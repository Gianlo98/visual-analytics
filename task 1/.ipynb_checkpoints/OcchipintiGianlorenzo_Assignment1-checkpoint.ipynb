{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e39a2d74",
   "metadata": {},
   "source": [
    "# S&DE Atelier: Visual Analytics 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f15f7c7",
   "metadata": {},
   "source": [
    "Instructor: Marco D'Ambros (marco.dambros@usi.ch)\n",
    "\n",
    "TA: Susanna Ardigò (susanna.ardigo@usi.ch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fe1be2",
   "metadata": {},
   "source": [
    "## Assignment 1\n",
    "Due 21 March 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b40198",
   "metadata": {},
   "source": [
    "The goal of this assignment is to use Python and Jupyter notebook to explore, analyze and visualize the *Used Cars*,  *Stack Overflow 2018 Developer Survey* and *Pandas QA on Stack Overflow* datasets available here: https://drive.google.com/drive/folders/1gI501mWsqIH2YZ5Ml-fMqnZnWYxMRdJl?usp=sharing. To solve the assignment you should apply the knowledge you gained from the theoretical and practical lectures. In particular, when creating tabular or graphical representations you should apply the principles you learned from theoretical lectures and use the technologies presented during practical lectures.  You may use either Seaborn, Matplotlib or Bokeh visualization libraries whenever applicable. Note that you are not allowed to use Pandas library because it has not been covered yet in the lectures. You should submit a Jyputer notebook (named ```SurenameName_Assignment1.ipynb```) that contains your solutions and the steps followed to arrive to these solutions. Please follow the structure of the assignment to solve the exercises. \n",
    "\n",
    "\n",
    "Please note that the datasets used for this assignment are described at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a7b496",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 1: Data quality (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfea2498",
   "metadata": {},
   "source": [
    "In the Used Cars dataset identify the missing and invalid values for the columns: `vehicle type`, `price`, `brand`, and `month of registration`. If needed, standardize the information and covert them to a unique value. Please specify for each column the number of missing or invalid instances. The prices are in euros and the range of accepted prices is between €1000 and €100000.\n",
    "Once you identified missing/invalid values for the given columns, remove all rows where one or more columns have invalid/missing data. \n",
    "Show the steps that you follow to reach the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beee018e",
   "metadata": {},
   "source": [
    "# Data parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ead64b7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dateCrawled',\n",
       " 'name',\n",
       " 'seller',\n",
       " 'offerType',\n",
       " 'price',\n",
       " 'abtest',\n",
       " 'vehicleType',\n",
       " 'yearOfRegistration',\n",
       " 'gearbox',\n",
       " 'powerPS',\n",
       " 'model',\n",
       " 'kilometer',\n",
       " 'monthOfRegistration',\n",
       " 'fuelType',\n",
       " 'brand',\n",
       " 'notRepairedDamage',\n",
       " 'dateCreated',\n",
       " 'nrOfPictures',\n",
       " 'postalCode',\n",
       " 'lastSeen']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "\n",
    "with open(\"used_cars_dataset.csv\", 'r', encoding='Windows-1252') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    rows = [row for row in csvreader]\n",
    "\n",
    "raw_header = rows.pop(0)\n",
    "\n",
    "raw_header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e64fc5ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "header = {}\n",
    "for x in zip(range(len(raw_header)), raw_header):\n",
    "    header[x[1]] = x[0]\n",
    "\n",
    "#header # Useful in the future with rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd13765d",
   "metadata": {},
   "source": [
    "As we can notice, we don't have empty columns and thus, all the rows will have at most 20 cols."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295b1ba9",
   "metadata": {},
   "source": [
    "# Data Correction & Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf2ba22f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "#\n",
    "# brand_counter = Counter([row[header[\"brand\"]] for row in rows])\n",
    "# brand_counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3bd838a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bmw |<=>|bmw', 1),\n",
       " ('saab|<=>|seat', 2),\n",
       " ('mazda|<=>|lada', 2),\n",
       " ('fiat|<=>|kia', 2),\n",
       " ('fiat|<=>|seat', 2),\n",
       " ('lancia|<=>|dacia', 2),\n",
       " ('smart|<=>|seat', 2),\n",
       " ('skoda|<=>|mazda', 3),\n",
       " ('skoda|<=>|lada', 3),\n",
       " ('skoda|<=>|honda', 3),\n",
       " ('skoda|<=>|kia', 3),\n",
       " ('audi|<=>|mazda', 3),\n",
       " ('audi|<=>|lada', 3),\n",
       " ('audi|<=>|mini', 3),\n",
       " ('subaru|<=>|smart', 3),\n",
       " ('saab|<=>|lada', 3),\n",
       " ('saab|<=>|fiat', 3),\n",
       " ('saab|<=>|smart', 3),\n",
       " ('saab|<=>|kia', 3),\n",
       " ('mazda|<=>|honda', 3),\n",
       " ('mazda|<=>|dacia', 3),\n",
       " ('lada|<=>|lancia', 3),\n",
       " ('lada|<=>|honda', 3),\n",
       " ('lada|<=>|dacia', 3),\n",
       " ('lada|<=>|kia', 3),\n",
       " ('opel|<=>|jeep', 3),\n",
       " ('opel|<=>|rover', 3),\n",
       " ('fiat|<=>|mini', 3),\n",
       " ('fiat|<=>|ford', 3),\n",
       " ('fiat|<=>|smart', 3),\n",
       " ('bmw|<=>|BMW', 3),\n",
       " ('bmw|<=>|kia', 3),\n",
       " ('jeep|<=>|seat', 3),\n",
       " ('honda|<=>|hyundai', 3),\n",
       " ('honda|<=>|ford', 3),\n",
       " ('dacia|<=>|kia', 3),\n",
       " ('mini|<=>|kia', 3),\n",
       " ('BMW|<=>|kia', 3),\n",
       " ('kia|<=>|seat', 3),\n",
       " ('skoda|<=>|audi', 4),\n",
       " ('skoda|<=>|saab', 4),\n",
       " ('skoda|<=>|dacia', 4),\n",
       " ('skoda|<=>|toyota', 4),\n",
       " ('skoda|<=>|ford', 4),\n",
       " ('skoda|<=>|smart', 4),\n",
       " ('skoda|<=>|seat', 4),\n",
       " ('volvo|<=>|opel', 4),\n",
       " ('volvo|<=>|honda', 4),\n",
       " ('volvo|<=>|rover', 4),\n",
       " ('volvo|<=>|ford', 4),\n",
       " ('audi|<=>|saab', 4),\n",
       " ('audi|<=>|opel', 4),\n",
       " ('audi|<=>|fiat', 4),\n",
       " ('audi|<=>|lancia', 4),\n",
       " ('audi|<=>|bmw ', 4),\n",
       " ('audi|<=>|bmw', 4),\n",
       " ('audi|<=>|jeep', 4),\n",
       " ('audi|<=>|suzuki', 4),\n",
       " ('audi|<=>|jaguar', 4),\n",
       " ('audi|<=>|honda', 4),\n",
       " ('audi|<=>|dacia', 4),\n",
       " ('audi|<=>|hyundai', 4),\n",
       " ('audi|<=>|BMW', 4),\n",
       " ('audi|<=>|ford', 4),\n",
       " ('audi|<=>|kia', 4),\n",
       " ('audi|<=>|seat', 4),\n",
       " ('subaru|<=>|saab', 4),\n",
       " ('subaru|<=>|suzuki', 4),\n",
       " ('subaru|<=>|seat', 4),\n",
       " ('saab|<=>|mazda', 4),\n",
       " ('saab|<=>|opel', 4),\n",
       " ('saab|<=>|bmw ', 4),\n",
       " ('saab|<=>|bmw', 4),\n",
       " ('saab|<=>|jeep', 4),\n",
       " ('saab|<=>|jaguar', 4),\n",
       " ('saab|<=>|dacia', 4),\n",
       " ('saab|<=>|mini', 4),\n",
       " ('saab|<=>|BMW', 4),\n",
       " ('saab|<=>|ford', 4),\n",
       " ('saab|<=>|nissan', 4),\n",
       " ('mazda|<=>|lancia', 4),\n",
       " ('mazda|<=>|jaguar', 4),\n",
       " ('mazda|<=>|mini', 4),\n",
       " ('mazda|<=>|ford', 4),\n",
       " ('mazda|<=>|smart', 4),\n",
       " ('mazda|<=>|kia', 4),\n",
       " ('lada|<=>|opel', 4),\n",
       " ('lada|<=>|fiat', 4),\n",
       " ('lada|<=>|bmw ', 4),\n",
       " ('lada|<=>|bmw', 4),\n",
       " ('lada|<=>|jeep', 4),\n",
       " ('lada|<=>|jaguar', 4),\n",
       " ('lada|<=>|mini', 4),\n",
       " ('lada|<=>|BMW', 4),\n",
       " ('lada|<=>|ford', 4),\n",
       " ('lada|<=>|smart', 4),\n",
       " ('lada|<=>|seat', 4),\n",
       " ('opel|<=>|fiat', 4),\n",
       " ('opel|<=>|bmw ', 4),\n",
       " ('opel|<=>|bmw', 4),\n",
       " ('opel|<=>|honda', 4),\n",
       " ('opel|<=>|mini', 4),\n",
       " ('opel|<=>|BMW', 4),\n",
       " ('opel|<=>|ford', 4),\n",
       " ('opel|<=>|kia', 4),\n",
       " ('opel|<=>|seat', 4),\n",
       " ('fiat|<=>|bmw ', 4),\n",
       " ('fiat|<=>|bmw', 4),\n",
       " ('fiat|<=>|jeep', 4),\n",
       " ('fiat|<=>|dacia', 4),\n",
       " ('fiat|<=>|BMW', 4),\n",
       " ('fiat|<=>|nissan', 4),\n",
       " ('lancia|<=>|honda', 4),\n",
       " ('lancia|<=>|mini', 4),\n",
       " ('lancia|<=>|kia', 4),\n",
       " ('bmw |<=>|jeep', 4),\n",
       " ('bmw |<=>|mini', 4),\n",
       " ('bmw |<=>|BMW', 4),\n",
       " ('bmw |<=>|ford', 4),\n",
       " ('bmw |<=>|smart', 4),\n",
       " ('bmw |<=>|kia', 4),\n",
       " ('bmw |<=>|seat', 4),\n",
       " ('bmw|<=>|jeep', 4),\n",
       " ('bmw|<=>|mini', 4),\n",
       " ('bmw|<=>|ford', 4),\n",
       " ('bmw|<=>|smart', 4),\n",
       " ('bmw|<=>|seat', 4),\n",
       " ('jeep|<=>|rover', 4),\n",
       " ('jeep|<=>|mini', 4),\n",
       " ('jeep|<=>|BMW', 4),\n",
       " ('jeep|<=>|ford', 4),\n",
       " ('jeep|<=>|kia', 4),\n",
       " ('renault|<=>|seat', 4),\n",
       " ('jaguar|<=>|dacia', 4),\n",
       " ('honda|<=>|dacia', 4),\n",
       " ('honda|<=>|rover', 4),\n",
       " ('honda|<=>|mini', 4),\n",
       " ('honda|<=>|toyota', 4),\n",
       " ('honda|<=>|kia', 4),\n",
       " ('dacia|<=>|mini', 4),\n",
       " ('dacia|<=>|daewoo', 4),\n",
       " ('rover|<=>|ford', 4),\n",
       " ('mini|<=>|BMW', 4),\n",
       " ('mini|<=>|ford', 4),\n",
       " ('mini|<=>|smart', 4),\n",
       " ('mini|<=>|seat', 4),\n",
       " ('BMW|<=>|ford', 4),\n",
       " ('BMW|<=>|seat', 4),\n",
       " ('ford|<=>|smart', 4),\n",
       " ('ford|<=>|kia', 4),\n",
       " ('ford|<=>|seat', 4),\n",
       " ('smart|<=>|kia', 4),\n",
       " ('kia|<=>|nissan', 4),\n",
       " ('nissan|<=>|seat', 4),\n",
       " ('skoda|<=>|volvo', 5),\n",
       " ('skoda|<=>|subaru', 5),\n",
       " ('skoda|<=>|opel', 5),\n",
       " ('skoda|<=>|fiat', 5),\n",
       " ('skoda|<=>|lancia', 5),\n",
       " ('skoda|<=>|bmw ', 5),\n",
       " ('skoda|<=>|bmw', 5),\n",
       " ('skoda|<=>|jeep', 5),\n",
       " ('skoda|<=>|suzuki', 5),\n",
       " ('skoda|<=>|jaguar', 5),\n",
       " ('skoda|<=>|rover', 5),\n",
       " ('skoda|<=>|mini', 5),\n",
       " ('skoda|<=>|hyundai', 5),\n",
       " ('skoda|<=>|BMW', 5),\n",
       " ('skoda|<=>|nissan', 5),\n",
       " ('volvo|<=>|audi', 5),\n",
       " ('volvo|<=>|saab', 5),\n",
       " ('volvo|<=>|mazda', 5),\n",
       " ('volvo|<=>|lada', 5),\n",
       " ('volvo|<=>|fiat', 5),\n",
       " ('volvo|<=>|bmw ', 5),\n",
       " ('volvo|<=>|bmw', 5),\n",
       " ('volvo|<=>|jeep', 5),\n",
       " ('volvo|<=>|dacia', 5),\n",
       " ('volvo|<=>|mini', 5),\n",
       " ('volvo|<=>|toyota', 5),\n",
       " ('volvo|<=>|BMW', 5),\n",
       " ('volvo|<=>|smart', 5),\n",
       " ('volvo|<=>|kia', 5),\n",
       " ('volvo|<=>|daewoo', 5),\n",
       " ('volvo|<=>|seat', 5),\n",
       " ('audi|<=>|subaru', 5),\n",
       " ('audi|<=>|renault', 5),\n",
       " ('audi|<=>|rover', 5),\n",
       " ('audi|<=>|smart', 5),\n",
       " ('audi|<=>|daewoo', 5),\n",
       " ('subaru|<=>|lada', 5),\n",
       " ('subaru|<=>|fiat', 5),\n",
       " ('subaru|<=>|bmw ', 5),\n",
       " ('subaru|<=>|bmw', 5),\n",
       " ('subaru|<=>|jaguar', 5),\n",
       " ('subaru|<=>|rover', 5),\n",
       " ('subaru|<=>|trabant', 5),\n",
       " ('subaru|<=>|ford', 5),\n",
       " ('subaru|<=>|kia', 5),\n",
       " ('saab|<=>|lancia', 5),\n",
       " ('saab|<=>|suzuki', 5),\n",
       " ('saab|<=>|honda', 5),\n",
       " ('saab|<=>|rover', 5),\n",
       " ('saab|<=>|trabant', 5),\n",
       " ('saab|<=>|daewoo', 5),\n",
       " ('mazda|<=>|opel', 5),\n",
       " ('mazda|<=>|fiat', 5),\n",
       " ('mazda|<=>|bmw ', 5),\n",
       " ('mazda|<=>|bmw', 5),\n",
       " ('mazda|<=>|jeep', 5),\n",
       " ('mazda|<=>|suzuki', 5),\n",
       " ('mazda|<=>|rover', 5),\n",
       " ('mazda|<=>|hyundai', 5),\n",
       " ('mazda|<=>|toyota', 5),\n",
       " ('mazda|<=>|BMW', 5),\n",
       " ('mazda|<=>|daewoo', 5),\n",
       " ('mazda|<=>|nissan', 5),\n",
       " ('mazda|<=>|seat', 5),\n",
       " ('lada|<=>|rover', 5),\n",
       " ('lada|<=>|hyundai', 5),\n",
       " ('lada|<=>|trabant', 5),\n",
       " ('lada|<=>|toyota', 5),\n",
       " ('lada|<=>|daewoo', 5),\n",
       " ('lada|<=>|nissan', 5),\n",
       " ('opel|<=>|dacia', 5),\n",
       " ('opel|<=>|toyota', 5),\n",
       " ('opel|<=>|smart', 5),\n",
       " ('opel|<=>|daewoo', 5),\n",
       " ('fiat|<=>|lancia', 5),\n",
       " ('fiat|<=>|renault', 5),\n",
       " ('fiat|<=>|jaguar', 5),\n",
       " ('fiat|<=>|honda', 5),\n",
       " ('fiat|<=>|rover', 5),\n",
       " ('fiat|<=>|daihatsu', 5),\n",
       " ('fiat|<=>|trabant', 5),\n",
       " ('fiat|<=>|toyota', 5),\n",
       " ('lancia|<=>|jaguar', 5),\n",
       " ('lancia|<=>|toyota', 5),\n",
       " ('lancia|<=>|daewoo', 5),\n",
       " ('bmw |<=>|honda', 5),\n",
       " ('bmw |<=>|dacia', 5),\n",
       " ('bmw |<=>|rover', 5),\n",
       " ('bmw |<=>|daewoo', 5),\n",
       " ('citroen|<=>|rover', 5),\n",
       " ('citroen|<=>|chevrolet', 5),\n",
       " ('citroen|<=>|nissan', 5),\n",
       " ('bmw|<=>|honda', 5),\n",
       " ('bmw|<=>|dacia', 5),\n",
       " ('bmw|<=>|rover', 5),\n",
       " ('bmw|<=>|daewoo', 5),\n",
       " ('jeep|<=>|jaguar', 5),\n",
       " ('jeep|<=>|honda', 5),\n",
       " ('jeep|<=>|dacia', 5),\n",
       " ('jeep|<=>|peugeot', 5),\n",
       " ('jeep|<=>|smart', 5),\n",
       " ('jeep|<=>|daewoo', 5),\n",
       " ('suzuki|<=>|jaguar', 5),\n",
       " ('suzuki|<=>|mini', 5),\n",
       " ('suzuki|<=>|hyundai', 5),\n",
       " ('suzuki|<=>|smart', 5),\n",
       " ('suzuki|<=>|kia', 5),\n",
       " ('suzuki|<=>|seat', 5),\n",
       " ('renault|<=>|peugeot', 5),\n",
       " ('renault|<=>|trabant', 5),\n",
       " ('renault|<=>|smart', 5),\n",
       " ('jaguar|<=>|honda', 5),\n",
       " ('jaguar|<=>|rover', 5),\n",
       " ('jaguar|<=>|smart', 5),\n",
       " ('jaguar|<=>|kia', 5),\n",
       " ('jaguar|<=>|daewoo', 5),\n",
       " ('jaguar|<=>|nissan', 5),\n",
       " ('jaguar|<=>|seat', 5),\n",
       " ('honda|<=>|BMW', 5),\n",
       " ('honda|<=>|smart', 5),\n",
       " ('honda|<=>|nissan', 5),\n",
       " ('honda|<=>|seat', 5),\n",
       " ('dacia|<=>|rover', 5),\n",
       " ('dacia|<=>|daihatsu', 5),\n",
       " ('dacia|<=>|toyota', 5),\n",
       " ('dacia|<=>|BMW', 5),\n",
       " ('dacia|<=>|ford', 5),\n",
       " ('dacia|<=>|smart', 5),\n",
       " ('dacia|<=>|nissan', 5),\n",
       " ('dacia|<=>|seat', 5),\n",
       " ('rover|<=>|mini', 5),\n",
       " ('rover|<=>|land_rover', 5),\n",
       " ('rover|<=>|chrysler', 5),\n",
       " ('rover|<=>|toyota', 5),\n",
       " ('rover|<=>|BMW', 5),\n",
       " ('rover|<=>|smart', 5),\n",
       " ('rover|<=>|kia', 5),\n",
       " ('rover|<=>|seat', 5),\n",
       " ('mini|<=>|hyundai', 5),\n",
       " ('mini|<=>|nissan', 5),\n",
       " ('peugeot|<=>|seat', 5),\n",
       " ('chrysler|<=>|chevrolet', 5),\n",
       " ('porsche|<=>|ford', 5),\n",
       " ('trabant|<=>|smart', 5),\n",
       " ('trabant|<=>|nissan', 5),\n",
       " ('trabant|<=>|seat', 5),\n",
       " ('toyota|<=>|ford', 5),\n",
       " ('toyota|<=>|smart', 5),\n",
       " ('toyota|<=>|kia', 5),\n",
       " ('toyota|<=>|seat', 5),\n",
       " ('BMW|<=>|smart', 5),\n",
       " ('smart|<=>|nissan', 5),\n",
       " ('daewoo|<=>|seat', 5),\n",
       " ('skoda|<=>|citroen', 6),\n",
       " ('skoda|<=>|trabant', 6),\n",
       " ('skoda|<=>|daewoo', 6),\n",
       " ('volvo|<=>|subaru', 6),\n",
       " ('volvo|<=>|lancia', 6),\n",
       " ('volvo|<=>|citroen', 6),\n",
       " ('volvo|<=>|suzuki', 6),\n",
       " ('volvo|<=>|jaguar', 6),\n",
       " ('volvo|<=>|peugeot', 6),\n",
       " ('volvo|<=>|porsche', 6),\n",
       " ('volvo|<=>|chevrolet', 6),\n",
       " ('volvo|<=>|nissan', 6),\n",
       " ('audi|<=>|peugeot', 6),\n",
       " ('audi|<=>|trabant', 6),\n",
       " ('audi|<=>|toyota', 6),\n",
       " ('audi|<=>|nissan', 6),\n",
       " ('subaru|<=>|mazda', 6),\n",
       " ('subaru|<=>|opel', 6),\n",
       " ('subaru|<=>|lancia', 6),\n",
       " ('subaru|<=>|jeep', 6),\n",
       " ('subaru|<=>|renault', 6),\n",
       " ('subaru|<=>|honda', 6),\n",
       " ('subaru|<=>|dacia', 6),\n",
       " ('subaru|<=>|mini', 6),\n",
       " ('subaru|<=>|peugeot', 6),\n",
       " ('subaru|<=>|daihatsu', 6),\n",
       " ('subaru|<=>|hyundai', 6),\n",
       " ('subaru|<=>|toyota', 6),\n",
       " ('subaru|<=>|BMW', 6),\n",
       " ('subaru|<=>|daewoo', 6),\n",
       " ('subaru|<=>|nissan', 6),\n",
       " ('saab|<=>|renault', 6),\n",
       " ('saab|<=>|daihatsu', 6),\n",
       " ('saab|<=>|hyundai', 6),\n",
       " ('saab|<=>|porsche', 6),\n",
       " ('saab|<=>|toyota', 6),\n",
       " ('mazda|<=>|renault', 6),\n",
       " ('mazda|<=>|daihatsu', 6),\n",
       " ('mazda|<=>|trabant', 6),\n",
       " ('lada|<=>|suzuki', 6),\n",
       " ('lada|<=>|renault', 6),\n",
       " ('lada|<=>|daihatsu', 6),\n",
       " ('alfa_romeo|<=>|land_rover', 6),\n",
       " ('opel|<=>|lancia', 6),\n",
       " ('opel|<=>|citroen', 6),\n",
       " ('opel|<=>|suzuki', 6),\n",
       " ('opel|<=>|renault', 6),\n",
       " ('opel|<=>|jaguar', 6),\n",
       " ('opel|<=>|peugeot', 6),\n",
       " ('opel|<=>|porsche', 6),\n",
       " ('opel|<=>|nissan', 6),\n",
       " ('fiat|<=>|citroen', 6),\n",
       " ('fiat|<=>|suzuki', 6),\n",
       " ('fiat|<=>|peugeot', 6),\n",
       " ('fiat|<=>|hyundai', 6),\n",
       " ('fiat|<=>|daewoo', 6),\n",
       " ('lancia|<=>|bmw ', 6),\n",
       " ('lancia|<=>|bmw', 6),\n",
       " ('lancia|<=>|jeep', 6),\n",
       " ('lancia|<=>|suzuki', 6),\n",
       " ('lancia|<=>|renault', 6),\n",
       " ('lancia|<=>|rover', 6),\n",
       " ('lancia|<=>|hyundai', 6),\n",
       " ('lancia|<=>|porsche', 6),\n",
       " ('lancia|<=>|trabant', 6),\n",
       " ('lancia|<=>|BMW', 6),\n",
       " ('lancia|<=>|ford', 6),\n",
       " ('lancia|<=>|smart', 6),\n",
       " ('lancia|<=>|nissan', 6),\n",
       " ('lancia|<=>|seat', 6),\n",
       " ('bmw |<=>|suzuki', 6),\n",
       " ('bmw |<=>|jaguar', 6),\n",
       " ('bmw |<=>|trabant', 6),\n",
       " ('bmw |<=>|toyota', 6),\n",
       " ('bmw |<=>|nissan', 6),\n",
       " ('citroen|<=>|jeep', 6),\n",
       " ('citroen|<=>|mini', 6),\n",
       " ('citroen|<=>|chrysler', 6),\n",
       " ('citroen|<=>|trabant', 6),\n",
       " ('citroen|<=>|toyota', 6),\n",
       " ('citroen|<=>|ford', 6),\n",
       " ('citroen|<=>|smart', 6),\n",
       " ('citroen|<=>|kia', 6),\n",
       " ('citroen|<=>|daewoo', 6),\n",
       " ('bmw|<=>|suzuki', 6),\n",
       " ('bmw|<=>|jaguar', 6),\n",
       " ('bmw|<=>|trabant', 6),\n",
       " ('bmw|<=>|toyota', 6),\n",
       " ('bmw|<=>|nissan', 6),\n",
       " ('jeep|<=>|suzuki', 6),\n",
       " ('jeep|<=>|renault', 6),\n",
       " ('jeep|<=>|toyota', 6),\n",
       " ('jeep|<=>|nissan', 6),\n",
       " ('suzuki|<=>|renault', 6),\n",
       " ('suzuki|<=>|honda', 6),\n",
       " ('suzuki|<=>|dacia', 6),\n",
       " ('suzuki|<=>|rover', 6),\n",
       " ('suzuki|<=>|peugeot', 6),\n",
       " ('suzuki|<=>|toyota', 6),\n",
       " ('suzuki|<=>|BMW', 6),\n",
       " ('suzuki|<=>|ford', 6),\n",
       " ('suzuki|<=>|daewoo', 6),\n",
       " ('suzuki|<=>|nissan', 6),\n",
       " ('renault|<=>|jaguar', 6),\n",
       " ('renault|<=>|honda', 6),\n",
       " ('renault|<=>|dacia', 6),\n",
       " ('renault|<=>|rover', 6),\n",
       " ('renault|<=>|mini', 6),\n",
       " ('renault|<=>|chevrolet', 6),\n",
       " ('renault|<=>|kia', 6),\n",
       " ('jaguar|<=>|mini', 6),\n",
       " ('jaguar|<=>|peugeot', 6),\n",
       " ('jaguar|<=>|daihatsu', 6),\n",
       " ('jaguar|<=>|hyundai', 6),\n",
       " ('jaguar|<=>|trabant', 6),\n",
       " ('jaguar|<=>|toyota', 6),\n",
       " ('jaguar|<=>|BMW', 6),\n",
       " ('jaguar|<=>|ford', 6),\n",
       " ('honda|<=>|porsche', 6),\n",
       " ('honda|<=>|trabant', 6),\n",
       " ('honda|<=>|daewoo', 6),\n",
       " ('dacia|<=>|hyundai', 6),\n",
       " ('dacia|<=>|porsche', 6),\n",
       " ('dacia|<=>|trabant', 6),\n",
       " ('rover|<=>|peugeot', 6),\n",
       " ('rover|<=>|porsche', 6),\n",
       " ('rover|<=>|chevrolet', 6),\n",
       " ('rover|<=>|trabant', 6),\n",
       " ('rover|<=>|daewoo', 6),\n",
       " ('rover|<=>|nissan', 6),\n",
       " ('mini|<=>|trabant', 6),\n",
       " ('mini|<=>|toyota', 6),\n",
       " ('mini|<=>|daewoo', 6),\n",
       " ('peugeot|<=>|hyundai', 6),\n",
       " ('peugeot|<=>|porsche', 6),\n",
       " ('peugeot|<=>|trabant', 6),\n",
       " ('peugeot|<=>|toyota', 6),\n",
       " ('peugeot|<=>|smart', 6),\n",
       " ('peugeot|<=>|daewoo', 6),\n",
       " ('daihatsu|<=>|kia', 6),\n",
       " ('daihatsu|<=>|daewoo', 6),\n",
       " ('daihatsu|<=>|seat', 6),\n",
       " ('hyundai|<=>|toyota', 6),\n",
       " ('hyundai|<=>|ford', 6),\n",
       " ('hyundai|<=>|kia', 6),\n",
       " ('hyundai|<=>|nissan', 6),\n",
       " ('hyundai|<=>|seat', 6),\n",
       " ('chrysler|<=>|porsche', 6),\n",
       " ('porsche|<=>|toyota', 6),\n",
       " ('porsche|<=>|nissan', 6),\n",
       " ('porsche|<=>|seat', 6),\n",
       " ('trabant|<=>|toyota', 6),\n",
       " ('trabant|<=>|kia', 6),\n",
       " ('trabant|<=>|daewoo', 6),\n",
       " ('toyota|<=>|BMW', 6),\n",
       " ('toyota|<=>|daewoo', 6),\n",
       " ('toyota|<=>|nissan', 6),\n",
       " ('BMW|<=>|daewoo', 6),\n",
       " ('BMW|<=>|nissan', 6),\n",
       " ('ford|<=>|daewoo', 6),\n",
       " ('ford|<=>|nissan', 6),\n",
       " ('smart|<=>|daewoo', 6),\n",
       " ('kia|<=>|daewoo', 6),\n",
       " ('daewoo|<=>|nissan', 6),\n",
       " ('skoda|<=>|renault', 7),\n",
       " ('skoda|<=>|peugeot', 7),\n",
       " ('skoda|<=>|daihatsu', 7),\n",
       " ('skoda|<=>|porsche', 7),\n",
       " ('volvo|<=>|volkswagen', 7),\n",
       " ('volvo|<=>|renault', 7),\n",
       " ('volvo|<=>|hyundai', 7),\n",
       " ('volvo|<=>|chrysler', 7),\n",
       " ('volvo|<=>|trabant', 7),\n",
       " ('audi|<=>|citroen', 7),\n",
       " ('audi|<=>|daihatsu', 7),\n",
       " ('audi|<=>|porsche', 7),\n",
       " ('subaru|<=>|citroen', 7),\n",
       " ('subaru|<=>|mitsubishi', 7),\n",
       " ('subaru|<=>|porsche', 7),\n",
       " ('saab|<=>|citroen', 7),\n",
       " ('saab|<=>|peugeot', 7),\n",
       " ('saab|<=>|chrysler', 7),\n",
       " ('volkswagen|<=>|porsche', 7),\n",
       " ('volkswagen|<=>|nissan', 7),\n",
       " ('mazda|<=>|citroen', 7),\n",
       " ('mazda|<=>|peugeot', 7),\n",
       " ('mazda|<=>|porsche', 7),\n",
       " ('lada|<=>|citroen', 7),\n",
       " ('lada|<=>|land_rover', 7),\n",
       " ('lada|<=>|peugeot', 7),\n",
       " ('lada|<=>|porsche', 7),\n",
       " ('alfa_romeo|<=>|citroen', 7),\n",
       " ('alfa_romeo|<=>|rover', 7),\n",
       " ('alfa_romeo|<=>|chevrolet', 7),\n",
       " ('alfa_romeo|<=>|daewoo', 7),\n",
       " ('opel|<=>|hyundai', 7),\n",
       " ('opel|<=>|chrysler', 7),\n",
       " ('opel|<=>|chevrolet', 7),\n",
       " ('opel|<=>|trabant', 7),\n",
       " ('fiat|<=>|porsche', 7),\n",
       " ('lancia|<=>|citroen', 7),\n",
       " ('lancia|<=>|land_rover', 7),\n",
       " ('lancia|<=>|peugeot', 7),\n",
       " ('lancia|<=>|daihatsu', 7),\n",
       " ('bmw |<=>|citroen', 7),\n",
       " ('bmw |<=>|renault', 7),\n",
       " ('bmw |<=>|peugeot', 7),\n",
       " ('bmw |<=>|hyundai', 7),\n",
       " ('bmw |<=>|porsche', 7),\n",
       " ('citroen|<=>|bmw', 7),\n",
       " ('citroen|<=>|suzuki', 7),\n",
       " ('citroen|<=>|renault', 7),\n",
       " ('citroen|<=>|jaguar', 7),\n",
       " ('citroen|<=>|honda', 7),\n",
       " ('citroen|<=>|dacia', 7),\n",
       " ('citroen|<=>|land_rover', 7),\n",
       " ('citroen|<=>|peugeot', 7),\n",
       " ('citroen|<=>|daihatsu', 7),\n",
       " ('citroen|<=>|hyundai', 7),\n",
       " ('citroen|<=>|porsche', 7),\n",
       " ('citroen|<=>|BMW', 7),\n",
       " ('citroen|<=>|seat', 7),\n",
       " ('bmw|<=>|renault', 7),\n",
       " ('bmw|<=>|peugeot', 7),\n",
       " ('bmw|<=>|hyundai', 7),\n",
       " ('bmw|<=>|porsche', 7),\n",
       " ('jeep|<=>|hyundai', 7),\n",
       " ('jeep|<=>|chrysler', 7),\n",
       " ('jeep|<=>|porsche', 7),\n",
       " ('jeep|<=>|chevrolet', 7),\n",
       " ('jeep|<=>|trabant', 7),\n",
       " ('suzuki|<=>|mitsubishi', 7),\n",
       " ('suzuki|<=>|porsche', 7),\n",
       " ('suzuki|<=>|trabant', 7),\n",
       " ('renault|<=>|daihatsu', 7),\n",
       " ('renault|<=>|hyundai', 7),\n",
       " ('renault|<=>|chrysler', 7),\n",
       " ('renault|<=>|porsche', 7),\n",
       " ('renault|<=>|toyota', 7),\n",
       " ('renault|<=>|BMW', 7),\n",
       " ('renault|<=>|ford', 7),\n",
       " ('renault|<=>|daewoo', 7),\n",
       " ('renault|<=>|nissan', 7),\n",
       " ('jaguar|<=>|chrysler', 7),\n",
       " ('jaguar|<=>|porsche', 7),\n",
       " ('honda|<=>|peugeot', 7),\n",
       " ('honda|<=>|daihatsu', 7),\n",
       " ('honda|<=>|chrysler', 7),\n",
       " ('honda|<=>|chevrolet', 7),\n",
       " ('dacia|<=>|peugeot', 7),\n",
       " ('rover|<=>|hyundai', 7),\n",
       " ('mini|<=>|peugeot', 7),\n",
       " ('mini|<=>|mitsubishi', 7),\n",
       " ('mini|<=>|daihatsu', 7),\n",
       " ('mini|<=>|porsche', 7),\n",
       " ('land_rover|<=>|chevrolet', 7),\n",
       " ('peugeot|<=>|chevrolet', 7),\n",
       " ('peugeot|<=>|BMW', 7),\n",
       " ('peugeot|<=>|ford', 7),\n",
       " ('peugeot|<=>|kia', 7),\n",
       " ('peugeot|<=>|nissan', 7),\n",
       " ('mitsubishi|<=>|nissan', 7),\n",
       " ('daihatsu|<=>|trabant', 7),\n",
       " ('daihatsu|<=>|toyota', 7),\n",
       " ('daihatsu|<=>|smart', 7),\n",
       " ('daihatsu|<=>|nissan', 7),\n",
       " ('hyundai|<=>|chrysler', 7),\n",
       " ('hyundai|<=>|porsche', 7),\n",
       " ('hyundai|<=>|trabant', 7),\n",
       " ('hyundai|<=>|BMW', 7),\n",
       " ('hyundai|<=>|smart', 7),\n",
       " ('hyundai|<=>|daewoo', 7),\n",
       " ('chrysler|<=>|trabant', 7),\n",
       " ('chrysler|<=>|toyota', 7),\n",
       " ('chrysler|<=>|ford', 7),\n",
       " ('chrysler|<=>|smart', 7),\n",
       " ('chrysler|<=>|nissan', 7),\n",
       " ('chrysler|<=>|seat', 7),\n",
       " ('porsche|<=>|trabant', 7),\n",
       " ('porsche|<=>|BMW', 7),\n",
       " ('porsche|<=>|smart', 7),\n",
       " ('porsche|<=>|kia', 7),\n",
       " ('porsche|<=>|daewoo', 7),\n",
       " ('chevrolet|<=>|smart', 7),\n",
       " ('chevrolet|<=>|daewoo', 7),\n",
       " ('chevrolet|<=>|seat', 7),\n",
       " ('trabant|<=>|BMW', 7),\n",
       " ('trabant|<=>|ford', 7),\n",
       " ('skoda|<=>|volkswagen', 8),\n",
       " ('skoda|<=>|chrysler', 8),\n",
       " ('skoda|<=>|chevrolet', 8),\n",
       " ('volvo|<=>|alfa_romeo', 8),\n",
       " ('volvo|<=>|daihatsu', 8),\n",
       " ('audi|<=>|land_rover', 8),\n",
       " ('audi|<=>|mitsubishi', 8),\n",
       " ('audi|<=>|chrysler', 8),\n",
       " ('subaru|<=>|alfa_romeo', 8),\n",
       " ('subaru|<=>|chrysler', 8),\n",
       " ('subaru|<=>|chevrolet', 8),\n",
       " ('saab|<=>|volkswagen', 8),\n",
       " ('volkswagen|<=>|lada', 8),\n",
       " ('volkswagen|<=>|opel', 8),\n",
       " ('volkswagen|<=>|citroen', 8),\n",
       " ('volkswagen|<=>|honda', 8),\n",
       " ('volkswagen|<=>|rover', 8),\n",
       " ('volkswagen|<=>|chrysler', 8),\n",
       " ('volkswagen|<=>|toyota', 8),\n",
       " ('volkswagen|<=>|smart', 8),\n",
       " ('volkswagen|<=>|kia', 8),\n",
       " ('volkswagen|<=>|seat', 8),\n",
       " ('mazda|<=>|land_rover', 8),\n",
       " ('mazda|<=>|chrysler', 8),\n",
       " ('lada|<=>|alfa_romeo', 8),\n",
       " ('lada|<=>|chrysler', 8),\n",
       " ('alfa_romeo|<=>|opel', 8),\n",
       " ('alfa_romeo|<=>|lancia', 8),\n",
       " ('alfa_romeo|<=>|jaguar', 8),\n",
       " ('alfa_romeo|<=>|ford', 8),\n",
       " ('alfa_romeo|<=>|smart', 8),\n",
       " ('opel|<=>|land_rover', 8),\n",
       " ('opel|<=>|daihatsu', 8),\n",
       " ('fiat|<=>|chrysler', 8),\n",
       " ('fiat|<=>|chevrolet', 8),\n",
       " ('lancia|<=>|chrysler', 8),\n",
       " ('bmw |<=>|daihatsu', 8),\n",
       " ('bmw |<=>|chrysler', 8),\n",
       " ('citroen|<=>|mitsubishi', 8),\n",
       " ('bmw|<=>|daihatsu', 8),\n",
       " ('bmw|<=>|chrysler', 8),\n",
       " ('jeep|<=>|daihatsu', 8),\n",
       " ('suzuki|<=>|daihatsu', 8),\n",
       " ('suzuki|<=>|chrysler', 8),\n",
       " ('jaguar|<=>|land_rover', 8),\n",
       " ('honda|<=>|land_rover', 8),\n",
       " ('dacia|<=>|chrysler', 8),\n",
       " ('rover|<=>|daihatsu', 8),\n",
       " ('mini|<=>|chrysler', 8),\n",
       " ('land_rover|<=>|chrysler', 8),\n",
       " ('land_rover|<=>|daewoo', 8),\n",
       " ('peugeot|<=>|daihatsu', 8),\n",
       " ('peugeot|<=>|chrysler', 8),\n",
       " ('mitsubishi|<=>|hyundai', 8),\n",
       " ('mitsubishi|<=>|porsche', 8),\n",
       " ('mitsubishi|<=>|trabant', 8),\n",
       " ('daihatsu|<=>|hyundai', 8),\n",
       " ('daihatsu|<=>|chrysler', 8),\n",
       " ('daihatsu|<=>|porsche', 8),\n",
       " ('daihatsu|<=>|BMW', 8),\n",
       " ('daihatsu|<=>|ford', 8),\n",
       " ('hyundai|<=>|chevrolet', 8),\n",
       " ('chrysler|<=>|BMW', 8),\n",
       " ('chrysler|<=>|kia', 8),\n",
       " ('chrysler|<=>|daewoo', 8),\n",
       " ('porsche|<=>|chevrolet', 8),\n",
       " ('chevrolet|<=>|trabant', 8),\n",
       " ('chevrolet|<=>|toyota', 8),\n",
       " ('chevrolet|<=>|ford', 8),\n",
       " ('skoda|<=>|alfa_romeo', 9),\n",
       " ('skoda|<=>|land_rover', 9),\n",
       " ('skoda|<=>|mitsubishi', 9),\n",
       " ('volvo|<=>|land_rover', 9),\n",
       " ('audi|<=>|volkswagen', 9),\n",
       " ('audi|<=>|alfa_romeo', 9),\n",
       " ('audi|<=>|chevrolet', 9),\n",
       " ('subaru|<=>|volkswagen', 9),\n",
       " ('subaru|<=>|land_rover', 9),\n",
       " ('saab|<=>|alfa_romeo', 9),\n",
       " ('saab|<=>|land_rover', 9),\n",
       " ('saab|<=>|mitsubishi', 9),\n",
       " ('saab|<=>|chevrolet', 9),\n",
       " ('volkswagen|<=>|mazda', 9),\n",
       " ('volkswagen|<=>|alfa_romeo', 9),\n",
       " ('volkswagen|<=>|fiat', 9),\n",
       " ('volkswagen|<=>|lancia', 9),\n",
       " ('volkswagen|<=>|bmw ', 9),\n",
       " ('volkswagen|<=>|bmw', 9),\n",
       " ('volkswagen|<=>|jeep', 9),\n",
       " ('volkswagen|<=>|suzuki', 9),\n",
       " ('volkswagen|<=>|renault', 9),\n",
       " ('volkswagen|<=>|jaguar', 9),\n",
       " ('volkswagen|<=>|dacia', 9),\n",
       " ('volkswagen|<=>|land_rover', 9),\n",
       " ('volkswagen|<=>|peugeot', 9),\n",
       " ('volkswagen|<=>|daihatsu', 9),\n",
       " ('volkswagen|<=>|hyundai', 9),\n",
       " ('volkswagen|<=>|chevrolet', 9),\n",
       " ('volkswagen|<=>|trabant', 9),\n",
       " ('volkswagen|<=>|ford', 9),\n",
       " ('volkswagen|<=>|daewoo', 9),\n",
       " ('mazda|<=>|alfa_romeo', 9),\n",
       " ('mazda|<=>|mitsubishi', 9),\n",
       " ('mazda|<=>|chevrolet', 9),\n",
       " ('lada|<=>|chevrolet', 9),\n",
       " ('alfa_romeo|<=>|fiat', 9),\n",
       " ('alfa_romeo|<=>|bmw ', 9),\n",
       " ('alfa_romeo|<=>|bmw', 9),\n",
       " ('alfa_romeo|<=>|jeep', 9),\n",
       " ('alfa_romeo|<=>|renault', 9),\n",
       " ('alfa_romeo|<=>|honda', 9),\n",
       " ('alfa_romeo|<=>|dacia', 9),\n",
       " ('alfa_romeo|<=>|peugeot', 9),\n",
       " ('alfa_romeo|<=>|daihatsu', 9),\n",
       " ('alfa_romeo|<=>|chrysler', 9),\n",
       " ('alfa_romeo|<=>|porsche', 9),\n",
       " ('alfa_romeo|<=>|trabant', 9),\n",
       " ('alfa_romeo|<=>|toyota', 9),\n",
       " ('alfa_romeo|<=>|kia', 9),\n",
       " ('alfa_romeo|<=>|seat', 9),\n",
       " ('fiat|<=>|mitsubishi', 9),\n",
       " ('lancia|<=>|mitsubishi', 9),\n",
       " ('lancia|<=>|chevrolet', 9),\n",
       " ('bmw |<=>|mitsubishi', 9),\n",
       " ('bmw |<=>|chevrolet', 9),\n",
       " ('bmw|<=>|mitsubishi', 9),\n",
       " ('bmw|<=>|chevrolet', 9),\n",
       " ('jeep|<=>|land_rover', 9),\n",
       " ('suzuki|<=>|chevrolet', 9),\n",
       " ('renault|<=>|land_rover', 9),\n",
       " ('renault|<=>|mitsubishi', 9),\n",
       " ('jaguar|<=>|mitsubishi', 9),\n",
       " ('jaguar|<=>|chevrolet', 9),\n",
       " ('dacia|<=>|land_rover', 9),\n",
       " ('dacia|<=>|mitsubishi', 9),\n",
       " ('dacia|<=>|chevrolet', 9),\n",
       " ('mini|<=>|land_rover', 9),\n",
       " ('mini|<=>|chevrolet', 9),\n",
       " ('land_rover|<=>|peugeot', 9),\n",
       " ('land_rover|<=>|daihatsu', 9),\n",
       " ('land_rover|<=>|hyundai', 9),\n",
       " ('land_rover|<=>|porsche', 9),\n",
       " ('land_rover|<=>|toyota', 9),\n",
       " ('land_rover|<=>|ford', 9),\n",
       " ('land_rover|<=>|smart', 9),\n",
       " ('land_rover|<=>|nissan', 9),\n",
       " ('peugeot|<=>|mitsubishi', 9),\n",
       " ('mitsubishi|<=>|daihatsu', 9),\n",
       " ('mitsubishi|<=>|toyota', 9),\n",
       " ('mitsubishi|<=>|smart', 9),\n",
       " ('mitsubishi|<=>|kia', 9),\n",
       " ('mitsubishi|<=>|seat', 9),\n",
       " ('daihatsu|<=>|chevrolet', 9),\n",
       " ('chevrolet|<=>|BMW', 9),\n",
       " ('chevrolet|<=>|kia', 9),\n",
       " ('chevrolet|<=>|nissan', 9),\n",
       " ('mercedes_benz|<=>|citroen', 10),\n",
       " ('mercedes_benz|<=>|chrysler', 10),\n",
       " ('mercedes_benz|<=>|porsche', 10),\n",
       " ('mercedes_benz|<=>|trabant', 10),\n",
       " ('volvo|<=>|mitsubishi', 10),\n",
       " ('volkswagen|<=>|mini', 10),\n",
       " ('volkswagen|<=>|mitsubishi', 10),\n",
       " ('volkswagen|<=>|BMW', 10),\n",
       " ('lada|<=>|mitsubishi', 10),\n",
       " ('alfa_romeo|<=>|suzuki', 10),\n",
       " ('alfa_romeo|<=>|mini', 10),\n",
       " ('alfa_romeo|<=>|mitsubishi', 10),\n",
       " ('alfa_romeo|<=>|hyundai', 10),\n",
       " ('alfa_romeo|<=>|BMW', 10),\n",
       " ('alfa_romeo|<=>|nissan', 10),\n",
       " ('opel|<=>|mitsubishi', 10),\n",
       " ('fiat|<=>|land_rover', 10),\n",
       " ('bmw |<=>|land_rover', 10),\n",
       " ('bmw|<=>|land_rover', 10),\n",
       " ('jeep|<=>|mitsubishi', 10),\n",
       " ('suzuki|<=>|land_rover', 10),\n",
       " ('honda|<=>|mitsubishi', 10),\n",
       " ('rover|<=>|mitsubishi', 10),\n",
       " ('land_rover|<=>|mitsubishi', 10),\n",
       " ('land_rover|<=>|trabant', 10),\n",
       " ('land_rover|<=>|BMW', 10),\n",
       " ('land_rover|<=>|kia', 10),\n",
       " ('land_rover|<=>|seat', 10),\n",
       " ('mitsubishi|<=>|chrysler', 10),\n",
       " ('mitsubishi|<=>|chevrolet', 10),\n",
       " ('mitsubishi|<=>|BMW', 10),\n",
       " ('mitsubishi|<=>|ford', 10),\n",
       " ('mitsubishi|<=>|daewoo', 10),\n",
       " ('sonstige_autos|<=>|seat', 10),\n",
       " ('mercedes_benz|<=>|subaru', 11),\n",
       " ('mercedes_benz|<=>|volkswagen', 11),\n",
       " ('mercedes_benz|<=>|mazda', 11),\n",
       " ('mercedes_benz|<=>|jeep', 11),\n",
       " ('mercedes_benz|<=>|renault', 11),\n",
       " ('mercedes_benz|<=>|rover', 11),\n",
       " ('mercedes_benz|<=>|mini', 11),\n",
       " ('mercedes_benz|<=>|land_rover', 11),\n",
       " ('mercedes_benz|<=>|peugeot', 11),\n",
       " ('mercedes_benz|<=>|mitsubishi', 11),\n",
       " ('mercedes_benz|<=>|chevrolet', 11),\n",
       " ('mercedes_benz|<=>|ford', 11),\n",
       " ('mercedes_benz|<=>|nissan', 11),\n",
       " ('mercedes_benz|<=>|seat', 11),\n",
       " ('volkswagen|<=>|sonstige_autos', 11),\n",
       " ('fiat|<=>|sonstige_autos', 11),\n",
       " ('lancia|<=>|sonstige_autos', 11),\n",
       " ('renault|<=>|sonstige_autos', 11),\n",
       " ('honda|<=>|sonstige_autos', 11),\n",
       " ('peugeot|<=>|sonstige_autos', 11),\n",
       " ('daihatsu|<=>|sonstige_autos', 11),\n",
       " ('porsche|<=>|sonstige_autos', 11),\n",
       " ('sonstige_autos|<=>|trabant', 11),\n",
       " ('sonstige_autos|<=>|toyota', 11),\n",
       " ('sonstige_autos|<=>|smart', 11),\n",
       " ('sonstige_autos|<=>|nissan', 11),\n",
       " ('skoda|<=>|mercedes_benz', 12),\n",
       " ('skoda|<=>|sonstige_autos', 12),\n",
       " ('mercedes_benz|<=>|audi', 12),\n",
       " ('mercedes_benz|<=>|saab', 12),\n",
       " ('mercedes_benz|<=>|lada', 12),\n",
       " ('mercedes_benz|<=>|alfa_romeo', 12),\n",
       " ('mercedes_benz|<=>|opel', 12),\n",
       " ('mercedes_benz|<=>|lancia', 12),\n",
       " ('mercedes_benz|<=>|bmw ', 12),\n",
       " ('mercedes_benz|<=>|bmw', 12),\n",
       " ('mercedes_benz|<=>|suzuki', 12),\n",
       " ('mercedes_benz|<=>|honda', 12),\n",
       " ('mercedes_benz|<=>|dacia', 12),\n",
       " ('mercedes_benz|<=>|daihatsu', 12),\n",
       " ('mercedes_benz|<=>|hyundai', 12),\n",
       " ('mercedes_benz|<=>|smart', 12),\n",
       " ('mercedes_benz|<=>|daewoo', 12),\n",
       " ('volvo|<=>|sonstige_autos', 12),\n",
       " ('audi|<=>|sonstige_autos', 12),\n",
       " ('subaru|<=>|sonstige_autos', 12),\n",
       " ('saab|<=>|sonstige_autos', 12),\n",
       " ('opel|<=>|sonstige_autos', 12),\n",
       " ('citroen|<=>|sonstige_autos', 12),\n",
       " ('suzuki|<=>|sonstige_autos', 12),\n",
       " ('jaguar|<=>|sonstige_autos', 12),\n",
       " ('dacia|<=>|sonstige_autos', 12),\n",
       " ('rover|<=>|sonstige_autos', 12),\n",
       " ('mini|<=>|sonstige_autos', 12),\n",
       " ('land_rover|<=>|sonstige_autos', 12),\n",
       " ('chevrolet|<=>|sonstige_autos', 12),\n",
       " ('sonstige_autos|<=>|kia', 12),\n",
       " ('sonstige_autos|<=>|daewoo', 12),\n",
       " ('mercedes_benz|<=>|volvo', 13),\n",
       " ('mercedes_benz|<=>|fiat', 13),\n",
       " ('mercedes_benz|<=>|jaguar', 13),\n",
       " ('mercedes_benz|<=>|sonstige_autos', 13),\n",
       " ('mercedes_benz|<=>|toyota', 13),\n",
       " ('mercedes_benz|<=>|BMW', 13),\n",
       " ('mercedes_benz|<=>|kia', 13),\n",
       " ('mazda|<=>|sonstige_autos', 13),\n",
       " ('lada|<=>|sonstige_autos', 13),\n",
       " ('alfa_romeo|<=>|sonstige_autos', 13),\n",
       " ('jeep|<=>|sonstige_autos', 13),\n",
       " ('mitsubishi|<=>|sonstige_autos', 13),\n",
       " ('hyundai|<=>|sonstige_autos', 13),\n",
       " ('chrysler|<=>|sonstige_autos', 13),\n",
       " ('sonstige_autos|<=>|ford', 13),\n",
       " ('bmw |<=>|sonstige_autos', 14),\n",
       " ('bmw|<=>|sonstige_autos', 14),\n",
       " ('sonstige_autos|<=>|BMW', 14)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "words = set([row[header['brand']] for row in rows])\n",
    "\n",
    "import itertools\n",
    "\n",
    "words_processed = [w for w in words]\n",
    "\n",
    "similarities = {}\n",
    "for w1, w2 in itertools.combinations(words_processed, 2):\n",
    "        similarities[f\"{w1}|<=>|{w2}\"] = distance(w1, w2)\n",
    "\n",
    "sorted(similarities.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6480a161",
   "metadata": {},
   "source": [
    "Looking at all the minimum values, we can notice that there is a bmw tag with a blank space (which will be removed in the standardization phase).\n",
    "For my knowledge of car's brand names, they seem all correct brand names without typos. Also capitalized names (like BMW) will be standardized to their respective lowercase version on the next phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7158a47c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bus|<=>|suv', 2),\n",
       " ('|<=>|bus', 3),\n",
       " ('|<=>|suv', 3),\n",
       " ('bus|<=>|coupe', 4),\n",
       " ('kombi|<=>|coupe', 4),\n",
       " ('suv|<=>|coupe', 4),\n",
       " ('|<=>|kombi', 5),\n",
       " ('|<=>|coupe', 5),\n",
       " ('bus|<=>|kombi', 5),\n",
       " ('bus|<=>|cabrio', 5),\n",
       " ('andere|<=>|coupe', 5),\n",
       " ('kombi|<=>|suv', 5),\n",
       " ('kombi|<=>|cabrio', 5),\n",
       " ('coupe|<=>|cabrio', 5),\n",
       " ('|<=>|andere', 6),\n",
       " ('|<=>|cabrio', 6),\n",
       " ('bus|<=>|andere', 6),\n",
       " ('andere|<=>|kombi', 6),\n",
       " ('andere|<=>|suv', 6),\n",
       " ('andere|<=>|cabrio', 6),\n",
       " ('limousine|<=>|coupe', 6),\n",
       " ('suv|<=>|cabrio', 6),\n",
       " ('bus|<=>|limousine', 7),\n",
       " ('limousine|<=>|kombi', 7),\n",
       " ('andere|<=>|limousine', 8),\n",
       " ('andere|<=>|kleinwagen', 8),\n",
       " ('limousine|<=>|kleinwagen', 8),\n",
       " ('limousine|<=>|suv', 8),\n",
       " ('limousine|<=>|cabrio', 8),\n",
       " ('|<=>|limousine', 9),\n",
       " ('kombi|<=>|kleinwagen', 9),\n",
       " ('kleinwagen|<=>|coupe', 9),\n",
       " ('|<=>|kleinwagen', 10),\n",
       " ('bus|<=>|kleinwagen', 10),\n",
       " ('kleinwagen|<=>|suv', 10),\n",
       " ('kleinwagen|<=>|cabrio', 10)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import distance\n",
    "\n",
    "words = set([row[header['vehicleType']] for row in rows])\n",
    "\n",
    "import itertools\n",
    "\n",
    "words_processed = [w for w in words]\n",
    "\n",
    "similarities = {}\n",
    "for w1, w2 in itertools.combinations(words_processed, 2):\n",
    "        similarities[f\"{w1}|<=>|{w2}\"] = distance(w1, w2)\n",
    "\n",
    "sorted(similarities.items(), key=lambda item: item[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09daea91",
   "metadata": {},
   "source": [
    "Also there we can see many blank field that will be removed but all the names looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a558b5f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311037"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter only  vehicle type, price, brand, and month of registration.\n",
    "# Constraints:\n",
    "# vehicle type: String\n",
    "# price: Number, euros between €1000 and €100000\n",
    "# brand: String, vehicle brand\n",
    "# month of registration: DateTime\n",
    "\n",
    "\n",
    "invalid_header_counter = [0] * len(header)\n",
    "## This function will remove all the invalid fields (invalid/out of range for numbers or empty for strings)\n",
    "def is_valid_entry(entry):\n",
    "    if len(entry) != len(header):\n",
    "        return False\n",
    "\n",
    "    if entry[header[\"vehicleType\"]] == '':\n",
    "        invalid_header_counter[header[\"vehicleType\"]] += 1\n",
    "        return False\n",
    "\n",
    "#     if not entry[header[\"price\"]].isdigit() or float(entry[header[\"price\"]]) < 1000 or float(entry[header[\"price\"]]) > 100000:\n",
    "    if not entry[header[\"price\"]].isdigit():\n",
    "        invalid_header_counter[header[\"price\"]] += 1\n",
    "        return False\n",
    "\n",
    "    if entry[header[\"brand\"]] == '':\n",
    "        invalid_header_counter[header[\"brand\"]] += 1\n",
    "        return False\n",
    "\n",
    "    if not entry[header[\"monthOfRegistration\"]].isdigit() or int(entry[header[\"monthOfRegistration\"]]) <= 0 or int(entry[header[\"monthOfRegistration\"]]) > 12:\n",
    "        invalid_header_counter[header[\"monthOfRegistration\"]] += 1\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# This will remove from the entry all the blank spaced and will standardize all the strings.\n",
    "def standardize_values(entry):\n",
    "    entry[header[\"vehicleType\"]] = entry[header[\"vehicleType\"]].strip().lower()\n",
    "    entry[header[\"brand\"]] = entry[header[\"brand\"]].strip().lower()\n",
    "    entry[header[\"price\"]] = float(entry[header[\"price\"]])\n",
    "    return entry\n",
    "\n",
    "cleaned_rows = [standardize_values(row) for row in rows if is_valid_entry(row)]\n",
    "len(cleaned_rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77c25532",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2016-03-24 10:58:45',\n",
       "  'A5_Sportback_2.7_Tdi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  18300.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '190',\n",
       "  '',\n",
       "  '125000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'ja',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '66954',\n",
       "  '2016-04-07 01:46:50'],\n",
       " ['2016-03-14 12:52:21',\n",
       "  'Jeep_Grand_Cherokee_\"Overland\"',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9800.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '163',\n",
       "  'grand',\n",
       "  '125000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'jeep',\n",
       "  '',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '90480',\n",
       "  '2016-04-05 12:47:46'],\n",
       " ['2016-03-17 16:54:04',\n",
       "  'GOLF_4_1_4__3TÜRER',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '91074',\n",
       "  '2016-03-17 17:40:17'],\n",
       " ['2016-03-31 17:25:20',\n",
       "  'Skoda_Fabia_1.4_TDI_PD_Classic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3600.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'fabia',\n",
       "  '90000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '60437',\n",
       "  '2016-04-06 10:17:21'],\n",
       " ['2016-04-04 17:36:23',\n",
       "  'BMW_316i___e36_Limousine___Bastlerfahrzeug__Export',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'ja',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '33775',\n",
       "  '2016-04-06 19:17:07'],\n",
       " ['2016-04-01 20:48:51',\n",
       "  'Peugeot_206_CC_110_Platinum',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2200.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  '2_reihe',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '67112',\n",
       "  '2016-04-05 18:18:39'],\n",
       " ['2016-03-21 18:54:38',\n",
       "  'VW_Derby_Bj_80__Scheunenfund',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1980',\n",
       "  'manuell',\n",
       "  '50',\n",
       "  'andere',\n",
       "  '40000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '19348',\n",
       "  '2016-03-25 16:47:58'],\n",
       " ['2016-04-04 23:42:13',\n",
       "  'Ford_C___Max_Titanium_1_0_L_EcoBoost',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14500.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2014',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'c_max',\n",
       "  '30000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '94505',\n",
       "  '2016-04-04 23:42:13'],\n",
       " ['2016-03-26 19:54:18',\n",
       "  'Mazda_3_1.6_Sport',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  '3_reihe',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'mazda',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '96224',\n",
       "  '2016-04-06 10:45:34'],\n",
       " ['2016-04-07 10:06:22',\n",
       "  'Volkswagen_Passat_Variant_2.0_TDI_Comfortline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2799.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-04-07 00:00:00',\n",
       "  '0',\n",
       "  '57290',\n",
       "  '2016-04-07 10:25:17'],\n",
       " ['2016-03-15 22:49:09',\n",
       "  'VW_Passat_Facelift_35i__\"7Sitzer\"',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  999.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '115',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '37269',\n",
       "  '2016-04-01 13:16:16'],\n",
       " ['2016-03-21 21:37:40',\n",
       "  'VW_PASSAT_1.9_TDI_131_PS_LEDER',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '2',\n",
       "  '',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '90762',\n",
       "  '2016-03-23 02:50:54'],\n",
       " ['2016-03-21 12:57:01',\n",
       "  'Nissan_Navara_2.5DPF_SE4x4_Klima_Sitzheizg_Bluetooth.Doppelkabine',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  17999.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '190',\n",
       "  'navara',\n",
       "  '70000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '04177',\n",
       "  '2016-04-06 07:45:42'],\n",
       " ['2016-03-20 10:25:19',\n",
       "  'Renault_Twingo_1.2_16V_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1750.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '75',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '65599',\n",
       "  '2016-04-06 13:16:07'],\n",
       " ['2016-03-23 15:48:05',\n",
       "  'Ford_C_MAX_2.0_TDCi_DPF_Titanium',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7550.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '136',\n",
       "  'c_max',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '88361',\n",
       "  '2016-04-05 18:45:11'],\n",
       " ['2016-04-01 22:55:47',\n",
       "  'Mercedes_Benz_A_160_Classic_Klima',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1850.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'a_klasse',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '49565',\n",
       "  '2016-04-05 22:46:05'],\n",
       " ['2016-04-01 19:56:48',\n",
       "  'Volkswagen_Scirocco_1.4_TSI_Sport',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10400.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '160',\n",
       "  'scirocco',\n",
       "  '100000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '75365',\n",
       "  '2016-04-05 16:45:49'],\n",
       " ['2016-03-27 11:38:00',\n",
       "  'BMW_530i_TÜV_7/17_Scheckheftgepflegt_sehr_guter_Zustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3699.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'automatik',\n",
       "  '231',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '68309',\n",
       "  '2016-04-07 06:44:26'],\n",
       " ['2016-03-12 19:43:07',\n",
       "  'Stadtflitzer',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  450.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '50',\n",
       "  'arosa',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '09526',\n",
       "  '2016-03-21 01:46:11'],\n",
       " ['2016-03-13 20:40:49',\n",
       "  'MERCEDES_200E__TÜV_04/2016',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1990',\n",
       "  'manuell',\n",
       "  '118',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'ja',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '35390',\n",
       "  '2016-03-13 20:40:49'],\n",
       " ['2016-03-18 21:44:09',\n",
       "  'BMW_530d_touring_Vollausstattung_NAVI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2002',\n",
       "  'automatik',\n",
       "  '193',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'ja',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '73765',\n",
       "  '2016-03-18 21:44:09'],\n",
       " ['2016-03-07 12:51:23',\n",
       "  'Honda_Civic_1.4_i_VTEC_Comfort',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6900.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '99',\n",
       "  'civic',\n",
       "  '60000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'honda',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '12621',\n",
       "  '2016-03-26 09:44:53'],\n",
       " ['2016-03-09 11:56:38',\n",
       "  'Volkswagen_T3_andere',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1990.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '1981',\n",
       "  'manuell',\n",
       "  '50',\n",
       "  'transporter',\n",
       "  '5000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '87471',\n",
       "  '2016-03-10 07:44:33'],\n",
       " ['2016-03-08 19:55:19',\n",
       "  'Fiat_Punto_1.2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  690.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'punto',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '86199',\n",
       "  '2016-03-09 11:45:28'],\n",
       " ['2016-04-03 15:48:11',\n",
       "  'Mercedes_Benz_E_250_D_Original_Zustand_!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3300.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'automatik',\n",
       "  '113',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '53879',\n",
       "  '2016-04-05 15:16:05'],\n",
       " ['2016-03-15 20:59:01',\n",
       "  'Golf_3_....._1.4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  245.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1994',\n",
       "  '',\n",
       "  '0',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '44145',\n",
       "  '2016-03-17 18:17:43'],\n",
       " ['2016-03-25 21:48:47',\n",
       "  'BMW_325i_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  18000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '218',\n",
       "  '3er',\n",
       "  '20000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '39179',\n",
       "  '2016-04-07 04:45:21'],\n",
       " ['2016-03-17 18:55:12',\n",
       "  'Mercedes_Benz_E_200_CDI_Automatik_Classic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '122',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '67071',\n",
       "  '2016-03-30 15:46:10'],\n",
       " ['2016-04-01 17:45:07',\n",
       "  'Abschleppwagen_Vw_LT_195.000_gruene_Plakette_TÜV_8/2017',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11900.0,\n",
       "  'test',\n",
       "  'andere',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '129',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '10551',\n",
       "  '2016-04-05 12:47:30'],\n",
       " ['2016-03-25 15:50:30',\n",
       "  'Mercedes_Camper_D407',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '1984',\n",
       "  'manuell',\n",
       "  '70',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '22767',\n",
       "  '2016-03-27 03:17:02'],\n",
       " ['2016-03-30 20:38:20',\n",
       "  'E_500_Avantgarde_AMG_Ausstattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'automatik',\n",
       "  '306',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '4',\n",
       "  '',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '33649',\n",
       "  '2016-04-03 11:44:49'],\n",
       " ['2016-03-24 00:52:09',\n",
       "  'BMW_E60_530XD',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '231',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '46119',\n",
       "  '2016-04-04 16:18:19'],\n",
       " ['2016-03-13 15:47:08',\n",
       "  'Mini_One_Pepper_Scheckheftgepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6990.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'one',\n",
       "  '100000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'mini',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '59174',\n",
       "  '2016-03-21 17:17:50'],\n",
       " ['2016-03-17 12:44:43',\n",
       "  'Smart_For_two_Klima_regensensor_uSw',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3900.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '61',\n",
       "  'fortwo',\n",
       "  '80000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  '',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '21073',\n",
       "  '2016-03-19 11:46:17'],\n",
       " ['2016-03-29 18:57:46',\n",
       "  'Renault_Clio_1.4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  590.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'clio',\n",
       "  '125000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '84180',\n",
       "  '2016-03-29 18:57:46'],\n",
       " ['2016-03-15 18:59:02',\n",
       "  'BMW_120d_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7999.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '177',\n",
       "  '1er',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '53604',\n",
       "  '2016-03-16 16:47:48'],\n",
       " ['2016-04-04 00:38:22',\n",
       "  'BMW_528i_touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2100.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'automatik',\n",
       "  '193',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '86157',\n",
       "  '2016-04-06 08:16:21'],\n",
       " ['2016-04-04 14:06:22',\n",
       "  'Mercedes_Benz_B180_Automatik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2012',\n",
       "  'automatik',\n",
       "  '109',\n",
       "  'b_klasse',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '35576',\n",
       "  '2016-04-05 12:09:29'],\n",
       " ['2016-03-30 08:50:37',\n",
       "  'BMW_120i_Cabrio_mit_M_Paket',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14800.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '1er',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '48691',\n",
       "  '2016-04-07 00:17:23'],\n",
       " ['2016-03-29 18:53:48',\n",
       "  'Bmw_318_d_Sport_Edition_Facelift',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7999.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '143',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '71083',\n",
       "  '2016-04-06 04:45:54'],\n",
       " ['2016-03-26 10:39:35',\n",
       "  'auto_opel_astra',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  550.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'ja',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '56759',\n",
       "  '2016-04-01 23:17:27'],\n",
       " ['2016-03-21 01:59:07',\n",
       "  'BMW_435i_Sport_coupe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  39600.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2014',\n",
       "  'automatik',\n",
       "  '306',\n",
       "  'andere',\n",
       "  '30000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '10435',\n",
       "  '2016-04-03 23:16:31'],\n",
       " ['2016-03-22 17:56:12',\n",
       "  'Smart_Cabrio_TÜV_bis_07/17',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3000.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '61',\n",
       "  'fortwo',\n",
       "  '80000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '12055',\n",
       "  '2016-03-22 17:56:12'],\n",
       " ['2016-03-31 08:57:44',\n",
       "  'Volkswagen_Golf_1.6_Comfortline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '39624',\n",
       "  '2016-04-06 03:15:47'],\n",
       " ['2016-03-29 15:48:15',\n",
       "  'TAUSCHE_BMW_E38_740i_g._SUV_/_GELÄNDEWAGEN_LESEN_LESEN',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '286',\n",
       "  '',\n",
       "  '150000',\n",
       "  '11',\n",
       "  '',\n",
       "  'sonstige_autos',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '53721',\n",
       "  '2016-04-06 01:44:38'],\n",
       " ['2016-03-26 07:54:29',\n",
       "  'Bmw_316i_compact.____200€',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  200.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '23560',\n",
       "  '2016-04-01 20:46:51'],\n",
       " ['2016-03-25 19:44:06',\n",
       "  'Audi_A8_3.0_TDI_quattro',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7499.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '232',\n",
       "  'a8',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'ja',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '71711',\n",
       "  '2016-04-07 01:44:24'],\n",
       " ['2016-03-30 12:49:54',\n",
       "  'Skoda_Fabia_1.2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'fabia',\n",
       "  '70000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '57076',\n",
       "  '2016-04-07 03:44:51'],\n",
       " ['2016-03-08 12:54:47',\n",
       "  'Volkswagen_Jetta_1.9_TDI_DSG_DPF_Sportline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '105',\n",
       "  'jetta',\n",
       "  '100000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '06242',\n",
       "  '2016-03-11 17:16:18'],\n",
       " ['2016-03-07 22:36:54',\n",
       "  'BMW_325_i_Cabrio_wenig_Kilometer',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14999.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '218',\n",
       "  '3er',\n",
       "  '50000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '01129',\n",
       "  '2016-03-15 10:17:59'],\n",
       " ['2016-03-15 14:50:24',\n",
       "  'Volkswagen_Scirocco_1.4_TSI___Top_Zustand_!!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9700.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '160',\n",
       "  'scirocco',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '55218',\n",
       "  '2016-04-06 04:17:09'],\n",
       " ['2016-03-21 12:47:55',\n",
       "  'Honda_HRV_reserviert!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2100.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'honda',\n",
       "  'nein',\n",
       "  '2016-02-09 00:00:00',\n",
       "  '0',\n",
       "  '85244',\n",
       "  '2016-03-31 16:46:09'],\n",
       " ['2016-03-15 09:51:05',\n",
       "  'Mercedes_Benz_190',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1280.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1992',\n",
       "  'automatik',\n",
       "  '109',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '86163',\n",
       "  '2016-04-05 19:18:21'],\n",
       " ['2016-03-08 01:36:42',\n",
       "  'Volkswagen_Polo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1993',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '08258',\n",
       "  '2016-04-05 23:46:00'],\n",
       " ['2016-03-05 16:45:05',\n",
       "  'Ford_Fiesta',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '50',\n",
       "  'fiesta',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '70327',\n",
       "  '2016-03-11 06:16:00'],\n",
       " ['2016-03-22 17:45:41',\n",
       "  'Mercedes_Benz_C_180_T_Kompressor_Avantgarde',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7999.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '156',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '83355',\n",
       "  '2016-03-25 09:47:43'],\n",
       " ['2016-03-31 18:46:36',\n",
       "  'VW_Golf_V_Plus_TDI_1_8',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6600.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '41812',\n",
       "  '2016-03-31 18:46:36'],\n",
       " ['2016-03-17 09:48:12',\n",
       "  'Nissan_Micra_1.2_CVT',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7999.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'micra',\n",
       "  '40000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '46145',\n",
       "  '2016-04-06 07:44:23'],\n",
       " ['2016-03-11 14:50:52',\n",
       "  'MB_Vito_Transporter_108_CDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1400.0,\n",
       "  'control',\n",
       "  'andere',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '82',\n",
       "  'vito',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '04668',\n",
       "  '2016-03-11 14:50:52'],\n",
       " ['2016-03-10 10:50:26',\n",
       "  'Mercedes_Benz_311_CDI_Sprinter_DOKA_6_SITZ._TÜV/AU_NEU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5000.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'sprinter',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '26382',\n",
       "  '2016-03-12 14:45:39'],\n",
       " ['2016-03-11 23:42:53',\n",
       "  'Mercedes_Benz_E_250_CDI_Mod.2011_Automatik_NAVI_XENON_Glasdach',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  20300.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '204',\n",
       "  'e_klasse',\n",
       "  '80000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '51491',\n",
       "  '2016-04-03 01:26:23'],\n",
       " ['2016-04-03 12:56:45',\n",
       "  'Bastler_Fahrzeug_mit_Rest_Tuev',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  350.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  '',\n",
       "  '150000',\n",
       "  '3',\n",
       "  '',\n",
       "  'fiat',\n",
       "  'ja',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '45665',\n",
       "  '2016-04-05 11:47:13'],\n",
       " ['2016-03-09 21:46:09',\n",
       "  'Ford_Escort_CLX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  600.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'escort',\n",
       "  '100000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'ja',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '37359',\n",
       "  '2016-04-05 23:44:25'],\n",
       " ['2016-03-23 18:59:37',\n",
       "  'Mercedes_Benz_C_220_T_CDI_Avantgarde',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3300.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'ja',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '29221',\n",
       "  '2016-03-23 18:59:37'],\n",
       " ['2016-03-12 19:46:54',\n",
       "  'SUBARU_FORESTER',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'automatik',\n",
       "  '122',\n",
       "  'forester',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'subaru',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '87497',\n",
       "  '2016-03-15 06:46:18'],\n",
       " ['2016-03-29 14:58:15',\n",
       "  'Volvo_XC90_AWD_D5_Kinetic_Leder_Standheizung_AHK_Gruene_Umweltpl',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8000.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '185',\n",
       "  'xc_reihe',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'volvo',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '96472',\n",
       "  '2016-04-03 23:18:17'],\n",
       " ['2016-03-13 19:52:34',\n",
       "  'Mitsubishi_L_400',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1800.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '87',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'mitsubishi',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '24116',\n",
       "  '2016-03-17 13:47:41'],\n",
       " ['2016-03-08 13:49:57',\n",
       "  'Fiesta_Titanium_1.25',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '82',\n",
       "  'fiesta',\n",
       "  '60000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '51065',\n",
       "  '2016-04-05 19:18:01'],\n",
       " ['2016-03-07 17:38:57',\n",
       "  'Renault_Scenic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2990.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '113',\n",
       "  'scenic',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '41836',\n",
       "  '2016-03-09 06:15:31'],\n",
       " ['2016-03-28 09:37:01',\n",
       "  'MERCEDES_BENZ_W124_250D_83KW_/_113PS___SCHLACHTFEST_!!!_Preis:_1€',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '113',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '40589',\n",
       "  '2016-04-06 12:15:54'],\n",
       " ['2016-03-23 11:53:21',\n",
       "  'A4_Diesel_Leder_Top___NAVI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2400.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '40210',\n",
       "  '2016-03-23 11:53:21'],\n",
       " ['2016-04-02 23:25:25',\n",
       "  'Senator_A2__3_0E__180PS__CD_Ausstattung.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1222.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1984',\n",
       "  'automatik',\n",
       "  '180',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '57290',\n",
       "  '2016-04-06 14:44:57'],\n",
       " ['2016-03-10 11:44:54',\n",
       "  'Audi_A1_1.2_TFSI_S_Line',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14500.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '86',\n",
       "  'a1',\n",
       "  '60000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '31582',\n",
       "  '2016-04-07 01:15:35'],\n",
       " ['2016-03-25 18:44:10',\n",
       "  'Volkswagen_Passat_1.9_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3850.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '30459',\n",
       "  '2016-04-07 00:16:18'],\n",
       " ['2016-03-31 10:53:10',\n",
       "  'BMW_318d_Aut.__Xenon__Navi__Sportsitze_FESTREIS!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  23490.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2013',\n",
       "  'automatik',\n",
       "  '143',\n",
       "  '3er',\n",
       "  '40000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '49356',\n",
       "  '2016-04-06 03:44:40'],\n",
       " ['2016-03-26 15:38:12',\n",
       "  'VW_t5_kastenwagen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6450.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '84',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '12',\n",
       "  '',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '44623',\n",
       "  '2016-04-06 03:17:37'],\n",
       " ['2016-04-01 12:45:41',\n",
       "  'Mercedes_A_Klasse____Top_Zustand___TÜV/AU___neu_!_Halbautomatik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1700.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '102',\n",
       "  'a_klasse',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '12305',\n",
       "  '2016-04-01 12:45:41'],\n",
       " ['2016-03-11 19:00:25',\n",
       "  'OPEL_COMBO_2008__TÜV_Neu_bis_02.2018__DIESEL___LKW_ZULLASUNG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4290.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'combo',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '23568',\n",
       "  '2016-04-05 22:45:32'],\n",
       " ['2016-03-28 00:56:46',\n",
       "  'Ford_Focus_Turnier_1.6_TDCi_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '74223',\n",
       "  '2016-04-02 11:47:00'],\n",
       " ['2016-03-15 17:41:31',\n",
       "  'Audi_A4_B5_1_6_Liter',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  499.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '85414',\n",
       "  '2016-03-22 20:48:22'],\n",
       " ['2016-03-16 16:44:10',\n",
       "  'Hyundai_Genesis_Coupe_GT_3.8_V6_Automatik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  22999.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2012',\n",
       "  'automatik',\n",
       "  '303',\n",
       "  'andere',\n",
       "  '50000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'hyundai',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '88167',\n",
       "  '2016-04-06 20:18:22'],\n",
       " ['2016-04-01 09:54:39',\n",
       "  'Audi_a4_2.5_TDI_Tausch_moeglich',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '97450',\n",
       "  '2016-04-01 09:54:39'],\n",
       " ['2016-03-26 07:36:20',\n",
       "  'verkaufe_Mercedes_Benz_220_CDI_Sport_8_fach_Bereift',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  900.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '37130',\n",
       "  '2016-03-29 11:44:47'],\n",
       " ['2016-03-21 10:47:53',\n",
       "  'BMW_316_automatikgetriebe_Klimaanlage',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'automatik',\n",
       "  '102',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '13357',\n",
       "  '2016-03-22 14:49:21'],\n",
       " ['2016-04-02 12:54:14',\n",
       "  'Audi_TT_Coupe_1.8_T_quattro',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3399.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '224',\n",
       "  'tt',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'ja',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '61276',\n",
       "  '2016-04-05 19:44:33'],\n",
       " ['2016-03-23 10:59:51',\n",
       "  'Volkswagen_Passat_Variant_1.6_Kombi_mit_TÜV_und_AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1300.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '04277',\n",
       "  '2016-03-29 04:46:48'],\n",
       " ['2016-03-22 17:44:26',\n",
       "  '530d_XDRIVE_235_PS',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7300.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '235',\n",
       "  '5er',\n",
       "  '100000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '83022',\n",
       "  '2016-03-22 17:44:26'],\n",
       " ['2016-03-26 15:47:58',\n",
       "  'Opel_Astra_Twin_Top_2.0_Turbo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8599.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '200',\n",
       "  'astra',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '79761',\n",
       "  '2016-04-06 04:15:35'],\n",
       " ['2016-04-03 03:57:26',\n",
       "  'Opel_Corsa_TÜV_bis_Ende_2016',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  350.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1998',\n",
       "  '',\n",
       "  '0',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '82110',\n",
       "  '2016-04-03 08:53:37'],\n",
       " ['2016-03-14 20:37:41',\n",
       "  'Audi_A6_2_5_TDi_Quattro_Avant_Automatic__Turbo__Kombi__132_KW__2_',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5900.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'automatik',\n",
       "  '178',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '88131',\n",
       "  '2016-04-06 07:17:00'],\n",
       " ['2016-03-20 18:38:02',\n",
       "  'Volkswagen_Golf_1.8_5V_Turbo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6300.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '265',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '56759',\n",
       "  '2016-04-07 01:45:06'],\n",
       " ['2016-04-04 09:50:15',\n",
       "  'Volkswagen_Corrado_1.8_G60',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3500.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1991',\n",
       "  'manuell',\n",
       "  '160',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '66111',\n",
       "  '2016-04-06 11:17:02'],\n",
       " ['2016-03-09 17:47:24',\n",
       "  'BMW_320d_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5950.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'ja',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '63526',\n",
       "  '2016-03-10 14:15:41'],\n",
       " ['2016-04-03 12:45:29',\n",
       "  'Honda_Jazz_1.2_S',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1699.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '77',\n",
       "  'jazz',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'honda',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '42105',\n",
       "  '2016-04-07 14:57:35'],\n",
       " ['2016-03-20 13:54:32',\n",
       "  'Volkswagen_vw_t4_Einzelstueck',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4699.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '79713',\n",
       "  '2016-03-25 13:45:30'],\n",
       " ['2016-03-23 12:54:52',\n",
       "  'Motorschaden_steuerrichmen_gerissen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  400.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '144',\n",
       "  'omega',\n",
       "  '150000',\n",
       "  '10',\n",
       "  '',\n",
       "  'opel',\n",
       "  'ja',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '45968',\n",
       "  '2016-04-05 12:45:13'],\n",
       " ['2016-03-12 22:57:58',\n",
       "  'Volkswagen_Polo_1.4_FSI_Team',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9290.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '86',\n",
       "  'polo',\n",
       "  '40000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '35630',\n",
       "  '2016-03-26 06:17:39'],\n",
       " ['2016-03-18 11:25:43',\n",
       "  'Mercedes_Benz_Kabrio__Limousine___mit_Gas_Umbau....',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'slk',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '57578',\n",
       "  '2016-03-23 08:44:31'],\n",
       " ['2016-04-01 17:56:34',\n",
       "  'Servo_Klima_Zv__usw',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1199.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '28832',\n",
       "  '2016-04-05 13:44:57'],\n",
       " ['2016-03-27 12:36:18',\n",
       "  'Ford_Focus_1_6l*Klima*TÜV*',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1750.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'focus',\n",
       "  '125000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '33330',\n",
       "  '2016-03-27 12:36:18'],\n",
       " ['2016-03-18 23:41:18',\n",
       "  'Volkswagen_Golf_CL',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1990',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '47447',\n",
       "  '2016-04-05 23:16:13'],\n",
       " ['2016-03-11 20:55:37',\n",
       "  'BMW_740i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'automatik',\n",
       "  '286',\n",
       "  '7er',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'ja',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '77866',\n",
       "  '2016-03-20 10:17:59'],\n",
       " ['2016-03-16 18:37:43',\n",
       "  'Opel_Combo_1.7_DTI___Kasten___Lkw_Zulassung___Euro3',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1100.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'combo',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '96224',\n",
       "  '2016-03-24 11:46:38'],\n",
       " ['2016-03-09 20:37:42',\n",
       "  'Mercedes_Benz_E_270_T_CDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2990.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '53520',\n",
       "  '2016-03-17 02:45:51'],\n",
       " ['2016-03-31 20:57:02',\n",
       "  'Volkswagen_Passat',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2750.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '90471',\n",
       "  '2016-04-06 16:45:33'],\n",
       " ['2016-03-28 20:45:46',\n",
       "  'Fabia_II_Combi_Greenline_1_2_TDI_DPF_mit_Gebrauchtwagengarantie',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6990.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'fabia',\n",
       "  '100000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '44894',\n",
       "  '2016-04-03 13:58:00'],\n",
       " ['2016-04-01 22:55:34',\n",
       "  'Audi_Cabriolet_Typ_89_2.0L',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4800.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  '80',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '38300',\n",
       "  '2016-04-05 23:46:14'],\n",
       " ['2016-03-21 16:45:30',\n",
       "  'Mercedes_Benz_C_220_CDI_DPF_Automatik_Avantgarde',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14499.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'c_klasse',\n",
       "  '125000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '13439',\n",
       "  '2016-04-06 15:45:36'],\n",
       " ['2016-04-03 15:58:09',\n",
       "  'Verkaufe_ein_toppen_golf5',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4300.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '28779',\n",
       "  '2016-04-05 15:46:23'],\n",
       " ['2016-03-24 14:38:41',\n",
       "  'Honda_Civic_1.6i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2499.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'civic',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'honda',\n",
       "  '',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '42853',\n",
       "  '2016-03-24 14:38:41'],\n",
       " ['2016-04-02 10:59:28',\n",
       "  'Alfa_Romeo_147_1.9_JTD_8V_M_Jet__DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3450.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  '147',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'alfa_romeo',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '86678',\n",
       "  '2016-04-04 07:51:11'],\n",
       " ['2016-04-01 15:53:08',\n",
       "  'Mercedes_Benz_C180_mit_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  350.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '49685',\n",
       "  '2016-04-05 10:48:41'],\n",
       " ['2016-03-13 01:57:12',\n",
       "  'BMW_530d_touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3550.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '184',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '72525',\n",
       "  '2016-04-06 08:45:19'],\n",
       " ['2016-03-28 19:06:18',\n",
       "  'Sehr_gepflegter_unfallfreier_Ford_C_Max_Style+_1_6_DPF_mit_AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8250.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'c_max',\n",
       "  '125000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '35606',\n",
       "  '2016-04-07 04:44:59'],\n",
       " ['2016-03-09 09:51:36',\n",
       "  'renault_clio_TÜV_NEU_bis_03_2018_inzahlungname_moeglich',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'clio',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '34516',\n",
       "  '2016-03-14 11:46:52'],\n",
       " ['2016-03-31 00:59:04',\n",
       "  'Mercedes_Benz_GLK_250_BlueTEC_4Matic_Standhzg_Alcantara_Voll',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  41900.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2014',\n",
       "  'automatik',\n",
       "  '204',\n",
       "  'glk',\n",
       "  '40000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '82131',\n",
       "  '2016-04-06 01:17:24'],\n",
       " ['2016-04-05 09:55:37',\n",
       "  'BMW_330d_schadow_line',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4200.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '204',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '53757',\n",
       "  '2016-04-07 13:16:46'],\n",
       " ['2016-03-27 12:59:00',\n",
       "  'Volkswagen_Golf_3',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '38640',\n",
       "  '2016-04-01 12:45:50'],\n",
       " ['2016-03-09 12:50:41',\n",
       "  'Peugeot_307_90_Grand_Filou',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '88',\n",
       "  '3_reihe',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'ja',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '66333',\n",
       "  '2016-03-20 06:18:20'],\n",
       " ['2016-03-22 10:50:53',\n",
       "  'Mercedes_C_200_Kompressor',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2300.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '160',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '40231',\n",
       "  '2016-03-22 11:41:11'],\n",
       " ['2016-03-29 07:56:29',\n",
       "  'Audi_A6_Avant_2_5tdi*Navi*Automatik*Euro4*',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '163',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '22525',\n",
       "  '2016-03-31 01:47:16'],\n",
       " ['2016-03-27 20:47:22',\n",
       "  'POLO_1.2_KILIMA_WIE_NEU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6799.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  '',\n",
       "  '60',\n",
       "  '',\n",
       "  '20000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '89077',\n",
       "  '2016-03-27 20:47:22'],\n",
       " ['2016-03-11 18:55:53',\n",
       "  'Opel_meriva_1.6_16_v_lpg__z16xe_no_OPC',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'meriva',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'lpg',\n",
       "  'opel',\n",
       "  'ja',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '27432',\n",
       "  '2016-03-12 23:47:10'],\n",
       " ['2016-04-03 18:58:25',\n",
       "  'BMW_Z4_roadster_2.2i_M_Paket__Klima__Xenon',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9450.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'z_reihe',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '65549',\n",
       "  '2016-04-07 11:15:22'],\n",
       " ['2016-03-10 16:57:31',\n",
       "  'Mercedes_Benz_C_270_CDI_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6890.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'c_klasse',\n",
       "  '125000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '55743',\n",
       "  '2016-04-01 20:47:57'],\n",
       " ['2016-03-23 14:45:57',\n",
       "  'Kia_Sorento__coole_Farbe_schaut......',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7500.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '194',\n",
       "  'sorento',\n",
       "  '5000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'kia',\n",
       "  '',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '34314',\n",
       "  '2016-04-05 15:47:51'],\n",
       " ['2016-04-01 16:06:24',\n",
       "  'Volkswagen_Golf_Function',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1993',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'golf',\n",
       "  '10000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '65929',\n",
       "  '2016-04-07 11:17:07'],\n",
       " ['2016-03-08 19:58:07',\n",
       "  'A_210_Evolution_AMG_Styling_absoluter_Festpreis',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1499.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'a_klasse',\n",
       "  '150000',\n",
       "  '10',\n",
       "  '',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '16727',\n",
       "  '2016-03-10 17:44:36'],\n",
       " ['2016-03-28 10:50:04',\n",
       "  'VW_Caddy_Lkw_Zulassung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1495.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  '',\n",
       "  '64',\n",
       "  '',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '99086',\n",
       "  '2016-04-04 11:45:46'],\n",
       " ['2016-03-30 10:50:00',\n",
       "  'Mitsubishi_Space_ohne_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  300.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'mitsubishi',\n",
       "  '',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '26340',\n",
       "  '2016-03-30 10:50:00'],\n",
       " ['2016-03-21 23:51:55',\n",
       "  'VW_Golf_CL__2er_Golf____TÜV_bis_02/2018',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  850.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1989',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '95485',\n",
       "  '2016-04-05 15:17:35'],\n",
       " ['2016-04-01 22:49:08',\n",
       "  'Volkswagen_T5_Transporter',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '17291',\n",
       "  '2016-04-05 21:45:06'],\n",
       " ['2016-03-16 12:53:29',\n",
       "  'Volvo_v40_mit_klima',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  300.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'v40',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volvo',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '47137',\n",
       "  '2016-03-25 10:18:16'],\n",
       " ['2016-03-28 01:36:15',\n",
       "  'Honda_Civic_1.4i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'civic',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'honda',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '12163',\n",
       "  '2016-04-02 11:47:57'],\n",
       " ['2016-03-14 12:54:41',\n",
       "  'Seat_Ibiza_1.2_12V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '64',\n",
       "  'ibiza',\n",
       "  '80000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '29633',\n",
       "  '2016-04-05 13:15:27'],\n",
       " ['2016-03-19 13:36:22',\n",
       "  'Ford_C_MAX_1.6_TDCi_Titanium__7_Sitzer_Topausstattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11890.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'c_max',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '97737',\n",
       "  '2016-04-06 18:48:10'],\n",
       " ['2016-03-28 13:46:45',\n",
       "  'BMW_323_Ci_Automatik__Sitzheizung__Sportauspuff__Alufelgen_17Zoll',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2900.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '53332',\n",
       "  '2016-03-30 04:17:59'],\n",
       " ['2016-03-07 12:38:19',\n",
       "  'Ford_Mustang_GT_V8_Cabrio_Premium_Neuwagenzustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  19750.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '305',\n",
       "  'mustang',\n",
       "  '50000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '38350',\n",
       "  '2016-03-12 20:18:29'],\n",
       " ['2016-03-16 12:46:39',\n",
       "  'Volkswagen_Eos_2.0_FSI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9800.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'eos',\n",
       "  '60000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '47198',\n",
       "  '2016-04-06 07:16:16'],\n",
       " ['2016-03-18 12:39:23',\n",
       "  'Ford_Fiesta_1.4_Ghia_Klima__Scheckheft__Tuev_NEU!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1990.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'fiesta',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '13469',\n",
       "  '2016-03-18 12:39:23'],\n",
       " ['2016-03-18 06:36:46',\n",
       "  'Mercedes_Benz_Vito_115_CDI_Extralang_DPF_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10500.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'vito',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '33729',\n",
       "  '2016-04-05 21:47:51'],\n",
       " ['2016-03-24 15:38:18',\n",
       "  'Opel_Astra_J_Design_Edition_1.7_CDTI_ecoflex',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'astra',\n",
       "  '70000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '34613',\n",
       "  '2016-04-05 11:56:32'],\n",
       " ['2016-03-22 10:55:42',\n",
       "  'Opel_Sintra_2.2__7_Sitzer',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  300.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '24147',\n",
       "  '2016-03-28 00:15:33'],\n",
       " ['2016-03-21 00:59:18',\n",
       "  'Mercedes_Benz_E320_Cdi_/TÜV_05.2017_//Vollaustattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2390.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '197',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '27474',\n",
       "  '2016-04-05 22:46:06'],\n",
       " ['2016-03-08 20:58:31',\n",
       "  'Skoda_Fabia_Combi_1.2_TSI_DSG_Style',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16449.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2015',\n",
       "  'automatik',\n",
       "  '110',\n",
       "  'fabia',\n",
       "  '10000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '95502',\n",
       "  '2016-04-06 01:17:03'],\n",
       " ['2016-03-12 17:47:07',\n",
       "  'BMW_e46_318i_touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2999.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '45891',\n",
       "  '2016-04-07 09:17:32'],\n",
       " ['2016-03-28 17:39:50',\n",
       "  'Audi_A4___1_8_TFSI___Xenon___Scheckheft___51_tkm___TOP_Zustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14300.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'a4',\n",
       "  '60000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '59494',\n",
       "  '2016-04-06 23:15:52'],\n",
       " ['2016-03-21 19:06:21',\n",
       "  'Volkswagen_Polo_1.4_United_1._Hand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5999.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'polo',\n",
       "  '125000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '65529',\n",
       "  '2016-04-05 20:47:12'],\n",
       " ['2016-03-10 18:45:59',\n",
       "  'Audi_A6_Avant_2.5_TDI_quattro_S_line',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3700.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '179',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '64846',\n",
       "  '2016-03-13 00:16:56'],\n",
       " ['2016-03-10 23:55:39',\n",
       "  'Volkswagen_Touran_1.9_TDI_DPF_Navi_PDC_Tempomat_AHK_TÜV_Neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8999.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'touran',\n",
       "  '90000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '26689',\n",
       "  '2016-03-17 09:16:40'],\n",
       " ['2016-03-31 16:57:18',\n",
       "  'Ford_Mustang_V8_390cui',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  25000.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1968',\n",
       "  'automatik',\n",
       "  '305',\n",
       "  'mustang',\n",
       "  '90000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '74547',\n",
       "  '2016-04-06 10:45:49'],\n",
       " ['2016-03-30 22:51:56',\n",
       "  'Audi_80_1.8_in_tadellosem_Zustand._Erst_71.000_km!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2790.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1992',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  '80',\n",
       "  '80000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '01219',\n",
       "  '2016-04-05 15:46:38'],\n",
       " ['2016-03-20 23:48:31',\n",
       "  'BMW_320i_Facelift',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13400.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '3er',\n",
       "  '100000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '64289',\n",
       "  '2016-04-07 11:17:36'],\n",
       " ['2016-03-25 15:56:48',\n",
       "  'Volkswagen_Golf_Plus_1.6_Goal',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4900.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '04288',\n",
       "  '2016-04-06 20:16:54'],\n",
       " ['2016-03-08 21:52:34',\n",
       "  'Volkswagen_Golf_R32_4Motion_DSG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11299.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '250',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '22083',\n",
       "  '2016-03-19 09:46:58'],\n",
       " ['2016-04-03 11:44:17',\n",
       "  'Golf_1_6_Family',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  550.0,\n",
       "  'control',\n",
       "  'andere',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '96175',\n",
       "  '2016-04-07 12:45:02'],\n",
       " ['2016-03-21 20:43:01',\n",
       "  'Hyundai_Getz_1.5_CRDi_VGT_GLS',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1400.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '88',\n",
       "  'getz',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'hyundai',\n",
       "  'ja',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '51147',\n",
       "  '2016-04-02 22:47:22'],\n",
       " ['2016-03-22 18:47:21',\n",
       "  'Opel_Insignia_2.0_CDTI_Sports_Tourer_Edition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'insignia',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '31832',\n",
       "  '2016-04-02 13:15:39'],\n",
       " ['2016-03-20 16:57:05',\n",
       "  'Volkswagen_Golf_Variant_1.9_TDI_Joker',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1800.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '01920',\n",
       "  '2016-04-06 22:47:14'],\n",
       " ['2016-03-08 09:56:53',\n",
       "  'A3_2.0_TDI_Sportback_DPF_quattro',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3200.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  '',\n",
       "  '0',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '41748',\n",
       "  '2016-03-08 09:56:53'],\n",
       " ['2016-04-04 21:43:31',\n",
       "  '***_ACHTUNG_FORD_KA_NEU_TÜV_BIS_04/2087_ZU_VERKAUFEN_***',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1350.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'ka',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '24103',\n",
       "  '2016-04-07 00:15:47'],\n",
       " ['2016-04-02 23:54:43',\n",
       "  'BMW_530d_Touring_Aut._TOP_Ausstattung!!!_Rabatt_!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '231',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '76332',\n",
       "  '2016-04-07 07:16:22'],\n",
       " ['2016-03-31 13:45:43',\n",
       "  'RENAULT_MEGANE_2.0_TURBO_COUPE_CABRIO_PRIVILEG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6900.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  'megane',\n",
       "  '100000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '84416',\n",
       "  '2016-04-06 06:16:29'],\n",
       " ['2016-03-30 16:39:39',\n",
       "  'BMW_730er_V8_Automatik_aus_3er_Hand_top_fahrbereit__TÜV:07.16!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  'automatik',\n",
       "  '0',\n",
       "  '7er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '12057',\n",
       "  '2016-04-01 10:45:49'],\n",
       " ['2016-04-01 18:45:45',\n",
       "  'Audi_A6_Avant_2.7_TDI_DPF_multitronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12999.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '179',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '25551',\n",
       "  '2016-04-05 14:18:28'],\n",
       " ['2016-03-31 12:58:26',\n",
       "  'BMW_BMW_535_d_xDrive_M_Sportpaket+Head_Up+Panorama++',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  18400.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2011',\n",
       "  'automatik',\n",
       "  '313',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '52477',\n",
       "  '2016-03-31 13:44:44'],\n",
       " ['2016-03-25 01:36:44',\n",
       "  'Volkswagen_Vento_1.8_CLX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  999.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '76597',\n",
       "  '2016-04-06 06:45:14'],\n",
       " ['2016-03-27 23:48:09',\n",
       "  'Audi_A3_2.0_TDI_Sportback_DPF_S_tronic_Navi_Xenon_19\"',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12899.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '140',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '51109',\n",
       "  '2016-04-06 02:15:20'],\n",
       " ['2016-03-31 19:52:39',\n",
       "  'BMW_318d_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '29683',\n",
       "  '2016-04-06 14:18:03'],\n",
       " ['2016-03-13 00:56:50',\n",
       "  'Smart_smart_&_pulse_cdi_TÜV_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2250.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  '',\n",
       "  '41',\n",
       "  'fortwo',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '78224',\n",
       "  '2016-03-30 01:47:18'],\n",
       " ['2016-04-02 10:51:46',\n",
       "  'Volkswagen_Golf_1.4_Trendline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'golf',\n",
       "  '50000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '34385',\n",
       "  '2016-04-02 10:51:46'],\n",
       " ['2016-04-05 11:48:01',\n",
       "  'BMW_130i_LCI_M_Paket_ab_Werk_Xenon_Schiebedach_18\"',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14900.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '265',\n",
       "  '1er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '14129',\n",
       "  '2016-04-05 11:48:01'],\n",
       " ['2016-03-17 13:47:55',\n",
       "  'Golf_3_75PS_Klima_Schiebedach',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '23879',\n",
       "  '2016-03-28 14:45:17'],\n",
       " ['2016-03-11 19:50:22',\n",
       "  'VW_Lupo_1_0_l_Tuev_6/2017_viele_Neuteile_verbaut',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'lupo',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '38707',\n",
       "  '2016-03-20 07:18:49'],\n",
       " ['2016-03-21 15:43:25',\n",
       "  'Audi_A3_1.8_Ambiente',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1250.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '63811',\n",
       "  '2016-03-30 01:18:23'],\n",
       " ['2016-04-04 17:48:29',\n",
       "  'VW_Polo_Coupe_86c_in_rot__guter_Zustand__alltagsauto',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  370.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1992',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'polo',\n",
       "  '125000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '30926',\n",
       "  '2016-04-04 17:48:29'],\n",
       " ['2016-03-12 08:55:50',\n",
       "  'AB_SOFORT_Renault_19__Chamade__R19__88PS__Ph2__TÜV_05/17',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  350.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1993',\n",
       "  'manuell',\n",
       "  '88',\n",
       "  'r19',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '06895',\n",
       "  '2016-03-16 08:16:48'],\n",
       " ['2016-03-15 23:55:52',\n",
       "  'BMW_325d_Coupe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8600.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '197',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '72116',\n",
       "  '2016-03-16 08:42:05'],\n",
       " ['2016-03-07 17:52:07',\n",
       "  'Volkswagen_Golf_Cabrio',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2222.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '1991',\n",
       "  'manuell',\n",
       "  '98',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '17495',\n",
       "  '2016-04-06 13:17:11'],\n",
       " ['2016-04-05 08:57:20',\n",
       "  'BMW_325i_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'automatik',\n",
       "  '218',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '88255',\n",
       "  '2016-04-07 12:17:52'],\n",
       " ['2016-03-29 23:57:50',\n",
       "  'Ford_C_Max_zu_verkaufen!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4000.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '115',\n",
       "  'c_max',\n",
       "  '125000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'ja',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '52525',\n",
       "  '2016-04-06 22:45:21'],\n",
       " ['2016-03-16 11:46:23',\n",
       "  'Mercedes_Benz_CDI_Kombi_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4650.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '12109',\n",
       "  '2016-03-20 09:44:22'],\n",
       " ['2016-03-23 11:42:10',\n",
       "  'Peugeot_206_1.4i_TÜV_NEU+8_Fach_Bereift+PANORAMADACH+4_TÜRER+',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1450.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  '2_reihe',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '21423',\n",
       "  '2016-04-01 15:15:53'],\n",
       " ['2016-03-08 13:55:52',\n",
       "  'Mondeo_TDCI_Bj._2002',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '115',\n",
       "  'mondeo',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  '',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '34253',\n",
       "  '2016-03-10 08:17:28'],\n",
       " ['2016-03-20 11:38:38',\n",
       "  'Seat_Cordoba_Vario_1.9_TDI_TÜV_NEU!!_Gruene_Plakette!_VHB!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'cordoba',\n",
       "  '150000',\n",
       "  '11',\n",
       "  '',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '67346',\n",
       "  '2016-03-26 19:18:11'],\n",
       " ['2016-03-05 15:55:47',\n",
       "  'Volkswagen_Golf_V_1.4_Top_Zustand_*TÜV*',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '100000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '06295',\n",
       "  '2016-03-16 06:45:15'],\n",
       " ['2016-03-14 15:56:43',\n",
       "  'Mercedes_Benz_C_200_T_CDI_Avantgarde_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5499.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '60318',\n",
       "  '2016-03-17 07:17:11'],\n",
       " ['2016-03-28 18:56:10',\n",
       "  'BMW_323i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '41470',\n",
       "  '2016-04-07 01:16:06'],\n",
       " ['2016-03-12 08:51:00',\n",
       "  'Suzuki_LJ_80_fuer_Bastler',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '1982',\n",
       "  'manuell',\n",
       "  '41',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'suzuki',\n",
       "  '',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '56337',\n",
       "  '2016-03-13 00:45:46'],\n",
       " ['2016-03-26 20:57:26',\n",
       "  'Audi_A4_Avant_2.0_TDI_Attraction_150PS_Xenon+_Navi+',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  22290.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'a4',\n",
       "  '50000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '84048',\n",
       "  '2016-04-06 11:45:22'],\n",
       " ['2016-03-22 11:59:28',\n",
       "  'Lancia_Phedra_2.0_16v_JTD_Emblema',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1440.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'lancia',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '83317',\n",
       "  '2016-03-22 11:59:28'],\n",
       " ['2016-03-21 14:56:24',\n",
       "  'Renault_Clio_1.4_RXE',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1150.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'clio',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '44879',\n",
       "  '2016-03-22 19:20:00'],\n",
       " ['2016-03-05 17:50:33',\n",
       "  'Volkswagen_Golf_2.0_GTD_Bremsen+Scheiben+TÜV_NEU_NAVI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15700.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'golf',\n",
       "  '90000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '32832',\n",
       "  '2016-03-11 16:17:33'],\n",
       " ['2016-03-17 06:56:39',\n",
       "  'BMW_316i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  899.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '85640',\n",
       "  '2016-03-20 07:17:29'],\n",
       " ['2016-04-01 14:38:34',\n",
       "  'Bmw_320i_Touring_mit_Lederausstattung_in_schwarz__Vollaustattung.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9200.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '85659',\n",
       "  '2016-04-05 08:50:10'],\n",
       " ['2016-03-20 12:58:06',\n",
       "  'Mercedes_Benz_Sprinter_313_CDI_Doka_3_5t',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3350.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '130',\n",
       "  'sprinter',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '96472',\n",
       "  '2016-03-21 15:21:23'],\n",
       " ['2016-04-01 19:58:29',\n",
       "  'Renault_Clio_II_1.2_i_16V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2700.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'clio',\n",
       "  '125000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'ja',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '34582',\n",
       "  '2016-04-05 17:26:59'],\n",
       " ['2016-03-10 10:46:01',\n",
       "  'Mazda_Premacy_1.9',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '114',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'mazda',\n",
       "  'ja',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '35043',\n",
       "  '2016-03-20 15:45:12'],\n",
       " ['2016-03-06 18:49:03',\n",
       "  'Nissan_350_Z',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10200.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'andere',\n",
       "  '90000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  '',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '71131',\n",
       "  '2016-04-06 22:17:28'],\n",
       " ['2016-04-03 11:59:39',\n",
       "  'Audi_A6_2.0_TDI_DPF_multitronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '140',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '48249',\n",
       "  '2016-04-03 11:59:39'],\n",
       " ['2016-03-30 20:42:52',\n",
       "  'Volkswagen_Golf_2.0_GTI_DSG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14700.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2012',\n",
       "  'automatik',\n",
       "  '211',\n",
       "  'golf',\n",
       "  '60000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '42283',\n",
       "  '2016-04-07 12:45:20'],\n",
       " ['2016-03-28 22:55:34',\n",
       "  'Schicker__roter__Sportflitzer_GOLF_4_1_6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '10777',\n",
       "  '2016-03-30 21:18:00'],\n",
       " ['2016-04-01 13:46:21',\n",
       "  'Mitsubishi_Colt_1300_GLi_|_original_65.500_Km_|_TÜV_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  895.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'colt',\n",
       "  '70000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mitsubishi',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '50823',\n",
       "  '2016-04-01 13:46:21'],\n",
       " ['2016-03-30 21:39:12',\n",
       "  'Volkswagen_6n',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  950.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '34270',\n",
       "  '2016-04-05 11:45:31'],\n",
       " ['2016-03-16 19:49:49',\n",
       "  'BMW_540ia_M_packet_Shadowline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6799.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'automatik',\n",
       "  '286',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'lpg',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '41836',\n",
       "  '2016-04-07 05:16:57'],\n",
       " ['2016-03-23 19:54:35',\n",
       "  'Opel_Corsa_MIT_TÜV__Geringer_Verbrauch__ALUFELGEN_!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '56',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '32756',\n",
       "  '2016-03-23 19:54:35'],\n",
       " ['2016-03-05 18:38:39',\n",
       "  'Volkswagen_Polo_1.2_TSI_DSG_MATCH',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11919.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'automatik',\n",
       "  '90',\n",
       "  'polo',\n",
       "  '30000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '06110',\n",
       "  '2016-04-07 04:46:28'],\n",
       " ['2016-03-21 10:44:34',\n",
       "  'Porsche_Cayman_2.9___PCM/Navi___Sport_Chrono___TÜV_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  25900.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '265',\n",
       "  'andere',\n",
       "  '125000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'porsche',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '93053',\n",
       "  '2016-03-25 06:17:46'],\n",
       " ['2016-03-25 08:52:58',\n",
       "  'vw_golf_3_90ps_lederausstattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  300.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1993',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '27412',\n",
       "  '2016-04-04 06:22:54'],\n",
       " ['2016-03-06 09:48:11',\n",
       "  'Audi_A1_1.6_TDI_Attraction',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'a1',\n",
       "  '125000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '24537',\n",
       "  '2016-03-09 22:18:06'],\n",
       " ['2016-03-07 10:49:21',\n",
       "  'Opel_Vectra_Edition_2000_klimaautomatic_Standheizung___gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'vectra',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '16816',\n",
       "  '2016-03-07 10:49:21'],\n",
       " ['2016-03-10 15:52:45',\n",
       "  'BMW_318i_touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '118',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'ja',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '51647',\n",
       "  '2016-03-10 15:52:45'],\n",
       " ['2016-03-27 21:50:59',\n",
       "  'Volkswagen_Lupo_1.4_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1690.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'lupo',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '45147',\n",
       "  '2016-04-01 23:44:57'],\n",
       " ['2016-04-04 21:52:13',\n",
       "  'Megane_CC',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3650.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '113',\n",
       "  'megane',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '21739',\n",
       "  '2016-04-07 00:45:06'],\n",
       " ['2016-03-20 19:49:21',\n",
       "  'Citroën_Berlingo_L1_1.6_HDi_75_Niveau_B',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4850.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'berlingo',\n",
       "  '80000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '42651',\n",
       "  '2016-03-20 19:49:21'],\n",
       " ['2016-03-09 15:50:39',\n",
       "  'Mercedes__C220__CDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1450.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'c_klasse',\n",
       "  '5000',\n",
       "  '8',\n",
       "  '',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '39120',\n",
       "  '2016-03-10 01:17:06'],\n",
       " ['2016-03-05 22:56:03',\n",
       "  'Volkswagen_Passat_Variant_2.0_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '93437',\n",
       "  '2016-03-06 04:40:29'],\n",
       " ['2016-04-03 02:57:31',\n",
       "  'Volkswagen_Passat_Variant_2.0_5V_Comfortline_Family',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3900.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '39218',\n",
       "  '2016-04-07 09:44:44'],\n",
       " ['2016-03-29 22:50:53',\n",
       "  'Volkswagen_Caravelle_Lang_DSG_Navi_Standheiz._VOLL',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  26999.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2013',\n",
       "  'automatik',\n",
       "  '140',\n",
       "  'transporter',\n",
       "  '80000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '83684',\n",
       "  '2016-04-06 11:47:14'],\n",
       " ['2016-03-09 10:51:52',\n",
       "  'Volkswagen_Passat_Variant_1.6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '23758',\n",
       "  '2016-03-09 11:41:16'],\n",
       " ['2016-03-11 00:55:09',\n",
       "  'BMW_730d_VOLLAUSSTATTUNG__E38_\"2.Hand\"',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5900.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '184',\n",
       "  '7er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '09405',\n",
       "  '2016-04-05 23:46:27'],\n",
       " ['2016-04-04 23:48:59',\n",
       "  'Audi_A6_3.0_TDI_competition_S_Line_LED_BOSE',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  56900.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2015',\n",
       "  'automatik',\n",
       "  '326',\n",
       "  'a6',\n",
       "  '5000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '45525',\n",
       "  '2016-04-07 04:16:57'],\n",
       " ['2016-03-12 15:55:41',\n",
       "  'Volkswagen_Golf_2.0',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1100.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '90439',\n",
       "  '2016-03-12 15:55:41'],\n",
       " ['2016-03-11 13:52:52',\n",
       "  'Mercedes_Benz_CLS_350_CDI_4Matic_7G_TRONIC',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  48000.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2014',\n",
       "  'automatik',\n",
       "  '265',\n",
       "  'andere',\n",
       "  '80000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '96123',\n",
       "  '2016-04-05 08:47:16'],\n",
       " ['2016-03-21 17:36:59',\n",
       "  'Renault_Clio_III_Sport_RS_201',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8400.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '201',\n",
       "  'clio',\n",
       "  '70000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '99713',\n",
       "  '2016-03-30 04:46:31'],\n",
       " ['2016-03-12 07:26:49',\n",
       "  'Tausche_Youngtimer_Audi_80_mit_knapp_62000_km',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1990',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  '80',\n",
       "  '70000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '55122',\n",
       "  '2016-03-26 23:15:49'],\n",
       " ['2016-03-29 14:54:44',\n",
       "  'VW_Golf_IV_1.4_16V_Motorschaden?',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '31275',\n",
       "  '2016-04-06 01:15:45'],\n",
       " ['2016-03-22 19:56:01',\n",
       "  'Mercedes_Benz_ML_350_CDI_4Matic_7G_TRONIC_DPF_BRABUS_AMG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  27500.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '224',\n",
       "  'm_klasse',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '54636',\n",
       "  '2016-04-06 13:16:23'],\n",
       " ['2016-03-25 12:25:20',\n",
       "  'Volkswagen_Touran_1.4_TSI_Highline_R_Line_7Sitze/Standheiz.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14950.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'touran',\n",
       "  '70000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '55129',\n",
       "  '2016-04-07 05:46:31'],\n",
       " ['2016-03-28 18:38:30',\n",
       "  'BMW_Z4_roadster_2.2i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14900.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'z_reihe',\n",
       "  '70000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '16321',\n",
       "  '2016-04-07 00:17:37'],\n",
       " ['2016-03-12 16:44:57',\n",
       "  'BMW_535i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2950.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'automatik',\n",
       "  '235',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '97332',\n",
       "  '2016-03-19 19:16:31'],\n",
       " ['2016-03-29 23:38:02',\n",
       "  'Audi_80_coupe_top_gepflegt_2_0',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  '',\n",
       "  '150000',\n",
       "  '8',\n",
       "  '',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '32791',\n",
       "  '2016-04-06 20:16:34'],\n",
       " ['2016-03-30 23:56:48',\n",
       "  'VW_Tiguan_2.0_TDI_DPF_BlueMotion_Technology',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12000.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'tiguan',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '01069',\n",
       "  '2016-03-31 07:43:22'],\n",
       " ['2016-03-15 19:58:30',\n",
       "  'Mercedes_Benz_C_220_T_CDI_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '22549',\n",
       "  '2016-04-04 23:18:49'],\n",
       " ['2016-04-04 17:44:53',\n",
       "  'Volkswagen_Golf_4_Cabrio_2.0l_Benzin',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2400.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2001',\n",
       "  '',\n",
       "  '115',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '47533',\n",
       "  '2016-04-06 19:18:03'],\n",
       " ['2016-03-17 22:52:09',\n",
       "  'Mitsubishi_3000_GT',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6000.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '286',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'mitsubishi',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '36093',\n",
       "  '2016-04-05 18:45:15'],\n",
       " ['2016-03-07 15:55:34',\n",
       "  'Ford_Fiesta_1.6_TDCi_Titanium',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8950.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'fiesta',\n",
       "  '60000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '50968',\n",
       "  '2016-04-05 12:15:41'],\n",
       " ['2016-03-06 12:38:13',\n",
       "  'Ford_Mustang',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  18700.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '213',\n",
       "  'mustang',\n",
       "  '80000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '97424',\n",
       "  '2016-04-06 05:44:39'],\n",
       " ['2016-03-20 21:47:47',\n",
       "  'VW_Polo_1_2_Match_Plus_Teilleder_Panorama_SHZ_PDC_Klimaautomatik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'polo',\n",
       "  '50000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '91722',\n",
       "  '2016-04-03 07:00:20'],\n",
       " ['2016-03-31 15:48:39',\n",
       "  'Volkswagen_Passat_3c_CC_2.0_TDI_DSG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11550.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '140',\n",
       "  'passat',\n",
       "  '125000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '26931',\n",
       "  '2016-03-31 15:48:39'],\n",
       " ['2016-03-30 16:47:47',\n",
       "  'BMW_316i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2400.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1989',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '95364',\n",
       "  '2016-04-03 06:53:38'],\n",
       " ['2016-03-19 16:40:33',\n",
       "  'VWTiguan_sehr_gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16150.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'tiguan',\n",
       "  '30000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '06502',\n",
       "  '2016-04-07 02:15:37'],\n",
       " ['2016-03-23 17:59:12',\n",
       "  'Sharan_dti',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3400.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'sharan',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '38259',\n",
       "  '2016-03-29 13:16:19'],\n",
       " ['2016-03-07 06:14:28',\n",
       "  'Audi_1.Hand_Navi_Leder_Xenon',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3000.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '179',\n",
       "  'tt',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '55120',\n",
       "  '2016-03-08 10:18:07'],\n",
       " ['2016-03-28 13:31:04',\n",
       "  'Volkswagen_Golf_1.9_TDI_Comfortline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '56235',\n",
       "  '2016-04-06 07:46:17'],\n",
       " ['2016-04-01 00:51:10',\n",
       "  'Mazda_6._Scheckheft',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  '6_reihe',\n",
       "  '5000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'mazda',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '54558',\n",
       "  '2016-04-05 00:44:27'],\n",
       " ['2016-03-22 13:54:57',\n",
       "  'Twingo_1_2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  300.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '58',\n",
       "  'twingo',\n",
       "  '125000',\n",
       "  '11',\n",
       "  '',\n",
       "  'renault',\n",
       "  '',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '42579',\n",
       "  '2016-03-22 13:54:57'],\n",
       " ['2016-03-18 21:37:39',\n",
       "  'Verkaufe_Renault_Megane_Classic_mit_Klima',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  890.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '107',\n",
       "  'megane',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'ja',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '41065',\n",
       "  '2016-03-18 21:37:39'],\n",
       " ['2016-03-10 17:47:26',\n",
       "  'Astra_G__neu_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1699.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  '',\n",
       "  '75',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '26817',\n",
       "  '2016-03-16 06:17:06'],\n",
       " ['2016-03-09 19:58:09',\n",
       "  'Mitsubishi_L_300',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2899.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'mitsubishi',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '13409',\n",
       "  '2016-04-05 14:18:51'],\n",
       " ['2016-04-04 18:56:38',\n",
       "  'Volkswagen_Golf_1.4_Comfortline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '80000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '22926',\n",
       "  '2016-04-06 21:16:27'],\n",
       " ['2016-03-29 00:56:26',\n",
       "  'Peugeot_308_175_THP_Sport_Plus__Navi__Panorama__Teilled.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6799.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '174',\n",
       "  '3_reihe',\n",
       "  '90000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '58095',\n",
       "  '2016-04-05 14:47:33'],\n",
       " ['2016-03-20 14:56:34',\n",
       "  'Opel_Astra_H_TÜV_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2650.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '96528',\n",
       "  '2016-03-20 14:56:34'],\n",
       " ['2016-03-16 19:47:27',\n",
       "  'Renault_Clio_1.6_16_V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '107',\n",
       "  'clio',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '14612',\n",
       "  '2016-04-07 04:46:17'],\n",
       " ['2016-03-19 10:54:59',\n",
       "  'Audi_A4_Cabriolet_3.0',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5299.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '220',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '80999',\n",
       "  '2016-04-02 14:46:07'],\n",
       " ['2016-03-19 04:37:03',\n",
       "  'Mercedes_Benz_E_220',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7150.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '70619',\n",
       "  '2016-03-29 18:45:38'],\n",
       " ['2016-03-31 14:39:03',\n",
       "  'Citroën_c4_Coupé_TÜV_2017',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2300.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '88',\n",
       "  'c4',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '49565',\n",
       "  '2016-04-05 18:46:30'],\n",
       " ['2016-03-13 13:38:54',\n",
       "  'Biete_BMW_318_i_Kombi_auch_Tausch',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  '',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '41199',\n",
       "  '2016-03-28 12:46:35'],\n",
       " ['2016-03-15 18:50:28',\n",
       "  'VW_Golf_Plus_DPF_140_PS_Rentnerfahrzeug',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '87656',\n",
       "  '2016-04-04 18:45:12'],\n",
       " ['2016-03-14 12:42:51',\n",
       "  'Fiat_Panda_4x4_Raritaet',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4400.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'panda',\n",
       "  '125000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '78112',\n",
       "  '2016-03-21 02:16:19'],\n",
       " ['2016-04-01 19:54:05',\n",
       "  'Audi_A3_2.0_TDI_Sportback_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7950.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '26419',\n",
       "  '2016-04-01 19:54:05'],\n",
       " ['2016-03-05 19:47:47',\n",
       "  'BMW_Z4_Coupe_3.0si_/Sommerfahrzeug',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16500.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '265',\n",
       "  'z_reihe',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '71083',\n",
       "  '2016-04-07 12:17:24'],\n",
       " ['2016-03-05 14:41:09',\n",
       "  'BMW_Z4_roadster_2.2i_Automatik_Prof_Navi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8100.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'z_reihe',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '38723',\n",
       "  '2016-03-19 20:47:36'],\n",
       " ['2016-03-10 12:57:13',\n",
       "  'Volkswagen_up!_ASG_high_up!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9750.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2013',\n",
       "  'automatik',\n",
       "  '75',\n",
       "  'up',\n",
       "  '60000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '99817',\n",
       "  '2016-04-07 06:45:08'],\n",
       " ['2016-04-03 21:06:19',\n",
       "  'Mazda_3_2.2_MZR_CD_DPF_Sports_Line',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7700.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '185',\n",
       "  '3_reihe',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'mazda',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '35510',\n",
       "  '2016-04-05 12:10:46'],\n",
       " ['2016-03-05 14:25:18',\n",
       "  'PEUGEOT_306_Cabriolet_1_6_Saint_Tropez',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '88',\n",
       "  '3_reihe',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  '',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '65624',\n",
       "  '2016-04-06 22:15:56'],\n",
       " ['2016-03-31 09:55:33',\n",
       "  'Smart_smart_&_passion',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1195.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '54',\n",
       "  'fortwo',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '86399',\n",
       "  '2016-03-31 10:45:59'],\n",
       " ['2016-03-15 15:41:08',\n",
       "  'Renault_Clio_Grandtour_1.2_16V_75_Dynamique',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9790.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2014',\n",
       "  'manuell',\n",
       "  '73',\n",
       "  'clio',\n",
       "  '30000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '10249',\n",
       "  '2016-03-29 20:18:04'],\n",
       " ['2016-03-10 17:39:51',\n",
       "  'Renault_Clio',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1849.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'clio',\n",
       "  '90000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '53940',\n",
       "  '2016-03-12 22:18:31'],\n",
       " ['2016-03-28 14:57:57',\n",
       "  'Smart_Fortwo_Coupe_cdi__schwarz_',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3300.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '41',\n",
       "  'fortwo',\n",
       "  '125000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '33719',\n",
       "  '2016-03-30 05:46:53'],\n",
       " ['2016-03-05 14:47:19',\n",
       "  'Volkswagen_Transporter_T4_70B_1F2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1991',\n",
       "  'manuell',\n",
       "  '77',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '59348',\n",
       "  '2016-04-07 09:45:43'],\n",
       " ['2016-03-26 13:53:42',\n",
       "  'Toyota_Celica_GT',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2600.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'toyota',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '09437',\n",
       "  '2016-04-06 01:47:01'],\n",
       " ['2016-03-16 17:38:52',\n",
       "  'Mercedes_Benz_C_220_T_CDI_Avantgarde',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11700.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '91717',\n",
       "  '2016-04-06 23:16:34'],\n",
       " ['2016-03-21 01:59:07',\n",
       "  'renault_gebraucht',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  600.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'scenic',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'ja',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '24143',\n",
       "  '2016-03-26 12:46:25'],\n",
       " ['2016-04-02 15:43:07',\n",
       "  'Hyundai_i30cw_1.4_Cl._Sparauto_LPG_Gas__1._Hand_scheckh.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4950.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'i_reihe',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'hyundai',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '87439',\n",
       "  '2016-04-06 14:16:46'],\n",
       " ['2016-03-30 21:38:18',\n",
       "  'LUPO_1.0_LPG_super_sparsam___8fach_bereift___TÜV_NEU_bis_03/2018!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '50',\n",
       "  'lupo',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '55543',\n",
       "  '2016-04-05 11:45:25'],\n",
       " ['2016-03-11 20:56:26',\n",
       "  'BMW_3er_318_Ci_Edition_Exclusive_M_Technic_CABRIO_8500€',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8499.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '51515',\n",
       "  '2016-04-06 04:44:25'],\n",
       " ['2016-03-09 15:45:23',\n",
       "  'Volkswagen_Golf_1.6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1700.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '37619',\n",
       "  '2016-03-10 11:17:48'],\n",
       " ['2016-03-19 23:54:43',\n",
       "  'Kia_Ceed_1.4_CVVT_Attract_blau_metallic_SHZ_HU_2_/17',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5750.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'ceed',\n",
       "  '100000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'kia',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '85665',\n",
       "  '2016-04-06 03:15:22'],\n",
       " ['2016-03-20 18:49:25',\n",
       "  'Renault_Kangoo_1.2_16V_Authentique',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  600.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'kangoo',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'ja',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '58553',\n",
       "  '2016-04-07 02:44:50'],\n",
       " ['2016-03-25 07:36:27',\n",
       "  '520_Navi__Klima___Automatik...',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8700.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '21682',\n",
       "  '2016-04-06 07:46:12'],\n",
       " ['2016-03-31 14:53:55',\n",
       "  'Mercedes_Benz_E220CDI_Kombi_Automatik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '125',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '53123',\n",
       "  '2016-04-06 08:17:03'],\n",
       " ['2016-03-20 11:44:15',\n",
       "  'Mazda_5_1.6_MZ_CD_Sendo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  18880.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2014',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  '5_reihe',\n",
       "  '20000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'mazda',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '59872',\n",
       "  '2016-04-06 08:46:43'],\n",
       " ['2016-03-19 17:48:55',\n",
       "  'BMW_520d_Touring_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11200.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '177',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '91301',\n",
       "  '2016-04-01 07:49:21'],\n",
       " ['2016-03-09 20:45:43',\n",
       "  'Camaro_3.4_..V6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3550.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1993',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'andere',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'chevrolet',\n",
       "  '',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '54634',\n",
       "  '2016-03-14 15:15:34'],\n",
       " ['2016-03-28 18:47:24',\n",
       "  'Seat_Ibiza_Sport',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3900.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'ibiza',\n",
       "  '125000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '53572',\n",
       "  '2016-04-07 00:46:29'],\n",
       " ['2016-03-10 18:53:17',\n",
       "  'BMW_118d_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13849.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  '1er',\n",
       "  '80000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '73485',\n",
       "  '2016-04-06 00:45:33'],\n",
       " ['2016-03-26 19:58:06',\n",
       "  'Skoda_Yeti_1.2_TSI_Ambition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13200.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'yeti',\n",
       "  '40000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '79400',\n",
       "  '2016-04-06 10:45:21'],\n",
       " ['2016-03-28 20:56:52',\n",
       "  'Skoda_Fabia_1.4_16V_Combi_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'fabia',\n",
       "  '125000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '81541',\n",
       "  '2016-04-07 03:45:06'],\n",
       " ['2016-04-02 20:50:59',\n",
       "  'Skoda_Fabia_1.2_HTP_Cool_Edition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2550.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'fabia',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '27419',\n",
       "  '2016-04-02 20:50:59'],\n",
       " ['2016-03-10 09:56:48',\n",
       "  'Skoda_Octavia_Combi_1.6_Gas_1.hand__Scheckheft',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3900.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'octavia',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'lpg',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '26419',\n",
       "  '2016-03-15 19:15:19'],\n",
       " ['2016-03-16 09:47:12',\n",
       "  'BMW_330i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2450.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '231',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '85748',\n",
       "  '2016-03-19 06:46:08'],\n",
       " ['2016-04-03 09:53:39',\n",
       "  'Opel_Corsa_B_1.2_TÜV_11/16_Webasto_Standheizung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'corsa',\n",
       "  '70000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '08496',\n",
       "  '2016-04-07 11:17:28'],\n",
       " ['2016-03-27 18:37:16',\n",
       "  'Volkswagen_Passat_Variant_1.9_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '35415',\n",
       "  '2016-03-29 14:30:36'],\n",
       " ['2016-03-23 17:49:30',\n",
       "  'Opel_Zafira_irmscher',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1799.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '116',\n",
       "  'zafira',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '64579',\n",
       "  '2016-03-25 01:45:48'],\n",
       " ['2016-03-10 17:58:38',\n",
       "  'Golf_3_mit_gas_Anlage',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  950.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '6',\n",
       "  '',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '53639',\n",
       "  '2016-03-23 16:21:38'],\n",
       " ['2016-03-15 19:46:06',\n",
       "  'Seat_Mii_1.0_Style_Salsa_Sport_Navi__Klima__Tempom...',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8150.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'mii',\n",
       "  '60000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '83646',\n",
       "  '2016-03-16 17:46:37'],\n",
       " ['2016-03-05 14:14:19',\n",
       "  'Mazda_RX_8',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3900.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '192',\n",
       "  'rx_reihe',\n",
       "  '125000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'mazda',\n",
       "  'nein',\n",
       "  '2016-03-03 00:00:00',\n",
       "  '0',\n",
       "  '57520',\n",
       "  '2016-03-29 05:18:01'],\n",
       " ['2016-03-08 08:57:34',\n",
       "  'Peugeot_107_Klima',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '68',\n",
       "  '',\n",
       "  '40000',\n",
       "  '12',\n",
       "  '',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '24536',\n",
       "  '2016-03-21 21:44:28'],\n",
       " ['2016-03-30 07:54:00',\n",
       "  'Opel_Corsa_1.4_ecoFLEX_Start/Stop_Color_Edition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9999.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2014',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'corsa',\n",
       "  '20000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '55294',\n",
       "  '2016-04-06 23:46:25'],\n",
       " ['2016-03-31 19:50:19',\n",
       "  '!!!_Ford_Mondeo__mit_TÜV___das_ist_wichtig_Tausche__auch_!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'mondeo',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '66117',\n",
       "  '2016-04-06 14:17:56'],\n",
       " ['2016-03-29 16:36:57',\n",
       "  'VW_Golf_1.8_Autogas_mit_Benzin_4_Tuere',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  699.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1992',\n",
       "  'manuell',\n",
       "  '66',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'lpg',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '67065',\n",
       "  '2016-04-06 02:15:37'],\n",
       " ['2016-04-05 10:54:41',\n",
       "  'opel_astra_f_cabriolet',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  999.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '115',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '56070',\n",
       "  '2016-04-05 12:26:18'],\n",
       " ['2016-04-03 15:49:38',\n",
       "  'Audi_A6_4B_4_2_V8',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6250.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  '',\n",
       "  '299',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '49733',\n",
       "  '2016-04-05 15:16:12'],\n",
       " ['2016-03-23 23:57:07',\n",
       "  'Peugeot__F__206_mit_Motorschaden',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  300.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  '',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'ja',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '84579',\n",
       "  '2016-03-25 15:22:13'],\n",
       " ['2016-03-30 13:53:41',\n",
       "  'BMW_635_CSi_!_Top_gepflegt_!_H_Kennz._!_TÜV_2017',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16500.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1983',\n",
       "  'manuell',\n",
       "  '218',\n",
       "  '6er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '42719',\n",
       "  '2016-04-01 08:18:11'],\n",
       " ['2016-03-14 23:52:32',\n",
       "  'Ford_Focus_1.8__FFV__flexifuel_Style',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'focus',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'andere',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '97199',\n",
       "  '2016-04-05 11:51:03'],\n",
       " ['2016-03-11 21:49:52',\n",
       "  'Corvette_Z06',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  49500.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2013',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '10000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'sonstige_autos',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '79774',\n",
       "  '2016-03-13 04:45:42'],\n",
       " ['2016-03-21 23:53:36',\n",
       "  'Golf_Cabrio',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1480.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '74',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '19246',\n",
       "  '2016-04-05 14:17:54'],\n",
       " ['2016-03-26 21:37:36',\n",
       "  'Fiat_Punto_188_SX__97.000km__TÜV__8_Fach_Bereifung__MP3_Radio',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1490.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'punto',\n",
       "  '100000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '59557',\n",
       "  '2016-04-06 14:15:47'],\n",
       " ['2016-03-30 20:41:10',\n",
       "  'Opel_Astra_1.7_CDTI_Caravan_DPF_Catch_me_now',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3200.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '47495',\n",
       "  '2016-04-05 09:52:46'],\n",
       " ['2016-04-04 10:53:57',\n",
       "  'Audi_A6_Avant_2.5_TDI_TÜV_10.2017',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3250.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '40789',\n",
       "  '2016-04-06 12:17:40'],\n",
       " ['2016-03-26 22:56:11',\n",
       "  'Volkswagen_Fox_1.2__KLIMA',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3400.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'fox',\n",
       "  '70000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '33790',\n",
       "  '2016-04-07 01:46:35'],\n",
       " ['2016-03-28 14:53:30',\n",
       "  'Renault_Twingo_mit_Faltdach',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1100.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '58',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '44623',\n",
       "  '2016-04-06 19:16:29'],\n",
       " ['2016-03-25 15:51:40',\n",
       "  'Opel_Vectra_2.2_DTI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1750.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'vectra',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '73431',\n",
       "  '2016-04-06 20:16:15'],\n",
       " ['2016-03-29 21:38:39',\n",
       "  'VW_Passat_2l_TDI_DSG_Automatik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3300.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '140',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '33775',\n",
       "  '2016-03-31 15:16:54'],\n",
       " ['2016-04-02 12:39:02',\n",
       "  'Chevrolet_Matiz_0.8_SE',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1350.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '52',\n",
       "  'matiz',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'chevrolet',\n",
       "  'ja',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '91207',\n",
       "  '2016-04-06 10:17:28'],\n",
       " ['2016-04-04 10:46:55',\n",
       "  'Fiat_Panda_1.2_4x4_Climbing',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8250.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'panda',\n",
       "  '80000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '42555',\n",
       "  '2016-04-06 11:45:34'],\n",
       " ['2016-03-24 21:50:08',\n",
       "  'Vw_beetle_highline_2_0_l_lederausstattung_+++',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '115',\n",
       "  'beetle',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '07987',\n",
       "  '2016-04-05 17:17:10'],\n",
       " ['2016-03-16 03:03:22',\n",
       "  'Audi_A6_Avant_2.5_TDI_GRÜNE_Plakette_VOLL',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3999.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '163',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '18442',\n",
       "  '2016-04-05 23:16:34'],\n",
       " ['2016-03-31 21:59:23',\n",
       "  'Mercedes_Benz_ML_63_AMG_4Matic_7G_TRONIC',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  21500.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '510',\n",
       "  'm_klasse',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'lpg',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '33178',\n",
       "  '2016-04-06 17:45:19'],\n",
       " ['2016-03-23 09:54:26',\n",
       "  'BMW_118i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  '1er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '85375',\n",
       "  '2016-03-29 02:16:45'],\n",
       " ['2016-03-12 12:40:29',\n",
       "  'Renault_Grand_Scenic__7_Sitzer__EURO_4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2950.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '91781',\n",
       "  '2016-03-17 10:17:56'],\n",
       " ['2016-03-18 21:06:25',\n",
       "  'Hyundai_i20',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4850.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '77',\n",
       "  'i_reihe',\n",
       "  '100000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'hyundai',\n",
       "  '',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '65936',\n",
       "  '2016-03-20 08:35:03'],\n",
       " ['2016-03-10 14:45:46',\n",
       "  'Fiat_Punto_1.2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1100.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'punto',\n",
       "  '60000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '87600',\n",
       "  '2016-03-11 02:16:04'],\n",
       " ['2016-03-20 04:36:43',\n",
       "  'Opel_Vectra_2_2_GTS_Tuev_12/2017',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3600.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '147',\n",
       "  'vectra',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '90439',\n",
       "  '2016-03-25 06:45:16'],\n",
       " ['2016-04-02 18:37:34',\n",
       "  'Volkswagen_Golf_1.4_Comfortline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7750.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '57392',\n",
       "  '2016-04-06 18:16:04'],\n",
       " ['2016-03-11 18:38:42',\n",
       "  'VW_Polo_1.2_*sehr_gepflegter_Zustand*',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2950.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'polo',\n",
       "  '90000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '41812',\n",
       "  '2016-03-12 23:16:58'],\n",
       " ['2016-03-08 08:56:07',\n",
       "  'BMW_520d_Touring_Aut.___NAVI___AHK___Scheckheft',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6880.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '83236',\n",
       "  '2016-03-09 07:15:46'],\n",
       " ['2016-03-27 14:51:02',\n",
       "  'Volkswagen_Golf_Plus_DSG_Navi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5600.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '140',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '42281',\n",
       "  '2016-04-07 11:45:06'],\n",
       " ['2016-03-07 07:55:46',\n",
       "  'Volkswagen_Golf_1.2TSI_BlueMotion_Technology_DSG_NEUWAGENGARANTIE',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10550.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2012',\n",
       "  'automatik',\n",
       "  '105',\n",
       "  'golf',\n",
       "  '60000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '44309',\n",
       "  '2016-03-22 01:18:19'],\n",
       " ['2016-03-13 14:23:08',\n",
       "  'Mercedes_Benz_A_180__BlueEFFICIENCY_',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  17500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'a_klasse',\n",
       "  '40000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '55481',\n",
       "  '2016-04-06 00:15:30'],\n",
       " ['2016-03-22 13:45:05',\n",
       "  'Seat_Ibiza_1.4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  999.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'ibiza',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '73340',\n",
       "  '2016-03-31 09:44:37'],\n",
       " ['2016-03-15 09:38:21',\n",
       "  'Opel_Astra_Kombi_mit_Anhaengerkupplung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '115',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '86859',\n",
       "  '2016-04-05 17:21:46'],\n",
       " ['2016-03-20 23:45:11',\n",
       "  'Audi_A4_1.8',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2700.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'a4',\n",
       "  '125000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '57072',\n",
       "  '2016-04-07 09:45:41'],\n",
       " ['2016-03-28 12:38:16',\n",
       "  'Opel_Corsa_Grand_Slam',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '85748',\n",
       "  '2016-03-30 02:46:26'],\n",
       " ['2016-03-30 12:58:13',\n",
       "  'Smart_smart_fortwo_cabrio_pulse_micro_hybrid_drive',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5555.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '71',\n",
       "  'fortwo',\n",
       "  '90000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '85540',\n",
       "  '2016-04-07 04:15:52'],\n",
       " ['2016-03-12 18:44:27',\n",
       "  'Mercedes_Benz_A_160_BlueEFFICIENCY_Avantgarde',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8450.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'a_klasse',\n",
       "  '70000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '21465',\n",
       "  '2016-04-07 11:44:22'],\n",
       " ['2016-03-15 13:58:27',\n",
       "  'Opel_Astra_G_Cabrio',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2750.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '147',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '44359',\n",
       "  '2016-04-06 04:45:01'],\n",
       " ['2016-03-09 01:38:20',\n",
       "  'Opel_corsa_b_tuev_3.2018',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '48683',\n",
       "  '2016-03-12 13:45:24'],\n",
       " ['2016-03-15 13:52:38',\n",
       "  'Ford_Focus_Trend',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  680.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '67661',\n",
       "  '2016-04-06 03:16:12'],\n",
       " ['2016-03-30 18:58:27',\n",
       "  'Opel_Corsa_B___________AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  444.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '58452',\n",
       "  '2016-04-01 13:45:31'],\n",
       " ['2016-03-24 20:46:31',\n",
       "  'Peugeot_207_75',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4650.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '73',\n",
       "  '2_reihe',\n",
       "  '70000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '50674',\n",
       "  '2016-03-24 20:46:31'],\n",
       " ['2016-03-14 15:49:16',\n",
       "  'Kia_Rio_1.4_EX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4450.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '97',\n",
       "  'rio',\n",
       "  '70000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'kia',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '45892',\n",
       "  '2016-03-14 15:49:16'],\n",
       " ['2016-03-29 13:56:25',\n",
       "  'Nissan_Micra_1.2_Tuev_Neu_Maerz_2018_OHNE_MÄNGEL_Top_Zustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1999.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '65',\n",
       "  'micra',\n",
       "  '80000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '70376',\n",
       "  '2016-03-29 13:56:25'],\n",
       " ['2016-03-05 15:43:56',\n",
       "  'Opel_Zafira_1.7_CDT_AHK_Regen_Lichtsensor__unfallfrei',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9650.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'zafira',\n",
       "  '100000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '49477',\n",
       "  '2016-03-06 03:44:26'],\n",
       " ['2016-03-11 15:37:14',\n",
       "  'W124_320_E_TÜV_03/2017_5_Gang_Automatik_KLIMA',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2700.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1992',\n",
       "  'automatik',\n",
       "  '220',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '71706',\n",
       "  '2016-03-16 03:16:36'],\n",
       " ['2016-03-22 12:55:10',\n",
       "  'Peugeot_806_HDi_je_parle_francais_la_110',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2200.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '54296',\n",
       "  '2016-03-26 13:46:26'],\n",
       " ['2016-03-08 19:48:23',\n",
       "  'Mercedes_Benz_A_140__2.Hand_Scheckheft_gepfl.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3850.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '82',\n",
       "  'a_klasse',\n",
       "  '125000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '59348',\n",
       "  '2016-03-21 03:46:22'],\n",
       " ['2016-04-04 12:43:25',\n",
       "  'Volkswagen_Touareg_3.0_V6_TDI_DPF_Aut._Individual',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15500.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '239',\n",
       "  'touareg',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '45355',\n",
       "  '2016-04-06 13:44:33'],\n",
       " ['2016-03-18 12:48:27',\n",
       "  'Corvette_C1_1959_top_Zustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  90500.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1959',\n",
       "  'automatik',\n",
       "  '295',\n",
       "  'andere',\n",
       "  '90000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'chevrolet',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '94032',\n",
       "  '2016-04-03 21:17:33'],\n",
       " ['2016-03-26 21:49:45',\n",
       "  'Alfa_Giulietta_2.0_JTDm_mit_QV_Ausstattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13950.0,\n",
       "  'control',\n",
       "  'andere',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  '',\n",
       "  '60000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'alfa_romeo',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '86830',\n",
       "  '2016-04-06 14:15:39'],\n",
       " ['2016-03-29 18:45:21',\n",
       "  'BMW_530D_E39',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3400.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'automatik',\n",
       "  '193',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '24143',\n",
       "  '2016-04-06 04:17:16'],\n",
       " ['2016-03-29 20:06:18',\n",
       "  'Seat_Ibiza_SC_1.4_TSI_DSG_Cupra',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '179',\n",
       "  'ibiza',\n",
       "  '50000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '60596',\n",
       "  '2016-04-06 09:17:57'],\n",
       " ['2016-03-26 15:48:15',\n",
       "  'Dacia_Logan_MCV_1.5_dCi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2800.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '68',\n",
       "  'logan',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'dacia',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '64859',\n",
       "  '2016-04-06 04:15:35'],\n",
       " ['2016-04-04 13:48:25',\n",
       "  'Mercedes_Benz_E220_|_220_E_|_TÜV_03/2017_|_w124_|_E_Klasse',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1490.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1992',\n",
       "  'automatik',\n",
       "  '136',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '13349',\n",
       "  '2016-04-06 15:16:20'],\n",
       " ['2016-03-10 10:54:55',\n",
       "  'Vw_Caddy_1_9_sdi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'caddy',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '38259',\n",
       "  '2016-03-10 11:44:13'],\n",
       " ['2016-04-01 13:50:10',\n",
       "  'Suzuki_Vitara_1_6_L_Benziner',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1199.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '1992',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'suzuki',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '35039',\n",
       "  '2016-04-07 00:16:16'],\n",
       " ['2016-03-21 07:37:10',\n",
       "  'Alfa_Romeo_Spiber',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3200.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'spider',\n",
       "  '150000',\n",
       "  '6',\n",
       "  '',\n",
       "  'alfa_romeo',\n",
       "  '',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '24939',\n",
       "  '2016-03-31 08:18:09'],\n",
       " ['2016-03-29 17:56:01',\n",
       "  'KIA_OPIRUS_3_5_V6_EX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '203',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'lpg',\n",
       "  'kia',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '23558',\n",
       "  '2016-04-06 03:45:27'],\n",
       " ['2016-03-13 22:46:52',\n",
       "  'Renault__Megane__Grandtour_Exception_2_0_dci_Scheckheft_blau',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6450.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'megane',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '25715',\n",
       "  '2016-04-06 19:17:57'],\n",
       " ['2016-04-03 16:06:23',\n",
       "  'Opel_Corsa_1.2_16V_Comfort_Tuev/AU_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1690.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '63263',\n",
       "  '2016-04-06 22:46:14'],\n",
       " ['2016-03-09 23:46:25',\n",
       "  'Volkswagen_Passat_2.5_TDI_V6_4Motion_Highline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1750.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '90768',\n",
       "  '2016-04-07 09:17:11'],\n",
       " ['2016-03-08 19:38:37',\n",
       "  'Mercedes_Benz_A_140_Classic_Klima_Scheckheft_DB_TÜV_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2990.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '82',\n",
       "  'a_klasse',\n",
       "  '70000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '23968',\n",
       "  '2016-03-15 07:44:28'],\n",
       " ['2016-03-24 18:55:31',\n",
       "  'Audi_A4_Avant_1_8_Lichtpaket__Benzin__MMI__Xenon__Leder',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '0',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '24837',\n",
       "  '2016-04-07 13:15:53'],\n",
       " ['2016-03-19 07:51:18',\n",
       "  'Vw_lupo_1.0_klima_servo_1_Jahr_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '50',\n",
       "  'lupo',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '65824',\n",
       "  '2016-04-06 23:46:32'],\n",
       " ['2016-03-20 20:45:27',\n",
       "  'Microcar_MC1',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3950.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '5',\n",
       "  '',\n",
       "  '70000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'sonstige_autos',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '90562',\n",
       "  '2016-04-03 04:17:08'],\n",
       " ['2016-03-25 11:49:12',\n",
       "  'Opel_Omega_mit_Hagelschaden',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '144',\n",
       "  'omega',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'ja',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '72820',\n",
       "  '2016-04-06 12:44:35'],\n",
       " ['2016-04-02 21:54:46',\n",
       "  'CITREON_JUMPER_2_2_2010_LKW__KASTEN_HOCH_UND_LANG_TOP',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'test',\n",
       "  'andere',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '07545',\n",
       "  '2016-04-02 22:44:37'],\n",
       " ['2016-03-29 14:54:44',\n",
       "  'Citroeen_Xara_1_4_inklusive_Neu_TÜV/AU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1299.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '13435',\n",
       "  '2016-04-06 01:15:42'],\n",
       " ['2016-04-02 17:54:29',\n",
       "  'VW_Touran_1.9_TDI_United',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5800.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  '',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '31319',\n",
       "  '2016-04-04 15:47:39'],\n",
       " ['2016-03-25 17:52:18',\n",
       "  'Mercedes_Benz_C_180_T_Kompressor_Automatik_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7600.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '143',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '27777',\n",
       "  '2016-04-06 23:17:08'],\n",
       " ['2016-03-26 17:39:51',\n",
       "  'Volkswagen_Golf_Variant_1.4_TSI_DSG_Highline_Xenon_Panorama',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13499.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '160',\n",
       "  'golf',\n",
       "  '60000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '04107',\n",
       "  '2016-04-06 06:16:08'],\n",
       " ['2016-03-08 19:55:56',\n",
       "  'Daihatsu_Cuore_1.0_Top',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1490.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '58',\n",
       "  'cuore',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'daihatsu',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '44145',\n",
       "  '2016-03-12 21:44:17'],\n",
       " ['2016-03-12 14:39:17',\n",
       "  'Volkswagen_Golf_1.4_Tour',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '31246',\n",
       "  '2016-04-06 23:17:03'],\n",
       " ['2016-04-04 10:57:55',\n",
       "  'BMW_318i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2350.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '65558',\n",
       "  '2016-04-05 12:14:20'],\n",
       " ['2016-03-12 16:48:12',\n",
       "  'Ford_S_Max_2.0_TDCi_DPF_Aut._Trend',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6350.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '140',\n",
       "  's_max',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '45699',\n",
       "  '2016-04-07 06:45:53'],\n",
       " ['2016-04-02 16:30:52',\n",
       "  'Mazda_Premacy_1.9_TÜV_NEU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'mazda',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '55128',\n",
       "  '2016-04-02 17:45:49'],\n",
       " ['2016-03-27 19:52:13',\n",
       "  'Opel_Vectra_1_6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'automatik',\n",
       "  '101',\n",
       "  'vectra',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '44809',\n",
       "  '2016-03-31 01:45:53'],\n",
       " ['2016-04-01 22:51:18',\n",
       "  'Audi_A4_1.8_T_quattro',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '300',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '18573',\n",
       "  '2016-04-05 21:45:11'],\n",
       " ['2016-04-04 02:02:24',\n",
       "  'Audi_A6Mit_AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4300.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '179',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '5',\n",
       "  '',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '21149',\n",
       "  '2016-04-06 08:45:28'],\n",
       " ['2016-03-07 21:55:49',\n",
       "  'Fiat_Marea_100_16V_Weekend_SX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  850.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '103',\n",
       "  'andere',\n",
       "  '125000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '20146',\n",
       "  '2016-04-06 10:46:18'],\n",
       " ['2016-04-03 21:37:38',\n",
       "  'Mercedes_Benz_A_160_Blue_Efficiency',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6700.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'a_klasse',\n",
       "  '70000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '74235',\n",
       "  '2016-04-05 22:18:19'],\n",
       " ['2016-03-30 07:54:22',\n",
       "  'Audi_A3',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  29980.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'a3',\n",
       "  '50000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '59846',\n",
       "  '2016-04-06 23:46:17'],\n",
       " ['2016-03-21 21:46:45',\n",
       "  'Opel_Corsa_1.2_16V___2_HAND__KLIMA__8_FACH_TÜV_NEU!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2238.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'corsa',\n",
       "  '125000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '01067',\n",
       "  '2016-03-24 11:46:38'],\n",
       " ['2016-03-26 10:37:21',\n",
       "  'BMW_coupe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3350.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '83024',\n",
       "  '2016-03-26 11:41:18'],\n",
       " ['2016-03-05 16:45:59',\n",
       "  'Renault_Modus_1.5_dCi_Bj_2005__220000_km_TÜV_NEU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '86',\n",
       "  'modus',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '13439',\n",
       "  '2016-04-07 07:17:06'],\n",
       " ['2016-03-19 18:38:15',\n",
       "  'Top_gepflegter_Audi_A6_Avant_S_Line_2.0_TDI_/_AHK_Standheizung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'a6',\n",
       "  '100000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '94060',\n",
       "  '2016-04-07 05:17:33'],\n",
       " ['2016-03-12 12:06:22',\n",
       "  'Audi_A2_1.4__TÜV+_Zahnriemen_NEU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4199.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'a2',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '56235',\n",
       "  '2016-04-06 17:16:45'],\n",
       " ['2016-03-29 15:38:17',\n",
       "  'SEAT_Ibiza__TÜV_neu_nichtraucher__scheckheft_gepflegt__23500_km',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6999.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '85',\n",
       "  'ibiza',\n",
       "  '30000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '01744',\n",
       "  '2016-04-06 01:15:34'],\n",
       " ['2016-03-23 20:52:50',\n",
       "  'VW_Polo_1.2_Trendline_Klima/_Scheckheftgepflegt/_TÜV_NEU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4490.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'polo',\n",
       "  '100000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '01993',\n",
       "  '2016-03-29 16:45:23'],\n",
       " ['2016-03-20 20:42:07',\n",
       "  'BMW_X6_xDrive30d_M_Sportpaket_Garantie_bis_2018',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  43900.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2013',\n",
       "  'automatik',\n",
       "  '245',\n",
       "  'x_reihe',\n",
       "  '40000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '50859',\n",
       "  '2016-04-07 04:46:53'],\n",
       " ['2016-03-05 14:06:49',\n",
       "  'Audi_A5_Cabrio_1.8_TFSI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  21950.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '160',\n",
       "  'a5',\n",
       "  '90000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '65474',\n",
       "  '2016-03-06 11:08:40'],\n",
       " ['2016-04-02 08:51:27',\n",
       "  'Mazda_323_Sporty',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  '3_reihe',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'mazda',\n",
       "  '',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '06132',\n",
       "  '2016-04-06 07:15:25'],\n",
       " ['2016-03-23 10:50:58',\n",
       "  'Mercedes_Benz_E_220_T_CDI_Automatik_Classic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8100.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '76872',\n",
       "  '2016-04-07 12:16:23'],\n",
       " ['2016-03-18 18:47:36',\n",
       "  'BMW_530i_Touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12650.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '258',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '96114',\n",
       "  '2016-04-05 22:45:26'],\n",
       " ['2016-04-01 12:49:18',\n",
       "  'Solides_Auto',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2400.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '88161',\n",
       "  '2016-04-07 09:17:47'],\n",
       " ['2016-03-17 18:52:52',\n",
       "  'Audi_A3_1.8_TFSI_Ambition_S_Line_Xenon',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '160',\n",
       "  'a3',\n",
       "  '70000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '73525',\n",
       "  '2016-04-07 06:16:47'],\n",
       " ['2016-03-08 10:49:40',\n",
       "  'Mercedes_Benz_E350_CGI_Coupé_AMG_|_LEDER_|_SCHIEBEDACH_|_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  24900.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '292',\n",
       "  'e_klasse',\n",
       "  '125000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '30453',\n",
       "  '2016-03-12 01:16:12'],\n",
       " ['2016-03-10 12:54:34',\n",
       "  'Volkswagen_Golf_Plus_1.9_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4300.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '60486',\n",
       "  '2016-03-16 08:17:23'],\n",
       " ['2016-03-27 15:44:25',\n",
       "  'Smart_smart_fortwo_coupe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2250.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  '',\n",
       "  '61',\n",
       "  'fortwo',\n",
       "  '80000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '66907',\n",
       "  '2016-03-27 15:44:25'],\n",
       " ['2016-03-20 12:06:18',\n",
       "  'Audi_A4_Avant_2.0_TDI_DPF_Ambiente',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10550.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  'a4',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '65549',\n",
       "  '2016-03-27 00:46:19'],\n",
       " ['2016-03-14 06:03:08',\n",
       "  'Seat_Ibiza',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'ibiza',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '14542',\n",
       "  '2016-03-17 20:16:30'],\n",
       " ['2016-04-05 00:55:53',\n",
       "  'Audi_A6_2.0_TFSI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '04275',\n",
       "  '2016-04-07 11:15:55'],\n",
       " ['2016-03-05 15:58:27',\n",
       "  'OPEL_Vectra_B_Kombi_2.0_DTI_mit_AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  890.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'vectra',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '82054',\n",
       "  '2016-03-12 21:17:43'],\n",
       " ['2016-03-21 12:56:08',\n",
       "  'Audi_A5_Cabrio_3.0_TDI_DPF_quattro_S_tronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  20500.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '239',\n",
       "  'a5',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '83735',\n",
       "  '2016-04-06 07:45:39'],\n",
       " ['2016-03-30 18:38:15',\n",
       "  'Alfa_Romeo_147_1.6_Twin_Spark',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2990.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  '147',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'alfa_romeo',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '74912',\n",
       "  '2016-04-07 09:44:33'],\n",
       " ['2016-03-26 08:49:58',\n",
       "  'MERCEDES_BENZ_C_200_T_CDI_AVANTGARDE',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9600.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '136',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '14513',\n",
       "  '2016-03-26 09:41:36'],\n",
       " ['2016-03-10 11:57:04',\n",
       "  'Opel_Corsa_City',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1099.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'corsa',\n",
       "  '30000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '15732',\n",
       "  '2016-03-22 00:46:44'],\n",
       " ['2016-03-22 11:45:47',\n",
       "  'Opel_Astra_1.6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1700.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'astra',\n",
       "  '100000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '82110',\n",
       "  '2016-03-28 00:16:22'],\n",
       " ['2016-04-01 14:59:23',\n",
       "  'Expert_Kasten_geschlossen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4760.0,\n",
       "  'test',\n",
       "  'andere',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '98',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'peugeot',\n",
       "  'ja',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '01809',\n",
       "  '2016-04-07 12:46:21'],\n",
       " ['2016-04-02 10:51:44',\n",
       "  'Ford_Galaxy_2.0_TDCI_Trend_EZ_04.12_Anhaengerkupplung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'galaxy',\n",
       "  '80000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '59590',\n",
       "  '2016-04-04 07:50:58'],\n",
       " ['2016-03-23 10:54:49',\n",
       "  'Audi_TT_Coupe_1.8_T',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5300.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '179',\n",
       "  'tt',\n",
       "  '125000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '55450',\n",
       "  '2016-04-07 11:16:02'],\n",
       " ['2016-03-09 12:55:36',\n",
       "  'Citroen_C3__TÜV_neu_Benzin__109_PS_1_6i_2003_Automatik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3555.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '109',\n",
       "  'c3',\n",
       "  '100000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '48683',\n",
       "  '2016-04-05 23:45:18'],\n",
       " ['2016-03-08 19:55:51',\n",
       "  'guenstig_Fiat_uno_zum_verkaufen_Tuev_bis_11.2016',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  350.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1989',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  '',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '33098',\n",
       "  '2016-03-11 04:44:31'],\n",
       " ['2016-03-31 23:52:32',\n",
       "  'Mercedes_Benz_Viano_2.2_CDI_kompakt_Function_DPF_Vollleder_AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  19900.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'viano',\n",
       "  '90000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '59348',\n",
       "  '2016-04-07 03:17:20'],\n",
       " ['2016-03-07 21:46:03',\n",
       "  'Mercedes_Benz_S_420_CDI_DPF_7G_TRONIC',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15200.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '320',\n",
       "  's_klasse',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '29664',\n",
       "  '2016-04-06 01:45:50'],\n",
       " ['2016-03-07 20:30:57',\n",
       "  'Mazda_121_mit_75_PS_TÜV_1/2018_115100_km',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  700.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  '1_reihe',\n",
       "  '125000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mazda',\n",
       "  '',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '56072',\n",
       "  '2016-03-11 00:45:42'],\n",
       " ['2016-03-06 19:46:28',\n",
       "  'BMW_318_mit_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  550.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '130',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '73431',\n",
       "  '2016-03-08 23:17:31'],\n",
       " ['2016-03-31 11:25:20',\n",
       "  'Volkswagen_Eos_2.0_TDI_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8700.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'eos',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '63263',\n",
       "  '2016-04-05 19:46:30'],\n",
       " ['2016-03-15 16:46:22',\n",
       "  'Fiat_Panda_0_9_TwinAir_Turbo__86_PS___Topausstattung_Klimaautom.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6990.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '86',\n",
       "  'panda',\n",
       "  '50000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '72793',\n",
       "  '2016-04-06 11:45:49'],\n",
       " ['2016-03-15 09:54:56',\n",
       "  'Citroen_C4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'c4',\n",
       "  '80000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'citroen',\n",
       "  '',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '52066',\n",
       "  '2016-03-18 03:16:45'],\n",
       " ['2016-03-19 16:57:24',\n",
       "  'Hyundai_Getz_1.1',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1650.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '63',\n",
       "  'getz',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'hyundai',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '58642',\n",
       "  '2016-03-19 17:41:47'],\n",
       " ['2016-03-26 13:38:11',\n",
       "  'Audi_A3_2.0_TDI__Pano',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7900.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '32839',\n",
       "  '2016-03-29 15:45:12'],\n",
       " ['2016-04-04 21:41:10',\n",
       "  'PEUGEOT_206_CC_CABRIO_TÜV08/2016_KLIMA_1.HAND_ZV_EFHB',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4290.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  '2_reihe',\n",
       "  '90000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '22549',\n",
       "  '2016-04-07 00:15:38'],\n",
       " ['2016-03-14 01:47:28',\n",
       "  'Volkswagen_Golf_1.2_TSI_BlueM_Technology_Cup_Standhzg',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14650.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2014',\n",
       "  'manuell',\n",
       "  '86',\n",
       "  'golf',\n",
       "  '20000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '38444',\n",
       "  '2016-04-05 01:15:23'],\n",
       " ['2016-03-24 18:50:52',\n",
       "  'BMW_318i_touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3299.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '08060',\n",
       "  '2016-04-07 13:15:43'],\n",
       " ['2016-03-29 17:44:47',\n",
       "  'Ford_Fiesta',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '81',\n",
       "  'fiesta',\n",
       "  '40000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '12679',\n",
       "  '2016-03-31 10:45:24'],\n",
       " ['2016-03-21 21:51:41',\n",
       "  'Peugeot_206_110',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  600.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '148',\n",
       "  '2_reihe',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'ja',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '39387',\n",
       "  '2016-04-05 00:15:35'],\n",
       " ['2016-03-20 20:47:52',\n",
       "  'Volkswagen_Golf_1.6_Goal',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '72074',\n",
       "  '2016-04-07 05:44:58'],\n",
       " ['2016-04-03 18:06:22',\n",
       "  'Opel_Astra_G_Caravan__Edition_100__Silber_Metallic.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '65474',\n",
       "  '2016-04-07 02:17:02'],\n",
       " ['2016-03-27 14:40:52',\n",
       "  'C_180_T_BlueEFFICIENCY_7G_TRONIC_Avantgarde',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8000.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2011',\n",
       "  'automatik',\n",
       "  '156',\n",
       "  'c_klasse',\n",
       "  '30000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '10115',\n",
       "  '2016-03-27 14:40:52'],\n",
       " ['2016-03-16 12:48:16',\n",
       "  'Bastlerfahrzeug_Peugeot_206',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  700.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  '2_reihe',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  '',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '12529',\n",
       "  '2016-04-06 07:46:07'],\n",
       " ['2016-03-23 15:42:44',\n",
       "  'Audi_A7_3.0_TDI_multitronic_V6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  27500.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '204',\n",
       "  'andere',\n",
       "  '100000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '49406',\n",
       "  '2016-03-23 15:42:44'],\n",
       " ['2016-03-08 19:48:28',\n",
       "  'Audi_A4_Avant_2.0_TDI_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'ja',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '72186',\n",
       "  '2016-04-05 15:17:49'],\n",
       " ['2016-04-02 07:56:21',\n",
       "  'Citroën_Xsara_Picasso_1.6_HDi_Exclusive',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2790.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '26133',\n",
       "  '2016-04-02 08:48:45'],\n",
       " ['2016-03-08 16:56:07',\n",
       "  'VW_Fox_2.Hand_1.2__112820_Km_Original__Scheckheft_VW__TÜV_08/2016',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2390.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'andere',\n",
       "  '125000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '45326',\n",
       "  '2016-03-10 12:16:19'],\n",
       " ['2016-03-14 20:55:56',\n",
       "  '320i_mit_Lackschaeden',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9900.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  '3er',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'ja',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '46047',\n",
       "  '2016-03-22 18:16:42'],\n",
       " ['2016-03-22 18:41:49',\n",
       "  'VW_Passat_Kombi_TDI_Mod._2010__Klima_Navi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '58093',\n",
       "  '2016-04-06 10:16:17'],\n",
       " ['2016-04-02 11:59:01',\n",
       "  'Ford_Mondeo_1.8_Turnier_Ambiente',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1450.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'mondeo',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '68647',\n",
       "  '2016-04-06 10:17:44'],\n",
       " ['2016-03-09 23:52:02',\n",
       "  'Sharan_1.9tdi_2_hand_tempomat_pdc_dvd',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3600.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'sharan',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '50354',\n",
       "  '2016-03-11 17:15:52'],\n",
       " ['2016-03-08 12:53:50',\n",
       "  'Smart_Smart_Brabus_Cabrio_Servo_Klima',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2295.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '54',\n",
       "  'fortwo',\n",
       "  '100000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '12043',\n",
       "  '2016-03-17 23:47:35'],\n",
       " ['2016-03-16 15:47:39',\n",
       "  'Bmw_530d_kombi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1699.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '51145',\n",
       "  '2016-04-06 17:46:18'],\n",
       " ['2016-03-23 13:45:11',\n",
       "  'Volkswagen_Golf_2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  950.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1991',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '94121',\n",
       "  '2016-03-26 06:16:42'],\n",
       " ['2016-03-06 19:48:31',\n",
       "  'Volkswagen_Polo_45',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '19303',\n",
       "  '2016-03-12 14:17:34'],\n",
       " ['2016-04-04 14:58:49',\n",
       "  'Volkswagen_Passat_Variant_1.6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '08309',\n",
       "  '2016-04-06 17:15:28'],\n",
       " ['2016-03-29 21:51:19',\n",
       "  'Nissan_Terrano_II_2.4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1250.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'andere',\n",
       "  '125000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '63607',\n",
       "  '2016-04-06 08:45:21'],\n",
       " ['2016-03-22 12:57:23',\n",
       "  'Toyota_Avensis_1.8_VVT_i_Combi_Hagelschaden',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3250.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '129',\n",
       "  'avensis',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'toyota',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '82494',\n",
       "  '2016-03-22 12:57:23'],\n",
       " ['2016-03-26 16:39:01',\n",
       "  'Viano___Vito___chekheft___diesel___gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13500.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '204',\n",
       "  'viano',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '03253',\n",
       "  '2016-03-29 18:47:25'],\n",
       " ['2016-03-15 09:55:13',\n",
       "  'BMW_323i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4999.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '80995',\n",
       "  '2016-04-05 18:44:30'],\n",
       " ['2016-03-14 22:57:33',\n",
       "  'Volkswagen_Tiguan_Sport_&_Style_/_8_fach_bereift_/_Panorama',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  18300.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'tiguan',\n",
       "  '40000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '92685',\n",
       "  '2016-04-07 05:46:14'],\n",
       " ['2016-04-01 09:37:51',\n",
       "  'VW_Golf_4_1_4l_2001_vollfahrbereit_mit_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '31061',\n",
       "  '2016-04-07 06:46:15'],\n",
       " ['2016-03-10 15:55:03',\n",
       "  'Mercedes_Benz_SL_280',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  36500.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '1981',\n",
       "  'manuell',\n",
       "  '185',\n",
       "  'sl',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '63897',\n",
       "  '2016-04-05 14:45:46'],\n",
       " ['2016-03-22 11:52:30',\n",
       "  'Opel_Corsa_C_1_2_in_Top_Zustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2700.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'corsa',\n",
       "  '125000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '34369',\n",
       "  '2016-04-06 00:46:36'],\n",
       " ['2016-04-02 21:43:28',\n",
       "  'Skoda_Roomster_1.2_12V_HTP_Style',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4990.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'roomster',\n",
       "  '60000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '52080',\n",
       "  '2016-04-02 21:43:28'],\n",
       " ['2016-03-21 13:43:24',\n",
       "  'BMW_325_Ci_m__packet_lpg_prins',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6800.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '192',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'lpg',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '53111',\n",
       "  '2016-03-21 13:43:24'],\n",
       " ['2016-03-19 19:46:25',\n",
       "  'Audi_S5_Exclusive___TipTronic_4.2L_V8_Quattro_Coupe_+',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  30900.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2011',\n",
       "  'automatik',\n",
       "  '354',\n",
       "  'a5',\n",
       "  '60000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '30159',\n",
       "  '2016-04-07 07:17:35'],\n",
       " ['2016-03-19 21:55:56',\n",
       "  'Renault_Clio_1.2_16V_75_Dynamique',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2999.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'clio',\n",
       "  '80000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '51143',\n",
       "  '2016-03-22 05:45:22'],\n",
       " ['2016-03-31 22:58:15',\n",
       "  'Fast_Jang_Timer_sucht_Übernehmer_!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1991',\n",
       "  'manuell',\n",
       "  '97',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '41836',\n",
       "  '2016-04-01 02:41:33'],\n",
       " ['2016-03-07 18:39:04',\n",
       "  'Mercedes_Benz_SL_380',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6500.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1981',\n",
       "  'automatik',\n",
       "  '204',\n",
       "  'sl',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'ja',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '13125',\n",
       "  '2016-03-09 07:46:08'],\n",
       " ['2016-03-22 12:52:55',\n",
       "  'Audi_Q5_3.0_TDI_quattro_S_tronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  28900.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '239',\n",
       "  'q5',\n",
       "  '125000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '89180',\n",
       "  '2016-04-06 01:44:23'],\n",
       " ['2016-03-24 19:54:22',\n",
       "  'Peugeot_206_135_Sport_S16',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1900.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '136',\n",
       "  '2_reihe',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '24536',\n",
       "  '2016-04-05 12:18:03'],\n",
       " ['2016-03-27 10:52:55',\n",
       "  'Volkswagen_Kaefer___Speedster___Dannert_Umbau',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4900.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1973',\n",
       "  'manuell',\n",
       "  '44',\n",
       "  'kaefer',\n",
       "  '90000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '16567',\n",
       "  '2016-04-07 06:44:31'],\n",
       " ['2016-04-04 11:06:24',\n",
       "  'Audi_A3_2.0_TDI_Sportback_DPF_Ambition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10490.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '65343',\n",
       "  '2016-04-05 12:13:38'],\n",
       " ['2016-03-08 16:53:28',\n",
       "  'Audi_A4_allroad_quattro_3.0_TDI_DPF_S_tronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  28499.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2011',\n",
       "  'automatik',\n",
       "  '239',\n",
       "  'a4',\n",
       "  '90000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '04416',\n",
       "  '2016-03-11 11:46:33'],\n",
       " ['2016-03-12 10:56:12',\n",
       "  'Suzuki_Ignis_Compact_Special1_3',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2100.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '83',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'suzuki',\n",
       "  '',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '50169',\n",
       "  '2016-04-06 03:46:17'],\n",
       " ['2016-03-06 21:36:24',\n",
       "  'VW_Passat_3BG_mit_LPG_Anlage',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3999.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'lpg',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '27711',\n",
       "  '2016-04-07 02:44:49'],\n",
       " ['2016-03-30 19:58:07',\n",
       "  'Peugeot_206',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1350.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  '2_reihe',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '30419',\n",
       "  '2016-04-07 11:45:57'],\n",
       " ['2016-03-05 15:52:50',\n",
       "  'Mazda_626_1.9i_LX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  600.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  '6_reihe',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'mazda',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '30627',\n",
       "  '2016-04-03 17:45:54'],\n",
       " ['2016-03-07 23:37:38',\n",
       "  'Opel_Corsa_1.3_CDTI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '70',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'ja',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '34414',\n",
       "  '2016-03-27 10:46:00'],\n",
       " ['2016-03-05 14:29:10',\n",
       "  'Opel_Insignia_Sportstourer',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2015',\n",
       "  'automatik',\n",
       "  '160',\n",
       "  'insignia',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '57632',\n",
       "  '2016-03-05 16:42:28'],\n",
       " ['2016-03-09 09:36:19',\n",
       "  'Mercedes_Benz_Vito_112_CDI_8_Sitze_Standheizung_Tempomat',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5000.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'vito',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '09599',\n",
       "  '2016-04-07 09:16:27'],\n",
       " ['2016-03-20 13:56:12',\n",
       "  'Opel_Corsa_1.3_CDTI_Style',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2400.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '96271',\n",
       "  '2016-03-28 12:47:17'],\n",
       " ['2016-03-14 12:52:24',\n",
       "  'Hyundai_Santa_Fe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4800.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '145',\n",
       "  'santa',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'hyundai',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '26188',\n",
       "  '2016-04-05 12:47:47'],\n",
       " ['2016-04-01 23:52:38',\n",
       "  'Volkswagen_Fox_1.2_Tuev_NEU_8_fach_bereit_Klima_elekt_FH',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3250.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'fox',\n",
       "  '100000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '04758',\n",
       "  '2016-04-06 03:17:40'],\n",
       " ['2016-03-26 08:53:33',\n",
       "  'Audi_A3_2.0_TDI_Sportback_DPF_quattro_S_line_Spor...',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11800.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '82541',\n",
       "  '2016-04-05 21:17:27'],\n",
       " ['2016-03-31 14:52:16',\n",
       "  'Touran_2.0_TDI_Highline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2800.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  '',\n",
       "  '0',\n",
       "  'touran',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '39167',\n",
       "  '2016-03-31 14:52:16'],\n",
       " ['2016-03-07 11:38:18',\n",
       "  'Mercedes_Benz_E_220_T_CDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '143',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '38518',\n",
       "  '2016-04-07 06:17:05'],\n",
       " ['2016-03-21 00:50:20',\n",
       "  'Mercedes_Benz_C_200_CDI_DPF_BlueEFFICIENCY_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9499.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '136',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '71540',\n",
       "  '2016-04-05 21:16:17'],\n",
       " ['2016-03-22 18:48:26',\n",
       "  'Volkswagen_Passat_Variant_1.8_5V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '78658',\n",
       "  '2016-04-06 11:16:16'],\n",
       " ['2016-04-01 08:37:29',\n",
       "  'Volkswagen_Lupo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1999.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'lupo',\n",
       "  '100000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '32549',\n",
       "  '2016-04-07 06:17:30'],\n",
       " ['2016-04-05 11:57:46',\n",
       "  'BMW_520i_touring_TÜV_Neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '38239',\n",
       "  '2016-04-05 12:44:52'],\n",
       " ['2016-03-28 19:54:32',\n",
       "  'Mercedes_Benz_C180',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  400.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'automatik',\n",
       "  '85',\n",
       "  'c_klasse',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'ja',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '32547',\n",
       "  '2016-03-28 19:54:32'],\n",
       " ['2016-03-11 18:44:15',\n",
       "  'Volkswagen_Touran_1.6_TDI_DPF_Comfortline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12950.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'touran',\n",
       "  '100000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '66386',\n",
       "  '2016-04-01 22:44:41'],\n",
       " ['2016-03-08 07:56:35',\n",
       "  'Audi_TT_1_8T_Klima_Leder_Bose_kompl._repariert_bitte_lesen_!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4999.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '180',\n",
       "  'tt',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '12307',\n",
       "  '2016-04-06 08:46:01'],\n",
       " ['2016-03-16 07:55:15',\n",
       "  'Mercedes_benz_c_200_vollausstattung_2_Hand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5200.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '163',\n",
       "  'c_klasse',\n",
       "  '125000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '14772',\n",
       "  '2016-03-17 07:47:42'],\n",
       " ['2016-03-23 00:55:16',\n",
       "  'Mini_cooper_s',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'cooper',\n",
       "  '90000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'mini',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '81667',\n",
       "  '2016-04-07 05:45:09'],\n",
       " ['2016-03-21 12:48:36',\n",
       "  'Saab_9_5_Troll_Hirsch_2.3t_Aero',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'automatik',\n",
       "  '230',\n",
       "  '',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'saab',\n",
       "  '',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '71691',\n",
       "  '2016-03-22 17:15:47'],\n",
       " ['2016-03-15 01:01:46',\n",
       "  'Seat__leon_cupra_r_auch_tausch',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9099.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '280',\n",
       "  'leon',\n",
       "  '125000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '89269',\n",
       "  '2016-04-05 15:15:40'],\n",
       " ['2016-03-21 11:56:46',\n",
       "  'Mini_Cooper_Chili_Panorama_Teilleder_Flame_Spoke_Modell_2007',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8200.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'cooper',\n",
       "  '90000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'mini',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '41466',\n",
       "  '2016-03-23 23:46:37'],\n",
       " ['2016-03-30 00:57:26',\n",
       "  'BMW_328i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '193',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'ja',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '44135',\n",
       "  '2016-04-06 21:47:39'],\n",
       " ['2016-03-08 14:56:43',\n",
       "  'Volkswagen_Golf_III',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '16909',\n",
       "  '2016-04-06 06:46:47'],\n",
       " ['2016-03-07 17:53:58',\n",
       "  'Volkswagen_Lupo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1190.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'lupo',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '23683',\n",
       "  '2016-03-07 17:53:58'],\n",
       " ['2016-03-31 23:41:53',\n",
       "  'Opel_Zafira_1.6_Comfort',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'zafira',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '98553',\n",
       "  '2016-03-31 23:41:53'],\n",
       " ['2016-03-10 20:47:58',\n",
       "  'Mazda_626_2.0_Touring_Edition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  '6_reihe',\n",
       "  '90000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mazda',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '22955',\n",
       "  '2016-04-06 08:15:56'],\n",
       " ['2016-03-28 21:47:37',\n",
       "  'Fiat_Punto_mit_neuen_TÜV_bis_Maerz_2018',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1150.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'punto',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '49610',\n",
       "  '2016-03-28 21:47:37'],\n",
       " ['2016-03-05 22:37:22',\n",
       "  'BMW_530d_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13980.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '231',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '47807',\n",
       "  '2016-03-06 03:42:13'],\n",
       " ['2016-03-30 02:37:57',\n",
       "  'Ford_C_Max_2.0Tdci_Ghia_Vollausstattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6499.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '136',\n",
       "  'c_max',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '59229',\n",
       "  '2016-04-06 22:45:42'],\n",
       " ['2016-03-23 03:02:45',\n",
       "  'Peugeot_407___2.0_HDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3499.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '140',\n",
       "  '4_reihe',\n",
       "  '30000',\n",
       "  '10',\n",
       "  '',\n",
       "  'peugeot',\n",
       "  'ja',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '50739',\n",
       "  '2016-04-07 07:44:29'],\n",
       " ['2016-03-21 10:50:50',\n",
       "  'E_280_T_CDI_Avantgarde_DPF___AHK__Klima__Airmatic__Volleder__Alu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8850.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'automatik',\n",
       "  '190',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '64347',\n",
       "  '2016-04-02 06:18:08'],\n",
       " ['2016-03-14 17:50:40',\n",
       "  'Ford_Tourneo_Connect_TDCi__DPF_gruene_Plakette_KLIMA___AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1990.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-01 00:00:00',\n",
       "  '0',\n",
       "  '82467',\n",
       "  '2016-03-21 10:45:42'],\n",
       " ['2016-03-23 00:57:12',\n",
       "  'VW_Golf_VI_Trendline_2.0_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7199.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '30655',\n",
       "  '2016-04-07 07:15:37'],\n",
       " ['2016-03-29 18:48:00',\n",
       "  'Audi_A5_3.0_TDI_Sportback',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16299.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '239',\n",
       "  'a5',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '82278',\n",
       "  '2016-04-06 04:45:41'],\n",
       " ['2016-03-21 15:59:54',\n",
       "  'BMW_316ti_Compact',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1999.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '44359',\n",
       "  '2016-03-24 05:15:42'],\n",
       " ['2016-03-10 18:45:10',\n",
       "  'Peugeot_206cc_Quiksilver_BJ_2005_95.000_km',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3800.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  '',\n",
       "  '100000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  '',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '46049',\n",
       "  '2016-04-05 23:44:21'],\n",
       " ['2016-03-26 10:36:55',\n",
       "  'Kia_Sportage_2.0_CRDi_2WD_EX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9990.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'sportage',\n",
       "  '100000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'kia',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '27729',\n",
       "  '2016-04-05 23:15:51'],\n",
       " ['2016-03-12 11:55:53',\n",
       "  'Audi_A4_Avant_2.7_TDI_DPF_S_line_Sportpaket__plus_',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13800.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '190',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '54317',\n",
       "  '2016-04-06 05:45:18'],\n",
       " ['2016-03-29 20:55:15',\n",
       "  'Renault_Twingo_1.2_16V_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2150.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '75',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '24601',\n",
       "  '2016-04-06 07:17:22'],\n",
       " ['2016-04-02 19:36:58',\n",
       "  'Volkswagen_Caddy_TDI_9K9AN6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'caddy',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '53498',\n",
       "  '2016-04-06 19:44:30'],\n",
       " ['2016-03-09 12:49:21',\n",
       "  'BMW_320d_DPF_Touring_/_Vollausstattung_/_TOP',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15750.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '184',\n",
       "  '3er',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '92260',\n",
       "  '2016-04-05 23:45:06'],\n",
       " ['2016-03-28 19:57:37',\n",
       "  'Golf_3_Variant_101Ps_tiefer_mit_Anhaengerkupplung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  990.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '84364',\n",
       "  '2016-04-07 02:47:02'],\n",
       " ['2016-03-17 19:51:48',\n",
       "  'Golf_4_1_4_Maschine',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  700.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '22047',\n",
       "  '2016-03-17 19:51:48'],\n",
       " ['2016-04-01 13:42:49',\n",
       "  'Mercedes_Benz_C_180_T',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '17039',\n",
       "  '2016-04-03 09:18:33'],\n",
       " ['2016-03-14 08:51:58',\n",
       "  'Audi_A8/S8_3.7_quattro_Automatikgetriebe_/_Zahnriemen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '260',\n",
       "  'a8',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '25926',\n",
       "  '2016-04-07 08:16:16'],\n",
       " ['2016-03-05 15:41:11',\n",
       "  'Volkswagen_Multivan_T4_2_4_L_Diesel_Camper_Drehsitz_Bett',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2700.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '1992',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '22301',\n",
       "  '2016-03-10 12:45:24'],\n",
       " ['2016-03-23 10:41:50',\n",
       "  'Opel_Corsa_1.3_CDTI_5Tuerer__Klimaanlage',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1650.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '42853',\n",
       "  '2016-03-30 18:47:25'],\n",
       " ['2016-03-08 21:37:14',\n",
       "  'VW_Golf_3_1_6l_Tuev/NEU!!!_5_Tuerig_Servo_SSD_WR',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  999.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '49163',\n",
       "  '2016-03-11 19:48:32'],\n",
       " ['2016-03-24 09:51:55',\n",
       "  'Mercedes_Benz_GLK_320_CDI_DPF_4Matic_7G_TRONIC',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  21500.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '224',\n",
       "  'glk',\n",
       "  '90000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '71691',\n",
       "  '2016-04-02 22:45:18'],\n",
       " ['2016-03-17 11:49:13',\n",
       "  'BMW_E_46_Caprio_2_5_ci',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3800.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '84489',\n",
       "  '2016-03-19 11:45:35'],\n",
       " ['2016-04-04 19:50:06',\n",
       "  'BMW_318i_Bastler_E46',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '25337',\n",
       "  '2016-04-04 19:50:06'],\n",
       " ['2016-03-31 23:54:31',\n",
       "  'Audi_A4_2.7_TDI_DPF_multitronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11900.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '179',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '83700',\n",
       "  '2016-04-07 04:16:25'],\n",
       " ['2016-03-09 14:52:18',\n",
       "  'VW_Golf_1_6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1996',\n",
       "  '',\n",
       "  '0',\n",
       "  'golf',\n",
       "  '80000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '07987',\n",
       "  '2016-03-17 14:17:58'],\n",
       " ['2016-03-27 11:46:59',\n",
       "  'Dodge_Caliber_2.0l_Sxt_cvt___Unfall___teilespender___ami___us_car',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2100.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '156',\n",
       "  'andere',\n",
       "  '90000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'chrysler',\n",
       "  'ja',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '08118',\n",
       "  '2016-04-01 10:44:39'],\n",
       " ['2016-04-03 12:40:34',\n",
       "  'MINI_Mini_Cooper',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5350.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'cooper',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'mini',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '82054',\n",
       "  '2016-04-07 14:57:06'],\n",
       " ['2016-03-20 11:56:51',\n",
       "  '320_Alpinweiss_Kohlenstoff',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '177',\n",
       "  '3er',\n",
       "  '125000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '47441',\n",
       "  '2016-03-20 11:56:51'],\n",
       " ['2016-04-04 14:53:21',\n",
       "  'VW_Lupo_Tuev/Asu_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1899.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'lupo',\n",
       "  '90000',\n",
       "  '7',\n",
       "  '',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '17506',\n",
       "  '2016-04-06 16:17:36'],\n",
       " ['2016-03-16 19:50:05',\n",
       "  'Chrysler_pt_cruiser_2.2_diesel_tausch_moeglich',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '130',\n",
       "  'ptcruiser',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'chrysler',\n",
       "  '',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '78661',\n",
       "  '2016-03-17 18:44:35'],\n",
       " ['2016-03-06 12:38:01',\n",
       "  'Mercedes_Benz_C_63_AMG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  41900.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2012',\n",
       "  'automatik',\n",
       "  '457',\n",
       "  'c_klasse',\n",
       "  '60000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '67657',\n",
       "  '2016-03-21 11:46:17'],\n",
       " ['2016-03-07 12:48:32',\n",
       "  'Mercedes_CLK_230_Automatik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5990.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2002',\n",
       "  'automatik',\n",
       "  '197',\n",
       "  'clk',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '09114',\n",
       "  '2016-03-16 05:48:30'],\n",
       " ['2016-03-22 21:50:36',\n",
       "  'Audi_A3_2.0_TDI_Sportback_Ambiente',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4499.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '52066',\n",
       "  '2016-04-06 17:15:54'],\n",
       " ['2016-03-12 17:50:58',\n",
       "  'Preis_nur_bis_morgen_14Uhr:_Schoener_Polo_HU/AU_02/2018',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '64',\n",
       "  '',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '04552',\n",
       "  '2016-03-12 17:50:58'],\n",
       " ['2016-03-18 15:36:20',\n",
       "  'Bmw_316i_Automatik_Getriebe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2999.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '105',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '8',\n",
       "  '',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '25335',\n",
       "  '2016-03-22 10:47:06'],\n",
       " ['2016-03-28 00:57:13',\n",
       "  'Golf_Plus_1_9_Diesel__Automatik__Zahnriehemen_Inspektion_Neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '104',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '53227',\n",
       "  '2016-04-06 09:45:29'],\n",
       " ['2016-03-23 11:57:36',\n",
       "  'Nissan_Primera_1.8_P12_\"_Prinz\"LPG_TÜV_neu!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2900.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'primera',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'lpg',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '58730',\n",
       "  '2016-04-01 16:46:42'],\n",
       " ['2016-04-02 19:38:54',\n",
       "  'Peugeot_407_HDi_135_Automatik_Tendance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1999.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '136',\n",
       "  '4_reihe',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '64289',\n",
       "  '2016-04-02 19:38:54'],\n",
       " ['2016-03-23 23:56:26',\n",
       "  'Hyundai_I10_1.2_Fifa_World_Cup_Edition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8799.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2014',\n",
       "  'manuell',\n",
       "  '87',\n",
       "  'i_reihe',\n",
       "  '30000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'hyundai',\n",
       "  '',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '96486',\n",
       "  '2016-03-31 23:15:40'],\n",
       " ['2016-03-07 23:56:28',\n",
       "  'Kia_Shuma_TÜV_bis_2018',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  970.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '88',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'kia',\n",
       "  '',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '13088',\n",
       "  '2016-03-08 06:42:47'],\n",
       " ['2016-03-18 14:07:01',\n",
       "  'Ford_Focus_Turnier_DI_Ghia',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1599.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '56479',\n",
       "  '2016-04-07 10:17:16'],\n",
       " ['2016-03-22 09:51:41',\n",
       "  'Renault_Espace_2.2_Family',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '107',\n",
       "  'espace',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '86551',\n",
       "  '2016-04-05 21:48:11'],\n",
       " ['2016-04-04 07:56:55',\n",
       "  'Opel_Vectra_A_CC_GT_/_1.Hand_/_77_tkm_/_Bastler_/_evntl._Tausch',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1991',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'vectra',\n",
       "  '80000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '25856',\n",
       "  '2016-04-06 08:45:33'],\n",
       " ['2016-03-26 13:57:27',\n",
       "  'Kia_Sportage',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10500.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'sportage',\n",
       "  '40000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'kia',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '67593',\n",
       "  '2016-04-06 02:17:10'],\n",
       " ['2016-03-18 12:43:41',\n",
       "  'Mercedes_Benz_S_280',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1969',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  's_klasse',\n",
       "  '80000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '46286',\n",
       "  '2016-04-05 22:16:44'],\n",
       " ['2016-03-27 09:56:27',\n",
       "  'BMW_E_46_Cabrio__Black_Saphir_Metallic__Facelift',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7400.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '192',\n",
       "  '',\n",
       "  '150000',\n",
       "  '8',\n",
       "  '',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '71067',\n",
       "  '2016-04-01 08:16:21'],\n",
       " ['2016-04-02 09:50:44',\n",
       "  'Opel_Frontera_2.2_16V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1698.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '136',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '30938',\n",
       "  '2016-04-04 07:17:41'],\n",
       " ['2016-03-12 22:57:06',\n",
       "  'Peugeot_206_zu_verkaufen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1850.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'andere',\n",
       "  '100000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  '',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '66809',\n",
       "  '2016-04-06 11:15:50'],\n",
       " ['2016-03-11 19:54:32',\n",
       "  '***VOLKSWAGEN_GOLF_5___V___TDI_SEHR_GEPFLEGT***',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '26736',\n",
       "  '2016-03-26 15:16:14'],\n",
       " ['2016-04-01 14:47:39',\n",
       "  'Ford_Galaxy_2.0_TDCi_DPF_Ambiente',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8450.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'galaxy',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '44359',\n",
       "  '2016-04-05 09:17:00'],\n",
       " ['2016-03-15 15:52:17',\n",
       "  'Mercedes_Benz_C_180_Kompressor_Sportcoupe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3700.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '38640',\n",
       "  '2016-04-06 08:16:04'],\n",
       " ['2016-03-28 14:57:24',\n",
       "  'Ford_Focus_Turnier_2.0_TDCi_DPF_Aut.EINPARKHELFE_TOP',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '136',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '45355',\n",
       "  '2016-03-31 23:46:20'],\n",
       " ['2016-03-28 19:40:52',\n",
       "  'BMW_335',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  21000.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '409',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '84030',\n",
       "  '2016-04-07 01:45:48'],\n",
       " ['2016-03-23 16:49:44',\n",
       "  'Mercedes_Benz_B_150',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'b_klasse',\n",
       "  '90000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '17348',\n",
       "  '2016-04-05 20:46:33'],\n",
       " ['2016-03-14 14:46:07',\n",
       "  'Audi_A4_Avant_1.8_TFSI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14750.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '160',\n",
       "  'a4',\n",
       "  '90000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '91361',\n",
       "  '2016-03-18 01:45:22'],\n",
       " ['2016-03-27 22:52:02',\n",
       "  'Audi_A8_d2_4_2Ltr_Quattro',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2200.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'automatik',\n",
       "  '299',\n",
       "  '',\n",
       "  '150000',\n",
       "  '5',\n",
       "  '',\n",
       "  'audi',\n",
       "  'ja',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '74182',\n",
       "  '2016-04-05 23:47:17'],\n",
       " ['2016-03-23 00:55:47',\n",
       "  'Opel_Vectra_B',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2250.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '147',\n",
       "  'vectra',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '22399',\n",
       "  '2016-04-07 05:45:04'],\n",
       " ['2016-03-10 18:06:17',\n",
       "  'Seat_Exeo_Kombi_bi_xenon',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'exeo',\n",
       "  '40000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '57520',\n",
       "  '2016-04-01 02:17:22'],\n",
       " ['2016-03-07 09:52:06',\n",
       "  'Mitsubishi_carisma_1.8gdi_klima_tuev_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1900.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  '',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mitsubishi',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '14943',\n",
       "  '2016-03-08 15:44:31'],\n",
       " ['2016-03-30 11:55:46',\n",
       "  'Oldsmobile_Cutlass',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15000.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1971',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '5000',\n",
       "  '4',\n",
       "  '',\n",
       "  'sonstige_autos',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '51145',\n",
       "  '2016-04-07 02:45:10'],\n",
       " ['2016-04-03 16:54:55',\n",
       "  'BMW_323_CI_Topasblau_HK__Leder__Alarm_/_viele_Neuteile!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2300.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '51109',\n",
       "  '2016-04-03 16:54:55'],\n",
       " ['2016-03-23 20:25:18',\n",
       "  '320_d_M_Paket',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2300.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  '',\n",
       "  '0',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '61440',\n",
       "  '2016-03-23 20:50:48'],\n",
       " ['2016-03-26 23:51:54',\n",
       "  'Mercedes_Benz_S_500_L_Amg_Prinz_Gas_Lpg',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12999.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'automatik',\n",
       "  '306',\n",
       "  's_klasse',\n",
       "  '90000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '50679',\n",
       "  '2016-04-07 03:15:20'],\n",
       " ['2016-04-02 22:46:59',\n",
       "  'Mercedes_Benz_SL_320',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  24800.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '1996',\n",
       "  'automatik',\n",
       "  '231',\n",
       "  'sl',\n",
       "  '50000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '53773',\n",
       "  '2016-04-06 23:45:42'],\n",
       " ['2016-03-25 14:50:45',\n",
       "  'Volkswagen_krankentransport_T5_7JD103/WF2/ZP1_Autm.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7800.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '131',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '63067',\n",
       "  '2016-04-07 08:44:25'],\n",
       " ['2016-03-19 20:54:28',\n",
       "  'Peugeot_207_95_VTi_Urban_Move',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6200.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  '2_reihe',\n",
       "  '100000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '10717',\n",
       "  '2016-04-07 10:17:49'],\n",
       " ['2016-03-11 23:36:16',\n",
       "  'Peugeot_206+_HDi_eco_70',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5300.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '68',\n",
       "  '2_reihe',\n",
       "  '100000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '83329',\n",
       "  '2016-03-17 04:17:42'],\n",
       " ['2016-03-30 21:46:21',\n",
       "  'Audi_A4_Avant_2.8_Quattro',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1849.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '193',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '91301',\n",
       "  '2016-04-05 11:50:23'],\n",
       " ['2016-03-22 17:57:57',\n",
       "  'Renault_Clio_1.2_ECON',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '58',\n",
       "  'clio',\n",
       "  '90000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '92224',\n",
       "  '2016-04-06 09:17:32'],\n",
       " ['2016-03-11 12:00:05',\n",
       "  'Volkswagen_wvPolo_9n',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2100.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '64',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '84307',\n",
       "  '2016-04-01 07:15:34'],\n",
       " ['2016-04-01 15:56:27',\n",
       "  'Dodge_RAM_1500_SLT_5_9l_V8_4X4_Benzin_/_Gas',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9900.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '250',\n",
       "  '',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'lpg',\n",
       "  'sonstige_autos',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '17309',\n",
       "  '2016-04-07 14:56:03'],\n",
       " ['2016-03-11 14:57:04',\n",
       "  'Beschreibung__Verkaufe_mein_BMW_e46_328i_Ausstattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1700.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '193',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '48157',\n",
       "  '2016-03-17 22:17:52'],\n",
       " ['2016-03-16 20:55:07',\n",
       "  'Fiat_Punto_85.000_km_mit_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  950.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  '',\n",
       "  '0',\n",
       "  'punto',\n",
       "  '90000',\n",
       "  '10',\n",
       "  '',\n",
       "  'fiat',\n",
       "  '',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '33729',\n",
       "  '2016-03-30 17:16:31'],\n",
       " ['2016-03-17 23:40:32',\n",
       "  'Mercedes_Benz_200',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'ja',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '97775',\n",
       "  '2016-04-05 12:21:17'],\n",
       " ['2016-04-04 22:43:34',\n",
       "  'Mercedes_Benz_A_160_BlueEFFICIENCY',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11850.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'a_klasse',\n",
       "  '60000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '82296',\n",
       "  '2016-04-07 01:16:42'],\n",
       " ['2016-03-17 14:56:29',\n",
       "  'C_220CDI_KOMBI_TÜV_NEU_DPF_MOPF_FACELIFT',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5900.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  '',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '58089',\n",
       "  '2016-03-20 16:50:14'],\n",
       " ['2016-04-03 21:56:01',\n",
       "  'Ford_Mondeo_5_Door_Wagon_1_8l_85kw',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  600.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'mondeo',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '18276',\n",
       "  '2016-04-03 22:42:17'],\n",
       " ['2016-03-23 11:50:35',\n",
       "  'Audi_A2_1_4_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'a2',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '24329',\n",
       "  '2016-03-29 04:47:32'],\n",
       " ['2016-03-17 15:50:28',\n",
       "  'Mercedes_A160_avangard_Ausstatung_mit_elektrischen_sonnendach',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1299.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '102',\n",
       "  'a_klasse',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '40880',\n",
       "  '2016-04-06 23:47:03'],\n",
       " ['2016-03-28 16:42:44',\n",
       "  'BMW_528i_touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'automatik',\n",
       "  '193',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '88400',\n",
       "  '2016-03-28 16:42:44'],\n",
       " ['2016-04-05 10:36:53',\n",
       "  'Touran_1.9_TDI_105_Ps_Familienauto',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4500.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'touran',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '66839',\n",
       "  '2016-04-07 13:16:13'],\n",
       " ['2016-03-09 21:52:45',\n",
       "  'Nissan_Pick_Up_4WD_King_Cab',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4000.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '99439',\n",
       "  '2016-04-06 00:15:48'],\n",
       " ['2016-03-24 22:53:51',\n",
       "  'Volkswagen_Polo_6N_TÜV_NEU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  950.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '49078',\n",
       "  '2016-03-28 05:16:58'],\n",
       " ['2016-03-29 12:57:17',\n",
       "  'Seat_Ibiza_1.9_TDI__131_PS_',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5950.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'ibiza',\n",
       "  '100000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '32312',\n",
       "  '2016-04-03 21:46:55'],\n",
       " ['2016-03-24 18:36:16',\n",
       "  'GOLF_5_1.9_TDi_Xenon_105_PS_Regen_Sensor',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2650.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '30457',\n",
       "  '2016-03-24 18:36:16'],\n",
       " ['2016-03-28 19:51:36',\n",
       "  'Audi_A4_3.0_tdi_Quattro',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '265',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '30900',\n",
       "  '2016-04-07 02:17:44'],\n",
       " ['2016-03-19 10:44:52',\n",
       "  'MINI_Mini_One_Navi_PDC_Klima',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9250.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'one',\n",
       "  '40000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'mini',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '30165',\n",
       "  '2016-03-29 23:16:15'],\n",
       " ['2016-04-01 19:55:37',\n",
       "  'Volkswagen_Passat_Variant_2.0_Pacific',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  850.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '96247',\n",
       "  '2016-04-01 19:55:37'],\n",
       " ['2016-03-11 13:49:25',\n",
       "  'Mercedes_E_200_CDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2990.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'automatik',\n",
       "  '116',\n",
       "  'e_klasse',\n",
       "  '30000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '13357',\n",
       "  '2016-04-01 13:17:04'],\n",
       " ['2016-03-31 21:37:03',\n",
       "  'Mercedes_Benz_C_220_CDI_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'c_klasse',\n",
       "  '90000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '22159',\n",
       "  '2016-04-06 18:18:01'],\n",
       " ['2016-03-31 13:55:39',\n",
       "  'Golf_2_1.3_Nz_GT_Ausstattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  499.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1992',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '16321',\n",
       "  '2016-04-06 06:44:51'],\n",
       " ['2016-03-30 17:44:47',\n",
       "  'BMW_523i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '94533',\n",
       "  '2016-04-07 08:17:04'],\n",
       " ['2016-03-15 18:46:09',\n",
       "  'Volkswagen_Golf_1.4_Comfortline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3499.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '36266',\n",
       "  '2016-04-06 19:18:08'],\n",
       " ['2016-03-12 10:58:09',\n",
       "  'Renault_Clio_3_1_2_l_16_v',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  '',\n",
       "  '100000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  '',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '17034',\n",
       "  '2016-03-13 01:46:52'],\n",
       " ['2016-03-23 16:53:58',\n",
       "  'Mercedes_Benz_A_160',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1699.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '102',\n",
       "  'a_klasse',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '20539',\n",
       "  '2016-04-05 20:47:05'],\n",
       " ['2016-03-28 22:55:09',\n",
       "  'Clio_3_zu_Verkaufen....schoenes_Auto',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2400.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'clio',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '30159',\n",
       "  '2016-04-05 12:45:47'],\n",
       " ['2016-03-24 10:50:35',\n",
       "  'Mercedes_W202_in_bestem_Zustand!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2999.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '122',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '13051',\n",
       "  '2016-04-07 01:46:57'],\n",
       " ['2016-03-27 19:53:32',\n",
       "  'BMW_Bmw_545i_Autogas_Vollausstattung!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'automatik',\n",
       "  '333',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '83349',\n",
       "  '2016-03-29 10:17:51'],\n",
       " ['2016-03-15 20:53:11',\n",
       "  'Ford_Fiesta_1.4_Fun_X_Klima__AK__TÜV_neu__gepflegt_!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3450.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '88',\n",
       "  'fiesta',\n",
       "  '100000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '66954',\n",
       "  '2016-03-25 21:16:25'],\n",
       " ['2016-04-04 21:48:23',\n",
       "  'Smart_for_Two_451_passion_mhd',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7500.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2013',\n",
       "  'automatik',\n",
       "  '71',\n",
       "  'fortwo',\n",
       "  '40000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '42697',\n",
       "  '2016-04-07 00:44:52'],\n",
       " ['2016-03-30 21:37:21',\n",
       "  'Dodge_Avenger_2.7_SXT',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6999.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '186',\n",
       "  '',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'sonstige_autos',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '81929',\n",
       "  '2016-04-05 11:18:33'],\n",
       " ['2016-03-05 19:57:52',\n",
       "  'Octavia_Combi_1.6_TDI_DPF_Ambition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'octavia',\n",
       "  '80000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '95183',\n",
       "  '2016-04-07 03:16:11'],\n",
       " ['2016-03-26 10:50:30',\n",
       "  'Nissan_Micra_K12',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2100.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '65',\n",
       "  'micra',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '10555',\n",
       "  '2016-03-26 11:41:07'],\n",
       " ['2016-04-01 07:55:36',\n",
       "  'Volkswagen_Passat_Variant_1.9_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3999.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '26655',\n",
       "  '2016-04-07 06:15:21'],\n",
       " ['2016-03-15 22:55:52',\n",
       "  'Volkswagen_Eos_2.0_FSI_Individual_Chrom_mit_abnehmbarer_AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13900.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'eos',\n",
       "  '80000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '79664',\n",
       "  '2016-03-17 02:45:31'],\n",
       " ['2016-03-18 19:46:40',\n",
       "  'BMW_316i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  550.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '57250',\n",
       "  '2016-04-05 22:47:01'],\n",
       " ['2016-03-21 14:47:07',\n",
       "  'Smart_Coupe_Tuev_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1690.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '0',\n",
       "  '',\n",
       "  '125000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '25421',\n",
       "  '2016-04-02 09:17:08'],\n",
       " ['2016-03-21 15:54:44',\n",
       "  'Twingo_mit_Motorschaden',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  250.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'ja',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '66399',\n",
       "  '2016-03-31 20:16:09'],\n",
       " ['2016-04-01 13:38:56',\n",
       "  'Volkswagen_Golf_4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '21629',\n",
       "  '2016-04-01 13:38:56'],\n",
       " ['2016-03-22 17:53:25',\n",
       "  'BMW_118i_Automatik_/_Sitzheizung/_PDC_hinten/_...',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '129',\n",
       "  '1er',\n",
       "  '90000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '74072',\n",
       "  '2016-03-24 02:15:39'],\n",
       " ['2016-03-09 21:47:40',\n",
       "  'Volkswagen_Scirocco_1.4_TSI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11540.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'scirocco',\n",
       "  '80000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '95666',\n",
       "  '2016-04-05 23:46:23'],\n",
       " ['2016-03-23 17:48:36',\n",
       "  'Peugeot_207_CC_Roland_Garros_120_PS',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13199.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  '2_reihe',\n",
       "  '30000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '46244',\n",
       "  '2016-04-05 22:15:29'],\n",
       " ['2016-03-10 14:36:55',\n",
       "  'Mercedes_Benz_CLK_Cabrio_200_Kompressor_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12790.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '163',\n",
       "  'clk',\n",
       "  '40000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '41539',\n",
       "  '2016-04-05 11:49:37'],\n",
       " ['2016-03-27 21:36:22',\n",
       "  'Ford_Mondeo_1.8_Turnier_Trend',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'mondeo',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '34260',\n",
       "  '2016-03-31 05:47:28'],\n",
       " ['2016-03-10 08:36:17',\n",
       "  'Volkswagen_Golf_Variant_1.2_TSI_BlueMotion_Technology_L...',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13750.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2015',\n",
       "  'manuell',\n",
       "  '86',\n",
       "  'golf',\n",
       "  '20000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '87488',\n",
       "  '2016-03-21 14:45:19'],\n",
       " ['2016-03-24 21:55:10',\n",
       "  'Opel_Corsa_1.3_CDTI_DPF_ecoFLEX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'corsa',\n",
       "  '80000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '33378',\n",
       "  '2016-03-24 23:43:50'],\n",
       " ['2016-03-22 12:45:44',\n",
       "  'Ford_C_Max',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3000.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'c_max',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '46397',\n",
       "  '2016-03-23 20:15:39'],\n",
       " ['2016-03-19 15:44:54',\n",
       "  'BMW_E38_730i_Leder_Navi_19Zoll_mit_neuen_Reifen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2300.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  'automatik',\n",
       "  '0',\n",
       "  '7er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '27446',\n",
       "  '2016-03-30 09:45:08'],\n",
       " ['2016-03-21 13:56:13',\n",
       "  'Mercedes_Benz_C_220_CDI_DPF_BE__Comand__PreSafe__7G_TRONIC_Edi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  21999.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2013',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'c_klasse',\n",
       "  '100000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '56727',\n",
       "  '2016-04-06 09:46:47'],\n",
       " ['2016-04-01 23:56:33',\n",
       "  'Audi_A6_Avant_3.0_TDI_DPF_quattro_tiptronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  24999.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2011',\n",
       "  'automatik',\n",
       "  '239',\n",
       "  'a6',\n",
       "  '80000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '67435',\n",
       "  '2016-04-06 04:16:23'],\n",
       " ['2016-03-23 17:37:20',\n",
       "  'Renault_Twingo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  550.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  '',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '67061',\n",
       "  '2016-03-25 02:19:29'],\n",
       " ['2016-03-10 22:53:58',\n",
       "  'Audi_A4_Avant_1.8_T',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2350.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '56841',\n",
       "  '2016-03-17 05:17:28'],\n",
       " ['2016-03-20 10:54:37',\n",
       "  'BMW_318_Cabrio_m_Optik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1750.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '56567',\n",
       "  '2016-03-21 07:37:01'],\n",
       " ['2016-04-02 22:57:47',\n",
       "  'VW_T5_Multivan_Special__AHK__Fahrradtraeger',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  28300.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '114',\n",
       "  'transporter',\n",
       "  '40000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '13467',\n",
       "  '2016-04-07 03:45:44'],\n",
       " ['2016-04-04 18:43:11',\n",
       "  'Volkswagen_Golf_Variant_1.6_TDI_DPF_Style',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14900.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'golf',\n",
       "  '50000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '21224',\n",
       "  '2016-04-06 20:44:50'],\n",
       " ['2016-03-08 16:42:59',\n",
       "  'Audi_A3_2.0_TDI_DPF_Ambition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'a3',\n",
       "  '80000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '34596',\n",
       "  '2016-03-11 11:15:54'],\n",
       " ['2016-03-22 19:25:23',\n",
       "  'Zuverlaessigen_Hyundai_Getz_1.1_zu_verkaufen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '63',\n",
       "  'getz',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'hyundai',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '60433',\n",
       "  '2016-04-07 12:16:23'],\n",
       " ['2016-04-02 14:51:22',\n",
       "  'Renault_Twingo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  150.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1993',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '9',\n",
       "  '',\n",
       "  'renault',\n",
       "  'ja',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '78224',\n",
       "  '2016-04-06 13:17:43'],\n",
       " ['2016-03-26 13:37:16',\n",
       "  'Polo_Limusine_1.4___74kw/101_ps',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3100.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '8',\n",
       "  '',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '12627',\n",
       "  '2016-04-06 01:16:12'],\n",
       " ['2016-03-20 11:53:15',\n",
       "  'Audi_A4_1_8_Turbo_163PS',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '163',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '97450',\n",
       "  '2016-03-26 20:16:52'],\n",
       " ['2016-04-05 20:25:33',\n",
       "  'Alfa_Romeo_159_2.2_JTS_Distinctive',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '185',\n",
       "  '159',\n",
       "  '125000',\n",
       "  '12',\n",
       "  '',\n",
       "  'alfa_romeo',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '56070',\n",
       "  '2016-04-05 20:25:33'],\n",
       " ['2016-03-23 17:59:14',\n",
       "  'Mazda_Demio_1.5i_55KW__75PS__AHK__Klima__4xFhb__ZvFb__Nebellampen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  480.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'mazda',\n",
       "  '',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '26629',\n",
       "  '2016-03-23 17:59:14'],\n",
       " ['2016-03-15 10:37:43',\n",
       "  'BMW_316i_TÜV_1/1/2018_unfallfrei_kein_Rost',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '45326',\n",
       "  '2016-04-05 20:46:50'],\n",
       " ['2016-04-05 10:51:06',\n",
       "  'Hyundai_Lantra',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  350.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'andere',\n",
       "  '125000',\n",
       "  '7',\n",
       "  '',\n",
       "  'hyundai',\n",
       "  '',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '87616',\n",
       "  '2016-04-05 10:51:06'],\n",
       " ['2016-03-28 14:53:09',\n",
       "  'Ford_Fiesta_1.0_Start_Stop_Titanium_TOP_Ausstattung_+',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10490.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'fiesta',\n",
       "  '30000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '50739',\n",
       "  '2016-04-06 19:16:28'],\n",
       " ['2016-03-26 12:47:39',\n",
       "  'Volkswagen_Multivan_4MOTION_MwSt._ausweisbar_Standheizung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  37990.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'transporter',\n",
       "  '50000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '25421',\n",
       "  '2016-04-06 00:44:41'],\n",
       " ['2016-03-22 01:53:10',\n",
       "  'Mercedes_Vito',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3800.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'vito',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'ja',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '45470',\n",
       "  '2016-04-03 14:19:53'],\n",
       " ['2016-03-31 00:28:47',\n",
       "  'Ford_Transit_Mit_TÜV/HU._Voll_Fahrbereit!_Fest_preis!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1290.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'transit',\n",
       "  '5000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '35435',\n",
       "  '2016-04-02 21:11:39'],\n",
       " ['2016-04-04 22:36:22',\n",
       "  'Volkswagen_Golf_1.6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '25489',\n",
       "  '2016-04-05 00:42:18'],\n",
       " ['2016-03-23 14:39:00',\n",
       "  'Nissan_Juke_1.6_Acenta_/_Haptic_Blue',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10999.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '117',\n",
       "  'juke',\n",
       "  '60000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '71134',\n",
       "  '2016-04-05 15:18:25'],\n",
       " ['2016-03-28 10:54:49',\n",
       "  'Audi_A1_Ambition_1.2_TFSI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13900.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '86',\n",
       "  'a1',\n",
       "  '30000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '07545',\n",
       "  '2016-04-06 12:45:24'],\n",
       " ['2016-03-14 18:30:47',\n",
       "  'Audi_A4_2.8',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '193',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'lpg',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '65474',\n",
       "  '2016-03-28 08:46:19'],\n",
       " ['2016-04-01 19:47:11',\n",
       "  'Audi_TT_Coupe_1.8_T_quattro__268_Ps',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5600.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '224',\n",
       "  'tt',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '59379',\n",
       "  '2016-04-05 16:44:39'],\n",
       " ['2016-04-03 16:58:42',\n",
       "  'Volkswagen_Golf_Cabrio_1.4_TSI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  18000.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'golf',\n",
       "  '20000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '49124',\n",
       "  '2016-04-05 16:46:42'],\n",
       " ['2016-04-03 10:56:51',\n",
       "  'Mercedes_Benz_B_150',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5300.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'b_klasse',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '18109',\n",
       "  '2016-04-05 09:51:32'],\n",
       " ['2016-03-23 17:37:58',\n",
       "  'BMW_X3_3.0d_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '218',\n",
       "  'x_reihe',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '82319',\n",
       "  '2016-04-05 21:18:48'],\n",
       " ['2016-03-22 15:50:59',\n",
       "  'BMW_316i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2850.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '71139',\n",
       "  '2016-04-06 05:45:42'],\n",
       " ['2016-03-06 11:38:19',\n",
       "  'Chrysler_sebring_cabrio.Weisse_Ledersitze._TOP._Tuning._GOLD',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3850.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '141',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'chrysler',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '13055',\n",
       "  '2016-04-06 11:36:39'],\n",
       " ['2016-04-01 12:58:04',\n",
       "  'Opel_Vertra_Verkauf_oder_Tausch',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '100',\n",
       "  'vectra',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '08468',\n",
       "  '2016-04-07 10:16:27'],\n",
       " ['2016-04-01 21:53:31',\n",
       "  'Peugeot_308_120_VTi_Sport_1._Hand_Scheckheft_gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6490.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  '3_reihe',\n",
       "  '70000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '41469',\n",
       "  '2016-04-05 20:18:28'],\n",
       " ['2016-03-12 04:36:23',\n",
       "  'Volkswagen_Golf_1.9_TDI_Sportline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4550.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '59077',\n",
       "  '2016-03-22 07:48:11'],\n",
       " ['2016-04-04 21:39:18',\n",
       "  'Ibiza_1.4_75_PS__Klima__HU/AU_01/2018__WR__SR__Scheckheftgepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2250.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'ibiza',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '09638',\n",
       "  '2016-04-07 00:15:32'],\n",
       " ['2016-04-02 20:48:56',\n",
       "  'Ford_ka_zu_verkaufen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  350.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '65',\n",
       "  'ka',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  '',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '10965',\n",
       "  '2016-04-02 20:48:56'],\n",
       " ['2016-03-29 23:53:31',\n",
       "  'Skoda_Fabia_1.9_TDI_PD_DPF_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7490.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'fabia',\n",
       "  '70000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '71638',\n",
       "  '2016-04-01 01:15:41'],\n",
       " ['2016-03-27 14:50:59',\n",
       "  'verkaufe_Ranault_Grand_Espace_IV_3_0dci___Voll___7Sitze_Leder...',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3250.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '178',\n",
       "  'espace',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '95444',\n",
       "  '2016-03-29 03:47:18'],\n",
       " ['2016-03-26 11:54:32',\n",
       "  'Volkswagen_Passat_Variant_1.9_TDI_Comfortline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '90',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '16831',\n",
       "  '2016-04-06 00:15:26'],\n",
       " ['2016-03-07 21:37:26',\n",
       "  'Opel_Vectra_B_1.8_16V_KLIMA_ALU_FACE_MFL_TÜV_7.2016',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  699.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'vectra',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '69469',\n",
       "  '2016-03-12 09:44:12'],\n",
       " ['2016-03-27 20:38:01',\n",
       "  'BMW_435d_Cabrio_xDrive_Sport_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  47999.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2014',\n",
       "  'automatik',\n",
       "  '313',\n",
       "  'andere',\n",
       "  '60000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '46569',\n",
       "  '2016-04-05 18:45:45'],\n",
       " ['2016-03-21 14:49:56',\n",
       "  'Sehr_schoener_gepflegter_VW_Touran_1.4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13500.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'touran',\n",
       "  '70000',\n",
       "  '1',\n",
       "  '',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '36145',\n",
       "  '2016-04-06 12:15:30'],\n",
       " ['2016-04-03 21:37:24',\n",
       "  'VW_T5_Multivan_1.9_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '32423',\n",
       "  '2016-04-05 22:18:18'],\n",
       " ['2016-03-25 08:37:02',\n",
       "  'Audi_A4_2.7_TDI_DPF_multitronic_S_line_Sportpaket...',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12990.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '190',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '27578',\n",
       "  '2016-04-06 08:17:51'],\n",
       " ['2016-03-30 17:58:44',\n",
       "  'Peugeot_106_Sketch',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  700.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  '1_reihe',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '59757',\n",
       "  '2016-04-07 08:46:18'],\n",
       " ['2016-04-05 00:54:03',\n",
       "  'Hyundai_Terracan_2.9_CRDi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2750.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'hyundai',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '69214',\n",
       "  '2016-04-07 09:46:51'],\n",
       " ['2016-03-28 14:51:11',\n",
       "  'Ford_Galaxy_2.0_TDCi_DPF_Aut._Titanium_GROSS_NAVI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15900.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2011',\n",
       "  'automatik',\n",
       "  '140',\n",
       "  'galaxy',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '68307',\n",
       "  '2016-03-31 23:16:51'],\n",
       " ['2016-03-16 15:38:57',\n",
       "  'Mercedes_Benz_C_180_CGI_BlueEFFICIENCY_Schiebedach_Avantgarde',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13499.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '156',\n",
       "  'c_klasse',\n",
       "  '100000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '31319',\n",
       "  '2016-03-20 14:18:08'],\n",
       " ['2016-03-05 14:25:04',\n",
       "  'Volkswagen_Golf_1.4_Edition_aus_2.Hand_mit_121tkm',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1300.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-04 00:00:00',\n",
       "  '0',\n",
       "  '65929',\n",
       "  '2016-03-06 11:11:24'],\n",
       " ['2016-03-19 15:54:15',\n",
       "  'Audi_A3_2.0_TFSI_quattro_Sportback_Ambition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7990.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '200',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '86706',\n",
       "  '2016-04-07 00:46:40'],\n",
       " ['2016-04-07 07:36:37',\n",
       "  'VW_POLO_6N',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  550.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'polo',\n",
       "  '5000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-04-07 00:00:00',\n",
       "  '0',\n",
       "  '45309',\n",
       "  '2016-04-07 07:36:37'],\n",
       " ['2016-04-03 14:57:50',\n",
       "  'Mercedes_C280_/_W202_gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  999.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '192',\n",
       "  '',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '91580',\n",
       "  '2016-04-05 14:15:45'],\n",
       " ['2016-03-17 18:38:57',\n",
       "  'Opel_COMFORT_1_6_coupe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  950.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '73431',\n",
       "  '2016-04-07 05:16:11'],\n",
       " ['2016-03-27 06:36:19',\n",
       "  'Citroën_C4_1.6_16V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4699.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'c4',\n",
       "  '100000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '61250',\n",
       "  '2016-04-01 04:45:39'],\n",
       " ['2016-03-09 14:50:51',\n",
       "  'Mercedes_Benz_E_220_T_CDI_Automatik_Elegance_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '40227',\n",
       "  '2016-03-10 10:45:25'],\n",
       " ['2016-03-31 10:36:17',\n",
       "  'Volkswagen_Golf_Plus_1.6_Tour_Edition_/_Scheckheft_/_1_Hand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5997.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '30179',\n",
       "  '2016-04-06 04:17:06'],\n",
       " ['2016-03-08 17:39:41',\n",
       "  'Golf_4_1.4_mit_Klimaanlaage',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  999.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '53474',\n",
       "  '2016-04-07 06:15:59'],\n",
       " ['2016-03-19 19:39:52',\n",
       "  'BMW_523i_touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'ja',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '97297',\n",
       "  '2016-04-07 06:46:44'],\n",
       " ['2016-04-02 23:51:26',\n",
       "  'Corsa_TÜV_Juli_2016',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  180.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '55',\n",
       "  'corsa',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '71522',\n",
       "  '2016-04-07 06:16:57'],\n",
       " ['2016-03-09 17:25:18',\n",
       "  'Volvo_V40__8_fach_bereift_TÜV_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  999.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '115',\n",
       "  'v40',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volvo',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '56579',\n",
       "  '2016-03-16 23:15:32'],\n",
       " ['2016-03-20 18:44:28',\n",
       "  'Volkswagen_Polo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '93059',\n",
       "  '2016-03-20 18:44:28'],\n",
       " ['2016-03-12 19:36:18',\n",
       "  'VW_Touran_1_6_Benzin___mit_Prins_Gasanlage_LPG__tanken_fuer_52Cent',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'touran',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'lpg',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '23611',\n",
       "  '2016-03-20 00:44:57'],\n",
       " ['2016-03-29 18:53:12',\n",
       "  'BMW_328i__M_Paket_ab_Werk',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5250.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1996',\n",
       "  'automatik',\n",
       "  '193',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '59077',\n",
       "  '2016-04-02 07:16:05'],\n",
       " ['2016-03-26 19:51:33',\n",
       "  'Audi_A3',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '35260',\n",
       "  '2016-04-06 09:47:00'],\n",
       " ['2016-04-03 22:51:30',\n",
       "  'Renault_Clio_1.2_16V_75_Expression',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4600.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'clio',\n",
       "  '80000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '88637',\n",
       "  '2016-04-06 00:45:18'],\n",
       " ['2016-03-25 18:46:10',\n",
       "  'Audi_A6_Avant_2.5_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '58644',\n",
       "  '2016-04-07 00:17:07'],\n",
       " ['2016-03-30 13:58:08',\n",
       "  'Renault_Twingo_mit_TÜV_03/2018_mit_Getriebeschaden',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2005',\n",
       "  'automatik',\n",
       "  '58',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'ja',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '50667',\n",
       "  '2016-04-01 08:18:17'],\n",
       " ['2016-04-02 19:53:34',\n",
       "  'VW_Golf_Cabrio_2.0_Highline_TÜV_Leder_Klima_BBS_elekt._Verdeck',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2799.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '116',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '19395',\n",
       "  '2016-04-06 20:17:24'],\n",
       " ['2016-03-09 13:54:44',\n",
       "  'BMW_520d__M_Paket_Touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9600.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '177',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '46395',\n",
       "  '2016-03-17 10:46:32'],\n",
       " ['2016-03-19 13:49:50',\n",
       "  'Opel_Zafira_A_1.6_16V_Comfort_T98__Starsilber__101_PS',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'test',\n",
       "  'andere',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'zafira',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '36419',\n",
       "  '2016-03-19 13:49:50'],\n",
       " ['2016-03-18 20:43:14',\n",
       "  'Fiat_Punto_1.2_8V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5999.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'punto',\n",
       "  '50000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '56253',\n",
       "  '2016-03-20 05:47:03'],\n",
       " ['2016-03-11 12:52:36',\n",
       "  'A2_1.4_TDI_TOP',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2200.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  '',\n",
       "  '0',\n",
       "  'a2',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '52393',\n",
       "  '2016-03-11 12:52:36'],\n",
       " ['2016-04-03 13:46:37',\n",
       "  '4er_Golf_GTI_2.3_V5',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '73579',\n",
       "  '2016-04-03 13:46:37'],\n",
       " ['2016-03-25 19:47:02',\n",
       "  'Mercedes_Benz_C_160_Kompressor_Sportcoupe__8_fach_Alubereift_',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7600.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'c_klasse',\n",
       "  '125000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '34454',\n",
       "  '2016-04-07 02:15:25'],\n",
       " ['2016-03-10 19:49:37',\n",
       "  'BMW_320_Ci',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7499.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '46145',\n",
       "  '2016-04-06 03:45:55'],\n",
       " ['2016-03-27 18:55:44',\n",
       "  'Volkswagen_Golf_1.6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1100.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '92709',\n",
       "  '2016-04-05 15:46:58'],\n",
       " ['2016-03-31 18:54:19',\n",
       "  'Renault_Safrane_2.0_16V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  890.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '136',\n",
       "  'andere',\n",
       "  '100000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '33154',\n",
       "  '2016-04-04 12:15:24'],\n",
       " ['2016-03-26 15:38:57',\n",
       "  'Volkswagen_Golf_1.6_CL_Europe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1099.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '21380',\n",
       "  '2016-04-06 03:17:40'],\n",
       " ['2016-03-30 20:49:23',\n",
       "  'Renault_Megane_1.9_dCi_FAP_Grandtour',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3999.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'megane',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '50765',\n",
       "  '2016-04-07 13:17:59'],\n",
       " ['2016-03-10 12:51:14',\n",
       "  'Skoda_Fabia_Bj.2011_mit_Klima',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4999.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'fabia',\n",
       "  '125000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '34587',\n",
       "  '2016-03-14 08:45:25'],\n",
       " ['2016-03-10 20:46:48',\n",
       "  'Volkswagen_Polo_1.4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2799.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'polo',\n",
       "  '90000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '25421',\n",
       "  '2016-03-20 11:16:15'],\n",
       " ['2016-03-24 17:49:49',\n",
       "  'Verkaufe_meinen_Treuen_Wagen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'carisma',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mitsubishi',\n",
       "  '',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '49696',\n",
       "  '2016-04-03 10:24:48'],\n",
       " ['2016-03-22 14:55:22',\n",
       "  'Mercedes_Benz_230_E_/_Rostfrei_/_\"Scheckheft\"_/_Garagenwagen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3299.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1989',\n",
       "  'automatik',\n",
       "  '132',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '63801',\n",
       "  '2016-04-06 05:15:32'],\n",
       " ['2016-03-07 21:57:39',\n",
       "  'TOP___BMW_E46_328i_Touring___160000___TÜV_02/2017',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3999.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '193',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '48159',\n",
       "  '2016-04-06 19:46:34'],\n",
       " ['2016-04-02 09:51:15',\n",
       "  'Ford_Escort_MK_1_Cosworth_H_Zulassung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  34900.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1968',\n",
       "  'manuell',\n",
       "  '220',\n",
       "  'escort',\n",
       "  '5000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '47445',\n",
       "  '2016-04-04 06:50:17'],\n",
       " ['2016-03-15 17:55:11',\n",
       "  'Volkswagen_Golf_1.4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '56244',\n",
       "  '2016-03-16 19:35:25'],\n",
       " ['2016-03-22 21:46:43',\n",
       "  'Zafira_Tourer_2.0_CDTI_Start_Stop_Navi_Kamera',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8800.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '165',\n",
       "  'zafira',\n",
       "  '30000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '10115',\n",
       "  '2016-03-22 21:46:43'],\n",
       " ['2016-03-05 20:49:52',\n",
       "  'Mercedes_Benz__E280__Sehr_guter_Zustand_!!!_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'automatik',\n",
       "  '204',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '51149',\n",
       "  '2016-03-09 09:16:33'],\n",
       " ['2016-03-18 01:57:22',\n",
       "  'Audi_S4_Avant_2.7_quattro',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12800.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '265',\n",
       "  'andere',\n",
       "  '100000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '14979',\n",
       "  '2016-04-05 21:18:35'],\n",
       " ['2016-03-09 20:38:22',\n",
       "  'Opel_Omega_2_0_116PS_TÜV_04/2016_Neue_Bremsen__Wasserpumpe_usw',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  398.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'omega',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '90513',\n",
       "  '2016-03-23 14:48:25'],\n",
       " ['2016-03-05 20:36:24',\n",
       "  'BMW_320i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2700.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '91522',\n",
       "  '2016-03-15 09:46:35'],\n",
       " ['2016-03-23 19:00:20',\n",
       "  'Opel_Astra_2.0_16V_Turbo_Coupe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6999.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '265',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '29664',\n",
       "  '2016-04-05 23:45:59'],\n",
       " ['2016-03-09 10:54:10',\n",
       "  'Honda_Accord_2.0_i_Comfort',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2190.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '155',\n",
       "  'accord',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'honda',\n",
       "  'ja',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '87437',\n",
       "  '2016-03-11 16:45:58'],\n",
       " ['2016-03-25 11:50:46',\n",
       "  'Audi_A4_Cabriolet_2.4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7450.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '76756',\n",
       "  '2016-04-06 12:44:54'],\n",
       " ['2016-03-19 10:51:07',\n",
       "  'diesel_sparbuechse_corsa_c',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1850.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'corsa',\n",
       "  '100000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '12355',\n",
       "  '2016-03-26 20:47:17'],\n",
       " ['2016-04-02 16:49:46',\n",
       "  'Audi_A4_1.8_T_AHK__Xenon__Navi__gepflegt!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9450.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  'a4',\n",
       "  '125000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '26676',\n",
       "  '2016-04-06 16:15:44'],\n",
       " ['2016-03-17 14:38:54',\n",
       "  'Ford_Focus_Turnier_1.6_TDCi_bis_18.03_angemeldet',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2999.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '66773',\n",
       "  '2016-03-17 14:38:54'],\n",
       " ['2016-03-26 16:53:56',\n",
       "  'Polo_9n3_ah_exclusive',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'polo',\n",
       "  '100000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '04600',\n",
       "  '2016-03-26 16:53:56'],\n",
       " ['2016-03-11 12:54:22',\n",
       "  'VW_GOLF_4_1.4_16V_TÜV_NEU_KLIMA_4X_ELEKTRIK_FENSTER',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1670.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '52388',\n",
       "  '2016-03-23 03:19:38'],\n",
       " ['2016-03-07 13:46:32',\n",
       "  'Toyota_Corolla_1.6_Combi_Sol',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1800.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'corolla',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'toyota',\n",
       "  'ja',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '53347',\n",
       "  '2016-03-10 10:46:17'],\n",
       " ['2016-03-17 20:58:30',\n",
       "  'MINI_Mini_One',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5299.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'one',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mini',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '53797',\n",
       "  '2016-04-07 10:15:30'],\n",
       " ['2016-03-15 16:37:39',\n",
       "  'Jaguar_XJ40__sehr_gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3750.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1991',\n",
       "  'automatik',\n",
       "  '234',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'jaguar',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '83435',\n",
       "  '2016-03-29 23:15:33'],\n",
       " ['2016-03-28 15:37:11',\n",
       "  'Ford_Focus_Kombi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  '',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '28832',\n",
       "  '2016-04-06 20:15:35'],\n",
       " ['2016-03-22 17:53:32',\n",
       "  'Mercedes_C200_Kompressor_mit_wenig_Kilometern',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4200.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  'c_klasse',\n",
       "  '100000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '35394',\n",
       "  '2016-04-07 07:46:43'],\n",
       " ['2016-03-21 18:41:46',\n",
       "  'Golf_4_GTI_1.8_Turbo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2549.0,\n",
       "  'test',\n",
       "  'andere',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '38112',\n",
       "  '2016-04-07 13:16:20'],\n",
       " ['2016-03-14 12:48:40',\n",
       "  'Skoda_Octavia_Scout_2_0_FSI_Allrad',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8380.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'octavia',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '94405',\n",
       "  '2016-04-05 12:47:36'],\n",
       " ['2016-03-27 18:51:29',\n",
       "  'Suzuki_Grand_Vitara_1.9_DDiS',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8000.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '129',\n",
       "  'grand',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'suzuki',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '95509',\n",
       "  '2016-04-05 15:46:49'],\n",
       " ['2016-03-19 12:55:55',\n",
       "  'Mercedes_Benz_E_240',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2599.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '89616',\n",
       "  '2016-04-06 17:46:14'],\n",
       " ['2016-03-10 17:39:00',\n",
       "  'Skoda_Octavia_1.9_TDi_Combi_SLX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2650.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'octavia',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '27374',\n",
       "  '2016-03-12 07:46:00'],\n",
       " ['2016-04-04 15:46:35',\n",
       "  'Volkswagen_Phaeton_3.0_V6_TDI_Automatik_Tuev_Leder_navi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '224',\n",
       "  'phaeton',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '26817',\n",
       "  '2016-04-06 17:17:09'],\n",
       " ['2016-04-04 21:54:07',\n",
       "  'Ford_Fiesta_1.3',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2410.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'fiesta',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '33442',\n",
       "  '2016-04-07 00:45:11'],\n",
       " ['2016-04-03 21:57:25',\n",
       "  'Mercedes_Benz_C_220_T_CDI_AMG_Leder_AHK_Harman_Vollausstattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14900.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '86641',\n",
       "  '2016-04-05 23:15:37'],\n",
       " ['2016-03-07 09:39:18',\n",
       "  'Mercedes_Benz_Vito_112_CDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2100.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'vito',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '94094',\n",
       "  '2016-03-07 16:17:30'],\n",
       " ['2016-03-28 12:46:22',\n",
       "  'Nissan_Micra_in_blau',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  300.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '55',\n",
       "  'micra',\n",
       "  '150000',\n",
       "  '2',\n",
       "  '',\n",
       "  'nissan',\n",
       "  '',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '49716',\n",
       "  '2016-04-06 15:44:59'],\n",
       " ['2016-03-27 14:51:10',\n",
       "  'Peugeot_206_CC_110_Roland_Garros',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3500.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  '2_reihe',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '51105',\n",
       "  '2016-03-27 14:51:10'],\n",
       " ['2016-03-14 13:25:27',\n",
       "  'Peugeot_406_Break_Sport',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2250.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '158',\n",
       "  '4_reihe',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '35781',\n",
       "  '2016-03-22 19:19:16'],\n",
       " ['2016-03-30 17:38:36',\n",
       "  'Fiat_Cinquecento_0.9_i.e._S',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '39',\n",
       "  'andere',\n",
       "  '80000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '92690',\n",
       "  '2016-04-07 08:16:35'],\n",
       " ['2016-03-17 23:39:33',\n",
       "  'BMW_X3_2.0d',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10200.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'x_reihe',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '94371',\n",
       "  '2016-03-26 06:45:34'],\n",
       " ['2016-03-17 18:54:42',\n",
       "  'Opel_Corsa_1.2_Easytronic_m.Seitenschaden',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '80',\n",
       "  'corsa',\n",
       "  '125000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'ja',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '33739',\n",
       "  '2016-03-17 18:54:42'],\n",
       " ['2016-03-19 13:43:42',\n",
       "  'Honda_Civic_1.4i_S_Cool',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'civic',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'honda',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '79618',\n",
       "  '2016-03-27 01:45:14'],\n",
       " ['2016-03-08 17:38:52',\n",
       "  'VW_Polo_6_N_fuer_Bastler',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  330.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'polo',\n",
       "  '125000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '30851',\n",
       "  '2016-04-07 06:16:00'],\n",
       " ['2016-03-16 08:53:55',\n",
       "  'Mercedes_C180_Limousine',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  550.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1993',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '44143',\n",
       "  '2016-03-19 06:16:25'],\n",
       " ['2016-03-16 17:53:12',\n",
       "  'Ford_Focus_Turn._1.8_TDCi_Futura',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1320.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '37081',\n",
       "  '2016-04-07 00:45:29'],\n",
       " ['2016-03-12 15:49:29',\n",
       "  'BMW_118d_DPF_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '143',\n",
       "  '1er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '12437',\n",
       "  '2016-04-05 01:18:30'],\n",
       " ['2016-03-28 23:53:09',\n",
       "  'Golf_V__5__FSI_2.0_Sport_150PS_auch_Tausch',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'golf',\n",
       "  '100000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '12051',\n",
       "  '2016-04-05 13:45:13'],\n",
       " ['2016-03-21 16:47:45',\n",
       "  'Seat_Ibiza__Klima__1._Hd.__Blau_Met._60_PS_Euro4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  899.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'ibiza',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '37688',\n",
       "  '2016-03-31 21:16:19'],\n",
       " ['2016-03-09 16:46:23',\n",
       "  'Volkswagen_Caravelle_Lang_DSG_2X_Schiebetueren_Leder_180PS',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2012',\n",
       "  'automatik',\n",
       "  '179',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '26382',\n",
       "  '2016-04-03 04:19:25'],\n",
       " ['2016-04-01 19:58:33',\n",
       "  'Porsche_Boxster_PDK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  43900.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2014',\n",
       "  'automatik',\n",
       "  '265',\n",
       "  'boxster',\n",
       "  '10000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'porsche',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '55128',\n",
       "  '2016-04-02 15:38:28'],\n",
       " ['2016-03-15 09:37:25',\n",
       "  'Audi_A5_3.0_TDI_Sportback_quattro_DPF_S_tronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  21500.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '239',\n",
       "  'a5',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '47269',\n",
       "  '2016-04-05 19:17:05'],\n",
       " ['2016-04-05 09:54:58',\n",
       "  'Seat_Cordoba',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  250.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'cordoba',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '98593',\n",
       "  '2016-04-07 13:16:41'],\n",
       " ['2016-03-09 08:53:31',\n",
       "  'Audi_a6_2.0_170_PS_2009_Unfall',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'ja',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '79822',\n",
       "  '2016-03-09 10:40:29'],\n",
       " ['2016-03-09 14:52:30',\n",
       "  'BMW_525',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '197',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '12103',\n",
       "  '2016-03-25 15:48:33'],\n",
       " ['2016-03-19 08:37:01',\n",
       "  'Renault_clio_Automatik_Klima_Tuev_Neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1499.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '90',\n",
       "  'clio',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '69181',\n",
       "  '2016-03-20 11:17:33'],\n",
       " ['2016-03-16 19:45:37',\n",
       "  'Volkswagen_Passat_Variant_2.0_TDI_BlueMotion_Comfortline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14990.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'passat',\n",
       "  '90000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '73525',\n",
       "  '2016-03-30 14:17:15'],\n",
       " ['2016-03-27 14:49:20',\n",
       "  'Renault_Megane_zu_verkaufen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '104',\n",
       "  'megane',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '31737',\n",
       "  '2016-04-01 14:45:24'],\n",
       " ['2016-03-10 07:36:57',\n",
       "  'BMW_530d_Touring_als_Kombi_3_Liter_218_PS',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5900.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '218',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '15366',\n",
       "  '2016-03-20 09:47:10'],\n",
       " ['2016-03-31 15:46:25',\n",
       "  'BMW_316i_Bastlerfahrzeug',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  200.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '06667',\n",
       "  '2016-03-31 15:46:25'],\n",
       " ['2016-03-06 12:51:24',\n",
       "  'Ford_Ford_Fiesta_1.4_TDCI._NEUER_TÜV_!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '68',\n",
       "  'fiesta',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '85774',\n",
       "  '2016-03-06 12:51:24'],\n",
       " ['2016-03-12 00:56:17',\n",
       "  'Mercedes_Benz_S_350_S_63_Facelift_AMG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  20500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '272',\n",
       "  's_klasse',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '68167',\n",
       "  '2016-03-17 04:17:42'],\n",
       " ['2016-03-31 21:58:03',\n",
       "  'Volkswagen_Golf_Plus_1.6_TDI_DPF_Style',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12999.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'golf',\n",
       "  '70000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '86529',\n",
       "  '2016-04-06 18:44:58'],\n",
       " ['2016-03-08 16:38:03',\n",
       "  'Skoda_Octavia_Combi_2.0_TDI_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'octavia',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '57334',\n",
       "  '2016-03-08 16:38:03'],\n",
       " ['2016-03-27 23:53:18',\n",
       "  'Toyota_Corolla_Verso_2.2_D_CAT_DPF_Executive',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5300.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '177',\n",
       "  'verso',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'toyota',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '63128',\n",
       "  '2016-04-06 09:15:32'],\n",
       " ['2016-03-28 19:52:56',\n",
       "  'Verkaufe_SUZUKI___CABRIO',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1250.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '50',\n",
       "  'swift',\n",
       "  '50000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'suzuki',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '03130',\n",
       "  '2016-04-07 02:17:46'],\n",
       " ['2016-03-07 14:45:31',\n",
       "  'Toyota_RAV_4_4x4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7900.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'rav',\n",
       "  '80000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'toyota',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '32694',\n",
       "  '2016-04-07 03:45:59'],\n",
       " ['2016-04-03 13:56:51',\n",
       "  'Ford_Cougar_2.5v6_24v_tausch',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2998.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '99974',\n",
       "  '2016-04-05 12:46:54'],\n",
       " ['2016-03-07 11:06:23',\n",
       "  'Mercedes_Benz_C_230',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  '',\n",
       "  '150',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '64546',\n",
       "  '2016-04-06 08:15:42'],\n",
       " ['2016-03-19 18:39:31',\n",
       "  'Peugeot_306_mit_Gasanlage_und_TÜV_bis_12/2016.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  350.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '81',\n",
       "  '3_reihe',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'ja',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '78713',\n",
       "  '2016-03-20 20:46:42'],\n",
       " ['2016-04-01 17:47:32',\n",
       "  'Mercedes_Benz_E_200_CDI_Automatik_Classic_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4499.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '122',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '48147',\n",
       "  '2016-04-03 14:18:42'],\n",
       " ['2016-03-18 23:47:11',\n",
       "  'BMW_320i_cabrio_M_Paket',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3900.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '35108',\n",
       "  '2016-04-06 03:45:09'],\n",
       " ['2016-03-22 10:25:18',\n",
       "  'Ford_Kuga_2.0_TDCi_4x4_Individual__Navi__Xenon__AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  25950.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2014',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  'kuga',\n",
       "  '30000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '57080',\n",
       "  '2016-04-06 07:44:24'],\n",
       " ['2016-03-19 08:36:43',\n",
       "  'Volkswagen_Multivan_T4_TDI_7DC_UY2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5900.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '41199',\n",
       "  '2016-03-29 19:18:15'],\n",
       " ['2016-03-15 17:49:35',\n",
       "  'Volkswagen_Caddy_1.6TDI_DPF_BMT_AHK_Life_Team__7_Si._',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12900.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'caddy',\n",
       "  '70000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '68623',\n",
       "  '2016-03-15 17:49:35'],\n",
       " ['2016-03-28 11:50:16',\n",
       "  'Nissan_Qashqai+2_1.6_acenta',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14800.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '117',\n",
       "  'qashqai',\n",
       "  '70000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '83052',\n",
       "  '2016-04-06 14:17:22'],\n",
       " ['2016-03-24 08:51:43',\n",
       "  'Golf_3_GL_mit_AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  250.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1993',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  '',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '29331',\n",
       "  '2016-03-30 08:18:28'],\n",
       " ['2016-03-17 19:43:13',\n",
       "  'Mitsubishi_Colt_CZ3_1.5_DI_D',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2499.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'colt',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'mitsubishi',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '33102',\n",
       "  '2016-04-07 07:17:02'],\n",
       " ['2016-03-15 12:50:56',\n",
       "  'Opel_Astra_1.8_16V_Coupe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1800.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'astra',\n",
       "  '125000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '60433',\n",
       "  '2016-03-23 12:47:26'],\n",
       " ['2016-03-17 11:54:39',\n",
       "  'Ford_Fiesta_1.3_Viva',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'fiesta',\n",
       "  '100000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '67551',\n",
       "  '2016-03-20 13:19:12'],\n",
       " ['2016-03-12 16:53:39',\n",
       "  'Volkswagen_Golf_Variant_1.9_TDI_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4200.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '50374',\n",
       "  '2016-03-12 16:53:39'],\n",
       " ['2016-04-02 04:36:18',\n",
       "  'Alfa_Romeo_159_1.9_jtdm_150ps_Mit_Tuev_|_behandelbar_|_Whatsapp_!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  '159',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'alfa_romeo',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '71254',\n",
       "  '2016-04-06 06:44:38'],\n",
       " ['2016-03-31 12:54:10',\n",
       "  'Ford_Mondeo_Kombi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  850.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '130',\n",
       "  'mondeo',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '64347',\n",
       "  '2016-03-31 13:42:10'],\n",
       " ['2016-03-20 18:58:17',\n",
       "  'VW_Passat_1_9_Tdi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4100.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '131',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '87766',\n",
       "  '2016-04-07 03:44:21'],\n",
       " ['2016-03-26 14:52:36',\n",
       "  'Suzuki_Swift_1.3_Comfort',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5900.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '92',\n",
       "  'swift',\n",
       "  '90000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'suzuki',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '79843',\n",
       "  '2016-04-06 02:46:27'],\n",
       " ['2016-03-25 12:51:39',\n",
       "  'Peugeot_Expert_baugleich_Citroen_Jumpy_HDI_L2H1',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9600.0,\n",
       "  'test',\n",
       "  'andere',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'andere',\n",
       "  '125000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '52066',\n",
       "  '2016-03-31 20:15:52'],\n",
       " ['2016-03-20 00:38:01',\n",
       "  'Gut_gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2799.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '135',\n",
       "  'scenic',\n",
       "  '5000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '82515',\n",
       "  '2016-03-26 13:46:29'],\n",
       " ['2016-03-05 18:58:07',\n",
       "  'BMW_120d_Facelift_M_Sportpaket',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7100.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '177',\n",
       "  '1er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '56070',\n",
       "  '2016-03-17 16:18:18'],\n",
       " ['2016-03-12 15:37:22',\n",
       "  '320_d__LEDER_NAVI_Bluethoot2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  '',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '26135',\n",
       "  '2016-03-12 15:37:22'],\n",
       " ['2016-03-21 19:53:48',\n",
       "  'Honda_civic_1.4_90_PS_mit_TUV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'civic',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'honda',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '94439',\n",
       "  '2016-03-21 19:53:48'],\n",
       " ['2016-03-27 13:55:27',\n",
       "  'Citroën_Jumper_35_L3H2_Heavy',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6800.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '67071',\n",
       "  '2016-04-07 09:46:23'],\n",
       " ['2016-03-15 17:48:56',\n",
       "  'Kia_Picanto_1.1_LX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2300.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '65',\n",
       "  'picanto',\n",
       "  '50000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'kia',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '10997',\n",
       "  '2016-03-16 15:18:40'],\n",
       " ['2016-04-06 18:06:35',\n",
       "  'Rover_75_Tourer_2.0_V6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2500.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'rover',\n",
       "  'ja',\n",
       "  '2016-04-06 00:00:00',\n",
       "  '0',\n",
       "  '56170',\n",
       "  '2016-04-06 18:25:20'],\n",
       " ['2016-03-28 14:43:23',\n",
       "  'Fiat_Punto___Diesel___Silber___1.9_JTD___80_PS___Gebraucht',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1099.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'punto',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'fiat',\n",
       "  '',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '22117',\n",
       "  '2016-03-31 22:45:41'],\n",
       " ['2016-03-08 14:53:30',\n",
       "  'Nissan_Micra_1.2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3999.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '65',\n",
       "  'micra',\n",
       "  '40000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '04639',\n",
       "  '2016-04-06 03:17:10'],\n",
       " ['2016-03-10 14:56:56',\n",
       "  'Mercedes_Benz_Mercedes_Benz_E_280_W210_4_Matic_Servis_Schekhef',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'automatik',\n",
       "  '204',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '63452',\n",
       "  '2016-04-05 11:49:13'],\n",
       " ['2016-03-17 18:56:07',\n",
       "  'Volkswagen_T5_California',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  44900.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2015',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'transporter',\n",
       "  '10000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '78166',\n",
       "  '2016-04-07 06:16:53'],\n",
       " ['2016-03-29 15:51:51',\n",
       "  'Opel_Corsa_1.3_Cdti_ERSTE_HAND_gruene_Plakette_Guter_Zustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2399.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '41569',\n",
       "  '2016-04-04 00:18:11'],\n",
       " ['2016-03-22 18:56:48',\n",
       "  'Opel_Zafira_1.8_Njoy_mit_Style_Paket',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2600.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'zafira',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '37539',\n",
       "  '2016-03-25 11:47:17'],\n",
       " ['2016-03-27 21:54:42',\n",
       "  'Volkswagen_Sharan_1.9_TDI_Family',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1950.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'sharan',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '41462',\n",
       "  '2016-03-27 21:54:42'],\n",
       " ['2016-03-07 13:53:53',\n",
       "  'Porsche_Boxster_Top_Gepflegt_Navi__Xenon__Hardtop',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10450.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '220',\n",
       "  'boxster',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'porsche',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '06844',\n",
       "  '2016-04-06 12:44:22'],\n",
       " ['2016-03-26 18:44:52',\n",
       "  'Top_transporter_mit_wenig_km',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3100.0,\n",
       "  'test',\n",
       "  'andere',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  '',\n",
       "  '70000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'kia',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '39218',\n",
       "  '2016-04-06 07:45:41'],\n",
       " ['2016-03-14 18:55:19',\n",
       "  'Smart_smart_crossblade',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14000.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '71',\n",
       "  'andere',\n",
       "  '5000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '66271',\n",
       "  '2016-04-02 05:18:16'],\n",
       " ['2016-03-17 19:40:22',\n",
       "  'Punto_Model_188',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  190.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'punto',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'ja',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '63911',\n",
       "  '2016-03-17 19:40:22'],\n",
       " ['2016-03-25 15:45:10',\n",
       "  'BMW_530d',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  17000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '231',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '78647',\n",
       "  '2016-04-06 19:16:53'],\n",
       " ['2016-03-25 12:49:40',\n",
       "  'BMW_318d_DPF_Touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11599.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '51147',\n",
       "  '2016-04-06 14:16:56'],\n",
       " ['2016-04-04 15:47:10',\n",
       "  'BMW_120d_M_paket_Sportsitze..',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9800.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  '1er',\n",
       "  '150000',\n",
       "  '10',\n",
       "  '',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '51469',\n",
       "  '2016-04-06 17:17:35'],\n",
       " ['2016-03-21 18:58:11',\n",
       "  'Renault_Clio_1.2_16V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1690.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'clio',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '70825',\n",
       "  '2016-04-06 21:47:14'],\n",
       " ['2016-03-21 05:36:19',\n",
       "  'Mercedes_Benz_E_350_CDI_DPF_Cabrio_BlueEFFICIENCY_7G_TRONIC',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  30600.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '231',\n",
       "  'e_klasse',\n",
       "  '80000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '71229',\n",
       "  '2016-03-21 09:42:42'],\n",
       " ['2016-03-27 11:55:07',\n",
       "  'Ford_Focus_Turnier_/_Kombi___131PS__2_0l',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '13593',\n",
       "  '2016-04-01 11:18:06'],\n",
       " ['2016-03-31 00:32:39',\n",
       "  'Renault_Megane_1.6_\"Bastler_Fahrzeug_oder_zum_ausschlachten\"',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  140.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'megane',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'ja',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '60435',\n",
       "  '2016-04-05 17:26:10'],\n",
       " ['2016-03-21 00:56:46',\n",
       "  'Mercedes_Benz_A_150_Avantgarde_W169_Automatik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5990.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '95',\n",
       "  'a_klasse',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '33014',\n",
       "  '2016-04-06 00:17:07'],\n",
       " ['2016-03-17 14:48:30',\n",
       "  'VW_Golf3_Variant_Kombi_1.9l',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1300.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '12',\n",
       "  '',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '01809',\n",
       "  '2016-03-19 14:20:31'],\n",
       " ['2016-03-20 16:41:53',\n",
       "  'Renault_Scenic_1.6_16V_RXE',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '107',\n",
       "  'scenic',\n",
       "  '125000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '79739',\n",
       "  '2016-03-30 07:46:08'],\n",
       " ['2016-03-21 14:54:00',\n",
       "  'Mercedes_Benz_C_220_CDI_Avantg._Panorama_PDC_SHZ_NAVI_VOLLEDER',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11900.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'c_klasse',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '45888',\n",
       "  '2016-04-06 12:15:36'],\n",
       " ['2016-04-05 13:25:18',\n",
       "  'BMW_316i',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1400.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '56073',\n",
       "  '2016-04-05 13:25:18'],\n",
       " ['2016-04-04 00:44:34',\n",
       "  'BMW_Z3_Roadster_Alpina...._Einer_der_ersten',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4700.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '117',\n",
       "  'z_reihe',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '31185',\n",
       "  '2016-04-06 03:44:28'],\n",
       " ['2016-04-03 13:41:24',\n",
       "  'Verkaufe_VW_caddy_1.9_sdi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1400.0,\n",
       "  'control',\n",
       "  'andere',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'caddy',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '18246',\n",
       "  '2016-04-03 13:41:24'],\n",
       " ['2016-03-27 20:55:55',\n",
       "  'Renault_Scenic_TOP_Familienauto',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1100.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '107',\n",
       "  'scenic',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '66740',\n",
       "  '2016-03-29 11:18:08'],\n",
       " ['2016-03-26 14:44:15',\n",
       "  'Opel_Vectra_2.8_V6_Turbo_Automatik_Cosmo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6990.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'automatik',\n",
       "  '230',\n",
       "  'vectra',\n",
       "  '100000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '46047',\n",
       "  '2016-04-06 02:17:16'],\n",
       " ['2016-03-07 18:57:15',\n",
       "  'Daewoo_Kalos',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  '',\n",
       "  '0',\n",
       "  'kalos',\n",
       "  '20000',\n",
       "  '8',\n",
       "  '',\n",
       "  'daewoo',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '08371',\n",
       "  '2016-03-07 18:57:15'],\n",
       " ['2016-04-06 19:25:24',\n",
       "  'Opel_Corsa_1.2_16V_Comfort',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1400.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-06 00:00:00',\n",
       "  '0',\n",
       "  '65428',\n",
       "  '2016-04-06 19:25:24'],\n",
       " ['2016-03-19 09:51:01',\n",
       "  'Skoda_Superb_Family__Kombi_2_0_l_Diesel_dunkelbraun',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16700.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2013',\n",
       "  '',\n",
       "  '140',\n",
       "  'superb',\n",
       "  '60000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '42549',\n",
       "  '2016-03-24 02:44:24'],\n",
       " ['2016-03-10 13:51:28',\n",
       "  'Fiat_Stilo_Multi_Wagon_1.6_16V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3300.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '103',\n",
       "  'stilo',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '44801',\n",
       "  '2016-03-21 00:46:31'],\n",
       " ['2016-03-23 07:54:25',\n",
       "  'Citroen_Berlingo_TÜV_neu_Anhaengerkupplung_Bremsen_neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1400.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'berlingo',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '92345',\n",
       "  '2016-03-27 12:18:10'],\n",
       " ['2016-04-03 14:39:43',\n",
       "  'Suzuki_Swift_1.0__D3_Norm_eFh_bei_Berlin',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  98.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '53',\n",
       "  'swift',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'suzuki',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '14542',\n",
       "  '2016-04-05 13:18:15'],\n",
       " ['2016-04-04 19:40:28',\n",
       "  'Hyundai__i30',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '99',\n",
       "  'i_reihe',\n",
       "  '30000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'hyundai',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '49824',\n",
       "  '2016-04-06 21:45:32'],\n",
       " ['2016-03-30 16:38:07',\n",
       "  'Toyota_Avensis_1.8_VVT_i_Combi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3800.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '129',\n",
       "  'avensis',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'toyota',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '26169',\n",
       "  '2016-04-07 07:15:47'],\n",
       " ['2016-03-18 17:42:48',\n",
       "  'BMW_316_ti_Compact__Klima',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  790.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-18 00:00:00',\n",
       "  '0',\n",
       "  '86368',\n",
       "  '2016-03-22 11:15:15'],\n",
       " ['2016-03-21 19:06:35',\n",
       "  'Mercedes_Benz_Vito_108_CDI_+_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1450.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '82',\n",
       "  'vito',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '55120',\n",
       "  '2016-03-23 11:32:18'],\n",
       " ['2016-03-27 22:54:11',\n",
       "  'Opel_zafira_1.9_CDTI_Navi__Panoramadach__Sitzheizung__Leder',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'zafira',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '27749',\n",
       "  '2016-04-06 06:17:07'],\n",
       " ['2016-03-26 19:47:19',\n",
       "  'Tausche_oder_verkaufe',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '26215',\n",
       "  '2016-03-26 19:47:19'],\n",
       " ['2016-03-24 12:39:51',\n",
       "  'SEAT_ALHAMBRA_1_9TDI_unfallfrei_AHK__1.ter_Hand__12_Jahre',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3650.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'alhambra',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '87779',\n",
       "  '2016-04-01 06:18:14'],\n",
       " ['2016-03-22 09:56:13',\n",
       "  'Fiat_Barchetta_1.8_16V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1999.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  'andere',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '83339',\n",
       "  '2016-03-23 17:48:06'],\n",
       " ['2016-03-25 12:42:09',\n",
       "  'Porsche_997_GT3_MKII_EIN_SAMMLERSTÜCK_IN_VOLLAUSSTATTUNG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  139997.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '435',\n",
       "  '911',\n",
       "  '20000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'porsche',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '71159',\n",
       "  '2016-04-06 13:45:07'],\n",
       " ['2016-04-04 19:36:22',\n",
       "  'Smart_Fortwo_cabrio',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3999.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '84',\n",
       "  'fortwo',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '50767',\n",
       "  '2016-04-04 19:36:22'],\n",
       " ['2016-03-26 13:50:33',\n",
       "  'Honda_Civic_Coupe_1.7_LS',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'civic',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'honda',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '08058',\n",
       "  '2016-03-26 13:50:33'],\n",
       " ['2016-03-25 16:50:13',\n",
       "  'Audi_A3',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5600.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'ja',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '51688',\n",
       "  '2016-03-25 16:50:13'],\n",
       " ['2016-04-02 00:58:10',\n",
       "  '**VW_T4_Multivan/Behindertengerecht**',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6500.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '68',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '3',\n",
       "  '',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '84072',\n",
       "  '2016-04-07 08:44:24'],\n",
       " ['2016-03-17 22:25:28',\n",
       "  'Citroën_C4_Grand_Picasso_2.0_HDi_FAP_EGS6_Tendance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '136',\n",
       "  'c4',\n",
       "  '125000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '35463',\n",
       "  '2016-04-07 02:15:35'],\n",
       " ['2016-03-06 00:57:18',\n",
       "  'Renault_Clio',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1799.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '82',\n",
       "  'clio',\n",
       "  '150000',\n",
       "  '5',\n",
       "  '',\n",
       "  'renault',\n",
       "  'ja',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '27729',\n",
       "  '2016-03-23 16:20:57'],\n",
       " ['2016-03-29 10:38:04',\n",
       "  'BMW_135i_Coupe_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  26500.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '306',\n",
       "  '1er',\n",
       "  '40000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '50933',\n",
       "  '2016-04-05 18:47:08'],\n",
       " ['2016-03-23 14:52:40',\n",
       "  'Renault_Twingo_1.3',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  710.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '54',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '17192',\n",
       "  '2016-03-26 08:15:30'],\n",
       " ['2016-03-24 15:50:15',\n",
       "  'Audi_A4_2.0_T_FSI_S_Line',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7890.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '200',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '56291',\n",
       "  '2016-03-29 17:17:35'],\n",
       " ['2016-03-30 22:52:22',\n",
       "  'VW_Passat_35i_Ersatzteillager',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  100.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '52134',\n",
       "  '2016-04-02 00:17:02'],\n",
       " ['2016-03-05 22:40:43',\n",
       "  'Mercedes_E__Klasse_T_Modell____w210_Kombi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '41352',\n",
       "  '2016-03-12 02:45:47'],\n",
       " ['2016-04-03 13:53:56',\n",
       "  'Mercedes_Benz_SLK_200_Kompressor',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14500.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  'slk',\n",
       "  '60000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '79650',\n",
       "  '2016-04-05 12:46:41'],\n",
       " ['2016-04-01 21:44:58',\n",
       "  'Audi_A6_Avant_2.8_FSI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13990.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '209',\n",
       "  'a6',\n",
       "  '100000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '02763',\n",
       "  '2016-04-05 19:16:57'],\n",
       " ['2016-03-22 15:55:07',\n",
       "  'Jeep_Grand_Cherokee_3.0I_Multijet_Overland',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  46950.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2013',\n",
       "  'automatik',\n",
       "  '250',\n",
       "  'grand',\n",
       "  '30000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'jeep',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '51427',\n",
       "  '2016-04-04 05:51:39'],\n",
       " ['2016-03-23 10:57:22',\n",
       "  'Mercedes_Benz_A_200_Avantgarde__Original_16570km_!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9490.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '136',\n",
       "  'a_klasse',\n",
       "  '20000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '09127',\n",
       "  '2016-04-07 12:16:20'],\n",
       " ['2016-03-08 22:06:21',\n",
       "  'BMW_320d_DPF_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6700.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '163',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '60439',\n",
       "  '2016-03-12 10:15:41'],\n",
       " ['2016-03-13 23:38:58',\n",
       "  'Volkswagen_3bg',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2990.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '45881',\n",
       "  '2016-04-01 00:47:15'],\n",
       " ['2016-03-09 16:47:45',\n",
       "  'Mercedes_Benz_C_240',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'c_klasse',\n",
       "  '125000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '67433',\n",
       "  '2016-04-07 05:46:07'],\n",
       " ['2016-03-30 00:56:03',\n",
       "  'BMW_320d_DPF_Aut.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '177',\n",
       "  '3er',\n",
       "  '125000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '22115',\n",
       "  '2016-04-06 22:45:49'],\n",
       " ['2016-03-28 17:42:46',\n",
       "  'MINI_Mini_One',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  16999.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2015',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'one',\n",
       "  '10000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'mini',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '82216',\n",
       "  '2016-04-06 23:15:55'],\n",
       " ['2016-03-10 13:56:39',\n",
       "  'Touareg_3.0_V6_TDI_DPF_Aut._Bi_Xen_1_Hand_Kamera',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10100.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '239',\n",
       "  'touareg',\n",
       "  '100000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '10115',\n",
       "  '2016-03-10 14:44:07'],\n",
       " ['2016-03-08 12:54:30',\n",
       "  'Caddy_1.9_TDI_DPF_Life',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3100.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  '',\n",
       "  '0',\n",
       "  'caddy',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '50321',\n",
       "  '2016-03-08 12:54:30'],\n",
       " ['2016-04-03 21:51:37',\n",
       "  'Audi_A4_kombi_Bj_12/10',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13200.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '143',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '33607',\n",
       "  '2016-04-05 23:15:28'],\n",
       " ['2016-03-05 18:50:21',\n",
       "  'Opel_Rekord_C_1900_S_Automatic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2900.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1971',\n",
       "  'automatik',\n",
       "  '90',\n",
       "  'andere',\n",
       "  '90000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '16356',\n",
       "  '2016-04-05 11:48:37'],\n",
       " ['2016-04-04 22:53:23',\n",
       "  'Toyota_Avensis_1.8_Kombi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1250.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '129',\n",
       "  'avensis',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'toyota',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '15566',\n",
       "  '2016-04-04 23:39:56'],\n",
       " ['2016-03-25 10:47:07',\n",
       "  'Ford_Ka_Bastlerauto',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'ka',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '31618',\n",
       "  '2016-04-06 10:17:08'],\n",
       " ['2016-03-28 19:47:05',\n",
       "  'Audi_A3',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1990.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '49084',\n",
       "  '2016-04-01 05:16:23'],\n",
       " ['2016-03-15 16:37:22',\n",
       "  'Brilliance_BS4_1.6_Comfort',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  '',\n",
       "  '70000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'sonstige_autos',\n",
       "  'ja',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '76829',\n",
       "  '2016-03-21 14:15:21'],\n",
       " ['2016-03-19 07:55:38',\n",
       "  'Mercedes_Benz_B_200_Turbo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  13500.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '193',\n",
       "  'b_klasse',\n",
       "  '60000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '41748',\n",
       "  '2016-04-06 04:17:29'],\n",
       " ['2016-03-27 23:37:22',\n",
       "  'Skoda_Octavia_Combi_1.9_TDI_DPF_DSG_Ambiente',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8999.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '105',\n",
       "  'octavia',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '77876',\n",
       "  '2016-04-06 07:45:08'],\n",
       " ['2016-03-17 00:36:59',\n",
       "  'BMW_320d_DPF',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14000.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '131',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '93077',\n",
       "  '2016-04-06 01:48:35'],\n",
       " ['2016-03-17 13:53:32',\n",
       "  'Audi_A4_1.9_TDI_TÜV_03/2018',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1385.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '50354',\n",
       "  '2016-03-28 15:45:13'],\n",
       " ['2016-03-19 21:36:21',\n",
       "  'Renault_Twingo_1.2_blau',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  999.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '43',\n",
       "  'twingo',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  '',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '33719',\n",
       "  '2016-03-30 21:18:32'],\n",
       " ['2016-03-30 16:55:38',\n",
       "  'Mercedes_Kombi_C200',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14430.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '146',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '45478',\n",
       "  '2016-04-03 07:15:25'],\n",
       " ['2016-03-29 15:56:18',\n",
       "  'VW_Polo_6N_blau',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  650.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '47918',\n",
       "  '2016-04-06 01:44:56'],\n",
       " ['2016-03-20 16:37:38',\n",
       "  'Volkswagen_Golf_1.4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '81549',\n",
       "  '2016-04-06 21:46:06'],\n",
       " ['2016-04-01 01:36:57',\n",
       "  'E39_520i_an_Bastler_weiterzugeben!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'ja',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '66687',\n",
       "  '2016-04-07 05:17:06'],\n",
       " ['2016-03-23 14:06:25',\n",
       "  'Skoda_Octavia_III_Combi_1.6_TDI_Green_tec_Ambition_AHK',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  14900.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'octavia',\n",
       "  '40000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '61462',\n",
       "  '2016-04-06 13:44:55'],\n",
       " ['2016-04-02 23:39:49',\n",
       "  'Barkas_B1000__1__Viertakt_motor',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3900.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1990',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  '',\n",
       "  '30000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'sonstige_autos',\n",
       "  '',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '15345',\n",
       "  '2016-04-07 01:46:23'],\n",
       " ['2016-03-13 20:25:18',\n",
       "  'Jeep_Grand_Cherokee_5.2_Limited_TÜV_NEU_03/2018',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3200.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '1995',\n",
       "  'automatik',\n",
       "  '211',\n",
       "  'grand',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'jeep',\n",
       "  'nein',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '64546',\n",
       "  '2016-03-14 06:49:04'],\n",
       " ['2016-03-15 21:48:31',\n",
       "  'Peugeot_Partner',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  '',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  '',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '66424',\n",
       "  '2016-03-17 18:47:59'],\n",
       " ['2016-03-24 19:46:43',\n",
       "  'Volkswagen_VW_Golf_I_1_Cabrio__Cabriolet____TÜV_bei_Verkauf',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5699.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1987',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '09120',\n",
       "  '2016-04-05 11:49:42'],\n",
       " ['2016-04-02 15:47:02',\n",
       "  'BMW_520i_touring___2._Hand__Neuwagenzustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4950.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '88521',\n",
       "  '2016-04-06 14:44:32'],\n",
       " ['2016-03-11 11:53:53',\n",
       "  'Audi_A4_1.8',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  700.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '56472',\n",
       "  '2016-03-22 09:44:49'],\n",
       " ['2016-03-15 20:47:47',\n",
       "  'Volkswagen_Golf_1.8T_Klimaauto_Glas_SD_SHZ_Temp_PDC_TÜV_NEU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4590.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '73660',\n",
       "  '2016-04-07 03:16:25'],\n",
       " ['2016-03-12 18:49:56',\n",
       "  'Mercedes_Benz_SLK_230_Kompressor__Klima__Sitzheizung__AMG_Felg',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6900.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '193',\n",
       "  'slk',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '33758',\n",
       "  '2016-04-07 12:46:38'],\n",
       " ['2016-04-03 18:36:22',\n",
       "  'Mercedes_Benz_A_170_Autotronic_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '116',\n",
       "  'a_klasse',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '93309',\n",
       "  '2016-04-05 18:46:10'],\n",
       " ['2016-03-20 23:06:24',\n",
       "  'Seat_Ibiza_SC_1.4_TSI_DSG_Cupra_TOP_gepflegt____NAVI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  10900.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '179',\n",
       "  'ibiza',\n",
       "  '80000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '60322',\n",
       "  '2016-04-06 06:17:49'],\n",
       " ['2016-03-26 23:48:03',\n",
       "  'Golf_3_Bon_Jovi_guter_Zustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  600.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '93486',\n",
       "  '2016-04-06 18:48:30'],\n",
       " ['2016-03-29 15:53:15',\n",
       "  'Golf_3_zum_ausschlachten_oder_herrichten',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  300.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1991',\n",
       "  '',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '93455',\n",
       "  '2016-04-06 01:44:50'],\n",
       " ['2016-03-21 14:50:20',\n",
       "  'Biete_Ford_Fiesta_1_4_TDCI__JH_1__mit_frischen_Tuev_zum_Verkauf_an',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1250.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '67',\n",
       "  'fiesta',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '53562',\n",
       "  '2016-03-21 15:06:48'],\n",
       " ['2016-03-17 09:49:16',\n",
       "  'Polo_6KV_Silbermetallik',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '48157',\n",
       "  '2016-04-06 06:16:26'],\n",
       " ['2016-03-24 20:47:26',\n",
       "  'Peugeot_206_CC___gepflegt___jetzt_in_den_CABRIO_SOMMER_starten',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5800.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '106',\n",
       "  '2_reihe',\n",
       "  '70000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '49124',\n",
       "  '2016-04-05 13:46:01'],\n",
       " ['2016-03-11 17:58:10',\n",
       "  'Verkaufe_Ford_escord',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1990',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  '',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'ja',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '29525',\n",
       "  '2016-03-16 05:46:20'],\n",
       " ['2016-03-28 10:57:54',\n",
       "  'Mercedes_CLK_2_6_V6_Tausche_gerne_gegen_SUV_TÜV_fast_Neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6000.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  'clk',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '45326',\n",
       "  '2016-04-06 12:45:25'],\n",
       " ['2016-03-26 19:48:43',\n",
       "  'Opel_Corsa_B_1.2_Webasto_Standheizung_TÜV_11/16',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '45',\n",
       "  'corsa',\n",
       "  '70000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '08496',\n",
       "  '2016-04-06 09:46:52'],\n",
       " ['2016-03-11 00:51:23',\n",
       "  'Skoda_Fabia_69_PS_13700_km',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5990.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'fabia',\n",
       "  '20000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '84335',\n",
       "  '2016-03-29 15:46:27'],\n",
       " ['2016-03-24 21:49:31',\n",
       "  'BMW_316ti_Compakt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-24 00:00:00',\n",
       "  '0',\n",
       "  '84032',\n",
       "  '2016-03-31 01:15:34'],\n",
       " ['2016-03-05 21:42:31',\n",
       "  'Saab_9_3_1.8_t_Sport_Kombi_Linear',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4250.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'saab',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '88690',\n",
       "  '2016-03-06 04:17:21'],\n",
       " ['2016-03-15 18:57:12',\n",
       "  'MX5_Mazda_NB_Schlachtfest__Reifen_Bremsen_Miata_Teile',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'mx_reihe',\n",
       "  '150000',\n",
       "  '11',\n",
       "  '',\n",
       "  'mazda',\n",
       "  'ja',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '89312',\n",
       "  '2016-04-06 21:16:58'],\n",
       " ['2016-04-01 23:57:34',\n",
       "  'Opel_Corsa_1.0_12V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '27798',\n",
       "  '2016-04-06 05:16:04'],\n",
       " ['2016-04-02 15:39:20',\n",
       "  'BMW_318i_Cabrio__Sondermodell_Design_Edition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4750.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '1991',\n",
       "  'manuell',\n",
       "  '113',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '72766',\n",
       "  '2016-04-02 15:39:20'],\n",
       " ['2016-03-06 21:38:54',\n",
       "  'BMW_M135i_Schnaeppchen_Top_Ausstattung_TÜV_NEU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  28250.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '320',\n",
       "  'm_reihe',\n",
       "  '40000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '78549',\n",
       "  '2016-04-07 02:17:36'],\n",
       " ['2016-03-09 12:57:10',\n",
       "  'Mercedes_Benz_C_200_T_Kompressor_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7400.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'automatik',\n",
       "  '163',\n",
       "  'c_klasse',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-09 00:00:00',\n",
       "  '0',\n",
       "  '63533',\n",
       "  '2016-04-06 01:47:22'],\n",
       " ['2016-03-20 01:57:09',\n",
       "  'Auto_verkauf',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'corsa',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '32257',\n",
       "  '2016-04-06 03:16:43'],\n",
       " ['2016-03-12 11:53:24',\n",
       "  'Fiat_stilo_abahrt__2.4_Liter_v5_tausch',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'stilo',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '80634',\n",
       "  '2016-03-19 08:44:52'],\n",
       " ['2016-03-13 14:07:17',\n",
       "  'Vw_lupo_1.0_motor_fuer_Bastler',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  390.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '50',\n",
       "  'lupo',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '61169',\n",
       "  '2016-03-13 17:21:25'],\n",
       " ['2016-04-04 00:58:53',\n",
       "  'Renault_Espace',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'espace',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '13589',\n",
       "  '2016-04-06 08:44:34'],\n",
       " ['2016-03-26 15:37:01',\n",
       "  'Ford_Transit_280_K_TDCi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3300.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'transit',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '28197',\n",
       "  '2016-04-06 03:17:34'],\n",
       " ['2016-04-02 15:37:16',\n",
       "  'Volkswagen_Polo_1.4_Goal_KLIMA/_PDC/_8xR/_TÜV_neu/_Gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4250.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'polo',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '91187',\n",
       "  '2016-04-02 15:37:16'],\n",
       " ['2016-03-21 13:58:18',\n",
       "  'Smart__roadster_softtouch_Turbo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4000.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '82',\n",
       "  'roadster',\n",
       "  '125000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '32425',\n",
       "  '2016-03-21 14:40:33'],\n",
       " ['2016-03-16 14:55:58',\n",
       "  'Alfa_Romeo_GT_Blackline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4200.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '166',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '5',\n",
       "  '',\n",
       "  'alfa_romeo',\n",
       "  '',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '44532',\n",
       "  '2016-03-28 12:18:13'],\n",
       " ['2016-03-11 19:38:44',\n",
       "  'Ford_Fiesta_1.0_EcoBoost_Start_Stop_SYNC_Edition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12800.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2014',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'fiesta',\n",
       "  '40000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '50129',\n",
       "  '2016-04-06 00:15:51'],\n",
       " ['2016-03-10 10:54:52',\n",
       "  'Skoda_Octavia_Combi_2.0_TDI_Green_tec_4x4_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  18500.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2013',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  'octavia',\n",
       "  '50000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '86899',\n",
       "  '2016-03-17 16:15:34'],\n",
       " ['2016-03-29 10:37:04',\n",
       "  'Audi_A4_1.6_Top_Zustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1600.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'a4',\n",
       "  '150000',\n",
       "  '2',\n",
       "  '',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '81827',\n",
       "  '2016-04-05 20:18:00'],\n",
       " ['2016-03-31 23:37:41',\n",
       "  'Verkaufe_Lanica_Ypsilon_ohne_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  300.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '85',\n",
       "  'ypsilon',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'lancia',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '88682',\n",
       "  '2016-04-07 01:44:35'],\n",
       " ['2016-03-17 11:36:56',\n",
       "  'Mercedes_A170_CDI_L_Elegance_Aut_El.fenst_Klima_DPF_Gruene_Plakett',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3280.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '95',\n",
       "  'a_klasse',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '90552',\n",
       "  '2016-03-29 05:45:14'],\n",
       " ['2016-04-07 13:25:40',\n",
       "  'Volkswagen_Golf_Variant_1.4_TSI_DSG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12950.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2011',\n",
       "  'automatik',\n",
       "  '160',\n",
       "  'golf',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-07 00:00:00',\n",
       "  '0',\n",
       "  '61250',\n",
       "  '2016-04-07 13:25:40'],\n",
       " ['2016-03-22 17:53:51',\n",
       "  'Volkswagen_Polo_1.2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1450.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '64',\n",
       "  'polo',\n",
       "  '80000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '82256',\n",
       "  '2016-03-24 02:15:41'],\n",
       " ['2016-04-04 19:36:20',\n",
       "  'Peugeot_206_75_JBL__Euro_4__TÜV_03/18__1._Hand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  '2_reihe',\n",
       "  '125000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '97653',\n",
       "  '2016-04-06 21:45:16'],\n",
       " ['2016-04-04 21:47:13',\n",
       "  'Renault_Scenic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1200.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '107',\n",
       "  'scenic',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '89081',\n",
       "  '2016-04-07 00:18:00'],\n",
       " ['2016-03-19 20:59:02',\n",
       "  'Mercedes_Benz_200',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4800.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '163',\n",
       "  'andere',\n",
       "  '100000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '78462',\n",
       "  '2016-04-07 09:17:07'],\n",
       " ['2016-03-20 18:56:40',\n",
       "  'Ford_C_MAX_Grand_Titanium___Schiebetueren_7_Sitzer_Topausstattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11890.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'c_max',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  '',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '97737',\n",
       "  '2016-04-07 02:45:30'],\n",
       " ['2016-03-14 20:26:01',\n",
       "  'Opel_Vectra_c_voll_fahrbereit',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '122',\n",
       "  'vectra',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '65929',\n",
       "  '2016-04-04 18:17:25'],\n",
       " ['2016-03-19 17:25:19',\n",
       "  'Audi_TT_Coupe_1.8_TFSI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  21800.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '160',\n",
       "  'tt',\n",
       "  '50000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '65428',\n",
       "  '2016-04-06 01:49:07'],\n",
       " ['2016-03-10 13:38:28',\n",
       "  'Ford_Focus_Turnier_DI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1450.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '19339',\n",
       "  '2016-04-07 08:15:25'],\n",
       " ['2016-04-03 23:54:52',\n",
       "  'Nissan_Vanette_LKW_ZULASSUNG_FÜR_EXPORT_KEIN_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  750.0,\n",
       "  'control',\n",
       "  'andere',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'nissan',\n",
       "  'ja',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '29633',\n",
       "  '2016-04-06 08:16:25'],\n",
       " ['2016-03-17 20:51:19',\n",
       "  'Passat_3b_sehr_gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2200.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '125',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '07743',\n",
       "  '2016-03-24 17:17:36'],\n",
       " ['2016-03-15 15:54:12',\n",
       "  'BMW_330_Ci',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11000.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '231',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '10243',\n",
       "  '2016-03-15 15:54:12'],\n",
       " ['2016-03-07 16:55:06',\n",
       "  'Volkswagen_Golf_VI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2800.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'ja',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '29553',\n",
       "  '2016-03-12 09:45:23'],\n",
       " ['2016-04-03 21:41:36',\n",
       "  'Chevrolet_Trailblazer__4_2LiterAutom_Benzin/Lpg_20_Zoll_Felgen',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4200.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '276',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'lpg',\n",
       "  'chevrolet',\n",
       "  'ja',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '71642',\n",
       "  '2016-04-05 22:18:38'],\n",
       " ['2016-03-30 12:56:02',\n",
       "  'Mitsubishi_Galant',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  550.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '136',\n",
       "  'galant',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'mitsubishi',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '02627',\n",
       "  '2016-04-07 03:45:00'],\n",
       " ['2016-03-22 17:54:09',\n",
       "  'Verkaufe_Opel_Corsa_D',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5000.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2007',\n",
       "  '',\n",
       "  '90',\n",
       "  'corsa',\n",
       "  '80000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '71131',\n",
       "  '2016-04-06 09:17:26'],\n",
       " ['2016-03-10 02:02:29',\n",
       "  'Mercedes_SEL500_Oldtimer',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3200.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1982',\n",
       "  'automatik',\n",
       "  '0',\n",
       "  's_klasse',\n",
       "  '150000',\n",
       "  '7',\n",
       "  '',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '55568',\n",
       "  '2016-03-27 22:15:32'],\n",
       " ['2016-03-08 19:41:01',\n",
       "  'Daewoo_Matiz_TÜV_Neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1250.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '52',\n",
       "  'matiz',\n",
       "  '125000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'daewoo',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '23858',\n",
       "  '2016-03-09 11:15:43'],\n",
       " ['2016-03-11 19:25:22',\n",
       "  'Verkaufe_treuen_Subaru_Justy_GX_als_BASTLERFAHRZEUG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  400.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '86',\n",
       "  'justy',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'subaru',\n",
       "  '',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '56179',\n",
       "  '2016-04-06 15:17:34'],\n",
       " ['2016-03-12 18:39:15',\n",
       "  'BMW_530d_Touring_Sport_Aut._Edition_Sport',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15900.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '235',\n",
       "  '5er',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '26954',\n",
       "  '2016-03-12 18:39:15'],\n",
       " ['2016-04-04 11:43:42',\n",
       "  'Hyundai_i30_TÜV._04.2018_/_11_2009_bj',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4499.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  'i_reihe',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'hyundai',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '45139',\n",
       "  '2016-04-06 12:17:53'],\n",
       " ['2016-03-08 12:47:47',\n",
       "  'Audi_S4_Cabriolet_tiptronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  17000.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '344',\n",
       "  'andere',\n",
       "  '125000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '22880',\n",
       "  '2016-03-08 19:21:27'],\n",
       " ['2016-03-11 13:48:41',\n",
       "  'Volkswagen_Beetle',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7350.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'beetle',\n",
       "  '70000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '48159',\n",
       "  '2016-03-15 23:44:41'],\n",
       " ['2016-03-15 12:39:13',\n",
       "  'BMW_BMW_520d_Touring_2.Hand_Navi_Xenon_PDC_8_FachAlu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15900.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '177',\n",
       "  '5er',\n",
       "  '100000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '87488',\n",
       "  '2016-04-06 00:16:25'],\n",
       " ['2016-03-07 13:57:06',\n",
       "  'Subaru_Impreza_WRX_STI_Sport',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  23000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '349',\n",
       "  'impreza',\n",
       "  '40000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'subaru',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '79677',\n",
       "  '2016-04-07 03:15:56'],\n",
       " ['2016-03-26 12:58:01',\n",
       "  'Prius_2_Executive_Voll_Leder_JBL_Sound_Navi_schwarz_TOP',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5750.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '100',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'hybrid',\n",
       "  'toyota',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '32427',\n",
       "  '2016-03-29 15:16:05'],\n",
       " ['2016-03-16 15:58:36',\n",
       "  'Nissan_Micra_1.0_K11_Style__San_Remo_',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  777.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'micra',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'nissan',\n",
       "  '',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '98630',\n",
       "  '2016-03-16 15:58:36'],\n",
       " ['2016-03-29 18:46:42',\n",
       "  'Opel_Corsa_1.0_12V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '58',\n",
       "  'corsa',\n",
       "  '125000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '64521',\n",
       "  '2016-04-06 04:44:51'],\n",
       " ['2016-03-06 14:49:30',\n",
       "  'Citroen_Grand_C4_Picasso',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9300.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'c4',\n",
       "  '80000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '64646',\n",
       "  '2016-04-06 11:45:55'],\n",
       " ['2016-03-25 18:48:23',\n",
       "  'Volkswagen_T5_Multivan_DSG_4MOTION_Highline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  29700.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2010',\n",
       "  'automatik',\n",
       "  '179',\n",
       "  'transporter',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '26180',\n",
       "  '2016-04-07 00:45:30'],\n",
       " ['2016-03-29 07:58:24',\n",
       "  'Verkaufe_meinen_Audi_90_Coupe_2.3_20V_quattro',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4300.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '1990',\n",
       "  'manuell',\n",
       "  '170',\n",
       "  '90',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '13595',\n",
       "  '2016-04-05 18:17:00'],\n",
       " ['2016-03-19 12:57:10',\n",
       "  'Skoda_Fabia_Kombi_Klima_1.Hand_Scheckheft_gepflegt__Skoda_',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3700.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '69',\n",
       "  'fabia',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '12526',\n",
       "  '2016-03-19 13:40:13'],\n",
       " ['2016-03-23 00:58:28',\n",
       "  'Peugeot_206cc_Cabrio_TÜV_bis_04/17_Super_Sommer_Auto_gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2699.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  '2_reihe',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '45881',\n",
       "  '2016-04-07 06:46:07'],\n",
       " ['2016-03-20 21:50:08',\n",
       "  'Volkswagen_Polo_1.4_Sportline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3200.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '12623',\n",
       "  '2016-03-26 00:47:12'],\n",
       " ['2016-03-07 11:49:12',\n",
       "  'BMW_535_Gran_Turismo',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  31500.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2011',\n",
       "  'automatik',\n",
       "  '299',\n",
       "  '5er',\n",
       "  '90000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '42489',\n",
       "  '2016-03-19 21:47:40'],\n",
       " ['2016-03-23 11:47:55',\n",
       "  'Skoda_Oktavia_1_9_TDI_Kombi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  599.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '110',\n",
       "  'octavia',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'skoda',\n",
       "  'ja',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '66839',\n",
       "  '2016-03-30 20:45:50'],\n",
       " ['2016-04-02 13:51:57',\n",
       "  'Volkswagen_Polo_1.4_United',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2008',\n",
       "  'manuell',\n",
       "  '80',\n",
       "  'polo',\n",
       "  '125000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '48165',\n",
       "  '2016-04-06 12:16:30'],\n",
       " ['2016-03-28 09:53:44',\n",
       "  'Mercedes_Benz_W123_240_TD_Stahlschiebedach_H_kennzeichen_01.2018',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6990.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1983',\n",
       "  'manuell',\n",
       "  '72',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-28 00:00:00',\n",
       "  '0',\n",
       "  '31737',\n",
       "  '2016-04-06 11:16:20'],\n",
       " ['2016-03-30 22:51:09',\n",
       "  'Peugeot_206_CC_110_Roland_Garros',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4900.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  '2_reihe',\n",
       "  '50000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '35641',\n",
       "  '2016-04-05 14:47:27'],\n",
       " ['2016-03-20 19:58:46',\n",
       "  'Daihatsu_Sirion_Sport',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  400.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '102',\n",
       "  'sirion',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'daihatsu',\n",
       "  'ja',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '16348',\n",
       "  '2016-03-20 19:58:46'],\n",
       " ['2016-04-04 13:25:44',\n",
       "  'opel_combo_in_bestzustand',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4500.0,\n",
       "  'test',\n",
       "  'andere',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '90',\n",
       "  'combo',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'ja',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '63263',\n",
       "  '2016-04-05 12:16:01'],\n",
       " ['2016-03-23 19:56:14',\n",
       "  'Opel_Astra_1.6_Caravan_Edition_100_Classic_TÜV_Neu',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1799.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'astra',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '21218',\n",
       "  '2016-04-06 01:15:51'],\n",
       " ['2016-04-02 11:45:42',\n",
       "  'Mitsubishi_Colt_2_Jahre_tuev',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'colt',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'mitsubishi',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '27753',\n",
       "  '2016-04-02 11:45:42'],\n",
       " ['2016-03-06 22:43:51',\n",
       "  'Dodge_RAM_B300_Mowag_Mowag_Dodge_V8_selten_Neuaufbau_US_Car_Van',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11500.0,\n",
       "  'control',\n",
       "  'bus',\n",
       "  '1985',\n",
       "  'automatik',\n",
       "  '160',\n",
       "  '',\n",
       "  '125000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'sonstige_autos',\n",
       "  '',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '04849',\n",
       "  '2016-04-05 18:17:04'],\n",
       " ['2016-03-15 19:55:46',\n",
       "  'Mercedes_Benz_A_170',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3999.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '95',\n",
       "  'a_klasse',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '79106',\n",
       "  '2016-04-07 00:44:22'],\n",
       " ['2016-04-02 01:55:41',\n",
       "  'Audi_A3_2.0_FSI_Sportback',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9100.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'a3',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '03058',\n",
       "  '2016-04-06 04:16:23'],\n",
       " ['2016-04-04 10:49:31',\n",
       "  'BMW_E32_730i_V8_OHNE_ROST_KEIN_TÜV_VOLL_FAHRBEREIT_IN_GRIECHENLAN',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2000.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1994',\n",
       "  'manuell',\n",
       "  '218',\n",
       "  '7er',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '50667',\n",
       "  '2016-04-06 11:46:31'],\n",
       " ['2016-03-25 18:54:36',\n",
       "  'Opel_Corsa_C_Sport',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1400.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  '',\n",
       "  '2016-03-25 00:00:00',\n",
       "  '0',\n",
       "  '32756',\n",
       "  '2016-03-28 19:17:43'],\n",
       " ['2016-03-05 22:51:25',\n",
       "  'Opel_Signum_1.9_CDTI_Automatik_Sport___Sehr_gepflegt',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5900.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2006',\n",
       "  'automatik',\n",
       "  '150',\n",
       "  'signum',\n",
       "  '125000',\n",
       "  '7',\n",
       "  'diesel',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-05 00:00:00',\n",
       "  '0',\n",
       "  '07318',\n",
       "  '2016-04-05 11:49:30'],\n",
       " ['2016-03-08 09:36:21',\n",
       "  'Audi_A2',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2300.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'a2',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '55595',\n",
       "  '2016-03-10 09:46:13'],\n",
       " ['2016-03-22 11:38:37',\n",
       "  'Lexus_GS_300',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9200.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2005',\n",
       "  'automatik',\n",
       "  '249',\n",
       "  '',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'sonstige_autos',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '95447',\n",
       "  '2016-04-05 23:16:55'],\n",
       " ['2016-03-30 11:37:14',\n",
       "  'Golf4_\"Generation\"1_4_Ltr_75_PS_16V_Tuev_Neu_!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1700.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2000',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '9',\n",
       "  '',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '54427',\n",
       "  '2016-04-01 06:16:42'],\n",
       " ['2016-03-06 11:42:02',\n",
       "  'Golf_Gti_Edition_25',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  11500.0,\n",
       "  'test',\n",
       "  'coupe',\n",
       "  '2001',\n",
       "  'manuell',\n",
       "  '179',\n",
       "  'golf',\n",
       "  '100000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-06 00:00:00',\n",
       "  '0',\n",
       "  '94124',\n",
       "  '2016-03-14 19:16:16'],\n",
       " ['2016-03-10 14:58:37',\n",
       "  'Skoda_Octavia_2.0_SLX',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1700.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'octavia',\n",
       "  '150000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '10243',\n",
       "  '2016-03-22 05:46:14'],\n",
       " ['2016-03-19 10:36:21',\n",
       "  'Chrysler_Crossfire_Roadster_3.2___6_Gang_Leder_Unfallfrei',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12990.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2007',\n",
       "  'manuell',\n",
       "  '218',\n",
       "  'crossfire',\n",
       "  '70000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'chrysler',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '20537',\n",
       "  '2016-03-28 11:16:49'],\n",
       " ['2016-03-15 17:55:44',\n",
       "  'Volkswagen_Polo_1.6_TDI_Trendline',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5799.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2009',\n",
       "  'manuell',\n",
       "  '75',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '96337',\n",
       "  '2016-04-06 17:15:36'],\n",
       " ['2016-03-12 19:59:21',\n",
       "  'BMW_320d_touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '150',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '89250',\n",
       "  '2016-03-15 07:45:11'],\n",
       " ['2016-03-31 19:45:19',\n",
       "  'Ford_Ka',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  500.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'ka',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-31 00:00:00',\n",
       "  '0',\n",
       "  '63589',\n",
       "  '2016-04-04 12:46:42'],\n",
       " ['2016-03-27 17:56:57',\n",
       "  'Toyota_Land_Cruiser_FJ_Cruiser',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'test',\n",
       "  'suv',\n",
       "  '2008',\n",
       "  'automatik',\n",
       "  '0',\n",
       "  'andere',\n",
       "  '30000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'toyota',\n",
       "  '',\n",
       "  '2016-03-27 00:00:00',\n",
       "  '0',\n",
       "  '63454',\n",
       "  '2016-03-27 17:56:57'],\n",
       " ['2016-03-30 20:53:48',\n",
       "  'Ford_Fiesta_diesel.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1850.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '65',\n",
       "  'fiesta',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  'nein',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '52146',\n",
       "  '2016-04-07 13:18:08'],\n",
       " ['2016-04-02 18:57:15',\n",
       "  'VW_Polo_6N',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  950.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'polo',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '51545',\n",
       "  '2016-04-02 18:57:15'],\n",
       " ['2016-04-04 13:53:02',\n",
       "  'Vw_Passat_3C_Kombi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  4600.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '80634',\n",
       "  '2016-04-04 13:53:02'],\n",
       " ['2016-03-23 11:52:01',\n",
       "  'Mercedes_W_124_____kein_Rost_!!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6900.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1988',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'e_klasse',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '86609',\n",
       "  '2016-04-05 11:18:23'],\n",
       " ['2016-04-04 11:41:10',\n",
       "  'Citroën_C3_Pluriel_1.4_Gold_Edition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3950.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '73',\n",
       "  'c3',\n",
       "  '70000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '18546',\n",
       "  '2016-04-06 12:17:44'],\n",
       " ['2016-03-22 20:45:05',\n",
       "  'Renault_Clio_/_mit_TÜV_/_Scheckheftgepf.',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  599.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1995',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'clio',\n",
       "  '90000',\n",
       "  '8',\n",
       "  'benzin',\n",
       "  'renault',\n",
       "  '',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '44809',\n",
       "  '2016-03-22 20:45:05'],\n",
       " ['2016-03-11 13:48:00',\n",
       "  'BMW_E36_328i_Ringtool',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  12900.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '237',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '4',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-11 00:00:00',\n",
       "  '0',\n",
       "  '59557',\n",
       "  '2016-04-07 09:45:47'],\n",
       " ['2016-03-29 20:41:31',\n",
       "  'Smart_smart_&_pure_cdi',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  3500.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  '',\n",
       "  '41',\n",
       "  'fortwo',\n",
       "  '150000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-03-29 00:00:00',\n",
       "  '0',\n",
       "  '76646',\n",
       "  '2016-04-06 06:45:27'],\n",
       " ['2016-03-14 22:59:02',\n",
       "  'Opel_Vectra_1.6_Sport_Tuning_mit_neuem_TÜV',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1799.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1998',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'vectra',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '55286',\n",
       "  '2016-04-05 11:49:40'],\n",
       " ['2016-04-05 12:38:39',\n",
       "  'Skoda_Fabia_1.2_TSI_Ambition',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  8500.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2012',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'fabia',\n",
       "  '80000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'skoda',\n",
       "  'nein',\n",
       "  '2016-04-05 00:00:00',\n",
       "  '0',\n",
       "  '51143',\n",
       "  '2016-04-05 12:38:39'],\n",
       " ['2016-03-14 15:45:13',\n",
       "  'Volkswagen_Passat_Variant_1.9_TDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1399.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '90',\n",
       "  'passat',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '27356',\n",
       "  '2016-04-05 18:45:35'],\n",
       " ['2016-04-02 18:49:48',\n",
       "  'Smart_smart_&_pure',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1790.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  '',\n",
       "  '50',\n",
       "  'fortwo',\n",
       "  '100000',\n",
       "  '10',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '73037',\n",
       "  '2016-04-02 18:49:48'],\n",
       " ['2016-03-30 16:56:32',\n",
       "  'MOTORSCHADEN',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '109',\n",
       "  '3_reihe',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'peugeot',\n",
       "  'ja',\n",
       "  '2016-03-30 00:00:00',\n",
       "  '0',\n",
       "  '70469',\n",
       "  '2016-03-30 17:43:52'],\n",
       " ['2016-04-03 08:59:10',\n",
       "  'Smart_cabrio_softouch_Klima_Passion',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5500.0,\n",
       "  'test',\n",
       "  'cabrio',\n",
       "  '2009',\n",
       "  'automatik',\n",
       "  '71',\n",
       "  'fortwo',\n",
       "  '80000',\n",
       "  '5',\n",
       "  'benzin',\n",
       "  'smart',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '51063',\n",
       "  '2016-04-05 07:46:36'],\n",
       " ['2016-03-12 14:52:26',\n",
       "  'Suzuki_Alto_Sehr_guter_Zustand_\"Klima\"',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5400.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2011',\n",
       "  'manuell',\n",
       "  '68',\n",
       "  'andere',\n",
       "  '30000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'suzuki',\n",
       "  'nein',\n",
       "  '2016-03-12 00:00:00',\n",
       "  '0',\n",
       "  '79261',\n",
       "  '2016-04-07 01:17:16'],\n",
       " ['2016-03-15 13:53:34',\n",
       "  'Mercedes_Benz_CLK_Coupe_270_CDI_VOLLAUSSTATTUNG',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7350.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2003',\n",
       "  'automatik',\n",
       "  '170',\n",
       "  'clk',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '82024',\n",
       "  '2016-04-06 03:46:23'],\n",
       " ['2016-03-26 08:56:16',\n",
       "  'Verkaufe/Tausche_Seat_Inca__VW_Caddy_aus_2002_1_9_SDI',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'test',\n",
       "  'andere',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '64',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'seat',\n",
       "  'nein',\n",
       "  '2016-03-26 00:00:00',\n",
       "  '0',\n",
       "  '12627',\n",
       "  '2016-03-26 08:56:16'],\n",
       " ['2016-03-13 09:45:29',\n",
       "  'Verkaufe_gepflegten_Mercedes_420SE_W126_!!TÜV_NEU!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15990.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '1988',\n",
       "  'automatik',\n",
       "  '224',\n",
       "  's_klasse',\n",
       "  '100000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  '',\n",
       "  '2016-03-13 00:00:00',\n",
       "  '0',\n",
       "  '96515',\n",
       "  '2016-03-22 20:51:47'],\n",
       " ['2016-03-19 17:56:27',\n",
       "  'BMW_320d/M_Paket/ca200ps/',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  7999.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '200',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'diesel',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-19 00:00:00',\n",
       "  '0',\n",
       "  '72461',\n",
       "  '2016-03-20 20:46:58'],\n",
       " ['2016-03-17 11:50:01',\n",
       "  'Mercedes_Vito_zu_verkaufen_!!',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5950.0,\n",
       "  'control',\n",
       "  'andere',\n",
       "  '2005',\n",
       "  'manuell',\n",
       "  '0',\n",
       "  'vito',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-17 00:00:00',\n",
       "  '0',\n",
       "  '22929',\n",
       "  '2016-04-06 12:15:30'],\n",
       " ['2016-04-04 17:54:30',\n",
       "  'Audi_A6_No_A4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5699.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2004',\n",
       "  'automatik',\n",
       "  '250',\n",
       "  'a6',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'audi',\n",
       "  '',\n",
       "  '2016-04-04 00:00:00',\n",
       "  '0',\n",
       "  '45896',\n",
       "  '2016-04-06 19:48:31'],\n",
       " ['2016-03-22 18:25:24',\n",
       "  'Lupo_Diesel_an_Bastler_abzugeben',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  800.0,\n",
       "  'test',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '60',\n",
       "  'lupo',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  '',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '35444',\n",
       "  '2016-04-07 09:17:31'],\n",
       " ['2016-04-03 10:36:48',\n",
       "  'Porsche_Cayman_S_1.Hand_PCCB_ATM116tkm_PASM_SportChrono',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  24400.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '295',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '2',\n",
       "  'benzin',\n",
       "  'porsche',\n",
       "  'nein',\n",
       "  '2016-04-03 00:00:00',\n",
       "  '0',\n",
       "  '35037',\n",
       "  '2016-04-06 16:47:06'],\n",
       " ['2016-03-21 20:39:54',\n",
       "  'Volkswagen_Lupo_1.0',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1212.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2002',\n",
       "  'manuell',\n",
       "  '50',\n",
       "  'lupo',\n",
       "  '20000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-21 00:00:00',\n",
       "  '0',\n",
       "  '41366',\n",
       "  '2016-04-01 02:46:00'],\n",
       " ['2016-03-22 20:58:31',\n",
       "  'Mercedes_Benz_A_170_Classic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2590.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  'a_klasse',\n",
       "  '150000',\n",
       "  '11',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-22 00:00:00',\n",
       "  '0',\n",
       "  '60326',\n",
       "  '2016-04-07 14:58:02'],\n",
       " ['2016-03-15 00:58:49',\n",
       "  'vw_sharan__projekt_zwo1.9tdi_motor_AHU',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  0.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '1996',\n",
       "  'manuell',\n",
       "  '113',\n",
       "  'sharan',\n",
       "  '150000',\n",
       "  '12',\n",
       "  'diesel',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '49661',\n",
       "  '2016-03-18 00:17:31'],\n",
       " ['2016-03-07 16:39:09',\n",
       "  'Mercedes_Benz_300_CE',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5400.0,\n",
       "  'control',\n",
       "  'coupe',\n",
       "  '1992',\n",
       "  'automatik',\n",
       "  '179',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-07 00:00:00',\n",
       "  '0',\n",
       "  '29633',\n",
       "  '2016-03-18 01:17:57'],\n",
       " ['2016-03-10 19:49:52',\n",
       "  'Mercedes_Benz_A_180_CDI_Autotronic_Elegance',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  6600.0,\n",
       "  'control',\n",
       "  'limousine',\n",
       "  '2007',\n",
       "  'automatik',\n",
       "  '109',\n",
       "  'a_klasse',\n",
       "  '125000',\n",
       "  '2',\n",
       "  'diesel',\n",
       "  'mercedes_benz',\n",
       "  'nein',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '51069',\n",
       "  '2016-03-22 14:46:58'],\n",
       " ['2016-04-02 17:39:21',\n",
       "  'Opel_Agila_1.0_12_V',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1000.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '58',\n",
       "  'agila',\n",
       "  '150000',\n",
       "  '9',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-02 00:00:00',\n",
       "  '0',\n",
       "  '64807',\n",
       "  '2016-04-02 17:39:21'],\n",
       " ['2016-03-16 13:56:45',\n",
       "  'Golf_4__1.6_schwarz_TÜV_3/2017',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  2400.0,\n",
       "  'control',\n",
       "  'kleinwagen',\n",
       "  '1999',\n",
       "  'automatik',\n",
       "  '101',\n",
       "  'golf',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'benzin',\n",
       "  'volkswagen',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '49324',\n",
       "  '2016-03-28 10:15:40'],\n",
       " ['2016-03-15 14:53:15',\n",
       "  'Dacia_Duster_1.6_16V_105_4x2_Ice_mit_sportlichen_Alufelgen_8_fach',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  9999.0,\n",
       "  'control',\n",
       "  'suv',\n",
       "  '2014',\n",
       "  'manuell',\n",
       "  '105',\n",
       "  'duster',\n",
       "  '30000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'dacia',\n",
       "  'nein',\n",
       "  '2016-03-15 00:00:00',\n",
       "  '0',\n",
       "  '72622',\n",
       "  '2016-03-19 09:16:01'],\n",
       " ['2016-03-10 11:53:00',\n",
       "  'FORD_FOCUS_TDCI__TOP_ZUSTAND_TÜV_bis_4/2017_____DIESEL',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1700.0,\n",
       "  'test',\n",
       "  'limousine',\n",
       "  '2004',\n",
       "  'manuell',\n",
       "  '115',\n",
       "  'focus',\n",
       "  '150000',\n",
       "  '6',\n",
       "  'diesel',\n",
       "  'ford',\n",
       "  '',\n",
       "  '2016-03-10 00:00:00',\n",
       "  '0',\n",
       "  '78532',\n",
       "  '2016-03-24 13:45:50'],\n",
       " ['2016-03-16 01:36:18',\n",
       "  'Citroën_Jumper_30_L2H1',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5100.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2010',\n",
       "  'manuell',\n",
       "  '120',\n",
       "  'andere',\n",
       "  '150000',\n",
       "  '10',\n",
       "  'diesel',\n",
       "  'citroen',\n",
       "  'nein',\n",
       "  '2016-03-16 00:00:00',\n",
       "  '0',\n",
       "  '33649',\n",
       "  '2016-03-25 04:44:54'],\n",
       " ['2016-03-20 17:45:44',\n",
       "  'Audi_A6_Avant_3.0_TDI_DPF_quattro_S_tronic',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  15000.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2012',\n",
       "  'automatik',\n",
       "  '245',\n",
       "  'a6',\n",
       "  '70000',\n",
       "  '5',\n",
       "  'diesel',\n",
       "  'audi',\n",
       "  'nein',\n",
       "  '2016-03-20 00:00:00',\n",
       "  '0',\n",
       "  '39110',\n",
       "  '2016-03-21 21:15:22'],\n",
       " ['2016-03-14 19:44:10',\n",
       "  'Peugeot_307_CC_mit_LPG_Fluessiggasanlage_und_Lederaustattung',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  5400.0,\n",
       "  'control',\n",
       "  'cabrio',\n",
       "  '2006',\n",
       "  'manuell',\n",
       "  '140',\n",
       "  '3_reihe',\n",
       "  '150000',\n",
       "  '3',\n",
       "  'benzin',\n",
       "  'peugeot',\n",
       "  'nein',\n",
       "  '2016-03-14 00:00:00',\n",
       "  '0',\n",
       "  '38104',\n",
       "  '2016-04-06 04:17:36'],\n",
       " ['2016-03-08 13:36:58',\n",
       "  'BMW_318i_touring',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1650.0,\n",
       "  'control',\n",
       "  'kombi',\n",
       "  '1997',\n",
       "  'manuell',\n",
       "  '116',\n",
       "  '3er',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'bmw',\n",
       "  'nein',\n",
       "  '2016-03-08 00:00:00',\n",
       "  '0',\n",
       "  '86391',\n",
       "  '2016-03-13 00:44:18'],\n",
       " ['2016-03-23 18:06:25',\n",
       "  'Fiat_Stilo_Multi_Wagon_1.6___Euro_4',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  700.0,\n",
       "  'test',\n",
       "  'kombi',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '103',\n",
       "  'stilo',\n",
       "  '150000',\n",
       "  '1',\n",
       "  'benzin',\n",
       "  'fiat',\n",
       "  'nein',\n",
       "  '2016-03-23 00:00:00',\n",
       "  '0',\n",
       "  '60437',\n",
       "  '2016-03-23 18:06:25'],\n",
       " ['2016-04-01 20:46:13',\n",
       "  'Opel_Zafira_1_6',\n",
       "  'privat',\n",
       "  'Angebot',\n",
       "  1995.0,\n",
       "  'test',\n",
       "  'bus',\n",
       "  '2003',\n",
       "  'manuell',\n",
       "  '101',\n",
       "  'zafira',\n",
       "  '150000',\n",
       "  '7',\n",
       "  'benzin',\n",
       "  'opel',\n",
       "  'nein',\n",
       "  '2016-04-01 00:00:00',\n",
       "  '0',\n",
       "  '44287',\n",
       "  '2016-04-03 17:46:36'],\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e43986d",
   "metadata": {},
   "source": [
    "Thanks to the function is_valid_entry, the size of the dataset has fallen down from 371528 to 255300 removing the illegal/missing values and ensuring the correctness of the values for the numerical types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e031a37",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row vehicleType has 37869 invalid values\n",
      "row monthOfRegistration has 22622 invalid values\n"
     ]
    }
   ],
   "source": [
    "for h in header.keys():\n",
    "    if invalid_header_counter[header[h]] != 0:\n",
    "        print(f\"row {h} has {invalid_header_counter[header[h]]} invalid values\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9685d2bc",
   "metadata": {},
   "source": [
    "Given the fact that the row brand always has a legal non-empty string let's check if the number of removed elements plus the number of cleaned rows match the number of total rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36a2de55",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(invalid_header_counter) + len(cleaned_rows) == len(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d15a8e1d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'audi': 28617,\n",
       "         'jeep': 701,\n",
       "         'volkswagen': 64660,\n",
       "         'skoda': 5158,\n",
       "         'bmw': 35164,\n",
       "         'peugeot': 9402,\n",
       "         'ford': 20943,\n",
       "         'mazda': 4738,\n",
       "         'nissan': 4221,\n",
       "         'renault': 14322,\n",
       "         'mercedes_benz': 31178,\n",
       "         'seat': 5835,\n",
       "         'honda': 2293,\n",
       "         'fiat': 7731,\n",
       "         'mini': 3144,\n",
       "         'smart': 4550,\n",
       "         'opel': 31932,\n",
       "         'sonstige_autos': 2715,\n",
       "         'subaru': 629,\n",
       "         'volvo': 2917,\n",
       "         'mitsubishi': 2491,\n",
       "         'hyundai': 3227,\n",
       "         'alfa_romeo': 1948,\n",
       "         'kia': 2253,\n",
       "         'suzuki': 1967,\n",
       "         'lancia': 380,\n",
       "         'porsche': 2046,\n",
       "         'citroen': 4382,\n",
       "         'toyota': 4191,\n",
       "         'chevrolet': 1589,\n",
       "         'dacia': 826,\n",
       "         'daihatsu': 632,\n",
       "         'saab': 480,\n",
       "         'chrysler': 1218,\n",
       "         'jaguar': 573,\n",
       "         'rover': 376,\n",
       "         'daewoo': 432,\n",
       "         'land_rover': 706,\n",
       "         'trabant': 295,\n",
       "         'lada': 175})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "brand_counter = Counter([row[header[\"brand\"]] for row in cleaned_rows])\n",
    "brand_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e5482",
   "metadata": {},
   "source": [
    "Doing an additional inspection over the values of the brand column, we can confirm that all the values are legal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2248fb01",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'coupe': 17585,\n",
       "         'suv': 14093,\n",
       "         'kleinwagen': 73185,\n",
       "         'limousine': 90157,\n",
       "         'cabrio': 21643,\n",
       "         'bus': 28737,\n",
       "         'kombi': 62816,\n",
       "         'andere': 2821})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vehicle_type_counter = Counter([row[header[\"vehicleType\"]] for row in cleaned_rows])\n",
    "vehicle_type_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120519da",
   "metadata": {},
   "source": [
    "And the same is valid for the column \"vehicle type\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16bf352",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "59683dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "f = open('cleaned_dataset.csv', 'w')\n",
    "\n",
    "writer = csv.writer(f)\n",
    "\n",
    "writer.writerow(raw_header)\n",
    "\n",
    "for row in cleaned_rows:\n",
    "    writer.writerow(row)\n",
    "# close the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "276d5ec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dateCrawled',\n",
       " 'name',\n",
       " 'seller',\n",
       " 'offerType',\n",
       " 'price',\n",
       " 'abtest',\n",
       " 'vehicleType',\n",
       " 'yearOfRegistration',\n",
       " 'gearbox',\n",
       " 'powerPS',\n",
       " 'model',\n",
       " 'kilometer',\n",
       " 'monthOfRegistration',\n",
       " 'fuelType',\n",
       " 'brand',\n",
       " 'notRepairedDamage',\n",
       " 'dateCreated',\n",
       " 'nrOfPictures',\n",
       " 'postalCode',\n",
       " 'lastSeen']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dc27b0",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 2: Data analysis (30 points) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd1d517",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'2016-03-24 10:58:45': 1,\n",
       "         '2016-03-14 12:52:21': 1,\n",
       "         '2016-03-17 16:54:04': 2,\n",
       "         '2016-03-31 17:25:20': 1,\n",
       "         '2016-04-01 20:48:51': 3,\n",
       "         '2016-04-04 23:42:13': 2,\n",
       "         '2016-03-26 19:54:18': 2,\n",
       "         '2016-04-07 10:06:22': 2,\n",
       "         '2016-03-21 21:37:40': 1,\n",
       "         '2016-03-21 12:57:01': 1,\n",
       "         '2016-03-20 10:25:19': 2,\n",
       "         '2016-03-23 15:48:05': 1,\n",
       "         '2016-04-01 22:55:47': 2,\n",
       "         '2016-04-01 19:56:48': 1,\n",
       "         '2016-03-27 11:38:00': 1,\n",
       "         '2016-03-18 21:44:09': 1,\n",
       "         '2016-03-07 12:51:23': 1,\n",
       "         '2016-03-09 11:56:38': 1,\n",
       "         '2016-04-03 15:48:11': 1,\n",
       "         '2016-03-25 21:48:47': 1,\n",
       "         '2016-03-17 18:55:12': 1,\n",
       "         '2016-04-01 17:45:07': 1,\n",
       "         '2016-03-25 15:50:30': 2,\n",
       "         '2016-03-30 20:38:20': 1,\n",
       "         '2016-03-24 00:52:09': 1,\n",
       "         '2016-03-13 15:47:08': 1,\n",
       "         '2016-03-17 12:44:43': 1,\n",
       "         '2016-03-15 18:59:02': 1,\n",
       "         '2016-04-04 00:38:22': 2,\n",
       "         '2016-04-04 14:06:22': 1,\n",
       "         '2016-03-30 08:50:37': 2,\n",
       "         '2016-03-29 18:53:48': 2,\n",
       "         '2016-03-21 01:59:07': 1,\n",
       "         '2016-03-22 17:56:12': 1,\n",
       "         '2016-03-31 08:57:44': 1,\n",
       "         '2016-03-25 19:44:06': 1,\n",
       "         '2016-03-30 12:49:54': 1,\n",
       "         '2016-03-08 12:54:47': 1,\n",
       "         '2016-03-07 22:36:54': 1,\n",
       "         '2016-03-15 14:50:24': 1,\n",
       "         '2016-03-21 12:47:55': 1,\n",
       "         '2016-03-15 09:51:05': 2,\n",
       "         '2016-03-22 17:45:41': 2,\n",
       "         '2016-03-31 18:46:36': 1,\n",
       "         '2016-03-17 09:48:12': 1,\n",
       "         '2016-03-11 14:50:52': 1,\n",
       "         '2016-03-10 10:50:26': 1,\n",
       "         '2016-03-11 23:42:53': 3,\n",
       "         '2016-03-23 18:59:37': 2,\n",
       "         '2016-03-12 19:46:54': 1,\n",
       "         '2016-03-29 14:58:15': 2,\n",
       "         '2016-03-13 19:52:34': 2,\n",
       "         '2016-03-08 13:49:57': 2,\n",
       "         '2016-03-07 17:38:57': 1,\n",
       "         '2016-03-23 11:53:21': 1,\n",
       "         '2016-04-02 23:25:25': 1,\n",
       "         '2016-03-10 11:44:54': 1,\n",
       "         '2016-03-25 18:44:10': 2,\n",
       "         '2016-03-31 10:53:10': 1,\n",
       "         '2016-03-26 15:38:12': 1,\n",
       "         '2016-04-01 12:45:41': 1,\n",
       "         '2016-03-11 19:00:25': 3,\n",
       "         '2016-03-28 00:56:46': 1,\n",
       "         '2016-03-16 16:44:10': 1,\n",
       "         '2016-04-01 09:54:39': 1,\n",
       "         '2016-04-02 12:54:14': 2,\n",
       "         '2016-03-23 10:59:51': 1,\n",
       "         '2016-03-22 17:44:26': 3,\n",
       "         '2016-03-26 15:47:58': 1,\n",
       "         '2016-03-14 20:37:41': 3,\n",
       "         '2016-03-20 18:38:02': 2,\n",
       "         '2016-04-04 09:50:15': 2,\n",
       "         '2016-03-09 17:47:24': 1,\n",
       "         '2016-04-03 12:45:29': 3,\n",
       "         '2016-03-20 13:54:32': 1,\n",
       "         '2016-03-12 22:57:58': 2,\n",
       "         '2016-03-18 11:25:43': 2,\n",
       "         '2016-04-01 17:56:34': 1,\n",
       "         '2016-03-27 12:36:18': 2,\n",
       "         '2016-03-18 23:41:18': 3,\n",
       "         '2016-03-11 20:55:37': 1,\n",
       "         '2016-03-16 18:37:43': 1,\n",
       "         '2016-03-09 20:37:42': 2,\n",
       "         '2016-03-31 20:57:02': 2,\n",
       "         '2016-03-28 20:45:46': 1,\n",
       "         '2016-04-01 22:55:34': 1,\n",
       "         '2016-03-21 16:45:30': 1,\n",
       "         '2016-04-03 15:58:09': 1,\n",
       "         '2016-03-24 14:38:41': 1,\n",
       "         '2016-04-02 10:59:28': 1,\n",
       "         '2016-03-13 01:57:12': 1,\n",
       "         '2016-03-28 19:06:18': 1,\n",
       "         '2016-03-31 00:59:04': 2,\n",
       "         '2016-04-05 09:55:37': 1,\n",
       "         '2016-03-27 12:59:00': 1,\n",
       "         '2016-03-22 10:50:53': 2,\n",
       "         '2016-03-29 07:56:29': 1,\n",
       "         '2016-03-27 20:47:22': 1,\n",
       "         '2016-04-03 18:58:25': 1,\n",
       "         '2016-03-10 16:57:31': 1,\n",
       "         '2016-03-23 14:45:57': 1,\n",
       "         '2016-03-08 19:58:07': 1,\n",
       "         '2016-03-28 10:50:04': 1,\n",
       "         '2016-04-01 22:49:08': 2,\n",
       "         '2016-03-28 01:36:15': 1,\n",
       "         '2016-03-14 12:54:41': 1,\n",
       "         '2016-03-19 13:36:22': 3,\n",
       "         '2016-03-28 13:46:45': 2,\n",
       "         '2016-03-07 12:38:19': 2,\n",
       "         '2016-03-16 12:46:39': 1,\n",
       "         '2016-03-18 12:39:23': 1,\n",
       "         '2016-03-18 06:36:46': 1,\n",
       "         '2016-03-24 15:38:18': 1,\n",
       "         '2016-03-21 00:59:18': 2,\n",
       "         '2016-03-08 20:58:31': 3,\n",
       "         '2016-03-12 17:47:07': 2,\n",
       "         '2016-03-28 17:39:50': 2,\n",
       "         '2016-03-21 19:06:21': 1,\n",
       "         '2016-03-10 18:45:59': 3,\n",
       "         '2016-03-10 23:55:39': 1,\n",
       "         '2016-03-31 16:57:18': 2,\n",
       "         '2016-03-30 22:51:56': 2,\n",
       "         '2016-03-20 23:48:31': 1,\n",
       "         '2016-03-25 15:56:48': 1,\n",
       "         '2016-03-08 21:52:34': 2,\n",
       "         '2016-03-21 20:43:01': 2,\n",
       "         '2016-03-22 18:47:21': 1,\n",
       "         '2016-03-20 16:57:05': 1,\n",
       "         '2016-03-08 09:56:53': 2,\n",
       "         '2016-04-04 21:43:31': 1,\n",
       "         '2016-04-02 23:54:43': 1,\n",
       "         '2016-03-31 13:45:43': 2,\n",
       "         '2016-03-30 16:39:39': 1,\n",
       "         '2016-04-01 18:45:45': 1,\n",
       "         '2016-03-31 12:58:26': 2,\n",
       "         '2016-03-27 23:48:09': 2,\n",
       "         '2016-03-31 19:52:39': 2,\n",
       "         '2016-03-13 00:56:50': 2,\n",
       "         '2016-04-02 10:51:46': 1,\n",
       "         '2016-04-05 11:48:01': 2,\n",
       "         '2016-03-11 19:50:22': 1,\n",
       "         '2016-03-21 15:43:25': 1,\n",
       "         '2016-03-15 23:55:52': 2,\n",
       "         '2016-03-07 17:52:07': 1,\n",
       "         '2016-04-05 08:57:20': 1,\n",
       "         '2016-03-29 23:57:50': 2,\n",
       "         '2016-03-16 11:46:23': 1,\n",
       "         '2016-03-23 11:42:10': 1,\n",
       "         '2016-03-20 11:38:38': 1,\n",
       "         '2016-03-05 15:55:47': 1,\n",
       "         '2016-03-14 15:56:43': 1,\n",
       "         '2016-03-28 18:56:10': 1,\n",
       "         '2016-03-12 08:51:00': 1,\n",
       "         '2016-03-26 20:57:26': 1,\n",
       "         '2016-03-22 11:59:28': 1,\n",
       "         '2016-03-21 14:56:24': 1,\n",
       "         '2016-03-05 17:50:33': 1,\n",
       "         '2016-04-01 14:38:34': 1,\n",
       "         '2016-03-20 12:58:06': 1,\n",
       "         '2016-04-01 19:58:29': 1,\n",
       "         '2016-03-06 18:49:03': 2,\n",
       "         '2016-04-03 11:59:39': 1,\n",
       "         '2016-03-30 20:42:52': 1,\n",
       "         '2016-03-28 22:55:34': 1,\n",
       "         '2016-03-16 19:49:49': 3,\n",
       "         '2016-03-05 18:38:39': 1,\n",
       "         '2016-03-21 10:44:34': 1,\n",
       "         '2016-03-06 09:48:11': 1,\n",
       "         '2016-03-27 21:50:59': 2,\n",
       "         '2016-04-04 21:52:13': 1,\n",
       "         '2016-03-20 19:49:21': 2,\n",
       "         '2016-03-09 15:50:39': 1,\n",
       "         '2016-03-05 22:56:03': 2,\n",
       "         '2016-04-03 02:57:31': 2,\n",
       "         '2016-03-29 22:50:53': 1,\n",
       "         '2016-03-11 00:55:09': 1,\n",
       "         '2016-04-04 23:48:59': 2,\n",
       "         '2016-03-12 15:55:41': 1,\n",
       "         '2016-03-11 13:52:52': 2,\n",
       "         '2016-03-21 17:36:59': 2,\n",
       "         '2016-03-22 19:56:01': 1,\n",
       "         '2016-03-25 12:25:20': 2,\n",
       "         '2016-03-28 18:38:30': 1,\n",
       "         '2016-03-12 16:44:57': 1,\n",
       "         '2016-03-29 23:38:02': 1,\n",
       "         '2016-03-30 23:56:48': 1,\n",
       "         '2016-03-15 19:58:30': 1,\n",
       "         '2016-04-04 17:44:53': 1,\n",
       "         '2016-03-17 22:52:09': 2,\n",
       "         '2016-03-07 15:55:34': 1,\n",
       "         '2016-03-06 12:38:13': 2,\n",
       "         '2016-03-20 21:47:47': 2,\n",
       "         '2016-03-31 15:48:39': 2,\n",
       "         '2016-03-30 16:47:47': 3,\n",
       "         '2016-03-19 16:40:33': 1,\n",
       "         '2016-03-23 17:59:12': 1,\n",
       "         '2016-03-07 06:14:28': 1,\n",
       "         '2016-03-28 13:31:04': 1,\n",
       "         '2016-04-01 00:51:10': 1,\n",
       "         '2016-03-10 17:47:26': 1,\n",
       "         '2016-03-09 19:58:09': 1,\n",
       "         '2016-04-04 18:56:38': 2,\n",
       "         '2016-03-29 00:56:26': 1,\n",
       "         '2016-03-20 14:56:34': 1,\n",
       "         '2016-03-16 19:47:27': 1,\n",
       "         '2016-03-19 10:54:59': 1,\n",
       "         '2016-03-19 04:37:03': 1,\n",
       "         '2016-03-31 14:39:03': 1,\n",
       "         '2016-03-13 13:38:54': 2,\n",
       "         '2016-03-15 18:50:28': 1,\n",
       "         '2016-03-14 12:42:51': 2,\n",
       "         '2016-04-01 19:54:05': 2,\n",
       "         '2016-03-05 19:47:47': 1,\n",
       "         '2016-03-05 14:41:09': 1,\n",
       "         '2016-03-10 12:57:13': 2,\n",
       "         '2016-04-03 21:06:19': 1,\n",
       "         '2016-03-05 14:25:18': 4,\n",
       "         '2016-03-31 09:55:33': 1,\n",
       "         '2016-03-15 15:41:08': 1,\n",
       "         '2016-03-10 17:39:51': 2,\n",
       "         '2016-03-28 14:57:57': 2,\n",
       "         '2016-03-26 13:53:42': 1,\n",
       "         '2016-03-16 17:38:52': 1,\n",
       "         '2016-04-02 15:43:07': 2,\n",
       "         '2016-03-30 21:38:18': 1,\n",
       "         '2016-03-11 20:56:26': 1,\n",
       "         '2016-03-09 15:45:23': 1,\n",
       "         '2016-03-19 23:54:43': 1,\n",
       "         '2016-03-25 07:36:27': 2,\n",
       "         '2016-03-31 14:53:55': 1,\n",
       "         '2016-03-20 11:44:15': 1,\n",
       "         '2016-03-19 17:48:55': 2,\n",
       "         '2016-03-09 20:45:43': 1,\n",
       "         '2016-03-28 18:47:24': 1,\n",
       "         '2016-03-10 18:53:17': 1,\n",
       "         '2016-03-26 19:58:06': 3,\n",
       "         '2016-03-28 20:56:52': 1,\n",
       "         '2016-04-02 20:50:59': 1,\n",
       "         '2016-03-10 09:56:48': 2,\n",
       "         '2016-03-16 09:47:12': 1,\n",
       "         '2016-03-23 17:49:30': 3,\n",
       "         '2016-03-15 19:46:06': 1,\n",
       "         '2016-03-05 14:14:19': 1,\n",
       "         '2016-03-08 08:57:34': 1,\n",
       "         '2016-03-30 07:54:00': 1,\n",
       "         '2016-04-03 15:49:38': 1,\n",
       "         '2016-03-30 13:53:41': 1,\n",
       "         '2016-03-14 23:52:32': 2,\n",
       "         '2016-03-11 21:49:52': 2,\n",
       "         '2016-03-21 23:53:36': 1,\n",
       "         '2016-03-26 21:37:36': 1,\n",
       "         '2016-03-30 20:41:10': 1,\n",
       "         '2016-04-04 10:53:57': 1,\n",
       "         '2016-03-26 22:56:11': 1,\n",
       "         '2016-03-28 14:53:30': 2,\n",
       "         '2016-03-25 15:51:40': 1,\n",
       "         '2016-03-29 21:38:39': 1,\n",
       "         '2016-04-02 12:39:02': 1,\n",
       "         '2016-04-04 10:46:55': 1,\n",
       "         '2016-03-24 21:50:08': 1,\n",
       "         '2016-03-16 03:03:22': 1,\n",
       "         '2016-03-31 21:59:23': 2,\n",
       "         '2016-03-23 09:54:26': 2,\n",
       "         '2016-03-12 12:40:29': 1,\n",
       "         '2016-03-18 21:06:25': 2,\n",
       "         '2016-03-10 14:45:46': 1,\n",
       "         '2016-03-20 04:36:43': 1,\n",
       "         '2016-04-02 18:37:34': 1,\n",
       "         '2016-03-11 18:38:42': 1,\n",
       "         '2016-03-08 08:56:07': 1,\n",
       "         '2016-03-27 14:51:02': 1,\n",
       "         '2016-03-07 07:55:46': 2,\n",
       "         '2016-03-13 14:23:08': 1,\n",
       "         '2016-03-15 09:38:21': 2,\n",
       "         '2016-03-20 23:45:11': 2,\n",
       "         '2016-03-30 12:58:13': 1,\n",
       "         '2016-03-12 18:44:27': 1,\n",
       "         '2016-03-15 13:58:27': 1,\n",
       "         '2016-03-24 20:46:31': 1,\n",
       "         '2016-03-14 15:49:16': 3,\n",
       "         '2016-03-29 13:56:25': 1,\n",
       "         '2016-03-05 15:43:56': 1,\n",
       "         '2016-03-11 15:37:14': 1,\n",
       "         '2016-03-22 12:55:10': 1,\n",
       "         '2016-03-08 19:48:23': 1,\n",
       "         '2016-04-04 12:43:25': 2,\n",
       "         '2016-03-18 12:48:27': 1,\n",
       "         '2016-03-26 21:49:45': 2,\n",
       "         '2016-03-29 18:45:21': 1,\n",
       "         '2016-03-29 20:06:18': 2,\n",
       "         '2016-03-26 15:48:15': 2,\n",
       "         '2016-04-04 13:48:25': 1,\n",
       "         '2016-04-01 13:50:10': 2,\n",
       "         '2016-03-21 07:37:10': 1,\n",
       "         '2016-03-29 17:56:01': 1,\n",
       "         '2016-03-13 22:46:52': 1,\n",
       "         '2016-04-03 16:06:23': 2,\n",
       "         '2016-03-09 23:46:25': 2,\n",
       "         '2016-03-08 19:38:37': 2,\n",
       "         '2016-03-24 18:55:31': 1,\n",
       "         '2016-03-19 07:51:18': 1,\n",
       "         '2016-03-20 20:45:27': 1,\n",
       "         '2016-04-02 21:54:46': 1,\n",
       "         '2016-03-29 14:54:44': 1,\n",
       "         '2016-04-02 17:54:29': 1,\n",
       "         '2016-03-25 17:52:18': 1,\n",
       "         '2016-03-26 17:39:51': 2,\n",
       "         '2016-03-08 19:55:56': 1,\n",
       "         '2016-03-12 14:39:17': 2,\n",
       "         '2016-04-04 10:57:55': 2,\n",
       "         '2016-03-12 16:48:12': 1,\n",
       "         '2016-03-27 19:52:13': 1,\n",
       "         '2016-04-01 22:51:18': 1,\n",
       "         '2016-04-04 02:02:24': 2,\n",
       "         '2016-04-03 21:37:38': 1,\n",
       "         '2016-03-30 07:54:22': 1,\n",
       "         '2016-03-21 21:46:45': 1,\n",
       "         '2016-03-26 10:37:21': 1,\n",
       "         '2016-03-05 16:45:59': 1,\n",
       "         '2016-03-19 18:38:15': 2,\n",
       "         '2016-03-12 12:06:22': 1,\n",
       "         '2016-03-29 15:38:17': 2,\n",
       "         '2016-03-23 20:52:50': 2,\n",
       "         '2016-03-20 20:42:07': 1,\n",
       "         '2016-03-05 14:06:49': 1,\n",
       "         '2016-04-02 08:51:27': 2,\n",
       "         '2016-03-23 10:50:58': 1,\n",
       "         '2016-03-18 18:47:36': 1,\n",
       "         '2016-04-01 12:49:18': 2,\n",
       "         '2016-03-17 18:52:52': 1,\n",
       "         '2016-03-08 10:49:40': 1,\n",
       "         '2016-03-10 12:54:34': 1,\n",
       "         '2016-03-27 15:44:25': 1,\n",
       "         '2016-03-20 12:06:18': 1,\n",
       "         '2016-04-05 00:55:53': 2,\n",
       "         '2016-03-21 12:56:08': 1,\n",
       "         '2016-03-30 18:38:15': 1,\n",
       "         '2016-03-26 08:49:58': 1,\n",
       "         '2016-03-10 11:57:04': 1,\n",
       "         '2016-03-22 11:45:47': 2,\n",
       "         '2016-04-01 14:59:23': 1,\n",
       "         '2016-04-02 10:51:44': 1,\n",
       "         '2016-03-23 10:54:49': 2,\n",
       "         '2016-03-09 12:55:36': 1,\n",
       "         '2016-03-31 23:52:32': 1,\n",
       "         '2016-03-07 21:46:03': 1,\n",
       "         '2016-03-31 11:25:20': 2,\n",
       "         '2016-03-15 16:46:22': 1,\n",
       "         '2016-03-15 09:54:56': 2,\n",
       "         '2016-03-19 16:57:24': 2,\n",
       "         '2016-03-26 13:38:11': 1,\n",
       "         '2016-04-04 21:41:10': 2,\n",
       "         '2016-03-14 01:47:28': 1,\n",
       "         '2016-03-24 18:50:52': 1,\n",
       "         '2016-03-29 17:44:47': 1,\n",
       "         '2016-03-20 20:47:52': 1,\n",
       "         '2016-04-03 18:06:22': 1,\n",
       "         '2016-03-27 14:40:52': 2,\n",
       "         '2016-03-23 15:42:44': 1,\n",
       "         '2016-03-08 19:48:28': 1,\n",
       "         '2016-04-02 07:56:21': 1,\n",
       "         '2016-03-08 16:56:07': 1,\n",
       "         '2016-03-14 20:55:56': 1,\n",
       "         '2016-03-22 18:41:49': 1,\n",
       "         '2016-04-02 11:59:01': 2,\n",
       "         '2016-03-09 23:52:02': 1,\n",
       "         '2016-03-08 12:53:50': 1,\n",
       "         '2016-03-16 15:47:39': 3,\n",
       "         '2016-03-29 21:51:19': 1,\n",
       "         '2016-03-22 12:57:23': 1,\n",
       "         '2016-03-26 16:39:01': 1,\n",
       "         '2016-03-15 09:55:13': 1,\n",
       "         '2016-03-14 22:57:33': 2,\n",
       "         '2016-04-01 09:37:51': 1,\n",
       "         '2016-03-10 15:55:03': 2,\n",
       "         '2016-03-22 11:52:30': 1,\n",
       "         '2016-04-02 21:43:28': 2,\n",
       "         '2016-03-21 13:43:24': 2,\n",
       "         '2016-03-19 19:46:25': 1,\n",
       "         '2016-03-19 21:55:56': 1,\n",
       "         '2016-03-31 22:58:15': 3,\n",
       "         '2016-03-07 18:39:04': 1,\n",
       "         '2016-03-22 12:52:55': 1,\n",
       "         '2016-03-24 19:54:22': 1,\n",
       "         '2016-03-27 10:52:55': 1,\n",
       "         '2016-04-04 11:06:24': 1,\n",
       "         '2016-03-08 16:53:28': 2,\n",
       "         '2016-03-12 10:56:12': 2,\n",
       "         '2016-03-06 21:36:24': 2,\n",
       "         '2016-03-30 19:58:07': 2,\n",
       "         '2016-03-05 14:29:10': 1,\n",
       "         '2016-03-09 09:36:19': 2,\n",
       "         '2016-03-20 13:56:12': 1,\n",
       "         '2016-03-14 12:52:24': 1,\n",
       "         '2016-04-01 23:52:38': 1,\n",
       "         '2016-03-26 08:53:33': 1,\n",
       "         '2016-03-31 14:52:16': 2,\n",
       "         '2016-03-07 11:38:18': 3,\n",
       "         '2016-03-21 00:50:20': 2,\n",
       "         '2016-03-22 18:48:26': 1,\n",
       "         '2016-04-01 08:37:29': 1,\n",
       "         '2016-04-05 11:57:46': 2,\n",
       "         '2016-03-11 18:44:15': 2,\n",
       "         '2016-03-08 07:56:35': 1,\n",
       "         '2016-03-16 07:55:15': 1,\n",
       "         '2016-03-23 00:55:16': 1,\n",
       "         '2016-03-15 01:01:46': 1,\n",
       "         '2016-03-21 11:56:46': 1,\n",
       "         '2016-03-30 00:57:26': 1,\n",
       "         '2016-03-07 17:53:58': 2,\n",
       "         '2016-03-31 23:41:53': 1,\n",
       "         '2016-03-10 20:47:58': 1,\n",
       "         '2016-03-28 21:47:37': 1,\n",
       "         '2016-03-05 22:37:22': 1,\n",
       "         '2016-03-30 02:37:57': 2,\n",
       "         '2016-03-23 03:02:45': 1,\n",
       "         '2016-03-21 10:50:50': 1,\n",
       "         '2016-03-14 17:50:40': 1,\n",
       "         '2016-03-23 00:57:12': 1,\n",
       "         '2016-03-29 18:48:00': 3,\n",
       "         '2016-03-21 15:59:54': 1,\n",
       "         '2016-03-10 18:45:10': 1,\n",
       "         '2016-03-26 10:36:55': 1,\n",
       "         '2016-03-12 11:55:53': 2,\n",
       "         '2016-03-29 20:55:15': 1,\n",
       "         '2016-04-02 19:36:58': 2,\n",
       "         '2016-03-09 12:49:21': 2,\n",
       "         '2016-03-14 08:51:58': 1,\n",
       "         '2016-03-05 15:41:11': 1,\n",
       "         '2016-03-23 10:41:50': 2,\n",
       "         '2016-03-24 09:51:55': 1,\n",
       "         '2016-03-17 11:49:13': 1,\n",
       "         '2016-03-31 23:54:31': 1,\n",
       "         '2016-03-09 14:52:18': 1,\n",
       "         '2016-03-27 11:46:59': 1,\n",
       "         '2016-04-03 12:40:34': 2,\n",
       "         '2016-03-20 11:56:51': 1,\n",
       "         '2016-04-04 14:53:21': 2,\n",
       "         '2016-03-16 19:50:05': 2,\n",
       "         '2016-03-06 12:38:01': 2,\n",
       "         '2016-03-07 12:48:32': 1,\n",
       "         '2016-03-22 21:50:36': 1,\n",
       "         '2016-03-12 17:50:58': 1,\n",
       "         '2016-03-18 15:36:20': 1,\n",
       "         '2016-03-28 00:57:13': 1,\n",
       "         '2016-03-23 11:57:36': 1,\n",
       "         '2016-04-02 19:38:54': 1,\n",
       "         '2016-03-23 23:56:26': 1,\n",
       "         '2016-03-18 14:07:01': 2,\n",
       "         '2016-04-04 07:56:55': 1,\n",
       "         '2016-03-26 13:57:27': 1,\n",
       "         '2016-03-18 12:43:41': 1,\n",
       "         '2016-03-27 09:56:27': 2,\n",
       "         '2016-04-02 09:50:44': 1,\n",
       "         '2016-03-12 22:57:06': 1,\n",
       "         '2016-03-11 19:54:32': 2,\n",
       "         '2016-04-01 14:47:39': 1,\n",
       "         '2016-03-15 15:52:17': 1,\n",
       "         '2016-03-28 14:57:24': 2,\n",
       "         '2016-03-28 19:40:52': 1,\n",
       "         '2016-03-23 16:49:44': 2,\n",
       "         '2016-03-14 14:46:07': 2,\n",
       "         '2016-03-27 22:52:02': 1,\n",
       "         '2016-03-23 00:55:47': 1,\n",
       "         '2016-03-10 18:06:17': 1,\n",
       "         '2016-03-07 09:52:06': 1,\n",
       "         '2016-03-30 11:55:46': 1,\n",
       "         '2016-04-03 16:54:55': 2,\n",
       "         '2016-03-23 20:25:18': 1,\n",
       "         '2016-03-26 23:51:54': 2,\n",
       "         '2016-04-02 22:46:59': 1,\n",
       "         '2016-03-25 14:50:45': 2,\n",
       "         '2016-03-19 20:54:28': 1,\n",
       "         '2016-03-11 23:36:16': 1,\n",
       "         '2016-03-30 21:46:21': 1,\n",
       "         '2016-03-22 17:57:57': 1,\n",
       "         '2016-03-11 12:00:05': 1,\n",
       "         '2016-04-01 15:56:27': 2,\n",
       "         '2016-03-11 14:57:04': 2,\n",
       "         '2016-04-04 22:43:34': 1,\n",
       "         '2016-03-17 14:56:29': 1,\n",
       "         '2016-03-23 11:50:35': 1,\n",
       "         '2016-03-17 15:50:28': 1,\n",
       "         '2016-04-05 10:36:53': 2,\n",
       "         '2016-03-09 21:52:45': 1,\n",
       "         '2016-03-29 12:57:17': 1,\n",
       "         '2016-03-24 18:36:16': 1,\n",
       "         '2016-03-28 19:51:36': 1,\n",
       "         '2016-03-19 10:44:52': 1,\n",
       "         '2016-03-11 13:49:25': 1,\n",
       "         '2016-03-31 21:37:03': 2,\n",
       "         '2016-03-30 17:44:47': 2,\n",
       "         '2016-03-15 18:46:09': 1,\n",
       "         '2016-03-12 10:58:09': 1,\n",
       "         '2016-03-23 16:53:58': 2,\n",
       "         '2016-03-28 22:55:09': 1,\n",
       "         '2016-03-24 10:50:35': 2,\n",
       "         '2016-03-27 19:53:32': 1,\n",
       "         '2016-03-15 20:53:11': 1,\n",
       "         '2016-04-04 21:48:23': 1,\n",
       "         '2016-03-30 21:37:21': 2,\n",
       "         '2016-03-05 19:57:52': 1,\n",
       "         '2016-03-26 10:50:30': 1,\n",
       "         '2016-04-01 07:55:36': 1,\n",
       "         '2016-03-15 22:55:52': 1,\n",
       "         '2016-03-21 14:47:07': 2,\n",
       "         '2016-04-01 13:38:56': 1,\n",
       "         '2016-03-22 17:53:25': 2,\n",
       "         '2016-03-09 21:47:40': 1,\n",
       "         '2016-03-23 17:48:36': 1,\n",
       "         '2016-03-10 14:36:55': 1,\n",
       "         '2016-03-27 21:36:22': 1,\n",
       "         '2016-03-10 08:36:17': 2,\n",
       "         '2016-03-24 21:55:10': 1,\n",
       "         '2016-03-22 12:45:44': 1,\n",
       "         '2016-03-19 15:44:54': 2,\n",
       "         '2016-03-21 13:56:13': 1,\n",
       "         '2016-04-01 23:56:33': 1,\n",
       "         '2016-03-10 22:53:58': 2,\n",
       "         '2016-03-20 10:54:37': 1,\n",
       "         '2016-04-02 22:57:47': 2,\n",
       "         '2016-04-04 18:43:11': 1,\n",
       "         '2016-03-08 16:42:59': 1,\n",
       "         '2016-03-22 19:25:23': 1,\n",
       "         '2016-03-26 13:37:16': 1,\n",
       "         '2016-03-20 11:53:15': 1,\n",
       "         '2016-04-05 20:25:33': 2,\n",
       "         '2016-03-15 10:37:43': 1,\n",
       "         '2016-03-28 14:53:09': 1,\n",
       "         '2016-03-26 12:47:39': 1,\n",
       "         '2016-03-22 01:53:10': 2,\n",
       "         '2016-03-31 00:28:47': 1,\n",
       "         '2016-04-04 22:36:22': 2,\n",
       "         '2016-03-23 14:39:00': 2,\n",
       "         '2016-03-28 10:54:49': 1,\n",
       "         '2016-03-14 18:30:47': 1,\n",
       "         '2016-04-01 19:47:11': 1,\n",
       "         '2016-04-03 16:58:42': 2,\n",
       "         '2016-04-03 10:56:51': 2,\n",
       "         '2016-03-23 17:37:58': 1,\n",
       "         '2016-03-22 15:50:59': 1,\n",
       "         '2016-03-06 11:38:19': 1,\n",
       "         '2016-04-01 21:53:31': 1,\n",
       "         '2016-03-12 04:36:23': 1,\n",
       "         '2016-04-04 21:39:18': 2,\n",
       "         '2016-03-29 23:53:31': 1,\n",
       "         '2016-03-27 14:50:59': 1,\n",
       "         '2016-03-26 11:54:32': 2,\n",
       "         '2016-03-27 20:38:01': 2,\n",
       "         '2016-03-21 14:49:56': 1,\n",
       "         '2016-04-03 21:37:24': 2,\n",
       "         '2016-03-25 08:37:02': 1,\n",
       "         '2016-04-05 00:54:03': 1,\n",
       "         '2016-03-28 14:51:11': 2,\n",
       "         '2016-03-16 15:38:57': 1,\n",
       "         '2016-03-05 14:25:04': 2,\n",
       "         '2016-03-19 15:54:15': 1,\n",
       "         '2016-03-27 06:36:19': 2,\n",
       "         '2016-03-09 14:50:51': 1,\n",
       "         '2016-03-31 10:36:17': 1,\n",
       "         '2016-03-19 19:39:52': 2,\n",
       "         '2016-03-20 18:44:28': 2,\n",
       "         '2016-03-12 19:36:18': 3,\n",
       "         '2016-03-29 18:53:12': 1,\n",
       "         '2016-03-26 19:51:33': 1,\n",
       "         '2016-04-03 22:51:30': 2,\n",
       "         '2016-03-25 18:46:10': 1,\n",
       "         '2016-04-02 19:53:34': 2,\n",
       "         '2016-03-09 13:54:44': 1,\n",
       "         '2016-03-19 13:49:50': 1,\n",
       "         '2016-03-18 20:43:14': 1,\n",
       "         '2016-03-11 12:52:36': 2,\n",
       "         '2016-04-03 13:46:37': 1,\n",
       "         '2016-03-25 19:47:02': 1,\n",
       "         '2016-03-10 19:49:37': 2,\n",
       "         '2016-03-27 18:55:44': 1,\n",
       "         '2016-03-26 15:38:57': 1,\n",
       "         '2016-03-30 20:49:23': 2,\n",
       "         '2016-03-10 12:51:14': 1,\n",
       "         '2016-03-10 20:46:48': 1,\n",
       "         '2016-03-24 17:49:49': 1,\n",
       "         '2016-03-22 14:55:22': 1,\n",
       "         '2016-03-07 21:57:39': 1,\n",
       "         '2016-04-02 09:51:15': 3,\n",
       "         '2016-03-22 21:46:43': 2,\n",
       "         '2016-03-05 20:49:52': 3,\n",
       "         '2016-03-18 01:57:22': 1,\n",
       "         '2016-03-05 20:36:24': 1,\n",
       "         '2016-03-23 19:00:20': 2,\n",
       "         '2016-03-09 10:54:10': 1,\n",
       "         '2016-03-25 11:50:46': 1,\n",
       "         '2016-03-19 10:51:07': 2,\n",
       "         '2016-04-02 16:49:46': 1,\n",
       "         '2016-03-17 14:38:54': 2,\n",
       "         '2016-03-26 16:53:56': 1,\n",
       "         '2016-03-11 12:54:22': 1,\n",
       "         '2016-03-07 13:46:32': 1,\n",
       "         '2016-03-17 20:58:30': 2,\n",
       "         '2016-03-15 16:37:39': 1,\n",
       "         '2016-03-22 17:53:32': 1,\n",
       "         '2016-03-21 18:41:46': 1,\n",
       "         '2016-03-14 12:48:40': 2,\n",
       "         '2016-03-27 18:51:29': 2,\n",
       "         '2016-03-19 12:55:55': 1,\n",
       "         '2016-03-10 17:39:00': 1,\n",
       "         '2016-04-04 15:46:35': 2,\n",
       "         '2016-04-04 21:54:07': 1,\n",
       "         '2016-04-03 21:57:25': 1,\n",
       "         '2016-03-07 09:39:18': 2,\n",
       "         '2016-03-27 14:51:10': 1,\n",
       "         '2016-03-14 13:25:27': 1,\n",
       "         '2016-03-17 23:39:33': 2,\n",
       "         '2016-03-17 18:54:42': 2,\n",
       "         '2016-03-19 13:43:42': 1,\n",
       "         '2016-03-16 17:53:12': 1,\n",
       "         '2016-03-12 15:49:29': 2,\n",
       "         '2016-03-28 23:53:09': 2,\n",
       "         '2016-03-09 16:46:23': 3,\n",
       "         '2016-04-01 19:58:33': 2,\n",
       "         '2016-03-15 09:37:25': 1,\n",
       "         '2016-03-09 14:52:30': 2,\n",
       "         '2016-03-19 08:37:01': 2,\n",
       "         '2016-03-16 19:45:37': 2,\n",
       "         '2016-03-10 07:36:57': 1,\n",
       "         '2016-03-06 12:51:24': 1,\n",
       "         '2016-03-12 00:56:17': 1,\n",
       "         '2016-03-31 21:58:03': 1,\n",
       "         '2016-03-08 16:38:03': 2,\n",
       "         '2016-03-27 23:53:18': 2,\n",
       "         '2016-03-28 19:52:56': 1,\n",
       "         '2016-03-07 14:45:31': 2,\n",
       "         '2016-04-03 13:56:51': 1,\n",
       "         '2016-04-01 17:47:32': 1,\n",
       "         '2016-03-18 23:47:11': 2,\n",
       "         '2016-03-22 10:25:18': 2,\n",
       "         '2016-03-19 08:36:43': 1,\n",
       "         '2016-03-15 17:49:35': 1,\n",
       "         '2016-03-28 11:50:16': 1,\n",
       "         '2016-03-17 19:43:13': 2,\n",
       "         '2016-03-15 12:50:56': 1,\n",
       "         '2016-03-17 11:54:39': 1,\n",
       "         '2016-03-12 16:53:39': 1,\n",
       "         '2016-04-02 04:36:18': 1,\n",
       "         '2016-03-20 18:58:17': 1,\n",
       "         '2016-03-26 14:52:36': 2,\n",
       "         '2016-03-25 12:51:39': 1,\n",
       "         '2016-03-20 00:38:01': 1,\n",
       "         '2016-03-05 18:58:07': 2,\n",
       "         '2016-03-12 15:37:22': 2,\n",
       "         '2016-03-27 13:55:27': 1,\n",
       "         '2016-03-15 17:48:56': 1,\n",
       "         '2016-04-06 18:06:35': 1,\n",
       "         '2016-03-28 14:43:23': 1,\n",
       "         '2016-03-08 14:53:30': 2,\n",
       "         '2016-03-10 14:56:56': 1,\n",
       "         '2016-03-17 18:56:07': 1,\n",
       "         '2016-03-29 15:51:51': 1,\n",
       "         '2016-03-22 18:56:48': 1,\n",
       "         '2016-03-27 21:54:42': 1,\n",
       "         '2016-03-07 13:53:53': 1,\n",
       "         '2016-03-26 18:44:52': 1,\n",
       "         '2016-03-14 18:55:19': 1,\n",
       "         '2016-03-25 15:45:10': 1,\n",
       "         '2016-03-25 12:49:40': 1,\n",
       "         '2016-04-04 15:47:10': 2,\n",
       "         '2016-03-21 18:58:11': 1,\n",
       "         '2016-03-21 05:36:19': 1,\n",
       "         '2016-03-27 11:55:07': 1,\n",
       "         '2016-03-21 00:56:46': 2,\n",
       "         '2016-03-17 14:48:30': 2,\n",
       "         '2016-03-20 16:41:53': 2,\n",
       "         '2016-03-21 14:54:00': 1,\n",
       "         '2016-04-05 13:25:18': 2,\n",
       "         '2016-04-04 00:44:34': 2,\n",
       "         '2016-04-03 13:41:24': 1,\n",
       "         '2016-03-27 20:55:55': 2,\n",
       "         '2016-03-26 14:44:15': 2,\n",
       "         '2016-04-06 19:25:24': 1,\n",
       "         '2016-03-19 09:51:01': 1,\n",
       "         '2016-03-10 13:51:28': 1,\n",
       "         '2016-03-23 07:54:25': 1,\n",
       "         '2016-04-04 19:40:28': 1,\n",
       "         '2016-03-30 16:38:07': 1,\n",
       "         '2016-03-21 19:06:35': 1,\n",
       "         '2016-03-27 22:54:11': 1,\n",
       "         '2016-03-24 12:39:51': 1,\n",
       "         '2016-03-22 09:56:13': 1,\n",
       "         '2016-04-04 19:36:22': 2,\n",
       "         '2016-03-26 13:50:33': 2,\n",
       "         '2016-03-25 16:50:13': 1,\n",
       "         '2016-04-02 00:58:10': 1,\n",
       "         '2016-03-17 22:25:28': 1,\n",
       "         '2016-03-06 00:57:18': 1,\n",
       "         '2016-03-29 10:38:04': 1,\n",
       "         '2016-03-24 15:50:15': 3,\n",
       "         '2016-03-05 22:40:43': 1,\n",
       "         '2016-04-03 13:53:56': 2,\n",
       "         '2016-04-01 21:44:58': 1,\n",
       "         '2016-03-22 15:55:07': 2,\n",
       "         '2016-03-23 10:57:22': 1,\n",
       "         '2016-03-08 22:06:21': 2,\n",
       "         '2016-03-13 23:38:58': 1,\n",
       "         '2016-03-09 16:47:45': 1,\n",
       "         '2016-03-30 00:56:03': 1,\n",
       "         '2016-03-28 17:42:46': 1,\n",
       "         '2016-03-10 13:56:39': 1,\n",
       "         '2016-03-08 12:54:30': 1,\n",
       "         '2016-04-03 21:51:37': 1,\n",
       "         '2016-03-05 18:50:21': 1,\n",
       "         '2016-04-04 22:53:23': 1,\n",
       "         '2016-03-28 19:47:05': 1,\n",
       "         '2016-03-19 07:55:38': 2,\n",
       "         '2016-03-27 23:37:22': 1,\n",
       "         '2016-03-17 00:36:59': 1,\n",
       "         '2016-03-17 13:53:32': 1,\n",
       "         '2016-03-30 16:55:38': 2,\n",
       "         '2016-03-20 16:37:38': 2,\n",
       "         '2016-04-01 01:36:57': 1,\n",
       "         '2016-03-23 14:06:25': 1,\n",
       "         '2016-04-02 23:39:49': 2,\n",
       "         '2016-03-13 20:25:18': 1,\n",
       "         '2016-03-24 19:46:43': 2,\n",
       "         '2016-04-02 15:47:02': 1,\n",
       "         '2016-03-15 20:47:47': 1,\n",
       "         '2016-03-12 18:49:56': 2,\n",
       "         '2016-04-03 18:36:22': 2,\n",
       "         '2016-03-20 23:06:24': 1,\n",
       "         '2016-03-21 14:50:20': 3,\n",
       "         '2016-03-24 20:47:26': 1,\n",
       "         '2016-03-28 10:57:54': 1,\n",
       "         '2016-03-11 00:51:23': 1,\n",
       "         '2016-03-05 21:42:31': 1,\n",
       "         '2016-04-01 23:57:34': 2,\n",
       "         '2016-04-02 15:39:20': 1,\n",
       "         '2016-03-06 21:38:54': 1,\n",
       "         '2016-03-09 12:57:10': 1,\n",
       "         '2016-03-20 01:57:09': 1,\n",
       "         '2016-03-26 15:37:01': 2,\n",
       "         '2016-04-02 15:37:16': 1,\n",
       "         '2016-03-21 13:58:18': 3,\n",
       "         '2016-03-16 14:55:58': 1,\n",
       "         '2016-03-11 19:38:44': 2,\n",
       "         '2016-03-10 10:54:52': 1,\n",
       "         '2016-03-29 10:37:04': 1,\n",
       "         '2016-03-17 11:36:56': 1,\n",
       "         '2016-04-07 13:25:40': 1,\n",
       "         '2016-03-22 17:53:51': 1,\n",
       "         '2016-04-04 19:36:20': 1,\n",
       "         '2016-04-04 21:47:13': 1,\n",
       "         '2016-03-19 20:59:02': 1,\n",
       "         '2016-03-20 18:56:40': 2,\n",
       "         '2016-03-14 20:26:01': 2,\n",
       "         '2016-03-19 17:25:19': 1,\n",
       "         '2016-03-10 13:38:28': 2,\n",
       "         '2016-03-17 20:51:19': 1,\n",
       "         '2016-03-15 15:54:12': 1,\n",
       "         '2016-03-07 16:55:06': 1,\n",
       "         '2016-04-03 21:41:36': 1,\n",
       "         '2016-03-22 17:54:09': 2,\n",
       "         '2016-03-10 02:02:29': 1,\n",
       "         '2016-03-08 19:41:01': 2,\n",
       "         '2016-03-12 18:39:15': 1,\n",
       "         '2016-04-04 11:43:42': 1,\n",
       "         '2016-03-08 12:47:47': 1,\n",
       "         '2016-03-11 13:48:41': 1,\n",
       "         '2016-03-15 12:39:13': 1,\n",
       "         '2016-03-07 13:57:06': 1,\n",
       "         '2016-03-26 12:58:01': 1,\n",
       "         '2016-03-29 18:46:42': 1,\n",
       "         '2016-03-06 14:49:30': 1,\n",
       "         '2016-03-25 18:48:23': 2,\n",
       "         '2016-03-29 07:58:24': 1,\n",
       "         '2016-03-19 12:57:10': 1,\n",
       "         '2016-03-23 00:58:28': 3,\n",
       "         '2016-03-20 21:50:08': 1,\n",
       "         '2016-03-07 11:49:12': 1,\n",
       "         '2016-04-02 13:51:57': 1,\n",
       "         '2016-03-28 09:53:44': 2,\n",
       "         '2016-03-30 22:51:09': 2,\n",
       "         '2016-04-04 13:25:44': 1,\n",
       "         '2016-03-23 19:56:14': 2,\n",
       "         '2016-03-06 22:43:51': 1,\n",
       "         '2016-03-15 19:55:46': 1,\n",
       "         '2016-04-02 01:55:41': 2,\n",
       "         '2016-04-04 10:49:31': 1,\n",
       "         '2016-03-25 18:54:36': 2,\n",
       "         '2016-03-05 22:51:25': 1,\n",
       "         '2016-03-08 09:36:21': 2,\n",
       "         '2016-03-22 11:38:37': 2,\n",
       "         '2016-03-30 11:37:14': 4,\n",
       "         '2016-03-06 11:42:02': 1,\n",
       "         '2016-03-10 14:58:37': 2,\n",
       "         '2016-03-19 10:36:21': 2,\n",
       "         '2016-03-15 17:55:44': 1,\n",
       "         '2016-03-12 19:59:21': 2,\n",
       "         '2016-03-30 20:53:48': 1,\n",
       "         '2016-04-04 13:53:02': 1,\n",
       "         '2016-03-23 11:52:01': 2,\n",
       "         '2016-04-04 11:41:10': 1,\n",
       "         '2016-03-11 13:48:00': 1,\n",
       "         '2016-03-29 20:41:31': 2,\n",
       "         '2016-03-14 22:59:02': 1,\n",
       "         '2016-04-05 12:38:39': 1,\n",
       "         '2016-03-14 15:45:13': 1,\n",
       "         '2016-04-02 18:49:48': 4,\n",
       "         '2016-03-30 16:56:32': 2,\n",
       "         '2016-04-03 08:59:10': 2,\n",
       "         '2016-03-12 14:52:26': 1,\n",
       "         '2016-03-15 13:53:34': 1,\n",
       "         '2016-03-13 09:45:29': 2,\n",
       "         '2016-03-19 17:56:27': 2,\n",
       "         '2016-03-17 11:50:01': 3,\n",
       "         '2016-04-04 17:54:30': 1,\n",
       "         '2016-04-03 10:36:48': 1,\n",
       "         '2016-03-21 20:39:54': 1,\n",
       "         '2016-03-22 20:58:31': 2,\n",
       "         '2016-03-07 16:39:09': 1,\n",
       "         '2016-03-10 19:49:52': 1,\n",
       "         '2016-04-02 17:39:21': 1,\n",
       "         '2016-03-16 13:56:45': 1,\n",
       "         '2016-03-15 14:53:15': 1,\n",
       "         '2016-03-10 11:53:00': 1,\n",
       "         '2016-03-16 01:36:18': 3,\n",
       "         '2016-03-20 17:45:44': 2,\n",
       "         '2016-03-14 19:44:10': 2,\n",
       "         '2016-03-08 13:36:58': 3,\n",
       "         '2016-04-01 20:46:13': 1,\n",
       "         '2016-03-10 14:49:15': 2,\n",
       "         '2016-03-21 20:43:35': 1,\n",
       "         '2016-03-28 19:40:12': 1,\n",
       "         '2016-04-02 09:53:46': 1,\n",
       "         '2016-04-03 21:58:03': 1,\n",
       "         '2016-03-17 19:45:15': 1,\n",
       "         '2016-04-01 12:45:03': 1,\n",
       "         '2016-03-29 23:57:13': 1,\n",
       "         '2016-03-17 13:55:20': 1,\n",
       "         '2016-03-25 10:46:31': 1,\n",
       "         '2016-03-31 14:56:16': 1,\n",
       "         '2016-03-19 18:52:10': 2,\n",
       "         '2016-04-02 12:50:07': 1,\n",
       "         '2016-04-04 16:49:02': 1,\n",
       "         '2016-03-28 12:46:21': 2,\n",
       "         '2016-03-17 12:52:15': 2,\n",
       "         '2016-03-11 00:59:04': 3,\n",
       "         '2016-04-01 15:50:10': 1,\n",
       "         '2016-03-20 19:43:09': 1,\n",
       "         '2016-04-03 19:06:19': 1,\n",
       "         '2016-03-27 01:58:19': 1,\n",
       "         '2016-03-14 18:43:26': 1,\n",
       "         '2016-03-26 19:37:50': 2,\n",
       "         '2016-03-27 21:40:29': 1,\n",
       "         '2016-03-12 09:37:02': 1,\n",
       "         '2016-03-05 22:40:26': 2,\n",
       "         '2016-03-19 10:25:21': 1,\n",
       "         '2016-04-03 10:51:38': 1,\n",
       "         '2016-03-22 00:38:14': 2,\n",
       "         '2016-03-09 20:42:10': 1,\n",
       "         '2016-03-19 21:57:25': 1,\n",
       "         '2016-03-29 11:51:21': 1,\n",
       "         '2016-03-27 12:52:57': 1,\n",
       "         '2016-03-19 15:57:48': 1,\n",
       "         '2016-04-03 22:42:54': 2,\n",
       "         '2016-03-05 15:51:19': 1,\n",
       "         '2016-03-17 20:38:17': 1,\n",
       "         '2016-03-14 19:56:40': 1,\n",
       "         '2016-03-20 12:40:48': 2,\n",
       "         '2016-03-28 18:54:12': 1,\n",
       "         '2016-03-15 09:49:11': 1,\n",
       "         '2016-04-02 11:55:33': 2,\n",
       "         '2016-03-23 22:38:02': 1,\n",
       "         '2016-04-04 13:56:09': 1,\n",
       "         '2016-04-02 14:51:54': 1,\n",
       "         '2016-03-17 20:37:39': 1,\n",
       "         '2016-03-17 09:50:38': 1,\n",
       "         '2016-03-19 23:54:54': 1,\n",
       "         '2016-03-11 10:37:38': 1,\n",
       "         '2016-04-04 13:59:43': 1,\n",
       "         '2016-04-03 21:56:00': 1,\n",
       "         '2016-03-30 17:57:34': 2,\n",
       "         '2016-03-19 12:57:25': 3,\n",
       "         '2016-03-22 16:52:10': 1,\n",
       "         '2016-03-20 12:49:53': 2,\n",
       "         '2016-03-08 20:43:16': 1,\n",
       "         '2016-03-28 07:37:03': 1,\n",
       "         '2016-03-31 08:41:39': 1,\n",
       "         '2016-03-10 17:38:59': 1,\n",
       "         '2016-04-04 16:54:34': 1,\n",
       "         '2016-03-30 21:47:21': 1,\n",
       "         '2016-04-04 00:37:00': 1,\n",
       "         '2016-03-12 20:37:19': 1,\n",
       "         '2016-03-18 14:38:17': 1,\n",
       "         '2016-03-29 15:53:54': 1,\n",
       "         '2016-03-17 14:51:19': 1,\n",
       "         '2016-03-08 01:59:56': 1,\n",
       "         '2016-04-02 08:54:19': 1,\n",
       "         '2016-03-07 11:58:11': 1,\n",
       "         '2016-04-01 20:57:52': 1,\n",
       "         '2016-04-04 03:03:03': 2,\n",
       "         '2016-03-23 17:52:07': 1,\n",
       "         '2016-03-30 12:50:32': 1,\n",
       "         '2016-04-01 14:56:45': 1,\n",
       "         '2016-04-03 13:47:09': 2,\n",
       "         '2016-03-28 11:37:02': 1,\n",
       "         '2016-04-03 22:46:42': 3,\n",
       "         '2016-03-24 15:50:28': 1,\n",
       "         '2016-04-03 14:37:27': 2,\n",
       "         '2016-03-11 20:59:54': 1,\n",
       "         '2016-03-30 13:52:11': 1,\n",
       "         '2016-03-29 20:37:34': 2,\n",
       "         '2016-03-12 01:00:15': 2,\n",
       "         '2016-04-03 22:37:37': 2,\n",
       "         '2016-03-27 03:36:42': 2,\n",
       "         '2016-03-12 21:48:51': 1,\n",
       "         '2016-03-09 15:51:13': 2,\n",
       "         '2016-03-21 00:59:14': 2,\n",
       "         '2016-03-24 12:50:04': 1,\n",
       "         '2016-03-24 15:38:12': 2,\n",
       "         '2016-03-05 16:30:54': 2,\n",
       "         '2016-03-10 19:58:45': 1,\n",
       "         '2016-04-03 18:41:09': 3,\n",
       "         '2016-03-26 17:45:48': 1,\n",
       "         '2016-03-20 19:38:18': 1,\n",
       "         '2016-03-07 19:38:47': 1,\n",
       "         '2016-03-28 16:58:02': 1,\n",
       "         '2016-03-22 14:39:12': 1,\n",
       "         '2016-03-14 11:54:03': 1,\n",
       "         '2016-03-22 23:48:23': 1,\n",
       "         '2016-03-07 23:56:32': 3,\n",
       "         '2016-03-30 11:49:40': 1,\n",
       "         '2016-04-03 01:36:20': 3,\n",
       "         '2016-03-21 20:49:17': 3,\n",
       "         '2016-04-01 18:56:52': 1,\n",
       "         '2016-03-23 18:39:15': 3,\n",
       "         '2016-03-10 19:59:04': 1,\n",
       "         '2016-03-05 19:52:38': 2,\n",
       "         '2016-04-02 16:43:44': 1,\n",
       "         '2016-03-11 19:54:09': 1,\n",
       "         '2016-03-09 12:47:44': 2,\n",
       "         '2016-03-16 16:49:59': 2,\n",
       "         '2016-03-20 21:37:03': 1,\n",
       "         '2016-04-02 20:52:06': 1,\n",
       "         '2016-03-15 14:53:33': 2,\n",
       "         '2016-03-15 19:55:47': 1,\n",
       "         '2016-04-04 14:00:05': 1,\n",
       "         '2016-03-24 21:49:25': 1,\n",
       "         '2016-04-03 14:58:07': 2,\n",
       "         '2016-03-25 19:37:40': 1,\n",
       "         '2016-03-20 16:44:31': 2,\n",
       "         '2016-03-12 20:49:14': 1,\n",
       "         '2016-04-04 18:52:14': 1,\n",
       "         '2016-03-16 19:40:49': 1,\n",
       "         '2016-04-01 18:58:18': 1,\n",
       "         '2016-03-18 21:06:20': 2,\n",
       "         '2016-03-09 09:57:05': 2,\n",
       "         '2016-03-28 11:57:56': 1,\n",
       "         '2016-03-15 22:53:37': 2,\n",
       "         '2016-03-17 20:44:07': 2,\n",
       "         '2016-03-25 12:56:37': 1,\n",
       "         '2016-03-23 22:54:10': 1,\n",
       "         '2016-04-04 16:54:56': 1,\n",
       "         '2016-03-12 11:54:25': 3,\n",
       "         '2016-04-03 16:52:01': 2,\n",
       "         '2016-03-11 20:48:54': 1,\n",
       "         '2016-03-07 17:49:09': 1,\n",
       "         '2016-03-26 22:56:36': 1,\n",
       "         '2016-03-23 14:06:18': 1,\n",
       "         '2016-03-07 10:59:03': 1,\n",
       "         '2016-03-17 16:49:24': 1,\n",
       "         '2016-03-10 22:51:31': 1,\n",
       "         '2016-03-26 10:53:37': 1,\n",
       "         '2016-03-24 14:54:19': 2,\n",
       "         '2016-03-11 23:52:14': 1,\n",
       "         '2016-03-11 19:58:53': 1,\n",
       "         '2016-03-05 14:19:10': 2,\n",
       "         '2016-03-30 18:56:46': 1,\n",
       "         '2016-03-27 19:50:42': 2,\n",
       "         '2016-03-18 00:51:44': 1,\n",
       "         '2016-03-24 15:53:50': 1,\n",
       "         '2016-03-07 09:53:50': 1,\n",
       "         '2016-03-23 11:57:35': 1,\n",
       "         '2016-03-26 10:50:10': 1,\n",
       "         '2016-03-24 14:38:58': 3,\n",
       "         '2016-04-03 13:43:30': 1,\n",
       "         '2016-03-21 21:52:34': 2,\n",
       "         '2016-04-01 16:49:34': 1,\n",
       "         '2016-04-01 19:45:46': 1,\n",
       "         '2016-03-20 13:50:32': 1,\n",
       "         '2016-04-03 16:54:32': 2,\n",
       "         '2016-03-16 18:56:46': 2,\n",
       "         '2016-03-30 22:51:36': 1,\n",
       "         '2016-03-12 19:50:28': 1,\n",
       "         '2016-03-28 22:46:27': 1,\n",
       "         '2016-03-26 17:57:25': 2,\n",
       "         '2016-03-20 20:43:50': 1,\n",
       "         '2016-03-19 11:57:04': 2,\n",
       "         '2016-03-11 18:25:26': 2,\n",
       "         '2016-04-05 00:40:01': 1,\n",
       "         '2016-03-23 09:50:43': 1,\n",
       "         '2016-03-29 22:48:21': 1,\n",
       "         '2016-04-04 20:50:38': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "row_counter =  Counter([row[header[\"dateCrawled\"]] for row in cleaned_rows])\n",
    "row_counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713c479",
   "metadata": {},
   "source": [
    "Please note that for Exercise 2 you should work on the clean Used Cars dataset without the missing and invalid data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d2b65",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45310016",
   "metadata": {},
   "source": [
    "**Exercise 2.1**\n",
    "\n",
    "We consider the norm to be that, for a given type of vehicle, on average the price of diesel is greater than the one of benzine. Provide a representation of the data which shows if, and to which extent, the various vehicle types conform to the norm.\n",
    "What relationship are you showing? Please justify the choice of the representation and your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62bafef2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19cc6aac",
   "metadata": {},
   "source": [
    "Let's create a function that, given a fuel type, compute the average for each kind of vehicleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da4a509d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "vehicle_rows = [[]] * len(vehicle_type_counter)\n",
    "vehicle_header = {}\n",
    "for x in zip(range(len(vehicle_type_counter)), vehicle_type_counter):\n",
    "    vehicle_header[x[1]] = x[0]\n",
    "\n",
    "\n",
    "def get_average_list(fuel):\n",
    "    i = 0\n",
    "    avg_list = [0] * len(vehicle_type_counter)\n",
    "    for vehicle_name in vehicle_header:\n",
    "        value = [row[header[\"price\"]] for row in cleaned_rows if row[header[\"vehicleType\"]] == vehicle_name and row[header[\"fuelType\"]] == fuel]\n",
    "        i += len(value)\n",
    "        avg_list[vehicle_header[vehicle_name]] = sum(value) / len(value)\n",
    "    return avg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63f1f0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74074d46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11492.059403437815,\n",
       " 10384.563336306868,\n",
       " 3923.169211810757,\n",
       " 10626.006778331746,\n",
       " 5748.519752618958,\n",
       " 5907.006580519057,\n",
       " 5571.508269251027,\n",
       " 5641.296116504855]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " get_average_list(\"benzin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81a05d",
   "metadata": {},
   "source": [
    "Now let's see how many vehicleTypes we have in out dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb69a800",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['coupe', 'suv', 'kleinwagen', 'cabrio', 'bus', 'limousine', 'kombi', 'andere'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vehicle_type_counter.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64d9931",
   "metadata": {},
   "source": [
    "The idea is, since we want to do a simple nominal comparison between the average price of benzine and diesel vehicles, to plot the data with a barplot with two series. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c08a520",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1002\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1002\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "  \n",
       "\n",
       "  const inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1002\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1002\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "\n",
       "  <div class=\"bk-root\" id=\"0d979c9d-1668-4f04-a3eb-12d3b642f0c4\" data-root-id=\"1004\"></div>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  const docs_json = {\"9e7cea49-edef-4e2a-bbb5-96fca380a665\":{\"defs\":[],\"roots\":{\"references\":[{\"attributes\":{\"below\":[{\"id\":\"1015\"}],\"center\":[{\"id\":\"1017\"},{\"id\":\"1021\"},{\"id\":\"1053\"}],\"height\":350,\"left\":[{\"id\":\"1018\"}],\"renderers\":[{\"id\":\"1041\"},{\"id\":\"1060\"}],\"title\":{\"id\":\"1005\"},\"toolbar\":{\"id\":\"1029\"},\"width\":900,\"x_range\":{\"id\":\"1007\"},\"x_scale\":{\"id\":\"1011\"},\"y_range\":{\"id\":\"1009\"},\"y_scale\":{\"id\":\"1013\"}},\"id\":\"1004\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"label\":{\"value\":\"diesel\"},\"renderers\":[{\"id\":\"1041\"}]},\"id\":\"1054\",\"type\":\"LegendItem\"},{\"attributes\":{\"label\":{\"value\":\"benzine\"},\"renderers\":[{\"id\":\"1060\"}]},\"id\":\"1072\",\"type\":\"LegendItem\"},{\"attributes\":{\"source\":{\"id\":\"1003\"}},\"id\":\"1061\",\"type\":\"CDSView\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"items\":[{\"id\":\"1054\"},{\"id\":\"1072\"}],\"orientation\":\"horizontal\"},\"id\":\"1053\",\"type\":\"Legend\"},{\"attributes\":{},\"id\":\"1026\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"1027\",\"type\":\"HelpTool\"},{\"attributes\":{\"coordinates\":null,\"group\":null,\"text\":\"Average price of diesel vs benzine\"},\"id\":\"1005\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"1013\",\"type\":\"LinearScale\"},{\"attributes\":{\"axis_label\":\"Vechicle type\",\"coordinates\":null,\"formatter\":{\"id\":\"1048\"},\"group\":null,\"major_label_policy\":{\"id\":\"1049\"},\"ticker\":{\"id\":\"1016\"}},\"id\":\"1015\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"coordinates\":null,\"fill_alpha\":0.5,\"fill_color\":\"lightgrey\",\"group\":null,\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":1.0,\"line_color\":\"black\",\"line_dash\":[4,4],\"line_width\":2,\"right_units\":\"screen\",\"syncable\":false,\"top_units\":\"screen\"},\"id\":\"1028\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"1009\",\"type\":\"DataRange1d\"},{\"attributes\":{\"factors\":[\"coupe\",\"suv\",\"kleinwagen\",\"cabrio\",\"bus\",\"limousine\",\"kombi\",\"andere\"],\"range_padding\":0.1},\"id\":\"1007\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"1049\",\"type\":\"AllLabels\"},{\"attributes\":{\"axis\":{\"id\":\"1015\"},\"coordinates\":null,\"grid_line_color\":null,\"group\":null,\"ticker\":null},\"id\":\"1017\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1016\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"axis_label\":\"Average price in euro\",\"coordinates\":null,\"formatter\":{\"id\":\"1073\"},\"group\":null,\"major_label_policy\":{\"id\":\"1046\"},\"ticker\":{\"id\":\"1019\"}},\"id\":\"1018\",\"type\":\"LinearAxis\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1003\"},\"glyph\":{\"id\":\"1038\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1040\"},\"nonselection_glyph\":{\"id\":\"1039\"},\"view\":{\"id\":\"1042\"}},\"id\":\"1041\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"fill_color\":{\"value\":\"#2D3A3A\"},\"hatch_color\":{\"value\":\"#2D3A3A\"},\"line_color\":{\"value\":\"#2D3A3A\"},\"top\":{\"field\":\"diesel\"},\"width\":{\"value\":0.2},\"x\":{\"field\":\"vehicleType\",\"transform\":{\"id\":\"1036\"}}},\"id\":\"1038\",\"type\":\"VBar\"},{\"attributes\":{\"data\":{\"benzine\":[11492.059403437815,10384.563336306868,3923.169211810757,10626.006778331746,5748.519752618958,5907.006580519057,5571.508269251027,5641.296116504855],\"diesel\":[15332.008890606881,15895.7843866171,4818.085625859698,13412.914459161148,8198.861908906296,9499.870135886349,7981.154897783995,5571.089743589743],\"vehicleType\":[\"coupe\",\"suv\",\"kleinwagen\",\"cabrio\",\"bus\",\"limousine\",\"kombi\",\"andere\"]},\"selected\":{\"id\":\"1051\"},\"selection_policy\":{\"id\":\"1050\"}},\"id\":\"1003\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#86b049\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"value\":\"#86b049\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#86b049\"},\"top\":{\"field\":\"benzine\"},\"width\":{\"value\":0.2},\"x\":{\"field\":\"vehicleType\",\"transform\":{\"id\":\"1055\"}}},\"id\":\"1058\",\"type\":\"VBar\"},{\"attributes\":{\"range\":{\"id\":\"1007\"}},\"id\":\"1055\",\"type\":\"Dodge\"},{\"attributes\":{},\"id\":\"1011\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"range\":{\"id\":\"1007\"},\"value\":-0.25},\"id\":\"1036\",\"type\":\"Dodge\"},{\"attributes\":{\"tools\":[{\"id\":\"1022\"},{\"id\":\"1023\"},{\"id\":\"1024\"},{\"id\":\"1025\"},{\"id\":\"1026\"},{\"id\":\"1027\"}]},\"id\":\"1029\",\"type\":\"Toolbar\"},{\"attributes\":{\"fill_color\":{\"value\":\"#86b049\"},\"hatch_color\":{\"value\":\"#86b049\"},\"line_color\":{\"value\":\"#86b049\"},\"top\":{\"field\":\"benzine\"},\"width\":{\"value\":0.2},\"x\":{\"field\":\"vehicleType\",\"transform\":{\"id\":\"1055\"}}},\"id\":\"1057\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1025\",\"type\":\"SaveTool\"},{\"attributes\":{\"overlay\":{\"id\":\"1028\"}},\"id\":\"1024\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"1022\",\"type\":\"PanTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#86b049\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"value\":\"#86b049\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#86b049\"},\"top\":{\"field\":\"benzine\"},\"width\":{\"value\":0.2},\"x\":{\"field\":\"vehicleType\",\"transform\":{\"id\":\"1055\"}}},\"id\":\"1059\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1023\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"1051\",\"type\":\"Selection\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#2D3A3A\"},\"hatch_alpha\":{\"value\":0.1},\"hatch_color\":{\"value\":\"#2D3A3A\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#2D3A3A\"},\"top\":{\"field\":\"diesel\"},\"width\":{\"value\":0.2},\"x\":{\"field\":\"vehicleType\",\"transform\":{\"id\":\"1036\"}}},\"id\":\"1039\",\"type\":\"VBar\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.2},\"fill_color\":{\"value\":\"#2D3A3A\"},\"hatch_alpha\":{\"value\":0.2},\"hatch_color\":{\"value\":\"#2D3A3A\"},\"line_alpha\":{\"value\":0.2},\"line_color\":{\"value\":\"#2D3A3A\"},\"top\":{\"field\":\"diesel\"},\"width\":{\"value\":0.2},\"x\":{\"field\":\"vehicleType\",\"transform\":{\"id\":\"1036\"}}},\"id\":\"1040\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"1046\",\"type\":\"AllLabels\"},{\"attributes\":{},\"id\":\"1019\",\"type\":\"BasicTicker\"},{\"attributes\":{\"axis\":{\"id\":\"1018\"},\"coordinates\":null,\"dimension\":1,\"group\":null,\"ticker\":null},\"id\":\"1021\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"1073\",\"type\":\"NumeralTickFormatter\"},{\"attributes\":{},\"id\":\"1048\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"1050\",\"type\":\"UnionRenderers\"},{\"attributes\":{\"coordinates\":null,\"data_source\":{\"id\":\"1003\"},\"glyph\":{\"id\":\"1057\"},\"group\":null,\"hover_glyph\":null,\"muted_glyph\":{\"id\":\"1059\"},\"nonselection_glyph\":{\"id\":\"1058\"},\"view\":{\"id\":\"1061\"}},\"id\":\"1060\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"source\":{\"id\":\"1003\"}},\"id\":\"1042\",\"type\":\"CDSView\"}],\"root_ids\":[\"1004\"]},\"title\":\"Bokeh Application\",\"version\":\"2.4.2\"}};\n",
       "  const render_items = [{\"docid\":\"9e7cea49-edef-4e2a-bbb5-96fca380a665\",\"root_ids\":[\"1004\"],\"roots\":{\"1004\":\"0d979c9d-1668-4f04-a3eb-12d3b642f0c4\"}}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    let attempts = 0;\n",
       "    const timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else {\n",
       "        attempts++;\n",
       "        if (attempts > 100) {\n",
       "          clearInterval(timer);\n",
       "          console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\");\n",
       "        }\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "1004"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.transform import dodge\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.models.formatters import NumeralTickFormatter\n",
    "output_notebook()\n",
    "\n",
    "vehicle_types = ['coupe', 'suv', 'kleinwagen', 'cabrio', 'bus', 'limousine', 'kombi', 'andere']\n",
    "fuel_type = ['diesel', 'benzine']\n",
    "\n",
    "data = {'vehicleType' : vehicle_types,\n",
    "        'diesel'   : get_average_list(\"diesel\"),\n",
    "        'benzine'   : get_average_list(\"benzin\")}\n",
    "\n",
    "source = ColumnDataSource(data=data)\n",
    "\n",
    "p = figure(x_range=vehicle_types, title=\"Average price of diesel vs benzine\",\n",
    "           height=350, width=900)\n",
    "\n",
    "\n",
    "p.vbar(x=dodge('vehicleType', -0.25, range=p.x_range), top='diesel', source=source,\n",
    "       width=0.2, color=\"#2D3A3A\", legend_label=\"diesel\")\n",
    "\n",
    "p.vbar(x=dodge('vehicleType',  0.0,  range=p.x_range), top='benzine', source=source,\n",
    "       width=0.2, color=\"#86b049\", legend_label=\"benzine\")\n",
    "\n",
    "p.yaxis.formatter = NumeralTickFormatter(format=\"0,0\") \n",
    "\n",
    "p.x_range.range_padding = 0.1\n",
    "p.xgrid.grid_line_color = None\n",
    "p.legend.location = \"top_right\"\n",
    "p.legend.orientation = \"horizontal\"\n",
    "p.xaxis.axis_label = \"Vechicle type\"\n",
    "p.yaxis.axis_label = \"Average price in euro\"\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34df424",
   "metadata": {},
   "source": [
    "The plot clearly states that, on average, the price of diesel vehicles is higher than the price of diesel ones. Moreover, that seems to be true for all the vehicle types in this dataset except for the type \"andere\" in which the average price appears to be very close. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1464468",
   "metadata": {},
   "source": [
    "The choice of the colors is not a coincidence; I choose green for the benzine and dark grey for the diesel because they seem to be a standard on fuel stations. I carefully chose these colors, avoiding a higher contrast or a higher saturation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a784d4",
   "metadata": {},
   "source": [
    "**Exercise 2.2**\n",
    "\n",
    "Find an appropriate way to show and compare the range of prices for the following ```brand```: *mercedes_benz*, *fiat*, *volvo*, *alfa_romeo* and *lancia*. Create a suitable graphical representation of this data. What kind of relationship are you showing? Describe what can be understood from the plot. Please justify your answer and your choice of the graphical representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943c0da6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now the focus is on the distribution of the prices of some brands. Let's filter the brand for the required ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "924047ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "involved_brands = [\"mercedes_benz\", \"volvo\", \"alfa_romeo\", \"fiat\", \"lancia\"]\n",
    "brand_prices = [0] * len(involved_brands)\n",
    "\n",
    "brand_price_header = {}\n",
    "for x in zip(range(len(involved_brands)), involved_brands):\n",
    "    brand_price_header[x[1]] = x[0]\n",
    "\n",
    "\n",
    "for brand_name in involved_brands:\n",
    "        brand_prices[brand_price_header[brand_name]] = [row[header[\"price\"]] for row in cleaned_rows if row[header[\"brand\"]] == brand_name]\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e4ff12",
   "metadata": {},
   "source": [
    "And, since we are talking about distributions, display them with a boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82209c8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "\n",
    "title = \"Distribution of brand prices\"\n",
    "\n",
    "plt.subplots(figsize=(15,5))\n",
    "plt.title(title)\n",
    "bp = sns.boxplot(data=brand_prices, orient=\"horizontal\")\n",
    "\n",
    "bp.set_yticklabels(involved_brands);\n",
    "\n",
    "bp.set_xlabel(\"Price\", fontsize = 15)\n",
    "bp.set_ylabel(\"Brand\", fontsize = 15)\n",
    "\n",
    "bp.set_xticks([x for x in range(0, 106000, 5000)])\n",
    "bp.xaxis.set_major_formatter(ticker.FuncFormatter(lambda x, pos: '{:,.0f}'.format(x/1000) + 'K'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e8b065",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "source": [
    "As we can immediately notice, the range price of Mercedes is the widest in this graph, starting from 1000 euros to more than 100K. However, prices are not evenly distributed across the entire range, but most of them reside under 25/27K euros.  \n",
    "Half of the Mercedes are sold at a price between 1K (constraint of the first exercise) and 6/7K, and this is the best value, especially if we compare it with Lancia, where half of them are sold at less than 2.5/3K. \n",
    "From this graph we can infer lots of other things, but the last thing that I want to mention is that all of the brands seem to have a higher spread on higher prices. If we look at the range between the median and the third quartile, it is always higher than the range between the first quartile and the median. \n",
    "At the end we can state that Fiat and Lancia have the lowest prices, followed by Alfa Romeo and Volvo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358847e7",
   "metadata": {},
   "source": [
    "---\n",
    "### Exercise 3: Data visualization and exploration (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b41933",
   "metadata": {},
   "source": [
    "Create an interactive dashboard composed of two tabs using Bokeh visualization library. The requirements for each tab are described as following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc01b05a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"1129\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  const force = true;\n",
       "\n",
       "  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  const JS_MIME_TYPE = 'application/javascript';\n",
       "  const HTML_MIME_TYPE = 'text/html';\n",
       "  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  const CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    const script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    const cell = handle.cell;\n",
       "\n",
       "    const id = cell.output_area._bokeh_element_id;\n",
       "    const server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id != null && id in Bokeh.index) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd_clean, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            const id = msg.content.text.trim();\n",
       "            if (id in Bokeh.index) {\n",
       "              Bokeh.index[id].model.document.clear();\n",
       "              delete Bokeh.index[id];\n",
       "            }\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd_destroy);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    const output_area = handle.output_area;\n",
       "    const output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      const bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      const script_attrs = bk_div.children[0].attributes;\n",
       "      for (let i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      const toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    const events = require('base/js/events');\n",
       "    const OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  const NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    const el = document.getElementById(\"1129\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) {\n",
       "        if (callback != null)\n",
       "          callback();\n",
       "      });\n",
       "    } finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.debug(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(css_urls, js_urls, callback) {\n",
       "    if (css_urls == null) css_urls = [];\n",
       "    if (js_urls == null) js_urls = [];\n",
       "\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = css_urls.length + js_urls.length;\n",
       "\n",
       "    function on_load() {\n",
       "      root._bokeh_is_loading--;\n",
       "      if (root._bokeh_is_loading === 0) {\n",
       "        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n",
       "        run_callbacks()\n",
       "      }\n",
       "    }\n",
       "\n",
       "    function on_error(url) {\n",
       "      console.error(\"failed to load \" + url);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < css_urls.length; i++) {\n",
       "      const url = css_urls[i];\n",
       "      const element = document.createElement(\"link\");\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.rel = \"stylesheet\";\n",
       "      element.type = \"text/css\";\n",
       "      element.href = url;\n",
       "      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n",
       "      document.body.appendChild(element);\n",
       "    }\n",
       "\n",
       "    for (let i = 0; i < js_urls.length; i++) {\n",
       "      const url = js_urls[i];\n",
       "      const element = document.createElement('script');\n",
       "      element.onload = on_load;\n",
       "      element.onerror = on_error.bind(null, url);\n",
       "      element.async = false;\n",
       "      element.src = url;\n",
       "      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.head.appendChild(element);\n",
       "    }\n",
       "  };\n",
       "\n",
       "  function inject_raw_css(css) {\n",
       "    const element = document.createElement(\"style\");\n",
       "    element.appendChild(document.createTextNode(css));\n",
       "    document.body.appendChild(element);\n",
       "  }\n",
       "\n",
       "  \n",
       "  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n",
       "  const css_urls = [];\n",
       "  \n",
       "\n",
       "  const inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    function(Bokeh) {\n",
       "    \n",
       "    \n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if (root.Bokeh !== undefined || force === true) {\n",
       "      \n",
       "    for (let i = 0; i < inline_js.length; i++) {\n",
       "      inline_js[i].call(root, root.Bokeh);\n",
       "    }\n",
       "    if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      const cell = $(document.getElementById(\"1129\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(css_urls, js_urls, function() {\n",
       "      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(\"1129\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  \n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.2.min.js\"];\n  const css_urls = [];\n  \n\n  const inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    function(Bokeh) {\n    \n    \n    }\n  ];\n\n  function run_inline_js() {\n    \n    if (root.Bokeh !== undefined || force === true) {\n      \n    for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n    if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(\"1129\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import csv\n",
    "from bokeh.models.widgets import Select, Slider, DateRangeSlider, MultiSelect\n",
    "\n",
    "from bokeh.layouts import row\n",
    "from bokeh.application import Application\n",
    "from bokeh.application.handlers import FunctionHandler\n",
    "from bokeh.models.ranges import Range1d\n",
    "from bokeh.palettes import Category20c\n",
    "from bokeh.models import ColumnDataSource\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.transform import dodge\n",
    "from bokeh.transform import factor_cmap\n",
    "from bokeh.plotting import figure, show, output_notebook, output_file, reset_output\n",
    "from bokeh.models import ColumnDataSource, CategoricalColorMapper, PrintfTickFormatter, NumeralTickFormatter, Legend, CDSView, GroupFilter, CustomJS, BoxSelectTool, FactorRange\n",
    "from bokeh.layouts import gridplot, column\n",
    "from bokeh.models.widgets import Div\n",
    "from bokeh.palettes import Spectral6, Pastel1, Category20c, Inferno256\n",
    "from bokeh.layouts import layout\n",
    "from bokeh.models import Panel, Tabs, FuncTickFormatter\n",
    "\n",
    "from collections import defaultdict\n",
    "from scipy import stats\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "output_notebook()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6c446ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Respondent': 0,\n",
       " 'Hobby': 1,\n",
       " 'OpenSource': 2,\n",
       " 'Country': 3,\n",
       " 'Student': 4,\n",
       " 'Employment': 5,\n",
       " 'FormalEducation': 6,\n",
       " 'UndergradMajor': 7,\n",
       " 'CompanySize': 8,\n",
       " 'DevType': 9,\n",
       " 'YearsCoding': 10,\n",
       " 'YearsCodingProf': 11,\n",
       " 'JobSatisfaction': 12,\n",
       " 'CareerSatisfaction': 13,\n",
       " 'HopeFiveYears': 14,\n",
       " 'JobSearchStatus': 15,\n",
       " 'LastNewJob': 16,\n",
       " 'AssessJob1': 17,\n",
       " 'AssessJob2': 18,\n",
       " 'AssessJob3': 19,\n",
       " 'AssessJob4': 20,\n",
       " 'AssessJob5': 21,\n",
       " 'AssessJob6': 22,\n",
       " 'AssessJob7': 23,\n",
       " 'AssessJob8': 24,\n",
       " 'AssessJob9': 25,\n",
       " 'AssessJob10': 26,\n",
       " 'AssessBenefits1': 27,\n",
       " 'AssessBenefits2': 28,\n",
       " 'AssessBenefits3': 29,\n",
       " 'AssessBenefits4': 30,\n",
       " 'AssessBenefits5': 31,\n",
       " 'AssessBenefits6': 32,\n",
       " 'AssessBenefits7': 33,\n",
       " 'AssessBenefits8': 34,\n",
       " 'AssessBenefits9': 35,\n",
       " 'AssessBenefits10': 36,\n",
       " 'AssessBenefits11': 37,\n",
       " 'JobContactPriorities1': 38,\n",
       " 'JobContactPriorities2': 39,\n",
       " 'JobContactPriorities3': 40,\n",
       " 'JobContactPriorities4': 41,\n",
       " 'JobContactPriorities5': 42,\n",
       " 'JobEmailPriorities1': 43,\n",
       " 'JobEmailPriorities2': 44,\n",
       " 'JobEmailPriorities3': 45,\n",
       " 'JobEmailPriorities4': 46,\n",
       " 'JobEmailPriorities5': 47,\n",
       " 'JobEmailPriorities6': 48,\n",
       " 'JobEmailPriorities7': 49,\n",
       " 'UpdateCV': 50,\n",
       " 'Currency': 51,\n",
       " 'Salary': 52,\n",
       " 'SalaryType': 53,\n",
       " 'ConvertedSalary': 54,\n",
       " 'CurrencySymbol': 55,\n",
       " 'CommunicationTools': 56,\n",
       " 'TimeFullyProductive': 57,\n",
       " 'EducationTypes': 58,\n",
       " 'SelfTaughtTypes': 59,\n",
       " 'TimeAfterBootcamp': 60,\n",
       " 'HackathonReasons': 61,\n",
       " 'AgreeDisagree1': 62,\n",
       " 'AgreeDisagree2': 63,\n",
       " 'AgreeDisagree3': 64,\n",
       " 'LanguageWorkedWith': 65,\n",
       " 'LanguageDesireNextYear': 66,\n",
       " 'DatabaseWorkedWith': 67,\n",
       " 'DatabaseDesireNextYear': 68,\n",
       " 'PlatformWorkedWith': 69,\n",
       " 'PlatformDesireNextYear': 70,\n",
       " 'FrameworkWorkedWith': 71,\n",
       " 'FrameworkDesireNextYear': 72,\n",
       " 'IDE': 73,\n",
       " 'OperatingSystem': 74,\n",
       " 'NumberMonitors': 75,\n",
       " 'Methodology': 76,\n",
       " 'VersionControl': 77,\n",
       " 'CheckInCode': 78,\n",
       " 'AdBlocker': 79,\n",
       " 'AdBlockerDisable': 80,\n",
       " 'AdBlockerReasons': 81,\n",
       " 'AdsAgreeDisagree1': 82,\n",
       " 'AdsAgreeDisagree2': 83,\n",
       " 'AdsAgreeDisagree3': 84,\n",
       " 'AdsActions': 85,\n",
       " 'AdsPriorities1': 86,\n",
       " 'AdsPriorities2': 87,\n",
       " 'AdsPriorities3': 88,\n",
       " 'AdsPriorities4': 89,\n",
       " 'AdsPriorities5': 90,\n",
       " 'AdsPriorities6': 91,\n",
       " 'AdsPriorities7': 92,\n",
       " 'AIDangerous': 93,\n",
       " 'AIInteresting': 94,\n",
       " 'AIResponsible': 95,\n",
       " 'AIFuture': 96,\n",
       " 'EthicsChoice': 97,\n",
       " 'EthicsReport': 98,\n",
       " 'EthicsResponsible': 99,\n",
       " 'EthicalImplications': 100,\n",
       " 'StackOverflowRecommend': 101,\n",
       " 'StackOverflowVisit': 102,\n",
       " 'StackOverflowHasAccount': 103,\n",
       " 'StackOverflowParticipate': 104,\n",
       " 'StackOverflowJobs': 105,\n",
       " 'StackOverflowDevStory': 106,\n",
       " 'StackOverflowJobsRecommend': 107,\n",
       " 'StackOverflowConsiderMember': 108,\n",
       " 'HypotheticalTools1': 109,\n",
       " 'HypotheticalTools2': 110,\n",
       " 'HypotheticalTools3': 111,\n",
       " 'HypotheticalTools4': 112,\n",
       " 'HypotheticalTools5': 113,\n",
       " 'WakeTime': 114,\n",
       " 'HoursComputer': 115,\n",
       " 'HoursOutside': 116,\n",
       " 'SkipMeals': 117,\n",
       " 'ErgonomicDevices': 118,\n",
       " 'Exercise': 119,\n",
       " 'Gender': 120,\n",
       " 'SexualOrientation': 121,\n",
       " 'EducationParents': 122,\n",
       " 'RaceEthnicity': 123,\n",
       " 'Age': 124,\n",
       " 'Dependents': 125,\n",
       " 'MilitaryUS': 126,\n",
       " 'SurveyTooLong': 127,\n",
       " 'SurveyEasy': 128}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "with open(\"survey_results_public.csv\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    survey_results_public_rows = [row for row in csvreader]\n",
    "\n",
    "with open(\"survey_results_schema.csv\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    survey_results_schema_rows = [row for row in csvreader]\n",
    "\n",
    "survey_results_header = {}\n",
    "survey_results_schema_rows.pop(0)\n",
    "survey_results_public_rows.pop(0)\n",
    "for x in zip(range(len(survey_results_schema_rows)), [row[0] for row in survey_results_schema_rows]):\n",
    "    survey_results_header[x[1]] = x[0]\n",
    "\n",
    "survey_results_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a090b96",
   "metadata": {},
   "source": [
    "#### Tab 1: Stack Overflow 2018 Developer Survey dataset\n",
    "- Show a graphical or tabular representation of the number of respondents by degree (column *FormalEducation*) and undergraduate major  (column *UndergradMajor*). The representation should allow to look up the number of respondents for a given degree and  undergraduate major (for example bachelor degree, mathematics or statistics)\n",
    "- Include one or more charts that show the trend between average salary (column ConvertedSalary) by company size and by wake time (for wake time, only include the time between 5am and 12pm).\n",
    "- The dashboard should be interactive. For instance, in the case of a bar plot, the plot should show the tooltip with the number of data points a bar represents. \n",
    "- Provide a way to interactively filter the data based on the respondents’ country (it should be possible to select multiple countries at the same time). This implies that all the components in the first tab should be updated according to the filter. Do you see a trend in the avg salary per wake time when you look at the data for all the countries? How does it compare with data from Italy only? And from Poland only?\n",
    "- Note that you should keep only posts where respondents' salary is not missing.\n",
    "\n",
    "*Note that you should not use DataCube for this exercise since there is an open issue in Bokeh to update the DataCube if the column data source is updated. For further information on this issue please refer to: https://github.com/bokeh/bokeh/issues/9657*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907aa694",
   "metadata": {},
   "source": [
    "# Tab 1\n",
    "\n",
    "Now we want to create the plots required in the first tab\n",
    "\n",
    "### 0) Country filter\n",
    "\n",
    "Since all plots of the first tab depend on a country filter, we want to create a list with all available country values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4304aa30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_label = list(set([row[survey_results_header[\"Country\"]] for row in survey_results_public_rows]))\n",
    "countries_label = [row for row in countries_label if row != \"NA\"]\n",
    "len(countries_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "833a2ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan',\n",
       " 'Albania',\n",
       " 'Algeria',\n",
       " 'Andorra',\n",
       " 'Angola',\n",
       " 'Antigua and Barbuda',\n",
       " 'Argentina',\n",
       " 'Armenia',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Azerbaijan',\n",
       " 'Bahamas',\n",
       " 'Bahrain',\n",
       " 'Bangladesh',\n",
       " 'Barbados',\n",
       " 'Belarus',\n",
       " 'Belgium',\n",
       " 'Belize',\n",
       " 'Benin',\n",
       " 'Bhutan',\n",
       " 'Bolivia',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Botswana',\n",
       " 'Brazil',\n",
       " 'Brunei Darussalam',\n",
       " 'Bulgaria',\n",
       " 'Burkina Faso',\n",
       " 'Burundi',\n",
       " 'Cambodia',\n",
       " 'Cameroon',\n",
       " 'Canada',\n",
       " 'Cape Verde',\n",
       " 'Central African Republic',\n",
       " 'Chile',\n",
       " 'China',\n",
       " 'Colombia',\n",
       " 'Congo, Republic of the...',\n",
       " 'Costa Rica',\n",
       " 'Croatia',\n",
       " 'Cuba',\n",
       " 'Cyprus',\n",
       " 'Czech Republic',\n",
       " \"Côte d'Ivoire\",\n",
       " \"Democratic People's Republic of Korea\",\n",
       " 'Democratic Republic of the Congo',\n",
       " 'Denmark',\n",
       " 'Djibouti',\n",
       " 'Dominica',\n",
       " 'Dominican Republic',\n",
       " 'Ecuador',\n",
       " 'Egypt',\n",
       " 'El Salvador',\n",
       " 'Eritrea',\n",
       " 'Estonia',\n",
       " 'Ethiopia',\n",
       " 'Fiji',\n",
       " 'Finland',\n",
       " 'France',\n",
       " 'Gabon',\n",
       " 'Gambia',\n",
       " 'Georgia',\n",
       " 'Germany',\n",
       " 'Ghana',\n",
       " 'Greece',\n",
       " 'Grenada',\n",
       " 'Guatemala',\n",
       " 'Guinea',\n",
       " 'Guinea-Bissau',\n",
       " 'Guyana',\n",
       " 'Haiti',\n",
       " 'Honduras',\n",
       " 'Hong Kong (S.A.R.)',\n",
       " 'Hungary',\n",
       " 'Iceland',\n",
       " 'India',\n",
       " 'Indonesia',\n",
       " 'Iran, Islamic Republic of...',\n",
       " 'Iraq',\n",
       " 'Ireland',\n",
       " 'Israel',\n",
       " 'Italy',\n",
       " 'Jamaica',\n",
       " 'Japan',\n",
       " 'Jordan',\n",
       " 'Kazakhstan',\n",
       " 'Kenya',\n",
       " 'Kuwait',\n",
       " 'Kyrgyzstan',\n",
       " 'Latvia',\n",
       " 'Lebanon',\n",
       " 'Lesotho',\n",
       " 'Liberia',\n",
       " 'Libyan Arab Jamahiriya',\n",
       " 'Liechtenstein',\n",
       " 'Lithuania',\n",
       " 'Luxembourg',\n",
       " 'Madagascar',\n",
       " 'Malawi',\n",
       " 'Malaysia',\n",
       " 'Maldives',\n",
       " 'Mali',\n",
       " 'Malta',\n",
       " 'Marshall Islands',\n",
       " 'Mauritania',\n",
       " 'Mauritius',\n",
       " 'Mexico',\n",
       " 'Micronesia, Federated States of...',\n",
       " 'Monaco',\n",
       " 'Mongolia',\n",
       " 'Montenegro',\n",
       " 'Morocco',\n",
       " 'Mozambique',\n",
       " 'Myanmar',\n",
       " 'Namibia',\n",
       " 'Nauru',\n",
       " 'Nepal',\n",
       " 'Netherlands',\n",
       " 'New Zealand',\n",
       " 'Nicaragua',\n",
       " 'Niger',\n",
       " 'Nigeria',\n",
       " 'North Korea',\n",
       " 'Norway',\n",
       " 'Oman',\n",
       " 'Other Country (Not Listed Above)',\n",
       " 'Pakistan',\n",
       " 'Palau',\n",
       " 'Panama',\n",
       " 'Paraguay',\n",
       " 'Peru',\n",
       " 'Philippines',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'Qatar',\n",
       " 'Republic of Korea',\n",
       " 'Republic of Moldova',\n",
       " 'Romania',\n",
       " 'Russian Federation',\n",
       " 'Rwanda',\n",
       " 'Saint Lucia',\n",
       " 'San Marino',\n",
       " 'Saudi Arabia',\n",
       " 'Senegal',\n",
       " 'Serbia',\n",
       " 'Sierra Leone',\n",
       " 'Singapore',\n",
       " 'Slovakia',\n",
       " 'Slovenia',\n",
       " 'Solomon Islands',\n",
       " 'Somalia',\n",
       " 'South Africa',\n",
       " 'South Korea',\n",
       " 'Spain',\n",
       " 'Sri Lanka',\n",
       " 'Sudan',\n",
       " 'Suriname',\n",
       " 'Swaziland',\n",
       " 'Sweden',\n",
       " 'Switzerland',\n",
       " 'Syrian Arab Republic',\n",
       " 'Taiwan',\n",
       " 'Tajikistan',\n",
       " 'Thailand',\n",
       " 'The former Yugoslav Republic of Macedonia',\n",
       " 'Timor-Leste',\n",
       " 'Togo',\n",
       " 'Trinidad and Tobago',\n",
       " 'Tunisia',\n",
       " 'Turkey',\n",
       " 'Turkmenistan',\n",
       " 'Uganda',\n",
       " 'Ukraine',\n",
       " 'United Arab Emirates',\n",
       " 'United Kingdom',\n",
       " 'United Republic of Tanzania',\n",
       " 'United States',\n",
       " 'Uruguay',\n",
       " 'Uzbekistan',\n",
       " 'Venezuela, Bolivarian Republic of...',\n",
       " 'Viet Nam',\n",
       " 'Yemen',\n",
       " 'Zambia',\n",
       " 'Zimbabwe']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_label = sorted(countries_label)\n",
    "countries_label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7875c66c",
   "metadata": {},
   "source": [
    "### 1) Graphical representation of the number of the number of respondents by degree and undergraduate\n",
    "\n",
    "First, let's check the values of those columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e3389dc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Bachelor’s degree (BA, BS, B.Eng., etc.)': 43659,\n",
       "         'Associate degree': 2970,\n",
       "         'Some college/university study without earning a degree': 11710,\n",
       "         'Master’s degree (MA, MS, M.Eng., MBA, etc.)': 21396,\n",
       "         'Secondary school (e.g. American high school, German Realschule or Gymnasium, etc.)': 8951,\n",
       "         'NA': 4152,\n",
       "         'Primary/elementary school': 1656,\n",
       "         'Professional degree (JD, MD, etc.)': 1447,\n",
       "         'I never completed any formal education': 700,\n",
       "         'Other doctoral degree (Ph.D, Ed.D., etc.)': 2214})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[survey_results_header[\"FormalEducation\"]] for row in survey_results_public_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d681c58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Mathematics or statistics': 2818,\n",
       "         'A natural science (ex. biology, chemistry, physics)': 3050,\n",
       "         'Computer science, computer engineering, or software engineering': 50336,\n",
       "         'Fine arts or performing arts (ex. graphic design, music, studio art)': 1135,\n",
       "         'Information systems, information technology, or system administration': 6507,\n",
       "         'Another engineering discipline (ex. civil, electrical, mechanical)': 6945,\n",
       "         'NA': 19819,\n",
       "         'A business discipline (ex. accounting, finance, marketing)': 1921,\n",
       "         'A social science (ex. anthropology, psychology, political science)': 1377,\n",
       "         'Web development or web design': 2418,\n",
       "         'A humanities discipline (ex. literature, history, philosophy)': 1590,\n",
       "         'A health science (ex. nursing, pharmacy, radiology)': 246,\n",
       "         'I never declared a major': 693})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[survey_results_header[\"UndergradMajor\"]] for row in survey_results_public_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750dc7df",
   "metadata": {},
   "source": [
    "Now we can remove the invalid values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5640d04f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "79036"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tab_1_graph_1_row_valid(row):\n",
    "    if row[survey_results_header[\"UndergradMajor\"]] == \"NA\" or row[survey_results_header[\"FormalEducation\"]] == \"NA\":\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "tab_1_graph_1_row = [row for row in survey_results_public_rows if tab_1_graph_1_row_valid(row)]\n",
    "len(tab_1_graph_1_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de24e2c",
   "metadata": {},
   "source": [
    "Now we can write the funciton to get the first plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e43b381",
   "metadata": {},
   "source": [
    "Let's get a list of entries for the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f291d50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bachelor’s degree (BA, BS, B.Eng., etc.)',\n",
       " 'Associate degree',\n",
       " 'Some college/university study without earning a degree',\n",
       " 'Master’s degree (MA, MS, M.Eng., MBA, etc.)',\n",
       " 'Professional degree (JD, MD, etc.)',\n",
       " 'Other doctoral degree (Ph.D, Ed.D., etc.)']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formal_education_labels = list(Counter([row[survey_results_header[\"FormalEducation\"]] for row in tab_1_graph_1_row]))\n",
    "formal_education_labels = [row for row in formal_education_labels if row != \"NA\"]\n",
    "formal_education_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "52f9b4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Afghanistan',\n",
       " 'Albania',\n",
       " 'Algeria',\n",
       " 'Andorra',\n",
       " 'Angola',\n",
       " 'Antigua and Barbuda',\n",
       " 'Argentina',\n",
       " 'Armenia',\n",
       " 'Australia',\n",
       " 'Austria',\n",
       " 'Azerbaijan',\n",
       " 'Bahamas',\n",
       " 'Bahrain',\n",
       " 'Bangladesh',\n",
       " 'Barbados',\n",
       " 'Belarus',\n",
       " 'Belgium',\n",
       " 'Belize',\n",
       " 'Benin',\n",
       " 'Bhutan',\n",
       " 'Bolivia',\n",
       " 'Bosnia and Herzegovina',\n",
       " 'Botswana',\n",
       " 'Brazil',\n",
       " 'Brunei Darussalam',\n",
       " 'Bulgaria',\n",
       " 'Burkina Faso',\n",
       " 'Burundi',\n",
       " 'Cambodia',\n",
       " 'Cameroon',\n",
       " 'Canada',\n",
       " 'Cape Verde',\n",
       " 'Central African Republic',\n",
       " 'Chile',\n",
       " 'China',\n",
       " 'Colombia',\n",
       " 'Congo, Republic of the...',\n",
       " 'Costa Rica',\n",
       " 'Croatia',\n",
       " 'Cuba',\n",
       " 'Cyprus',\n",
       " 'Czech Republic',\n",
       " \"Côte d'Ivoire\",\n",
       " \"Democratic People's Republic of Korea\",\n",
       " 'Democratic Republic of the Congo',\n",
       " 'Denmark',\n",
       " 'Djibouti',\n",
       " 'Dominica',\n",
       " 'Dominican Republic',\n",
       " 'Ecuador',\n",
       " 'Egypt',\n",
       " 'El Salvador',\n",
       " 'Eritrea',\n",
       " 'Estonia',\n",
       " 'Ethiopia',\n",
       " 'Fiji',\n",
       " 'Finland',\n",
       " 'France',\n",
       " 'Gabon',\n",
       " 'Gambia',\n",
       " 'Georgia',\n",
       " 'Germany',\n",
       " 'Ghana',\n",
       " 'Greece',\n",
       " 'Grenada',\n",
       " 'Guatemala',\n",
       " 'Guinea',\n",
       " 'Guinea-Bissau',\n",
       " 'Guyana',\n",
       " 'Haiti',\n",
       " 'Honduras',\n",
       " 'Hong Kong (S.A.R.)',\n",
       " 'Hungary',\n",
       " 'Iceland',\n",
       " 'India',\n",
       " 'Indonesia',\n",
       " 'Iran, Islamic Republic of...',\n",
       " 'Iraq',\n",
       " 'Ireland',\n",
       " 'Israel',\n",
       " 'Italy',\n",
       " 'Jamaica',\n",
       " 'Japan',\n",
       " 'Jordan',\n",
       " 'Kazakhstan',\n",
       " 'Kenya',\n",
       " 'Kuwait',\n",
       " 'Kyrgyzstan',\n",
       " 'Latvia',\n",
       " 'Lebanon',\n",
       " 'Lesotho',\n",
       " 'Liberia',\n",
       " 'Libyan Arab Jamahiriya',\n",
       " 'Liechtenstein',\n",
       " 'Lithuania',\n",
       " 'Luxembourg',\n",
       " 'Madagascar',\n",
       " 'Malawi',\n",
       " 'Malaysia',\n",
       " 'Maldives',\n",
       " 'Mali',\n",
       " 'Malta',\n",
       " 'Marshall Islands',\n",
       " 'Mauritania',\n",
       " 'Mauritius',\n",
       " 'Mexico',\n",
       " 'Micronesia, Federated States of...',\n",
       " 'Monaco',\n",
       " 'Mongolia',\n",
       " 'Montenegro',\n",
       " 'Morocco',\n",
       " 'Mozambique',\n",
       " 'Myanmar',\n",
       " 'Namibia',\n",
       " 'Nauru',\n",
       " 'Nepal',\n",
       " 'Netherlands',\n",
       " 'New Zealand',\n",
       " 'Nicaragua',\n",
       " 'Niger',\n",
       " 'Nigeria',\n",
       " 'North Korea',\n",
       " 'Norway',\n",
       " 'Oman',\n",
       " 'Other Country (Not Listed Above)',\n",
       " 'Pakistan',\n",
       " 'Palau',\n",
       " 'Panama',\n",
       " 'Paraguay',\n",
       " 'Peru',\n",
       " 'Philippines',\n",
       " 'Poland',\n",
       " 'Portugal',\n",
       " 'Qatar',\n",
       " 'Republic of Korea',\n",
       " 'Republic of Moldova',\n",
       " 'Romania',\n",
       " 'Russian Federation',\n",
       " 'Rwanda',\n",
       " 'Saint Lucia',\n",
       " 'San Marino',\n",
       " 'Saudi Arabia',\n",
       " 'Senegal',\n",
       " 'Serbia',\n",
       " 'Sierra Leone',\n",
       " 'Singapore',\n",
       " 'Slovakia',\n",
       " 'Slovenia',\n",
       " 'Solomon Islands',\n",
       " 'Somalia',\n",
       " 'South Africa',\n",
       " 'South Korea',\n",
       " 'Spain',\n",
       " 'Sri Lanka',\n",
       " 'Sudan',\n",
       " 'Suriname',\n",
       " 'Swaziland',\n",
       " 'Sweden',\n",
       " 'Switzerland',\n",
       " 'Syrian Arab Republic',\n",
       " 'Taiwan',\n",
       " 'Tajikistan',\n",
       " 'Thailand',\n",
       " 'The former Yugoslav Republic of Macedonia',\n",
       " 'Timor-Leste',\n",
       " 'Togo',\n",
       " 'Trinidad and Tobago',\n",
       " 'Tunisia',\n",
       " 'Turkey',\n",
       " 'Turkmenistan',\n",
       " 'Uganda',\n",
       " 'Ukraine',\n",
       " 'United Arab Emirates',\n",
       " 'United Kingdom',\n",
       " 'United Republic of Tanzania',\n",
       " 'United States',\n",
       " 'Uruguay',\n",
       " 'Uzbekistan',\n",
       " 'Venezuela, Bolivarian Republic of...',\n",
       " 'Viet Nam',\n",
       " 'Yemen',\n",
       " 'Zambia',\n",
       " 'Zimbabwe']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "countries_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1667a350",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Computer science, computer engineering, or software engineering', 27750),\n",
       " ('Information systems, information technology, or system administration',\n",
       "  3741),\n",
       " ('Another engineering discipline', 3498),\n",
       " ('Web development or web design', 1099),\n",
       " ('Mathematics or statistics', 1051),\n",
       " ('A natural science', 992),\n",
       " ('A business discipline', 986),\n",
       " ('A humanities discipline', 812),\n",
       " ('A social science', 735),\n",
       " ('Fine arts or performing arts', 594),\n",
       " ('I never declared a major', 99),\n",
       " ('A health science', 92)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_undergrad_major_count_list(selected_formal_education, selected_countries): \n",
    "    def remove_pharentesis(name):\n",
    "        if \"(\" in name:\n",
    "            return name[:name.index(\"(\")].strip()\n",
    "        else:\n",
    "            return name\n",
    "        \n",
    "    def valid_row(row):\n",
    "        if row[survey_results_header[\"Country\"]] not in selected_countries:\n",
    "            return False\n",
    "        \n",
    "        if row[survey_results_header[\"FormalEducation\"]] != selected_formal_education:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    formal_education_rows = [row for row in tab_1_graph_1_row if valid_row(row)]\n",
    "    undergrad_major_rows = [row[survey_results_header[\"UndergradMajor\"]] for row in formal_education_rows]\n",
    "    counter_rows = [x for x in Counter(undergrad_major_rows).most_common()]\n",
    "\n",
    "    return [(remove_pharentesis(x[0]), x[1]) for x in counter_rows]\n",
    "\n",
    "get_undergrad_major_count_list(formal_education_labels[0], countries_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c9b38",
   "metadata": {},
   "source": [
    "Since we have to allow to look up the number of respondents for a given degree and undergraduate major, we will create a select dropdown that allows the user to select a degree type, and then, we will display the count of all the undergraduate majors for that degree. \n",
    "\n",
    "So, if we want to know the number of respondents with a bachelor's degree with a major in mathematics or statistics, we have to select \"Bachelor's degree\" from the select dropdown and identify the column with the label \"Mathematics or statistics.\" We can also compare the number of respondents with a specific major given a formal education type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29388406",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_1_graph_1():\n",
    "    \n",
    "    default_selected_countries = countries_label\n",
    "    \n",
    "    #This is the default value of the select component\n",
    "    default_formal_education_selection = formal_education_labels[0]\n",
    "    \n",
    "    undergrad_major_data = get_undergrad_major_count_list(default_formal_education_selection, default_selected_countries)\n",
    "    \n",
    "    data = {\n",
    "            'UndergradMajor' : [row[0] for row in undergrad_major_data],\n",
    "            'count'   : [row[1] for row in undergrad_major_data]\n",
    "       }\n",
    "    \n",
    "    source = ColumnDataSource(data=data)\n",
    "\n",
    "    TOOLTIPS = [\n",
    "        (\"UndergradMajor\", \"@UndergradMajor\"),\n",
    "        (\"Count\", \"@count{0,0}\"),\n",
    "    ]\n",
    "    \n",
    "    p = figure(\n",
    "            x_range=[row[0] for row in undergrad_major_data], \n",
    "            title=\"Number of respondents by undergraduate major\",\n",
    "            plot_width=950, \n",
    "            plot_height=700,\n",
    "            tooltips=TOOLTIPS\n",
    "    )\n",
    "    \n",
    "    p.xaxis.axis_label = \"Undergraduate Major\"\n",
    "    p.yaxis.axis_label = \"Number of respondents\"\n",
    "\n",
    "    p.yaxis.formatter = NumeralTickFormatter(format=\"0,0\") \n",
    "    \n",
    "    \n",
    "    p.vbar(\n",
    "        x='UndergradMajor', \n",
    "        top='count', \n",
    "        source=source, \n",
    "        width=0.9)\n",
    "\n",
    "    p.xaxis.major_label_orientation = -0.95\n",
    "    \n",
    "\n",
    "    \n",
    "    # Select the formal education for the first graph\n",
    "    select_formal_education = Select(\n",
    "        title='Formal Education', \n",
    "        value=default_formal_education_selection, \n",
    "        options=formal_education_labels\n",
    "    )\n",
    "\n",
    "    \n",
    "    def filter_data(selected_countries = countries_label):\n",
    "        selected_formal_education = select_formal_education.value\n",
    "        undergrad_major_data = get_undergrad_major_count_list(selected_formal_education, selected_countries)\n",
    "        new_data = {\n",
    "                    'UndergradMajor' : [row[0] for row in undergrad_major_data],\n",
    "                    'count'   : [row[1] for row in undergrad_major_data]\n",
    "               }\n",
    "        return ColumnDataSource(data=new_data)\n",
    "\n",
    "\n",
    "    def update_data(attr, old, new):\n",
    "        updated_source = filter_data()\n",
    "        source.data.update(updated_source.data)\n",
    "        \n",
    "    def update_selected_countries(selected_countries = []):\n",
    "        updated_source = filter_data(selected_countries)\n",
    "        source.data.update(updated_source.data)\n",
    "\n",
    "    select_formal_education.on_change('value', update_data)\n",
    "\n",
    "    \n",
    "    return (layout([select_formal_education, p]), update_selected_countries)\n",
    "\n",
    "## Uncomment this to have a preview of the plot\n",
    "\n",
    "# def wrapper(doc):\n",
    "#     p, upd = panel_1_graph_1()\n",
    "#     doc.add_root(p)\n",
    "    \n",
    "# handler = FunctionHandler(wrapper)\n",
    "# app = Application(handler)\n",
    "# show(app, notebook_url=\"http://localhost:8889\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cfb7be",
   "metadata": {},
   "source": [
    "### 2.1) Graphical representation of the Average Salary trend by company size\n",
    "\n",
    "First, let's check the legal values of company size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71232a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98855"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_1_graph_2_row = [row for row in survey_results_public_rows]\n",
    "len(tab_1_graph_2_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e99dfa00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'20 to 99 employees': 16996,\n",
       "         '10,000 or more employees': 9757,\n",
       "         '100 to 499 employees': 14011,\n",
       "         '10 to 19 employees': 8007,\n",
       "         '500 to 999 employees': 4630,\n",
       "         '1,000 to 4,999 employees': 7634,\n",
       "         '5,000 to 9,999 employees': 3017,\n",
       "         'Fewer than 10 employees': 7479,\n",
       "         'NA': 27324})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[survey_results_header[\"CompanySize\"]] for row in survey_results_public_rows])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e55d05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['500 to 999 employees',\n",
       " '1,000 to 4,999 employees',\n",
       " '5,000 to 9,999 employees',\n",
       " '100 to 499 employees',\n",
       " 'Fewer than 10 employees',\n",
       " '10 to 19 employees',\n",
       " '20 to 99 employees',\n",
       " '10,000 or more employees']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_size_labels = list(set([row[survey_results_header[\"CompanySize\"]] for row in survey_results_public_rows]))\n",
    "company_size_labels = [row for row in company_size_labels if row != \"NA\"]\n",
    "company_size_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc96764",
   "metadata": {},
   "source": [
    "Since I would like to show the categorical data sorted by the size of the company, I must manually sort these values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "614893ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fewer than 10 employees',\n",
       " '10 to 19 employees',\n",
       " '20 to 99 employees',\n",
       " '100 to 499 employees',\n",
       " '500 to 999 employees',\n",
       " '1,000 to 4,999 employees',\n",
       " '5,000 to 9,999 employees',\n",
       " '10,000 or more employees']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_size_labels = [\n",
    "    'Fewer than 10 employees',\n",
    "    '10 to 19 employees',\n",
    "    '20 to 99 employees',\n",
    "    '100 to 499 employees',\n",
    "    '500 to 999 employees',\n",
    "    '1,000 to 4,999 employees',\n",
    "    '5,000 to 9,999 employees',\n",
    "    '10,000 or more employees'  \n",
    "]\n",
    "company_size_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8199e8d",
   "metadata": {},
   "source": [
    "Now let's inspect the content of ConvertedSalary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "190db42a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NA', 51153),\n",
       " ('0', 842),\n",
       " ('120000', 524),\n",
       " ('1e+05', 497),\n",
       " ('80000', 396),\n",
       " ('1e+06', 382),\n",
       " ('110000', 371),\n",
       " ('90000', 364),\n",
       " ('150000', 357),\n",
       " ('60000', 351),\n",
       " ('75000', 337),\n",
       " ('130000', 333),\n",
       " ('70000', 319),\n",
       " ('73433', 317),\n",
       " ('61194', 308),\n",
       " ('85000', 288),\n",
       " ('125000', 276),\n",
       " ('115000', 255),\n",
       " ('105000', 255),\n",
       " ('48955', 246),\n",
       " ('44064', 245),\n",
       " ('140000', 243),\n",
       " ('65000', 237),\n",
       " ('2e+06', 227),\n",
       " ('95000', 218),\n",
       " ('50000', 215),\n",
       " ('29376', 214),\n",
       " ('55075', 211),\n",
       " ('135000', 182),\n",
       " ('58752', 181),\n",
       " ('36716', 181),\n",
       " ('67313', 178),\n",
       " ('36720', 167),\n",
       " ('62507', 162),\n",
       " ('30000', 161),\n",
       " ('160000', 161),\n",
       " ('51408', 153),\n",
       " ('79552', 151),\n",
       " ('55000', 150),\n",
       " ('85671', 149),\n",
       " ('14688', 147),\n",
       " ('12000', 144),\n",
       " ('69452', 143),\n",
       " ('24000', 141),\n",
       " ('42836', 138),\n",
       " ('22032', 134),\n",
       " ('55562', 131),\n",
       " ('41671', 127),\n",
       " ('51403', 127),\n",
       " ('180000', 124),\n",
       " ('36000', 122),\n",
       " ('48000', 120),\n",
       " ('73428', 119),\n",
       " ('2e+05', 119),\n",
       " ('72000', 118),\n",
       " ('83342', 117),\n",
       " ('48616', 116),\n",
       " ('145000', 113),\n",
       " ('58746', 111),\n",
       " ('97910', 111),\n",
       " ('80521', 106),\n",
       " ('17628', 105),\n",
       " ('40000', 103),\n",
       " ('44060', 102),\n",
       " ('82000', 102),\n",
       " ('76397', 99),\n",
       " ('32316', 98),\n",
       " ('18000', 98),\n",
       " ('3756', 95),\n",
       " ('64417', 95),\n",
       " ('6000', 95),\n",
       " ('122388', 93),\n",
       " ('46992', 92),\n",
       " ('48313', 92),\n",
       " ('91791', 90),\n",
       " ('39164', 89),\n",
       " ('30597', 87),\n",
       " ('72469', 85),\n",
       " ('5640', 85),\n",
       " ('170000', 85),\n",
       " ('2820', 84),\n",
       " ('4692', 84),\n",
       " ('35244', 83),\n",
       " ('45000', 79),\n",
       " ('155000', 78),\n",
       " ('66084', 77),\n",
       " ('175000', 77),\n",
       " ('26436', 76),\n",
       " ('90288', 76),\n",
       " ('41124', 76),\n",
       " ('78000', 75),\n",
       " ('68000', 75),\n",
       " ('34726', 74),\n",
       " ('97233', 74),\n",
       " ('165000', 74),\n",
       " ('96000', 73),\n",
       " ('102000', 73),\n",
       " ('60391', 72),\n",
       " ('38184', 72),\n",
       " ('46507', 72),\n",
       " ('92000', 71),\n",
       " ('95968', 69),\n",
       " ('56365', 69),\n",
       " ('58340', 68),\n",
       " ('9396', 68),\n",
       " ('11748', 67),\n",
       " ('23496', 67),\n",
       " ('84000', 67),\n",
       " ('63642', 66),\n",
       " ('7827', 66),\n",
       " ('33780', 65),\n",
       " ('66089', 64),\n",
       " ('21228', 63),\n",
       " ('29373', 63),\n",
       " ('108000', 63),\n",
       " ('15654', 63),\n",
       " ('42000', 62),\n",
       " ('110149', 61),\n",
       " ('52872', 60),\n",
       " ('20000', 59),\n",
       " ('9393', 59),\n",
       " ('20556', 59),\n",
       " ('103000', 59),\n",
       " ('250000', 57),\n",
       " ('18780', 57),\n",
       " ('112000', 57),\n",
       " ('87000', 57),\n",
       " ('18785', 57),\n",
       " ('52783', 56),\n",
       " ('23482', 56),\n",
       " ('24972', 56),\n",
       " ('12524', 55),\n",
       " ('63000', 55),\n",
       " ('38893', 55),\n",
       " ('1884', 54),\n",
       " ('30840', 54),\n",
       " ('34269', 54),\n",
       " ('52000', 54),\n",
       " ('66000', 54),\n",
       " ('104030', 53),\n",
       " ('88000', 53),\n",
       " ('76000', 53),\n",
       " ('54000', 53),\n",
       " ('48468', 53),\n",
       " ('73000', 53),\n",
       " ('31848', 53),\n",
       " ('12732', 53),\n",
       " ('62000', 53),\n",
       " ('52339', 52),\n",
       " ('79973', 52),\n",
       " ('93000', 52),\n",
       " ('16152', 52),\n",
       " ('10620', 52),\n",
       " ('45283', 52),\n",
       " ('88116', 52),\n",
       " ('8808', 52),\n",
       " ('44449', 52),\n",
       " ('39648', 51),\n",
       " ('96626', 51),\n",
       " ('49932', 51),\n",
       " ('24478', 51),\n",
       " ('3000', 51),\n",
       " ('14089', 50),\n",
       " ('71976', 50),\n",
       " ('40388', 50),\n",
       " ('11232', 50),\n",
       " ('18720', 50),\n",
       " ('111123', 50),\n",
       " ('25000', 49),\n",
       " ('68537', 49),\n",
       " ('138904', 49),\n",
       " ('7344', 49),\n",
       " ('56298', 48),\n",
       " ('68443', 48),\n",
       " ('66674', 48),\n",
       " ('70985', 48),\n",
       " ('13212', 47),\n",
       " ('7512', 47),\n",
       " ('97000', 47),\n",
       " ('19092', 47),\n",
       " ('58000', 47),\n",
       " ('40261', 47),\n",
       " ('28236', 47),\n",
       " ('31821', 47),\n",
       " ('74000', 46),\n",
       " ('15000', 46),\n",
       " ('22452', 46),\n",
       " ('14400', 46),\n",
       " ('10958', 45),\n",
       " ('6576', 45),\n",
       " ('106000', 45),\n",
       " ('63979', 45),\n",
       " ('107000', 45),\n",
       " ('98000', 45),\n",
       " ('56000', 45),\n",
       " ('35000', 45),\n",
       " ('88119', 45),\n",
       " ('11268', 44),\n",
       " ('50005', 44),\n",
       " ('3e+05', 43),\n",
       " ('57000', 43),\n",
       " ('37428', 43),\n",
       " ('14976', 43),\n",
       " ('53851', 43),\n",
       " ('67000', 43),\n",
       " ('104178', 42),\n",
       " ('17220', 42),\n",
       " ('25476', 42),\n",
       " ('104000', 42),\n",
       " ('52627', 41),\n",
       " ('4696', 41),\n",
       " ('45838', 41),\n",
       " ('94000', 41),\n",
       " ('6612', 41),\n",
       " ('75880', 41),\n",
       " ('118000', 41),\n",
       " ('7488', 41),\n",
       " ('42348', 40),\n",
       " ('64866', 40),\n",
       " ('29940', 40),\n",
       " ('83000', 40),\n",
       " ('132000', 40),\n",
       " ('33045', 40),\n",
       " ('2256', 40),\n",
       " ('190000', 40),\n",
       " ('14856', 40),\n",
       " ('9600', 40),\n",
       " ('28178', 40),\n",
       " ('51394', 39),\n",
       " ('42588', 39),\n",
       " ('16980', 39),\n",
       " ('3384', 39),\n",
       " ('6262', 39),\n",
       " ('10284', 39),\n",
       " ('76495', 39),\n",
       " ('10000', 38),\n",
       " ('41612', 38),\n",
       " ('26925', 38),\n",
       " ('37504', 38),\n",
       " ('88573', 38),\n",
       " ('36115', 38),\n",
       " ('86000', 38),\n",
       " ('55812', 38),\n",
       " ('54336', 38),\n",
       " ('87971', 37),\n",
       " ('74856', 37),\n",
       " ('31309', 37),\n",
       " ('117492', 37),\n",
       " ('57522', 37),\n",
       " ('61680', 36),\n",
       " ('59880', 36),\n",
       " ('8496', 36),\n",
       " ('21916', 36),\n",
       " ('8448', 36),\n",
       " ('47731', 36),\n",
       " ('27900', 36),\n",
       " ('119960', 36),\n",
       " ('77000', 35),\n",
       " ('125014', 35),\n",
       " ('49404', 35),\n",
       " ('9360', 35),\n",
       " ('8400', 35),\n",
       " ('19104', 35),\n",
       " ('6372', 35),\n",
       " ('42456', 34),\n",
       " ('22030', 34),\n",
       " ('7200', 34),\n",
       " ('5880', 34),\n",
       " ('220000', 34),\n",
       " ('126000', 34),\n",
       " ('30559', 34),\n",
       " ('7044', 34),\n",
       " ('4800', 34),\n",
       " ('146865', 34),\n",
       " ('26196', 34),\n",
       " ('91000', 33),\n",
       " ('50179', 33),\n",
       " ('21168', 33),\n",
       " ('44916', 33),\n",
       " ('37940', 32),\n",
       " ('113000', 32),\n",
       " ('59729', 32),\n",
       " ('104269', 32),\n",
       " ('15600', 32),\n",
       " ('62418', 32),\n",
       " ('15024', 32),\n",
       " ('35292', 32),\n",
       " ('116000', 32),\n",
       " ('25701', 32),\n",
       " ('69761', 32),\n",
       " ('17640', 32),\n",
       " ('1500', 31),\n",
       " ('33337', 31),\n",
       " ('55981', 31),\n",
       " ('44287', 31),\n",
       " ('146868', 31),\n",
       " ('4128', 30),\n",
       " ('59980', 30),\n",
       " ('81000', 30),\n",
       " ('52404', 30),\n",
       " ('127000', 30),\n",
       " ('11741', 30),\n",
       " ('64000', 30),\n",
       " ('125123', 30),\n",
       " ('2400', 30),\n",
       " ('13152', 30),\n",
       " ('122000', 30),\n",
       " ('77104', 30),\n",
       " ('83224', 30),\n",
       " ('104678', 30),\n",
       " ('19200', 30),\n",
       " ('27600', 30),\n",
       " ('45528', 30),\n",
       " ('166685', 30),\n",
       " ('47904', 29),\n",
       " ('440592', 29),\n",
       " ('587460', 29),\n",
       " ('89000', 29),\n",
       " ('103965', 29),\n",
       " ('80776', 29),\n",
       " ('123000', 29),\n",
       " ('61000', 29),\n",
       " ('38820', 29),\n",
       " ('9552', 28),\n",
       " ('65285', 28),\n",
       " ('185000', 28),\n",
       " ('4884', 28),\n",
       " ('3252', 28),\n",
       " ('10332', 28),\n",
       " ('10175', 28),\n",
       " ('83972', 28),\n",
       " ('27781', 28),\n",
       " ('32209', 28),\n",
       " ('56951', 28),\n",
       " ('105324', 28),\n",
       " ('144000', 28),\n",
       " ('53000', 27),\n",
       " ('4404', 27),\n",
       " ('7428', 27),\n",
       " ('67977', 27),\n",
       " ('111000', 27),\n",
       " ('72230', 27),\n",
       " ('5616', 27),\n",
       " ('26400', 27),\n",
       " ('33972', 26),\n",
       " ('142000', 26),\n",
       " ('67368', 26),\n",
       " ('43060', 26),\n",
       " ('52932', 26),\n",
       " ('37571', 26),\n",
       " ('734328', 26),\n",
       " ('6575', 26),\n",
       " ('117000', 26),\n",
       " ('36084', 26),\n",
       " ('128000', 26),\n",
       " ('35492', 26),\n",
       " ('16848', 26),\n",
       " ('63896', 25),\n",
       " ('47227', 25),\n",
       " ('12480', 25),\n",
       " ('45876', 25),\n",
       " ('69000', 25),\n",
       " ('500052', 25),\n",
       " ('240000', 25),\n",
       " ('24696', 25),\n",
       " ('20351', 25),\n",
       " ('111963', 25),\n",
       " ('29724', 24),\n",
       " ('95462', 24),\n",
       " ('8076', 24),\n",
       " ('31788', 24),\n",
       " ('660900', 24),\n",
       " ('101000', 24),\n",
       " ('134627', 24),\n",
       " ('118068', 24),\n",
       " ('13104', 24),\n",
       " ('18358', 24),\n",
       " ('28956', 24),\n",
       " ('124000', 24),\n",
       " ('3757', 23),\n",
       " ('61118', 23),\n",
       " ('64620', 23),\n",
       " ('63156', 23),\n",
       " ('70500', 23),\n",
       " ('57276', 23),\n",
       " ('67560', 23),\n",
       " ('25003', 23),\n",
       " ('5636', 23),\n",
       " ('31948', 23),\n",
       " ('87768', 22),\n",
       " ('3600', 22),\n",
       " ('40282', 22),\n",
       " ('47984', 22),\n",
       " ('13306', 22),\n",
       " ('99967', 22),\n",
       " ('39136', 22),\n",
       " ('59970', 22),\n",
       " ('5076', 22),\n",
       " ('79000', 22),\n",
       " ('18360', 22),\n",
       " ('100652', 22),\n",
       " ('25047', 22),\n",
       " ('51983', 22),\n",
       " ('31764', 22),\n",
       " ('3180', 22),\n",
       " ('18696', 22),\n",
       " ('37404', 22),\n",
       " ('11016', 22),\n",
       " ('40392', 21),\n",
       " ('107964', 21),\n",
       " ('8610', 21),\n",
       " ('33684', 21),\n",
       " ('156000', 21),\n",
       " ('87500', 21),\n",
       " ('14112', 21),\n",
       " ('4320', 21),\n",
       " ('38208', 21),\n",
       " ('147000', 21),\n",
       " ('20836', 21),\n",
       " ('138000', 21),\n",
       " ('66027', 21),\n",
       " ('10800', 21),\n",
       " ('56460', 21),\n",
       " ('133000', 21),\n",
       " ('21600', 21),\n",
       " ('26612', 20),\n",
       " ('5479', 20),\n",
       " ('19068', 20),\n",
       " ('13200', 20),\n",
       " ('153000', 20),\n",
       " ('28800', 20),\n",
       " ('16908', 20),\n",
       " ('137000', 20),\n",
       " ('43985', 20),\n",
       " ('5256', 20),\n",
       " ('16800', 20),\n",
       " ('62556', 20),\n",
       " ('24408', 20),\n",
       " ('76368', 20),\n",
       " ('57975', 20),\n",
       " ('12348', 20),\n",
       " ('78900', 20),\n",
       " ('1200', 20),\n",
       " ('46963', 19),\n",
       " ('32000', 19),\n",
       " ('86120', 19),\n",
       " ('79175', 19),\n",
       " ('120782', 19),\n",
       " ('71000', 19),\n",
       " ('11652', 19),\n",
       " ('6204', 19),\n",
       " ('136000', 19),\n",
       " ('750084', 19),\n",
       " ('15900', 19),\n",
       " ('28149', 19),\n",
       " ('19824', 19),\n",
       " ('97500', 19),\n",
       " ('80772', 19),\n",
       " ('56892', 19),\n",
       " ('114000', 19),\n",
       " ('112730', 19),\n",
       " ('134000', 19),\n",
       " ('70841', 18),\n",
       " ('4512', 18),\n",
       " ('54173', 18),\n",
       " ('82344', 18),\n",
       " ('27000', 18),\n",
       " ('41856', 18),\n",
       " ('13800', 18),\n",
       " ('16272', 18),\n",
       " ('84264', 18),\n",
       " ('56148', 18),\n",
       " ('102804', 18),\n",
       " ('129000', 18),\n",
       " ('4248', 18),\n",
       " ('73619', 18),\n",
       " ('150288', 18),\n",
       " ('46332', 18),\n",
       " ('38000', 18),\n",
       " ('24156', 18),\n",
       " ('78328', 18),\n",
       " ('109000', 18),\n",
       " ('152794', 18),\n",
       " ('210000', 18),\n",
       " ('1879', 17),\n",
       " ('47000', 17),\n",
       " ('4476', 17),\n",
       " ('514032', 17),\n",
       " ('35988', 17),\n",
       " ('75008', 17),\n",
       " ('23172', 17),\n",
       " ('154000', 17),\n",
       " ('881196', 17),\n",
       " ('99000', 17),\n",
       " ('121000', 17),\n",
       " ('94455', 17),\n",
       " ('36235', 17),\n",
       " ('38400', 17),\n",
       " ('13956', 17),\n",
       " ('80564', 17),\n",
       " ('62807', 17),\n",
       " ('23352', 17),\n",
       " ('82500', 17),\n",
       " ('195000', 17),\n",
       " ('34752', 17),\n",
       " ('114696', 17),\n",
       " ('5000', 17),\n",
       " ('16644', 17),\n",
       " ('6732', 16),\n",
       " ('7140', 16),\n",
       " ('32874', 16),\n",
       " ('5424', 16),\n",
       " ('7500', 16),\n",
       " ('143000', 16),\n",
       " ('1200000', 16),\n",
       " ('666744', 16),\n",
       " ('16437', 16),\n",
       " ('11271', 16),\n",
       " ('15876', 16),\n",
       " ('112344', 16),\n",
       " ('6660', 16),\n",
       " ('54755', 16),\n",
       " ('127957', 16),\n",
       " ('92600', 16),\n",
       " ('22248', 16),\n",
       " ('6012', 16),\n",
       " ('51000', 16),\n",
       " ('3948', 16),\n",
       " ('367164', 16),\n",
       " ('12888', 16),\n",
       " ('12216', 16),\n",
       " ('2712', 16),\n",
       " ('88752', 16),\n",
       " ('16116', 16),\n",
       " ('68868', 16),\n",
       " ('40536', 16),\n",
       " ('183582', 16),\n",
       " ('34440', 15),\n",
       " ('22560', 15),\n",
       " ('10584', 15),\n",
       " ('50400', 15),\n",
       " ('9540', 15),\n",
       " ('15924', 15),\n",
       " ('833424', 15),\n",
       " ('34512', 15),\n",
       " ('84547', 15),\n",
       " ('90567', 15),\n",
       " ('122880', 15),\n",
       " ('7884', 15),\n",
       " ('5304', 15),\n",
       " ('39987', 15),\n",
       " ('14687', 15),\n",
       " ('4488', 15),\n",
       " ('68063', 15),\n",
       " ('7800', 15),\n",
       " ('1668', 15),\n",
       " ('73084', 15),\n",
       " ('31200', 15),\n",
       " ('62500', 15),\n",
       " ('954624', 15),\n",
       " ('28176', 15),\n",
       " ('19582', 15),\n",
       " ('51159', 15),\n",
       " ('146000', 15),\n",
       " ('70212', 15),\n",
       " ('49440', 15),\n",
       " ('60216', 14),\n",
       " ('936', 14),\n",
       " ('32400', 14),\n",
       " ('21000', 14),\n",
       " ('616836', 14),\n",
       " ('56352', 14),\n",
       " ('960000', 14),\n",
       " ('84448', 14),\n",
       " ('47736', 14),\n",
       " ('20268', 14),\n",
       " ('3792', 14),\n",
       " ('16668', 14),\n",
       " ('49000', 14),\n",
       " ('2448', 14),\n",
       " ('116268', 14),\n",
       " ('92500', 14),\n",
       " ('19056', 14),\n",
       " ('70138', 14),\n",
       " ('20400', 14),\n",
       " ('38199', 14),\n",
       " ('20806', 14),\n",
       " ('225000', 14),\n",
       " ('74657', 14),\n",
       " ('1800000', 14),\n",
       " ('73500', 14),\n",
       " ('69024', 14),\n",
       " ('143952', 14),\n",
       " ('45144', 13),\n",
       " ('37572', 13),\n",
       " ('55392', 13),\n",
       " ('12708', 13),\n",
       " ('187848', 13),\n",
       " ('11604', 13),\n",
       " ('64380', 13),\n",
       " ('76514', 13),\n",
       " ('16884', 13),\n",
       " ('25428', 13),\n",
       " ('205000', 13),\n",
       " ('20664', 13),\n",
       " ('59988', 13),\n",
       " ('74885', 13),\n",
       " ('40332', 13),\n",
       " ('5820', 13),\n",
       " ('54790', 13),\n",
       " ('33600', 13),\n",
       " ('65776', 13),\n",
       " ('1800', 13),\n",
       " ('13320', 13),\n",
       " ('20592', 13),\n",
       " ('528720', 13),\n",
       " ('75975', 13),\n",
       " ('89266', 13),\n",
       " ('26064', 13),\n",
       " ('469968', 13),\n",
       " ('6504', 13),\n",
       " ('11000', 13),\n",
       " ('14088', 13),\n",
       " ('1560000', 13),\n",
       " ('93842', 13),\n",
       " ('2628', 13),\n",
       " ('62880', 13),\n",
       " ('33048', 13),\n",
       " ('91284', 13),\n",
       " ('28584', 13),\n",
       " ('4344', 13),\n",
       " ('9732', 13),\n",
       " ('5400', 13),\n",
       " ('3744', 13),\n",
       " ('46000', 13),\n",
       " ('7514', 13),\n",
       " ('28056', 13),\n",
       " ('131000', 13),\n",
       " ('807756', 13),\n",
       " ('9108', 13),\n",
       " ('45092', 13),\n",
       " ('583392', 13),\n",
       " ('660', 13),\n",
       " ('22548', 13),\n",
       " ('25200', 13),\n",
       " ('1080000', 12),\n",
       " ('9000', 12),\n",
       " ('6384', 12),\n",
       " ('5448', 12),\n",
       " ('141000', 12),\n",
       " ('28000', 12),\n",
       " ('50904', 12),\n",
       " ('28068', 12),\n",
       " ('72209', 12),\n",
       " ('19332', 12),\n",
       " ('148000', 12),\n",
       " ('8268', 12),\n",
       " ('29170', 12),\n",
       " ('34000', 12),\n",
       " ('29743', 12),\n",
       " ('94788', 12),\n",
       " ('100011', 12),\n",
       " ('38916', 12),\n",
       " ('62001', 12),\n",
       " ('2208', 12),\n",
       " ('63516', 12),\n",
       " ('11676', 12),\n",
       " ('62618', 12),\n",
       " ('45420', 12),\n",
       " ('69248', 12),\n",
       " ('44496', 12),\n",
       " ('152000', 12),\n",
       " ('18048', 12),\n",
       " ('21300', 12),\n",
       " ('41000', 12),\n",
       " ('14820', 12),\n",
       " ('77300', 12),\n",
       " ('4200', 12),\n",
       " ('63762', 12),\n",
       " ('41871', 12),\n",
       " ('2818', 12),\n",
       " ('53949', 12),\n",
       " ('47688', 12),\n",
       " ('27168', 12),\n",
       " ('53892', 12),\n",
       " ('37500', 12),\n",
       " ('162000', 12),\n",
       " ('112716', 12),\n",
       " ('19404', 12),\n",
       " ('23844', 12),\n",
       " ('46704', 12),\n",
       " ('2940', 12),\n",
       " ('11976', 12),\n",
       " ('1440000', 12),\n",
       " ('138900', 12),\n",
       " ('44000', 11),\n",
       " ('9768', 11),\n",
       " ('59172', 11),\n",
       " ('9984', 11),\n",
       " ('72500', 11),\n",
       " ('45897', 11),\n",
       " ('171343', 11),\n",
       " ('7860', 11),\n",
       " ('139000', 11),\n",
       " ('149000', 11),\n",
       " ('24228', 11),\n",
       " ('176000', 11),\n",
       " ('50700', 11),\n",
       " ('275000', 11),\n",
       " ('466716', 11),\n",
       " ('37056', 11),\n",
       " ('28644', 11),\n",
       " ('87701', 11),\n",
       " ('12396', 11),\n",
       " ('33000', 11),\n",
       " ('6e+05', 11),\n",
       " ('39000', 11),\n",
       " ('600060', 11),\n",
       " ('14100', 11),\n",
       " ('400044', 11),\n",
       " ('61200', 11),\n",
       " ('230000', 11),\n",
       " ('82890', 11),\n",
       " ('12720', 11),\n",
       " ('108395', 11),\n",
       " ('6768', 11),\n",
       " ('23254', 11),\n",
       " ('8328', 11),\n",
       " ('59000', 11),\n",
       " ('17134', 11),\n",
       " ('411228', 11),\n",
       " ('77786', 11),\n",
       " ('52500', 11),\n",
       " ('29664', 11),\n",
       " ('62400', 11),\n",
       " ('159947', 11),\n",
       " ('23820', 11),\n",
       " ('69036', 11),\n",
       " ('70859', 11),\n",
       " ('159104', 11),\n",
       " ('119000', 11),\n",
       " ('80393', 11),\n",
       " ('9672', 11),\n",
       " ('98304', 11),\n",
       " ('3336', 11),\n",
       " ('50326', 11),\n",
       " ('128834', 11),\n",
       " ('52668', 11),\n",
       " ('12500', 11),\n",
       " ('26472', 11),\n",
       " ('173650', 11),\n",
       " ('33528', 10),\n",
       " ('45895', 10),\n",
       " ('56172', 10),\n",
       " ('3900', 10),\n",
       " ('280000', 10),\n",
       " ('3914', 10),\n",
       " ('9012', 10),\n",
       " ('6360', 10),\n",
       " ('41916', 10),\n",
       " ('8640', 10),\n",
       " ('23208', 10),\n",
       " ('10476', 10),\n",
       " ('23614', 10),\n",
       " ('37000', 10),\n",
       " ('49923', 10),\n",
       " ('46750', 10),\n",
       " ('14964', 10),\n",
       " ('19980', 10),\n",
       " ('108840', 10),\n",
       " ('21133', 10),\n",
       " ('416712', 10),\n",
       " ('25704', 10),\n",
       " ('50728', 10),\n",
       " ('5292', 10),\n",
       " ('17796', 10),\n",
       " ('5088', 10),\n",
       " ('22225', 10),\n",
       " ('57600', 10),\n",
       " ('53076', 10),\n",
       " ('44400', 10),\n",
       " ('2688', 10),\n",
       " ('35421', 10),\n",
       " ('4704', 10),\n",
       " ('293736', 10),\n",
       " ('47640', 10),\n",
       " ('43440', 10),\n",
       " ('176244', 10),\n",
       " ('31572', 10),\n",
       " ('102018', 10),\n",
       " ('10140', 10),\n",
       " ('43755', 10),\n",
       " ('50094', 10),\n",
       " ('7596', 10),\n",
       " ('3576', 10),\n",
       " ('71500', 10),\n",
       " ('28608', 10),\n",
       " ('67500', 10),\n",
       " ('2664', 10),\n",
       " ('74080', 10),\n",
       " ('2892', 10),\n",
       " ('93924', 10),\n",
       " ('3672', 10),\n",
       " ('15420', 10),\n",
       " ('220296', 10),\n",
       " ('46702', 10),\n",
       " ('25788', 10),\n",
       " ('51534', 10),\n",
       " ('91969', 9),\n",
       " ('39776', 9),\n",
       " ('47922', 9),\n",
       " ('14652', 9),\n",
       " ('37152', 9),\n",
       " ('46260', 9),\n",
       " ('63192', 9),\n",
       " ('131959', 9),\n",
       " ('57500', 9),\n",
       " ('720000', 9),\n",
       " ('114771', 9),\n",
       " ('43000', 9),\n",
       " ('59034', 9),\n",
       " ('64254', 9),\n",
       " ('118344', 9),\n",
       " ('168000', 9),\n",
       " ('558084', 9),\n",
       " ('5016', 9),\n",
       " ('67638', 9),\n",
       " ('102806', 9),\n",
       " ('1440', 9),\n",
       " ('76500', 9),\n",
       " ('46752', 9),\n",
       " ('14292', 9),\n",
       " ('17376', 9),\n",
       " ('54813', 9),\n",
       " ('14304', 9),\n",
       " ('5772', 9),\n",
       " ('26700', 9),\n",
       " ('1320', 9),\n",
       " ('180575', 9),\n",
       " ('172000', 9),\n",
       " ('88629', 9),\n",
       " ('10104', 9),\n",
       " ('4e+05', 9),\n",
       " ('22699', 9),\n",
       " ('82836', 9),\n",
       " ('27684', 9),\n",
       " ('38136', 9),\n",
       " ('157000', 9),\n",
       " ('37452', 9),\n",
       " ('215000', 9),\n",
       " ('6444', 9),\n",
       " ('23000', 9),\n",
       " ('10200', 9),\n",
       " ('77232', 9),\n",
       " ('46416', 9),\n",
       " ('8453', 9),\n",
       " ('1565', 9),\n",
       " ('83416', 9),\n",
       " ('3192', 9),\n",
       " ('15720', 9),\n",
       " ('3504', 9),\n",
       " ('5160', 9),\n",
       " ('20964', 9),\n",
       " ('34608', 9),\n",
       " ('95642', 9),\n",
       " ('30600', 9),\n",
       " ('52089', 9),\n",
       " ('77844', 9),\n",
       " ('65979', 9),\n",
       " ('3372', 9),\n",
       " ('64590', 9),\n",
       " ('9348', 9),\n",
       " ('54023', 9),\n",
       " ('18336', 9),\n",
       " ('132180', 9),\n",
       " ('308412', 9),\n",
       " ('8352', 9),\n",
       " ('14000', 9),\n",
       " ('2160', 9),\n",
       " ('208356', 9),\n",
       " ('86895', 9),\n",
       " ('98620', 9),\n",
       " ('95460', 9),\n",
       " ('89832', 9),\n",
       " ('8052', 9),\n",
       " ('32412', 8),\n",
       " ('8140', 8),\n",
       " ('84816', 8),\n",
       " ('6312', 8),\n",
       " ('4728', 8),\n",
       " ('2172', 8),\n",
       " ('12132', 8),\n",
       " ('12096', 8),\n",
       " ('20172', 8),\n",
       " ('16200', 8),\n",
       " ('6240', 8),\n",
       " ('40584', 8),\n",
       " ('24420', 8),\n",
       " ('8988', 8),\n",
       " ('7116', 8),\n",
       " ('6948', 8),\n",
       " ('93760', 8),\n",
       " ('20880', 8),\n",
       " ('76492', 8),\n",
       " ('1332', 8),\n",
       " ('58392', 8),\n",
       " ('9706', 8),\n",
       " ('15972', 8),\n",
       " ('130337', 8),\n",
       " ('24324', 8),\n",
       " ('5009', 8),\n",
       " ('10632', 8),\n",
       " ('22440', 8),\n",
       " ('33816', 8),\n",
       " ('31536', 8),\n",
       " ('102500', 8),\n",
       " ('46533', 8),\n",
       " ('61380', 8),\n",
       " ('13150', 8),\n",
       " ('41760', 8),\n",
       " ('3131', 8),\n",
       " ('62412', 8),\n",
       " ('9756', 8),\n",
       " ('24', 8),\n",
       " ('8820', 8),\n",
       " ('4860', 8),\n",
       " ('10848', 8),\n",
       " ('716748', 8),\n",
       " ('14832', 8),\n",
       " ('18768', 8),\n",
       " ('182000', 8),\n",
       " ('38552', 8),\n",
       " ('12636', 8),\n",
       " ('38676', 8),\n",
       " ('112500', 8),\n",
       " ('98500', 8),\n",
       " ('83172', 8),\n",
       " ('42500', 8),\n",
       " ('38112', 8),\n",
       " ('22800', 8),\n",
       " ('78202', 8),\n",
       " ('101808', 8),\n",
       " ('11220', 8),\n",
       " ('73576', 8),\n",
       " ('12239', 8),\n",
       " ('26640', 8),\n",
       " ('10008', 8),\n",
       " ('47505', 8),\n",
       " ('135955', 8),\n",
       " ('36542', 8),\n",
       " ('1500000', 8),\n",
       " ('13524', 8),\n",
       " ('65868', 8),\n",
       " ('450000', 8),\n",
       " ('22932', 8),\n",
       " ('166000', 8),\n",
       " ('338136', 8),\n",
       " ('360000', 8),\n",
       " ('19447', 8),\n",
       " ('151000', 8),\n",
       " ('63684', 8),\n",
       " ('11640', 8),\n",
       " ('3324', 8),\n",
       " ('32232', 8),\n",
       " ('107500', 8),\n",
       " ('63636', 8),\n",
       " ('264', 8),\n",
       " ('119910', 8),\n",
       " ('433380', 8),\n",
       " ('8232', 8),\n",
       " ('7632', 8),\n",
       " ('31932', 8),\n",
       " ('1128', 8),\n",
       " ('40428', 8),\n",
       " ('115860', 8),\n",
       " ('135550', 8),\n",
       " ('36', 8),\n",
       " ('128507', 7),\n",
       " ('916764', 7),\n",
       " ('10440', 7),\n",
       " ('61580', 7),\n",
       " ('20808', 7),\n",
       " ('57170', 7),\n",
       " ('53144', 7),\n",
       " ('36005', 7),\n",
       " ('84731', 7),\n",
       " ('48828', 7),\n",
       " ('99056', 7),\n",
       " ('4680', 7),\n",
       " ('89343', 7),\n",
       " ('45019', 7),\n",
       " ('26392', 7),\n",
       " ('533388', 7),\n",
       " ('123959', 7),\n",
       " ('12648', 7),\n",
       " ('41328', 7),\n",
       " ('24308', 7),\n",
       " ('5844', 7),\n",
       " ('11544', 7),\n",
       " ('2640', 7),\n",
       " ...]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[survey_results_header[\"ConvertedSalary\"]] for row in survey_results_public_rows]).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d55713",
   "metadata": {},
   "source": [
    "All of them seems to be legal values except for ones with \"NA\". Let's remove them from our dataset when we have to compute the average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "520fb25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('10,000 or more employees', 130062.06115357888),\n",
       " ('10 to 19 employees', 83704.84450034301),\n",
       " ('100 to 499 employees', 96816.24517872497),\n",
       " ('20 to 99 employees', 88625.15631713555),\n",
       " ('1,000 to 4,999 employees', 105271.31927577831),\n",
       " ('500 to 999 employees', 104451.50937031485),\n",
       " ('5,000 to 9,999 employees', 105323.71121177803),\n",
       " ('Fewer than 10 employees', 77923.73962550607)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_company_size(selected_countries):  \n",
    "    def valid_row(row):\n",
    "        if row[survey_results_header[\"Country\"]] not in selected_countries:\n",
    "            return False\n",
    "        \n",
    "        if row[survey_results_header[\"CompanySize\"]] not in company_size_labels:\n",
    "            return False\n",
    "        \n",
    "        if row[survey_results_header[\"ConvertedSalary\"]]  == \"NA\":\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    salary_dict = defaultdict(lambda: [])\n",
    "    rows = [row for row in tab_1_graph_2_row if valid_row(row)]\n",
    "    \n",
    "    for row in rows:\n",
    "        salary_dict[row[survey_results_header[\"CompanySize\"]]].append(float(row[survey_results_header[\"ConvertedSalary\"]]))\n",
    "\n",
    "    for x in salary_dict:\n",
    "        salary_dict[x] = stats.describe(salary_dict[x]).mean\n",
    "        \n",
    "    return [(x, salary_dict[x]) for x in salary_dict]\n",
    "\n",
    "\n",
    "get_average_company_size(countries_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "39095b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def panel_1_graph_2():\n",
    "    \n",
    "    default_selected_countries = countries_label\n",
    "\n",
    "    trend_data = get_average_company_size(default_selected_countries)\n",
    "    \n",
    "    data = {\n",
    "            'CompanySize' : [row[0] for row in trend_data],\n",
    "            'avg'   : [row[1] for row in trend_data]\n",
    "       }\n",
    "    \n",
    "    source = ColumnDataSource(data=data)\n",
    "\n",
    "    TOOLTIPS = [\n",
    "        (\"CompanySize\", \"@CompanySize\"),\n",
    "        (\"Average\", \"@avg{0,0.00}\"),\n",
    "    ]\n",
    "    \n",
    "    p = figure(\n",
    "            x_range=company_size_labels,\n",
    "            title=\"Average salary by company size\",\n",
    "            plot_width=500, \n",
    "#             plot_height=700,\n",
    "            tooltips=TOOLTIPS\n",
    "    )\n",
    "    \n",
    "    p.xaxis.axis_label = \"Number of employees\"\n",
    "    p.yaxis.axis_label = \"Average salary (in dollars)\"\n",
    "    p.yaxis.formatter = NumeralTickFormatter(format=\"0,0\") \n",
    "    \n",
    "\n",
    "    p.vbar(\n",
    "        x='CompanySize', \n",
    "        top='avg', \n",
    "        source=source, \n",
    "        width=0.9)\n",
    "\n",
    "    p.xaxis.major_label_orientation = 0.55\n",
    "    \n",
    "    \n",
    "    def filter_data(selected_countries = countries_label):\n",
    "        trend_data = get_average_company_size(selected_countries)\n",
    "        new_data = {\n",
    "                    'CompanySize' : [row[0] for row in trend_data],\n",
    "                    'avg'   : [row[1] for row in trend_data]\n",
    "               }\n",
    "        return ColumnDataSource(data=new_data)\n",
    "        \n",
    "    def update_selected_countries(selected_countries = []):\n",
    "        updated_source = filter_data(selected_countries)\n",
    "        source.data.update(updated_source.data)\n",
    "    \n",
    "    return (layout([p]), update_selected_countries)\n",
    "\n",
    "## Uncomment this to have a preview of the plot\n",
    "\n",
    "# def wrapper(doc):\n",
    "#     p, upd = panel_1_graph_2()\n",
    "#     doc.add_root(p)\n",
    "    \n",
    "# handler = FunctionHandler(wrapper)\n",
    "# app = Application(handler)\n",
    "# show(app, notebook_url=\"http://localhost:8889\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5927ab",
   "metadata": {},
   "source": [
    "### 2.2) Graphical representation of the Average Salary trend by WakeTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "031442b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Between 5:00 - 6:00 AM': 8922,\n",
       "         'Between 6:01 - 7:00 AM': 20322,\n",
       "         'NA': 26709,\n",
       "         'Before 5:00 AM': 1730,\n",
       "         'Between 7:01 - 8:00 AM': 21250,\n",
       "         'Between 9:01 - 10:00 AM': 3453,\n",
       "         'I do not have a set schedule': 3842,\n",
       "         'Between 8:01 - 9:00 AM': 10592,\n",
       "         'Between 10:01 - 11:00 AM': 1020,\n",
       "         'Between 11:01 AM - 12:00 PM': 387,\n",
       "         'After 12:01 PM': 301,\n",
       "         'I work night shifts': 327})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[survey_results_header[\"WakeTime\"]] for row in survey_results_public_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa52bf",
   "metadata": {},
   "source": [
    "Same as we did before, let's create a custom order excluding the unwanted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61244506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Between 5:00 - 6:00 AM',\n",
       " 'Between 6:01 - 7:00 AM',\n",
       " 'Between 7:01 - 8:00 AM',\n",
       " 'Between 8:01 - 9:00 AM',\n",
       " 'Between 9:01 - 10:00 AM',\n",
       " 'Between 10:01 - 11:00 AM',\n",
       " 'Between 11:01 AM - 12:00 PM']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "waketime_labels = [\n",
    "    'Between 5:00 - 6:00 AM',\n",
    "    'Between 6:01 - 7:00 AM',\n",
    "    'Between 7:01 - 8:00 AM',\n",
    "    'Between 8:01 - 9:00 AM',\n",
    "    'Between 9:01 - 10:00 AM',\n",
    "    'Between 10:01 - 11:00 AM',\n",
    "    'Between 11:01 AM - 12:00 PM'\n",
    "]\n",
    "waketime_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1374d3f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Between 6:01 - 7:00 AM', 102210.88438698038),\n",
       " ('Between 7:01 - 8:00 AM', 94755.63425437671),\n",
       " ('Between 9:01 - 10:00 AM', 80079.01123069724),\n",
       " ('Between 8:01 - 9:00 AM', 90289.8809624314),\n",
       " ('Between 5:00 - 6:00 AM', 109336.22882851094),\n",
       " ('Between 10:01 - 11:00 AM', 69036.42434782609),\n",
       " ('Between 11:01 AM - 12:00 PM', 60238.74585635359)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_average_waketime(selected_countries):  \n",
    "    def valid_row(row):\n",
    "        if row[survey_results_header[\"Country\"]] not in selected_countries:\n",
    "            return False\n",
    "        \n",
    "        if row[survey_results_header[\"WakeTime\"]] not in waketime_labels:\n",
    "            return False\n",
    "        \n",
    "        if row[survey_results_header[\"ConvertedSalary\"]]  == \"NA\":\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    salary_dict = defaultdict(lambda: [])\n",
    "    rows = [row for row in tab_1_graph_2_row if valid_row(row)]\n",
    "    \n",
    "    for row in rows:\n",
    "        salary_dict[row[survey_results_header[\"WakeTime\"]]].append(float(row[survey_results_header[\"ConvertedSalary\"]]))\n",
    "\n",
    "    for x in salary_dict:\n",
    "        salary_dict[x] = stats.describe(salary_dict[x]).mean\n",
    "        \n",
    "    return [(x, salary_dict[x]) for x in salary_dict]\n",
    "\n",
    "\n",
    "get_average_waketime(countries_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d0f48dab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def panel_1_graph_3():\n",
    "    \n",
    "    default_selected_countries = countries_label\n",
    "\n",
    "    trend_data = get_average_waketime(default_selected_countries)\n",
    "    \n",
    "    data = {\n",
    "            'WakeTime' : [row[0] for row in trend_data],\n",
    "            'avg'   : [row[1] for row in trend_data]\n",
    "       }\n",
    "    \n",
    "    source = ColumnDataSource(data=data)\n",
    "\n",
    "    TOOLTIPS = [\n",
    "        (\"WakeTime\", \"@WakeTime\"),\n",
    "        (\"Average\", \"@avg{0,0.00}\"),\n",
    "    ]\n",
    "    \n",
    "    p = figure(\n",
    "            x_range=waketime_labels,\n",
    "            title=\"Average salary by wake time\",\n",
    "            plot_width=500, \n",
    "#             plot_height=700,\n",
    "            tooltips=TOOLTIPS\n",
    "    )\n",
    "    \n",
    "    p.xaxis.axis_label = \"Wake Time\"\n",
    "    p.yaxis.axis_label = \"Average salary (in dollars)\"\n",
    "    p.yaxis.formatter = NumeralTickFormatter(format=\"0,0\") \n",
    "    \n",
    "\n",
    "\n",
    "    p.vbar(\n",
    "        x='WakeTime', \n",
    "        top='avg', \n",
    "        source=source, \n",
    "        width=0.9)\n",
    "\n",
    "    p.xaxis.major_label_orientation = 0.55\n",
    "    \n",
    "    \n",
    "    def filter_data(selected_countries = countries_label):\n",
    "        trend_data = get_average_waketime(selected_countries)\n",
    "        new_data = {\n",
    "                    'WakeTime' : [row[0] for row in trend_data],\n",
    "                    'avg'   : [row[1] for row in trend_data]\n",
    "               }\n",
    "        return ColumnDataSource(data=new_data)\n",
    "        \n",
    "    def update_selected_countries(selected_countries = []):\n",
    "        updated_source = filter_data(selected_countries)\n",
    "        source.data.update(updated_source.data)\n",
    "    \n",
    "    return (layout([p]), update_selected_countries)\n",
    "\n",
    "## Uncomment this to have a preview of the plot\n",
    "\n",
    "# def wrapper(doc):\n",
    "#     p, upd = panel_1_graph_3()\n",
    "#     doc.add_root(p)\n",
    "    \n",
    "# handler = FunctionHandler(wrapper)\n",
    "# app = Application(handler)\n",
    "# show(app, notebook_url=\"http://localhost:8889\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda9caeb",
   "metadata": {},
   "source": [
    "### 3) Pack all the plots in the same panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2cbdd6f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1130\">\n",
       "  (function() {\n",
       "    const xhr = new XMLHttpRequest()\n",
       "    xhr.responseType = 'blob';\n",
       "    xhr.open('GET', \"http://localhost:54862/autoload.js?bokeh-autoload-element=1130&bokeh-absolute-url=http://localhost:54862&resources=none\", true);\n",
       "    \n",
       "    xhr.onload = function (event) {\n",
       "      const script = document.createElement('script');\n",
       "      const src = URL.createObjectURL(event.target.response);\n",
       "      script.src = src;\n",
       "      document.body.appendChild(script);\n",
       "    };\n",
       "    xhr.send();\n",
       "  })();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "61ce9cccb85c426e96caedfda3969e6a"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def panel_1():\n",
    "    #This is the default value of the select component\n",
    "    default_selected_countries = countries_label\n",
    "\n",
    "    p1, update_p1 = panel_1_graph_1()\n",
    "    p2, update_p2 = panel_1_graph_2()\n",
    "    p3, update_p3 = panel_1_graph_3()\n",
    "    \n",
    "    # MultiSelect of the countries\n",
    "    select_countries = MultiSelect(\n",
    "        title=\"Respondents country\", \n",
    "        options=countries_label, \n",
    "        width=200,\n",
    "        height=300,\n",
    "        value=default_selected_countries\n",
    "    )\n",
    "\n",
    "\n",
    "    def update_data(attr, old, new):\n",
    "        selected_countries = select_countries.value\n",
    "        update_p1(selected_countries)\n",
    "        update_p2(selected_countries)\n",
    "        update_p3(selected_countries)\n",
    "        \n",
    "    select_countries.on_change('value', update_data)\n",
    "    \n",
    "    return layout([[select_countries], [p3, p2], [p1]])\n",
    "\n",
    "## Uncomment this to have a preview of the plot\n",
    "\n",
    "def wrapper(doc):\n",
    "     doc.add_root(panel_1())\n",
    "        \n",
    "handler = FunctionHandler(wrapper)\n",
    "app = Application(handler)\n",
    "show(app, notebook_url=\"http://localhost:8889\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbaa152b",
   "metadata": {},
   "source": [
    "## Final Consideration\n",
    "\n",
    "Looking at the above graphs, we can notice a trend in the average salary by looking at the data grouped by company size and wake time. \n",
    "\n",
    "### Consideration selecting all countries (Global)\n",
    "\n",
    "Looking at the average salary by company size, a higher salary corresponds to a larger company, with a rising trend. \n",
    "Looking at the average salary by wake time, higher salaries (with an average of 110K dollars) are matched to respondents who woke up earlier between 5 AM and 6 AM; the trend seems to fall on respondents who woke up late in the morning between 11:01 AM and 12:00 PM. So, this graph suggests that people who woke up earlier in the morning are more productive than those who woke up late. \n",
    "\n",
    "### Consideration selecting only Italy\n",
    "The average salary of Italian people by wake time seems to follow the same trend of the chart with all countries with a big exception. People who woke up between 10:01 AM and 11:00 AM have an average salary of 331K dollars, more than three times the higher than average on the global chart. \n",
    "\n",
    "### Consideration selecting only Poland\n",
    "Poland is far from the trend of both Italy and Global. Here all the averages are close to themselves, with an average that lives between 35K and 43K dollars. People who woke up late in the morning between 11:01 AM and 12:00 PM, on average, earn three times more than the other respondents with an average salary of 150K. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea8045",
   "metadata": {},
   "source": [
    "#### Tab 2: Pandas QA on Stack Overflow dataset\n",
    "- Show a graphical representation that allows investigating the relationship between Pandas QA scores and views.\n",
    "- Interactively filter the data based on the post date (column Created). \n",
    "- Provide a tooltip that shows the title of the corresponding post. What is the title of three outliers of your choice? \n",
    "- Distinguish between the accepted and not accepted answers. What do you notice for the posts with high numbers of views?  \n",
    "- Note that you should keep only posts where views are >= 1000. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce412bb",
   "metadata": {},
   "source": [
    "# Tab 2\n",
    "\n",
    "Let's create the plots for the second tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71ee2c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OcchipintiGianlorenzo_Assignment1.ipynb survey_results_schema.csv\r\n",
      "SO_pandas.csv                           used_cars_dataset.csv\r\n",
      "survey_results_public.csv\r\n"
     ]
    }
   ],
   "source": [
    "! ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9567420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'': 0,\n",
       " 'Post Link': 1,\n",
       " 'Type': 2,\n",
       " 'Title': 3,\n",
       " 'Markdown': 4,\n",
       " 'Tags': 5,\n",
       " 'Created': 6,\n",
       " 'Last Edit': 7,\n",
       " 'Edited By': 8,\n",
       " 'Score': 9,\n",
       " 'Favorites': 10,\n",
       " 'Views': 11,\n",
       " 'Answers': 12,\n",
       " 'Accepted': 13,\n",
       " 'CW': 14,\n",
       " 'Closed': 15}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"SO_pandas.csv\", 'r') as csvfile:\n",
    "    csvreader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
    "    pandas_rows = [row for row in csvreader]\n",
    "    \n",
    "# survey_results_header = {}\n",
    "# survey_results_schema_rows.pop(0)\n",
    "# survey_results_public_rows.pop(0)\n",
    "# for x in zip(range(len(survey_results_schema_rows)), [row[0] for row in survey_results_schema_rows]):\n",
    "#     survey_results_header[x[1]] = x[0]\n",
    "\n",
    "# survey_results_header\n",
    "row_header = pandas_rows.pop(0)\n",
    "pandas_header = {}\n",
    "\n",
    "for x in zip(range(len(row_header)), row_header):\n",
    "    pandas_header[x[1]] = x[0]\n",
    "    \n",
    "pandas_header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7d8e92",
   "metadata": {},
   "source": [
    "Let's check the values of the column Score, Views, Accepted, Title and Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9079cb80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'0': 12738,\n",
       "         '2': 19245,\n",
       "         '4': 6023,\n",
       "         '6': 1880,\n",
       "         '1': 26048,\n",
       "         '5': 3344,\n",
       "         '7': 1246,\n",
       "         '8': 779,\n",
       "         '17': 130,\n",
       "         '22': 67,\n",
       "         '21': 73,\n",
       "         '62': 8,\n",
       "         '16': 151,\n",
       "         '33': 22,\n",
       "         '95': 11,\n",
       "         '12': 269,\n",
       "         '3': 11318,\n",
       "         '20': 91,\n",
       "         '39': 26,\n",
       "         '19': 96,\n",
       "         '10': 431,\n",
       "         '13': 265,\n",
       "         '9': 585,\n",
       "         '73': 11,\n",
       "         '76': 4,\n",
       "         '38': 23,\n",
       "         '54': 12,\n",
       "         '23': 78,\n",
       "         '263': 1,\n",
       "         '11': 368,\n",
       "         '25': 52,\n",
       "         '37': 23,\n",
       "         '26': 59,\n",
       "         '44': 20,\n",
       "         '45': 20,\n",
       "         '31': 34,\n",
       "         '-1': 108,\n",
       "         '61': 10,\n",
       "         '105': 4,\n",
       "         '65': 7,\n",
       "         '24': 56,\n",
       "         '34': 35,\n",
       "         '124': 2,\n",
       "         '-2': 16,\n",
       "         '28': 47,\n",
       "         '14': 215,\n",
       "         '69': 5,\n",
       "         '30': 39,\n",
       "         '18': 117,\n",
       "         '29': 45,\n",
       "         '183': 3,\n",
       "         '15': 173,\n",
       "         '27': 49,\n",
       "         '108': 6,\n",
       "         '64': 8,\n",
       "         '50': 14,\n",
       "         '51': 13,\n",
       "         '56': 11,\n",
       "         '32': 27,\n",
       "         '422': 1,\n",
       "         '36': 30,\n",
       "         '47': 15,\n",
       "         '46': 13,\n",
       "         '43': 23,\n",
       "         '96': 3,\n",
       "         '35': 32,\n",
       "         '137': 3,\n",
       "         '42': 24,\n",
       "         '87': 4,\n",
       "         '83': 6,\n",
       "         '68': 12,\n",
       "         '60': 12,\n",
       "         '41': 18,\n",
       "         '58': 8,\n",
       "         '48': 21,\n",
       "         '-3': 5,\n",
       "         '78': 5,\n",
       "         '152': 3,\n",
       "         '53': 13,\n",
       "         '118': 3,\n",
       "         '49': 17,\n",
       "         '208': 2,\n",
       "         '160': 1,\n",
       "         '451': 1,\n",
       "         '52': 17,\n",
       "         '106': 2,\n",
       "         '70': 6,\n",
       "         '63': 6,\n",
       "         '-7': 1,\n",
       "         '136': 2,\n",
       "         '93': 3,\n",
       "         '148': 1,\n",
       "         '71': 11,\n",
       "         '79': 6,\n",
       "         '84': 5,\n",
       "         '149': 2,\n",
       "         '143': 3,\n",
       "         '119': 2,\n",
       "         '80': 5,\n",
       "         '89': 7,\n",
       "         '40': 19,\n",
       "         '77': 12,\n",
       "         '129': 2,\n",
       "         '97': 3,\n",
       "         '107': 3,\n",
       "         '59': 8,\n",
       "         '145': 2,\n",
       "         '85': 5,\n",
       "         '91': 5,\n",
       "         '187': 3,\n",
       "         '81': 3,\n",
       "         '66': 6,\n",
       "         '281': 1,\n",
       "         '82': 7,\n",
       "         '313': 1,\n",
       "         '100': 4,\n",
       "         '74': 9,\n",
       "         '109': 2,\n",
       "         '147': 2,\n",
       "         '103': 5,\n",
       "         '86': 7,\n",
       "         '350': 1,\n",
       "         '110': 2,\n",
       "         '452': 1,\n",
       "         '554': 1,\n",
       "         '101': 3,\n",
       "         '164': 2,\n",
       "         '114': 3,\n",
       "         '167': 1,\n",
       "         '217': 2,\n",
       "         '185': 1,\n",
       "         '680': 1,\n",
       "         '102': 6,\n",
       "         '131': 1,\n",
       "         '279': 1,\n",
       "         '72': 8,\n",
       "         '218': 2,\n",
       "         '130': 1,\n",
       "         '98': 2,\n",
       "         '67': 6,\n",
       "         '200': 2,\n",
       "         '441': 2,\n",
       "         '113': 2,\n",
       "         '155': 1,\n",
       "         '141': 4,\n",
       "         '99': 1,\n",
       "         '126': 3,\n",
       "         '55': 11,\n",
       "         '207': 1,\n",
       "         '319': 2,\n",
       "         '470': 1,\n",
       "         '190': 1,\n",
       "         '262': 2,\n",
       "         '163': 1,\n",
       "         '193': 1,\n",
       "         '134': 1,\n",
       "         '250': 1,\n",
       "         '280': 1,\n",
       "         '153': 1,\n",
       "         '176': 2,\n",
       "         '75': 5,\n",
       "         '376': 1,\n",
       "         '142': 2,\n",
       "         '216': 1,\n",
       "         '390': 1,\n",
       "         '158': 1,\n",
       "         '239': 1,\n",
       "         '178': 2,\n",
       "         '288': 1,\n",
       "         '203': 1,\n",
       "         '286': 1,\n",
       "         '431': 1,\n",
       "         '245': 1,\n",
       "         '182': 3,\n",
       "         '90': 1,\n",
       "         '57': 2,\n",
       "         '223': 1,\n",
       "         '236': 1,\n",
       "         '150': 1,\n",
       "         '211': 1,\n",
       "         '104': 2,\n",
       "         '246': 1,\n",
       "         '209': 1,\n",
       "         '128': 3,\n",
       "         '230': 2,\n",
       "         '260': 2,\n",
       "         '151': 4,\n",
       "         '252': 1,\n",
       "         '-4': 1,\n",
       "         '139': 4,\n",
       "         '92': 5,\n",
       "         '127': 1,\n",
       "         '709': 1,\n",
       "         '175': 1,\n",
       "         '268': 1,\n",
       "         '295': 1,\n",
       "         '88': 2,\n",
       "         '154': 1,\n",
       "         '574': 1,\n",
       "         '123': 1,\n",
       "         '282': 1,\n",
       "         '499': 1,\n",
       "         '188': 2,\n",
       "         '133': 1,\n",
       "         '971': 1,\n",
       "         '222': 2,\n",
       "         '189': 1,\n",
       "         '330': 1,\n",
       "         '180': 1,\n",
       "         '191': 1,\n",
       "         '206': 2,\n",
       "         '138': 1,\n",
       "         '-6': 1,\n",
       "         '174': 1,\n",
       "         '304': 1,\n",
       "         '132': 2,\n",
       "         '184': 2,\n",
       "         '332': 1,\n",
       "         '235': 1,\n",
       "         '595': 1,\n",
       "         '165': 1,\n",
       "         '116': 1,\n",
       "         '111': 1,\n",
       "         '177': 1,\n",
       "         '144': 1,\n",
       "         '492': 1,\n",
       "         '707': 1,\n",
       "         '204': 1,\n",
       "         '258': 1,\n",
       "         '2116': 1,\n",
       "         '214': 2,\n",
       "         '296': 1,\n",
       "         '234': 1,\n",
       "         '404': 2,\n",
       "         '407': 1,\n",
       "         '121': 1,\n",
       "         '445': 1,\n",
       "         '675': 1,\n",
       "         '238': 1,\n",
       "         '251': 1,\n",
       "         '192': 1,\n",
       "         '307': 1,\n",
       "         '400': 1,\n",
       "         '413': 1,\n",
       "         '391': 1,\n",
       "         '2531': 1,\n",
       "         '361': 1,\n",
       "         '241': 1,\n",
       "         '233': 1,\n",
       "         '242': 1,\n",
       "         '401': 1,\n",
       "         '120': 1,\n",
       "         '140': 1,\n",
       "         '94': 1,\n",
       "         '112': 1,\n",
       "         '570': 1,\n",
       "         '335': 1,\n",
       "         '227': 1,\n",
       "         '179': 1})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[pandas_header[\"Score\"]] for row in pandas_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c15cc78f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'1312.0': 9,\n",
       "         '438.0': 35,\n",
       "         '734.0': 25,\n",
       "         '18242.0': 1,\n",
       "         '271.0': 43,\n",
       "         '385.0': 39,\n",
       "         '206.0': 70,\n",
       "         '3689.0': 2,\n",
       "         '1051.0': 11,\n",
       "         '1890.0': 4,\n",
       "         '3258.0': 4,\n",
       "         '8178.0': 1,\n",
       "         '7590.0': 1,\n",
       "         '1068.0': 16,\n",
       "         '1080.0': 13,\n",
       "         '1138.0': 10,\n",
       "         '10560.0': 1,\n",
       "         '11309.0': 1,\n",
       "         '817.0': 26,\n",
       "         '8657.0': 1,\n",
       "         '901.0': 13,\n",
       "         '223.0': 78,\n",
       "         '1311.0': 3,\n",
       "         '382.0': 54,\n",
       "         '373.0': 44,\n",
       "         '4405.0': 3,\n",
       "         '1315.0': 9,\n",
       "         '1043.0': 12,\n",
       "         '87.0': 232,\n",
       "         '1486.0': 6,\n",
       "         '12452.0': 1,\n",
       "         '260.0': 53,\n",
       "         '3771.0': 1,\n",
       "         '155.0': 98,\n",
       "         '11664.0': 1,\n",
       "         '45.0': 665,\n",
       "         '1000.0': 9,\n",
       "         '12442.0': 1,\n",
       "         '2970.0': 2,\n",
       "         '6726.0': 3,\n",
       "         '377.0': 45,\n",
       "         '530.0': 35,\n",
       "         '11717.0': 1,\n",
       "         '31179.0': 2,\n",
       "         '596.0': 29,\n",
       "         '6642.0': 3,\n",
       "         '748.0': 17,\n",
       "         '124.0': 110,\n",
       "         '3560.0': 6,\n",
       "         '3109.0': 5,\n",
       "         '8866.0': 3,\n",
       "         '5079.0': 1,\n",
       "         '30295.0': 1,\n",
       "         '407.0': 54,\n",
       "         '178.0': 67,\n",
       "         '409412.0': 1,\n",
       "         '22026.0': 1,\n",
       "         '1076.0': 8,\n",
       "         '113.0': 152,\n",
       "         '992.0': 10,\n",
       "         '2343.0': 7,\n",
       "         '384.0': 35,\n",
       "         '1483.0': 8,\n",
       "         '4068.0': 3,\n",
       "         '81.0': 249,\n",
       "         '112.0': 136,\n",
       "         '5185.0': 3,\n",
       "         '874.0': 8,\n",
       "         '3414.0': 4,\n",
       "         '134.0': 98,\n",
       "         '109.0': 173,\n",
       "         '4030.0': 2,\n",
       "         '24832.0': 1,\n",
       "         '32007.0': 1,\n",
       "         '114.0': 142,\n",
       "         '11145.0': 1,\n",
       "         '7948.0': 3,\n",
       "         '413.0': 46,\n",
       "         '2241.0': 5,\n",
       "         '987.0': 17,\n",
       "         '953.0': 16,\n",
       "         '55.0': 511,\n",
       "         '29009.0': 2,\n",
       "         '24372.0': 2,\n",
       "         '11021.0': 3,\n",
       "         '156873.0': 1,\n",
       "         '34505.0': 1,\n",
       "         '3670.0': 3,\n",
       "         '9154.0': 1,\n",
       "         '294.0': 56,\n",
       "         '65.0': 372,\n",
       "         '315.0': 43,\n",
       "         '10222.0': 1,\n",
       "         '47.0': 579,\n",
       "         '8869.0': 3,\n",
       "         '1356.0': 14,\n",
       "         '732.0': 24,\n",
       "         '8722.0': 1,\n",
       "         '2949.0': 1,\n",
       "         '80.0': 283,\n",
       "         '531.0': 26,\n",
       "         '14393.0': 1,\n",
       "         '2536.0': 3,\n",
       "         '3190.0': 4,\n",
       "         '93.0': 215,\n",
       "         '75.0': 339,\n",
       "         '154.0': 92,\n",
       "         '721.0': 16,\n",
       "         '288453.0': 1,\n",
       "         '72.0': 318,\n",
       "         '1510.0': 11,\n",
       "         '3730.0': 1,\n",
       "         '1757.0': 4,\n",
       "         '2520.0': 5,\n",
       "         '1803.0': 5,\n",
       "         '295.0': 52,\n",
       "         '1211.0': 6,\n",
       "         '69268.0': 1,\n",
       "         '28527.0': 1,\n",
       "         '894.0': 11,\n",
       "         '52269.0': 1,\n",
       "         '185.0': 84,\n",
       "         '89.0': 228,\n",
       "         '20546.0': 1,\n",
       "         '419.0': 42,\n",
       "         '3120.0': 3,\n",
       "         '143.0': 120,\n",
       "         '2106.0': 3,\n",
       "         '704.0': 18,\n",
       "         '77.0': 303,\n",
       "         '261743.0': 1,\n",
       "         '1229.0': 7,\n",
       "         '3751.0': 1,\n",
       "         '264.0': 61,\n",
       "         '7142.0': 1,\n",
       "         '92256.0': 1,\n",
       "         '979.0': 17,\n",
       "         '1178.0': 12,\n",
       "         '2128.0': 5,\n",
       "         '92.0': 175,\n",
       "         '1030.0': 17,\n",
       "         '1534.0': 7,\n",
       "         '346.0': 48,\n",
       "         '5536.0': 1,\n",
       "         '835.0': 23,\n",
       "         '820.0': 19,\n",
       "         '219.0': 67,\n",
       "         '6598.0': 1,\n",
       "         '6698.0': 1,\n",
       "         '58.0': 509,\n",
       "         '109030.0': 2,\n",
       "         '1873.0': 4,\n",
       "         '8538.0': 1,\n",
       "         '4820.0': 2,\n",
       "         '78.0': 295,\n",
       "         '16615.0': 1,\n",
       "         '1078.0': 9,\n",
       "         '99.0': 163,\n",
       "         '1863.0': 6,\n",
       "         '240.0': 79,\n",
       "         '4768.0': 2,\n",
       "         '765.0': 21,\n",
       "         '175.0': 75,\n",
       "         '668.0': 17,\n",
       "         '70.0': 350,\n",
       "         '4457.0': 1,\n",
       "         '230.0': 55,\n",
       "         '54885.0': 1,\n",
       "         '105.0': 165,\n",
       "         '436.0': 48,\n",
       "         '19687.0': 2,\n",
       "         '2504.0': 6,\n",
       "         '117.0': 143,\n",
       "         '1185.0': 12,\n",
       "         '1299.0': 8,\n",
       "         '2288.0': 4,\n",
       "         '8698.0': 1,\n",
       "         '12100.0': 1,\n",
       "         '234.0': 65,\n",
       "         '157.0': 67,\n",
       "         '132.0': 105,\n",
       "         '238.0': 71,\n",
       "         '3187.0': 7,\n",
       "         '879.0': 16,\n",
       "         '1387.0': 11,\n",
       "         '1039.0': 10,\n",
       "         '52792.0': 1,\n",
       "         '13170.0': 1,\n",
       "         '320.0': 55,\n",
       "         '861.0': 16,\n",
       "         '1790.0': 4,\n",
       "         '1050.0': 7,\n",
       "         '4441.0': 1,\n",
       "         '6484.0': 1,\n",
       "         '36825.0': 1,\n",
       "         '2136.0': 6,\n",
       "         '139.0': 106,\n",
       "         '1020.0': 11,\n",
       "         '3536.0': 2,\n",
       "         '1237.0': 5,\n",
       "         '575.0': 28,\n",
       "         '130.0': 107,\n",
       "         '400866.0': 1,\n",
       "         '22578.0': 1,\n",
       "         '211.0': 69,\n",
       "         '790.0': 16,\n",
       "         '2719.0': 3,\n",
       "         '338.0': 39,\n",
       "         '1823.0': 7,\n",
       "         '478.0': 45,\n",
       "         '1223.0': 10,\n",
       "         '620.0': 21,\n",
       "         '12714.0': 1,\n",
       "         '934.0': 13,\n",
       "         '59.0': 472,\n",
       "         '21432.0': 1,\n",
       "         '499.0': 33,\n",
       "         '2749.0': 4,\n",
       "         '101.0': 182,\n",
       "         '5238.0': 1,\n",
       "         '270.0': 46,\n",
       "         '91.0': 226,\n",
       "         '168.0': 86,\n",
       "         '195.0': 74,\n",
       "         '1298.0': 8,\n",
       "         '49266.0': 1,\n",
       "         '950.0': 11,\n",
       "         '11553.0': 1,\n",
       "         '3224.0': 6,\n",
       "         '1151.0': 8,\n",
       "         '318.0': 48,\n",
       "         '73.0': 311,\n",
       "         '788.0': 21,\n",
       "         '167.0': 97,\n",
       "         '579.0': 32,\n",
       "         '96.0': 196,\n",
       "         '583.0': 25,\n",
       "         '527.0': 25,\n",
       "         '3195.0': 2,\n",
       "         '5516.0': 2,\n",
       "         '237.0': 76,\n",
       "         '1713.0': 2,\n",
       "         '494.0': 33,\n",
       "         '4692.0': 2,\n",
       "         '8004.0': 2,\n",
       "         '12385.0': 1,\n",
       "         '2724.0': 4,\n",
       "         '181.0': 61,\n",
       "         '53.0': 526,\n",
       "         '47737.0': 2,\n",
       "         '426.0': 33,\n",
       "         '637.0': 14,\n",
       "         '9161.0': 3,\n",
       "         '172.0': 67,\n",
       "         '1061.0': 9,\n",
       "         '63.0': 435,\n",
       "         '88036.0': 1,\n",
       "         '780.0': 9,\n",
       "         '543.0': 23,\n",
       "         '2950.0': 2,\n",
       "         '176.0': 63,\n",
       "         '33.0': 600,\n",
       "         '1771.0': 8,\n",
       "         '4921.0': 2,\n",
       "         '100.0': 182,\n",
       "         '54557.0': 1,\n",
       "         '2009.0': 5,\n",
       "         '1929.0': 5,\n",
       "         '1779.0': 6,\n",
       "         '644.0': 24,\n",
       "         '429.0': 43,\n",
       "         '1708.0': 6,\n",
       "         '994.0': 7,\n",
       "         '357.0': 46,\n",
       "         '1007.0': 14,\n",
       "         '754.0': 11,\n",
       "         '110.0': 134,\n",
       "         '337.0': 40,\n",
       "         '129.0': 105,\n",
       "         '3455.0': 3,\n",
       "         '142.0': 93,\n",
       "         '601.0': 26,\n",
       "         '3254.0': 1,\n",
       "         '6987.0': 1,\n",
       "         '2056.0': 6,\n",
       "         '90.0': 223,\n",
       "         '1613.0': 5,\n",
       "         '3799.0': 1,\n",
       "         '14111.0': 1,\n",
       "         '3795.0': 1,\n",
       "         '1609.0': 14,\n",
       "         '56721.0': 1,\n",
       "         '495.0': 45,\n",
       "         '4578.0': 2,\n",
       "         '1627.0': 5,\n",
       "         '1181.0': 14,\n",
       "         '174.0': 72,\n",
       "         '4380.0': 1,\n",
       "         '6306.0': 2,\n",
       "         '4923.0': 3,\n",
       "         '103.0': 178,\n",
       "         '4300.0': 4,\n",
       "         '3453.0': 3,\n",
       "         '1084.0': 12,\n",
       "         '600.0': 25,\n",
       "         '6944.0': 2,\n",
       "         '868.0': 18,\n",
       "         '1093.0': 5,\n",
       "         '5660.0': 2,\n",
       "         '525.0': 24,\n",
       "         '565.0': 24,\n",
       "         '1941.0': 4,\n",
       "         '12659.0': 2,\n",
       "         '7411.0': 2,\n",
       "         '2409.0': 2,\n",
       "         '756.0': 13,\n",
       "         '9035.0': 1,\n",
       "         '57.0': 485,\n",
       "         '16489.0': 1,\n",
       "         '122.0': 121,\n",
       "         '46.0': 617,\n",
       "         '1956.0': 10,\n",
       "         '2821.0': 3,\n",
       "         '173.0': 75,\n",
       "         '179.0': 69,\n",
       "         '418.0': 46,\n",
       "         '44.0': 709,\n",
       "         '1074.0': 15,\n",
       "         '1157.0': 10,\n",
       "         '526.0': 32,\n",
       "         '685.0': 15,\n",
       "         '1494.0': 8,\n",
       "         '3827.0': 1,\n",
       "         '807.0': 14,\n",
       "         '3891.0': 8,\n",
       "         '1940.0': 2,\n",
       "         '156.0': 85,\n",
       "         '263.0': 66,\n",
       "         '642.0': 23,\n",
       "         '1218.0': 12,\n",
       "         '43.0': 668,\n",
       "         '296.0': 52,\n",
       "         '653.0': 18,\n",
       "         '31588.0': 1,\n",
       "         '3590.0': 1,\n",
       "         '16386.0': 1,\n",
       "         '2675.0': 4,\n",
       "         '17433.0': 2,\n",
       "         '56.0': 493,\n",
       "         '1546.0': 14,\n",
       "         '383.0': 38,\n",
       "         '24263.0': 1,\n",
       "         '2003.0': 5,\n",
       "         '906.0': 22,\n",
       "         '548.0': 20,\n",
       "         '770.0': 16,\n",
       "         '391.0': 29,\n",
       "         '2354.0': 10,\n",
       "         '32189.0': 1,\n",
       "         '40.0': 709,\n",
       "         '6182.0': 1,\n",
       "         '97.0': 199,\n",
       "         '955.0': 17,\n",
       "         '4111.0': 5,\n",
       "         '7571.0': 2,\n",
       "         '590.0': 27,\n",
       "         '935.0': 17,\n",
       "         '9589.0': 1,\n",
       "         '2213.0': 4,\n",
       "         '42.0': 697,\n",
       "         '5494.0': 3,\n",
       "         '713.0': 23,\n",
       "         '2630.0': 5,\n",
       "         '1424.0': 12,\n",
       "         '633.0': 18,\n",
       "         '35.0': 709,\n",
       "         '2826.0': 6,\n",
       "         '3671.0': 3,\n",
       "         '1188.0': 6,\n",
       "         '428.0': 48,\n",
       "         '1117.0': 15,\n",
       "         '1324.0': 12,\n",
       "         '62.0': 410,\n",
       "         '299.0': 58,\n",
       "         '652.0': 28,\n",
       "         '12743.0': 2,\n",
       "         '1385.0': 7,\n",
       "         '1134.0': 10,\n",
       "         '3793.0': 1,\n",
       "         '2860.0': 2,\n",
       "         '268.0': 50,\n",
       "         '854.0': 11,\n",
       "         '6977.0': 2,\n",
       "         '2785.0': 5,\n",
       "         '421.0': 47,\n",
       "         '258.0': 65,\n",
       "         '102.0': 196,\n",
       "         '1694.0': 4,\n",
       "         '2063.0': 2,\n",
       "         '334.0': 49,\n",
       "         '38.0': 695,\n",
       "         '45249.0': 1,\n",
       "         '3924.0': 3,\n",
       "         '13779.0': 1,\n",
       "         '1294.0': 10,\n",
       "         '54.0': 591,\n",
       "         '205.0': 80,\n",
       "         '1199.0': 6,\n",
       "         '12417.0': 2,\n",
       "         '867.0': 13,\n",
       "         '14099.0': 1,\n",
       "         '929.0': 16,\n",
       "         '15170.0': 3,\n",
       "         '1675.0': 3,\n",
       "         '9310.0': 1,\n",
       "         '1210.0': 11,\n",
       "         '106.0': 184,\n",
       "         '1380.0': 6,\n",
       "         '852.0': 13,\n",
       "         '482.0': 28,\n",
       "         '665.0': 15,\n",
       "         '7608.0': 1,\n",
       "         '450.0': 28,\n",
       "         '814.0': 17,\n",
       "         '362.0': 48,\n",
       "         '8998.0': 1,\n",
       "         '414.0': 40,\n",
       "         '307.0': 50,\n",
       "         '2021.0': 3,\n",
       "         '276.0': 59,\n",
       "         '36215.0': 1,\n",
       "         '973.0': 17,\n",
       "         '3696.0': 2,\n",
       "         '1314.0': 7,\n",
       "         '6210.0': 1,\n",
       "         '150205.0': 2,\n",
       "         '865.0': 19,\n",
       "         '535.0': 29,\n",
       "         '4086.0': 4,\n",
       "         '4613.0': 1,\n",
       "         '183.0': 96,\n",
       "         '1408.0': 12,\n",
       "         '229.0': 62,\n",
       "         '3964.0': 4,\n",
       "         '7492.0': 2,\n",
       "         '8286.0': 1,\n",
       "         '14759.0': 1,\n",
       "         '1840.0': 7,\n",
       "         '2570.0': 1,\n",
       "         '39.0': 675,\n",
       "         '226.0': 66,\n",
       "         '541.0': 27,\n",
       "         '94.0': 192,\n",
       "         '166.0': 80,\n",
       "         '5090.0': 1,\n",
       "         '196.0': 69,\n",
       "         '536.0': 17,\n",
       "         '136.0': 112,\n",
       "         '657.0': 23,\n",
       "         '272.0': 58,\n",
       "         '7259.0': 1,\n",
       "         '6334.0': 1,\n",
       "         '2052.0': 5,\n",
       "         '510.0': 26,\n",
       "         '3142.0': 4,\n",
       "         '972.0': 15,\n",
       "         '975.0': 14,\n",
       "         '113844.0': 3,\n",
       "         '3165.0': 2,\n",
       "         '1939.0': 6,\n",
       "         '34157.0': 1,\n",
       "         '3641.0': 2,\n",
       "         '1715.0': 9,\n",
       "         '3272.0': 2,\n",
       "         '67.0': 361,\n",
       "         '1617.0': 3,\n",
       "         '862.0': 13,\n",
       "         '6831.0': 1,\n",
       "         '9351.0': 3,\n",
       "         '2378.0': 1,\n",
       "         '1367.0': 13,\n",
       "         '13903.0': 1,\n",
       "         '17499.0': 1,\n",
       "         '131.0': 131,\n",
       "         '317.0': 57,\n",
       "         '94807.0': 2,\n",
       "         '2050.0': 6,\n",
       "         '673.0': 19,\n",
       "         '1466.0': 5,\n",
       "         '2927.0': 5,\n",
       "         '8449.0': 2,\n",
       "         '63027.0': 1,\n",
       "         '552.0': 36,\n",
       "         '1673.0': 6,\n",
       "         '2273.0': 5,\n",
       "         '350.0': 41,\n",
       "         '10751.0': 1,\n",
       "         '2048.0': 3,\n",
       "         '145.0': 113,\n",
       "         '940.0': 9,\n",
       "         '6265.0': 1,\n",
       "         '8002.0': 1,\n",
       "         '254.0': 48,\n",
       "         '345.0': 41,\n",
       "         '1526.0': 5,\n",
       "         '961.0': 15,\n",
       "         '9410.0': 1,\n",
       "         '736.0': 16,\n",
       "         '491.0': 33,\n",
       "         '188.0': 76,\n",
       "         '1542.0': 6,\n",
       "         '990.0': 9,\n",
       "         '2198.0': 4,\n",
       "         '313.0': 58,\n",
       "         '703.0': 24,\n",
       "         '5654.0': 1,\n",
       "         '1320.0': 7,\n",
       "         '1776.0': 6,\n",
       "         '3594.0': 3,\n",
       "         '1209.0': 6,\n",
       "         '368.0': 49,\n",
       "         '10573.0': 1,\n",
       "         '50.0': 621,\n",
       "         '232.0': 61,\n",
       "         '390.0': 39,\n",
       "         '406.0': 43,\n",
       "         '293.0': 43,\n",
       "         '1437.0': 9,\n",
       "         '375.0': 48,\n",
       "         '1547.0': 10,\n",
       "         '1523.0': 5,\n",
       "         '423.0': 40,\n",
       "         '1817.0': 5,\n",
       "         '203.0': 76,\n",
       "         '11502.0': 1,\n",
       "         '1971.0': 8,\n",
       "         '61.0': 454,\n",
       "         '6433.0': 1,\n",
       "         '4290.0': 3,\n",
       "         '427.0': 34,\n",
       "         '3950.0': 3,\n",
       "         '1692.0': 5,\n",
       "         '785.0': 18,\n",
       "         '744.0': 18,\n",
       "         '1880.0': 10,\n",
       "         '1423.0': 7,\n",
       "         '1633.0': 8,\n",
       "         '343.0': 42,\n",
       "         '632.0': 28,\n",
       "         '140.0': 113,\n",
       "         '360.0': 47,\n",
       "         '623.0': 27,\n",
       "         '1243.0': 12,\n",
       "         '910.0': 13,\n",
       "         '244.0': 56,\n",
       "         '386.0': 41,\n",
       "         '214.0': 56,\n",
       "         '336.0': 53,\n",
       "         '5634.0': 2,\n",
       "         '207.0': 87,\n",
       "         '2119.0': 6,\n",
       "         '125.0': 152,\n",
       "         '12037.0': 2,\n",
       "         '458.0': 36,\n",
       "         '194.0': 66,\n",
       "         '8354.0': 1,\n",
       "         '1241.0': 8,\n",
       "         '14026.0': 1,\n",
       "         '11122.0': 1,\n",
       "         '2713.0': 5,\n",
       "         '311.0': 49,\n",
       "         '966.0': 15,\n",
       "         '4989.0': 1,\n",
       "         '917.0': 10,\n",
       "         '2130.0': 5,\n",
       "         '151.0': 85,\n",
       "         '104.0': 170,\n",
       "         '287.0': 45,\n",
       "         '4320.0': 2,\n",
       "         '1300.0': 6,\n",
       "         '4070.0': 2,\n",
       "         '511.0': 35,\n",
       "         '3154.0': 4,\n",
       "         '95.0': 188,\n",
       "         '672.0': 27,\n",
       "         '603.0': 28,\n",
       "         '86.0': 237,\n",
       "         '347.0': 40,\n",
       "         '515.0': 37,\n",
       "         '1976.0': 8,\n",
       "         '339.0': 36,\n",
       "         '34.0': 735,\n",
       "         '1306.0': 9,\n",
       "         '10488.0': 1,\n",
       "         '3130.0': 3,\n",
       "         '27983.0': 1,\n",
       "         '123.0': 128,\n",
       "         '32544.0': 1,\n",
       "         '64.0': 415,\n",
       "         '1763.0': 6,\n",
       "         '369.0': 46,\n",
       "         '1643.0': 3,\n",
       "         '832.0': 17,\n",
       "         '278.0': 56,\n",
       "         '202.0': 76,\n",
       "         '10296.0': 2,\n",
       "         '5918.0': 2,\n",
       "         '2152.0': 7,\n",
       "         '289.0': 66,\n",
       "         '1100.0': 14,\n",
       "         '711.0': 25,\n",
       "         '5199.0': 1,\n",
       "         '997.0': 16,\n",
       "         '3957.0': 3,\n",
       "         '165.0': 77,\n",
       "         '1725.0': 7,\n",
       "         '324.0': 77,\n",
       "         '1085.0': 18,\n",
       "         '866.0': 21,\n",
       "         '1995.0': 6,\n",
       "         '2487.0': 3,\n",
       "         '126.0': 131,\n",
       "         '13276.0': 1,\n",
       "         '291.0': 71,\n",
       "         '574.0': 28,\n",
       "         '233.0': 70,\n",
       "         '1816.0': 7,\n",
       "         '3020.0': 3,\n",
       "         '518.0': 28,\n",
       "         '5344.0': 3,\n",
       "         '273.0': 53,\n",
       "         '778.0': 11,\n",
       "         '163.0': 78,\n",
       "         '683.0': 16,\n",
       "         '88.0': 209,\n",
       "         '121.0': 138,\n",
       "         '599.0': 19,\n",
       "         '192.0': 79,\n",
       "         '4606.0': 5,\n",
       "         '661.0': 21,\n",
       "         '650.0': 34,\n",
       "         '144.0': 103,\n",
       "         '153.0': 69,\n",
       "         '1047.0': 10,\n",
       "         '949.0': 22,\n",
       "         '680.0': 24,\n",
       "         '456.0': 39,\n",
       "         '729.0': 17,\n",
       "         '1388.0': 10,\n",
       "         '452.0': 28,\n",
       "         '372.0': 41,\n",
       "         '28029.0': 1,\n",
       "         '119.0': 122,\n",
       "         '872.0': 15,\n",
       "         '7563.0': 1,\n",
       "         '487.0': 31,\n",
       "         '9039.0': 2,\n",
       "         '3296.0': 3,\n",
       "         '1194.0': 9,\n",
       "         '51.0': 597,\n",
       "         '1133.0': 19,\n",
       "         '60.0': 489,\n",
       "         '186.0': 59,\n",
       "         '1280.0': 10,\n",
       "         '331.0': 52,\n",
       "         '7113.0': 3,\n",
       "         '1113.0': 9,\n",
       "         '111.0': 142,\n",
       "         '4595.0': 2,\n",
       "         '210.0': 67,\n",
       "         '79.0': 293,\n",
       "         '851.0': 22,\n",
       "         '913.0': 12,\n",
       "         '379.0': 57,\n",
       "         '7200.0': 2,\n",
       "         '199.0': 65,\n",
       "         '25085.0': 1,\n",
       "         '74.0': 343,\n",
       "         '948.0': 14,\n",
       "         '66.0': 403,\n",
       "         '298.0': 49,\n",
       "         '9359.0': 1,\n",
       "         '1345.0': 12,\n",
       "         '2005.0': 3,\n",
       "         '308.0': 66,\n",
       "         '83.0': 253,\n",
       "         '1473.0': 6,\n",
       "         '364.0': 65,\n",
       "         '135.0': 119,\n",
       "         '36.0': 682,\n",
       "         '204.0': 64,\n",
       "         '469.0': 35,\n",
       "         '182.0': 62,\n",
       "         '6426.0': 1,\n",
       "         '2589.0': 3,\n",
       "         '1192.0': 8,\n",
       "         '1343.0': 7,\n",
       "         '1263.0': 13,\n",
       "         '1008.0': 20,\n",
       "         '787.0': 19,\n",
       "         '4032.0': 1,\n",
       "         '57064.0': 1,\n",
       "         '323.0': 59,\n",
       "         '309.0': 49,\n",
       "         '235.0': 64,\n",
       "         '2986.0': 2,\n",
       "         '1233.0': 14,\n",
       "         '37.0': 727,\n",
       "         '1321.0': 4,\n",
       "         '755.0': 14,\n",
       "         '48.0': 641,\n",
       "         '29959.0': 2,\n",
       "         '306.0': 41,\n",
       "         '4986.0': 2,\n",
       "         '30.0': 578,\n",
       "         '329.0': 47,\n",
       "         '1182.0': 4,\n",
       "         '881.0': 19,\n",
       "         '415.0': 24,\n",
       "         '681.0': 19,\n",
       "         '2655.0': 2,\n",
       "         '5424.0': 3,\n",
       "         '545.0': 21,\n",
       "         '5481.0': 2,\n",
       "         '1091.0': 9,\n",
       "         '628.0': 21,\n",
       "         '521.0': 33,\n",
       "         '10111.0': 3,\n",
       "         '26.0': 482,\n",
       "         '28204.0': 2,\n",
       "         '32.0': 622,\n",
       "         '500.0': 41,\n",
       "         '158.0': 76,\n",
       "         '69.0': 368,\n",
       "         '285.0': 63,\n",
       "         '984.0': 15,\n",
       "         '21.0': 298,\n",
       "         '697.0': 11,\n",
       "         '236.0': 45,\n",
       "         '1037.0': 9,\n",
       "         '2281.0': 4,\n",
       "         '533.0': 32,\n",
       "         '381.0': 47,\n",
       "         '1058.0': 9,\n",
       "         '27.0': 554,\n",
       "         '164.0': 102,\n",
       "         '363.0': 32,\n",
       "         '481.0': 44,\n",
       "         '120.0': 129,\n",
       "         '4663.0': 3,\n",
       "         '667.0': 13,\n",
       "         '4244.0': 1,\n",
       "         '1264.0': 14,\n",
       "         '749.0': 14,\n",
       "         '7870.0': 1,\n",
       "         '693.0': 23,\n",
       "         '1267.0': 14,\n",
       "         '3543.0': 4,\n",
       "         '310.0': 54,\n",
       "         '2372.0': 6,\n",
       "         '547.0': 24,\n",
       "         '1245.0': 12,\n",
       "         '292.0': 56,\n",
       "         '3218.0': 4,\n",
       "         '1734.0': 6,\n",
       "         '502.0': 37,\n",
       "         '464.0': 31,\n",
       "         '12530.0': 1,\n",
       "         '1582.0': 6,\n",
       "         '1903.0': 5,\n",
       "         '169.0': 70,\n",
       "         '1228.0': 9,\n",
       "         '209.0': 57,\n",
       "         '4273.0': 1,\n",
       "         '10815.0': 2,\n",
       "         '127.0': 113,\n",
       "         '731.0': 22,\n",
       "         '4607.0': 2,\n",
       "         '2323.0': 4,\n",
       "         '5755.0': 1,\n",
       "         '1786.0': 6,\n",
       "         '431.0': 49,\n",
       "         '698.0': 31,\n",
       "         '1049.0': 9,\n",
       "         '2311.0': 5,\n",
       "         '1413.0': 13,\n",
       "         '643.0': 27,\n",
       "         '675.0': 19,\n",
       "         '2580.0': 3,\n",
       "         '727.0': 13,\n",
       "         '137.0': 107,\n",
       "         '1362.0': 12,\n",
       "         '1414.0': 12,\n",
       "         '190.0': 76,\n",
       "         '1481.0': 6,\n",
       "         '41.0': 674,\n",
       "         '1135.0': 5,\n",
       "         '2890.0': 3,\n",
       "         '1137.0': 7,\n",
       "         '958.0': 10,\n",
       "         '564.0': 33,\n",
       "         '138.0': 109,\n",
       "         '332.0': 56,\n",
       "         '389.0': 45,\n",
       "         '1877.0': 2,\n",
       "         '118.0': 153,\n",
       "         '397.0': 46,\n",
       "         '4442.0': 2,\n",
       "         '877.0': 20,\n",
       "         '2966.0': 5,\n",
       "         '11349.0': 1,\n",
       "         '546.0': 34,\n",
       "         '10001.0': 3,\n",
       "         '516.0': 32,\n",
       "         '216.0': 68,\n",
       "         '714.0': 19,\n",
       "         '1583.0': 9,\n",
       "         '23696.0': 1,\n",
       "         '9751.0': 1,\n",
       "         '6721.0': 3,\n",
       "         '2163.0': 3,\n",
       "         '10437.0': 3,\n",
       "         '6933.0': 3,\n",
       "         '1770.0': 4,\n",
       "         '2069.0': 6,\n",
       "         '14539.0': 1,\n",
       "         '4248.0': 5,\n",
       "         '': 46,\n",
       "         '180.0': 77,\n",
       "         '1488.0': 10,\n",
       "         '16426.0': 1,\n",
       "         '2793.0': 2,\n",
       "         '2363.0': 6,\n",
       "         '443.0': 33,\n",
       "         '267.0': 62,\n",
       "         '9997.0': 2,\n",
       "         '2768.0': 3,\n",
       "         '341.0': 39,\n",
       "         '304.0': 47,\n",
       "         '162.0': 99,\n",
       "         '566.0': 19,\n",
       "         '300.0': 54,\n",
       "         '2166.0': 4,\n",
       "         '1251.0': 2,\n",
       "         '76549.0': 1,\n",
       "         '6733.0': 3,\n",
       "         '68.0': 363,\n",
       "         '17529.0': 1,\n",
       "         '115.0': 148,\n",
       "         '939.0': 20,\n",
       "         '2330.0': 2,\n",
       "         '612.0': 24,\n",
       "         '4671.0': 1,\n",
       "         '11196.0': 1,\n",
       "         '614.0': 25,\n",
       "         '1125.0': 11,\n",
       "         '71.0': 354,\n",
       "         '571.0': 29,\n",
       "         '465.0': 35,\n",
       "         '715.0': 18,\n",
       "         '496.0': 30,\n",
       "         '490.0': 34,\n",
       "         '367.0': 44,\n",
       "         '2203.0': 2,\n",
       "         '6432.0': 2,\n",
       "         '2262.0': 4,\n",
       "         '394.0': 46,\n",
       "         '1798.0': 2,\n",
       "         '695.0': 21,\n",
       "         '1634.0': 5,\n",
       "         '1177.0': 5,\n",
       "         '84.0': 257,\n",
       "         '10689.0': 1,\n",
       "         '691.0': 25,\n",
       "         '1005.0': 6,\n",
       "         '844.0': 13,\n",
       "         '1465.0': 5,\n",
       "         '353.0': 56,\n",
       "         '141.0': 85,\n",
       "         '14861.0': 1,\n",
       "         '2639.0': 2,\n",
       "         '184.0': 73,\n",
       "         '1226.0': 12,\n",
       "         '197.0': 69,\n",
       "         '5911.0': 1,\n",
       "         '31.0': 664,\n",
       "         '1896.0': 4,\n",
       "         '4429.0': 2,\n",
       "         '470.0': 30,\n",
       "         '1111.0': 8,\n",
       "         '751.0': 17,\n",
       "         '7152.0': 1,\n",
       "         '830.0': 19,\n",
       "         '871.0': 16,\n",
       "         '312.0': 56,\n",
       "         '284.0': 54,\n",
       "         '1034.0': 10,\n",
       "         '3005.0': 2,\n",
       "         '836.0': 16,\n",
       "         '2852.0': 5,\n",
       "         '559.0': 23,\n",
       "         '340.0': 52,\n",
       "         '76.0': 322,\n",
       "         '243.0': 57,\n",
       "         '52.0': 563,\n",
       "         '1742.0': 5,\n",
       "         '220.0': 59,\n",
       "         '23527.0': 1,\n",
       "         '1103.0': 13,\n",
       "         '171.0': 69,\n",
       "         '10451.0': 4,\n",
       "         '19.0': 206,\n",
       "         '161.0': 90,\n",
       "         '593.0': 24,\n",
       "         '2235.0': 2,\n",
       "         '604.0': 33,\n",
       "         '400.0': 27,\n",
       "         '116.0': 149,\n",
       "         '1203.0': 15,\n",
       "         '4232.0': 3,\n",
       "         '1162.0': 5,\n",
       "         '6525.0': 1,\n",
       "         '8095.0': 1,\n",
       "         '480.0': 50,\n",
       "         '420.0': 39,\n",
       "         '1090.0': 10,\n",
       "         '5787.0': 2,\n",
       "         '1756.0': 4,\n",
       "         '2853.0': 3,\n",
       "         '2276.0': 3,\n",
       "         '6646.0': 2,\n",
       "         '13735.0': 2,\n",
       "         '826.0': 18,\n",
       "         '12376.0': 1,\n",
       "         '912.0': 12,\n",
       "         '663.0': 26,\n",
       "         '2238.0': 6,\n",
       "         '177.0': 88,\n",
       "         '6495.0': 1,\n",
       "         '3031.0': 1,\n",
       "         '1236.0': 11,\n",
       "         '5824.0': 2,\n",
       "         '1484.0': 9,\n",
       "         '1307.0': 11,\n",
       "         '160.0': 78,\n",
       "         '2670.0': 1,\n",
       "         '275.0': 51,\n",
       "         '771.0': 13,\n",
       "         '648.0': 19,\n",
       "         '17126.0': 1,\n",
       "         '960.0': 15,\n",
       "         '474.0': 39,\n",
       "         '943.0': 11,\n",
       "         '78906.0': 1,\n",
       "         '883.0': 14,\n",
       "         '1796.0': 6,\n",
       "         '85.0': 241,\n",
       "         '2370.0': 3,\n",
       "         '191.0': 73,\n",
       "         '6278.0': 2,\n",
       "         '4670.0': 1,\n",
       "         '568.0': 16,\n",
       "         '1141.0': 8,\n",
       "         '931.0': 13,\n",
       "         '833.0': 10,\n",
       "         '2049.0': 4,\n",
       "         '666.0': 36,\n",
       "         '277.0': 37,\n",
       "         '396.0': 47,\n",
       "         '1073.0': 13,\n",
       "         '335.0': 39,\n",
       "         '2704.0': 2,\n",
       "         '98.0': 243,\n",
       "         '10973.0': 3,\n",
       "         '1329.0': 6,\n",
       "         '328.0': 52,\n",
       "         '1762.0': 6,\n",
       "         '1749.0': 5,\n",
       "         '2030.0': 1,\n",
       "         '445.0': 32,\n",
       "         '1065.0': 14,\n",
       "         '221.0': 79,\n",
       "         '314.0': 47,\n",
       "         '13291.0': 1,\n",
       "         '19963.0': 1,\n",
       "         '3082.0': 4,\n",
       "         '537.0': 20,\n",
       "         '870.0': 14,\n",
       "         '585.0': 19,\n",
       "         '2246.0': 7,\n",
       "         '2551.0': 2,\n",
       "         '719.0': 17,\n",
       "         '1674.0': 8,\n",
       "         '150.0': 87,\n",
       "         '412.0': 44,\n",
       "         '4294.0': 1,\n",
       "         '10679.0': 2,\n",
       "         '266.0': 63,\n",
       "         '523.0': 23,\n",
       "         '885.0': 11,\n",
       "         ...})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[pandas_header[\"Views\"]] for row in pandas_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "84e8c6a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2015-05-27 10:38:38', 2),\n",
       " ('2015-06-22 20:03:10', 2),\n",
       " ('2015-07-01 16:27:53', 2),\n",
       " ('2016-03-24 17:34:53', 2),\n",
       " ('2017-05-11 13:21:47', 2),\n",
       " ('2017-09-14 16:21:25', 2),\n",
       " ('2017-05-11 21:32:47', 2),\n",
       " ('2017-11-22 16:34:32', 2),\n",
       " ('2018-01-24 17:17:15', 2),\n",
       " ('2018-03-09 17:50:22', 2),\n",
       " ('2018-04-23 15:43:07', 2),\n",
       " ('2018-10-20 03:10:34', 2),\n",
       " ('2018-08-29 19:31:46', 2),\n",
       " ('2018-07-19 18:27:24', 2),\n",
       " ('2018-09-26 18:14:39', 2),\n",
       " ('2017-08-05 19:17:03', 2),\n",
       " ('2016-05-13 18:00:40', 2),\n",
       " ('2016-06-15 17:46:38', 2),\n",
       " ('2017-07-14 03:01:39', 2),\n",
       " ('2017-10-25 05:54:35', 2),\n",
       " ('2016-03-16 19:43:24', 2),\n",
       " ('2018-02-14 19:44:55', 2),\n",
       " ('2017-09-27 11:21:37', 2),\n",
       " ('2017-12-28 13:14:30', 2),\n",
       " ('2017-09-05 03:27:51', 2),\n",
       " ('2017-09-07 05:25:30', 2),\n",
       " ('2017-09-06 07:39:22', 2),\n",
       " ('2017-09-05 17:55:24', 2),\n",
       " ('2017-08-01 00:34:57', 2),\n",
       " ('2017-09-08 08:41:31', 2),\n",
       " ('2018-06-08 10:44:49', 2),\n",
       " ('2017-11-06 08:23:11', 2),\n",
       " ('2017-11-07 07:34:39', 2),\n",
       " ('2017-11-07 14:08:52', 2),\n",
       " ('2017-09-15 07:20:40', 2),\n",
       " ('2015-04-22 14:21:45', 2),\n",
       " ('2015-07-31 06:23:56', 2),\n",
       " ('2018-02-09 11:43:02', 2),\n",
       " ('2018-02-12 13:54:20', 2),\n",
       " ('2017-07-05 21:16:21', 2),\n",
       " ('2018-11-11 16:02:51', 2),\n",
       " ('2018-03-13 15:08:27', 2),\n",
       " ('2017-07-25 20:12:20', 2),\n",
       " ('2018-05-08 09:14:35', 2),\n",
       " ('2018-04-05 12:07:19', 2),\n",
       " ('2018-05-21 07:58:07', 2),\n",
       " ('2018-05-12 16:54:46', 2),\n",
       " ('2018-07-10 15:06:57', 2),\n",
       " ('2018-05-16 03:18:07', 2),\n",
       " ('2018-05-28 04:20:57', 2),\n",
       " ('2018-01-12 21:22:25', 2),\n",
       " ('2018-01-16 20:57:48', 2),\n",
       " ('2018-10-27 10:02:51', 2),\n",
       " ('2018-01-18 23:14:03', 2),\n",
       " ('2018-12-26 04:36:11', 2),\n",
       " ('2018-12-14 12:39:21', 2),\n",
       " ('2018-06-25 04:09:07', 2),\n",
       " ('2018-03-27 14:38:50', 2),\n",
       " ('2018-05-29 11:57:59', 2),\n",
       " ('2018-06-02 17:50:58', 2),\n",
       " ('2018-12-10 03:12:39', 2),\n",
       " ('2018-12-06 06:41:15', 2),\n",
       " ('2016-05-09 23:32:12', 2),\n",
       " ('2016-07-13 08:06:50', 2),\n",
       " ('2016-05-22 17:39:23', 2),\n",
       " ('2016-10-24 20:08:00', 2),\n",
       " ('2016-11-23 16:33:56', 2),\n",
       " ('2015-10-30 12:50:31', 2),\n",
       " ('2016-06-14 20:07:05', 2),\n",
       " ('2016-06-17 17:30:43', 2),\n",
       " ('2016-06-07 23:32:54', 2),\n",
       " ('2016-12-30 10:24:58', 2),\n",
       " ('2016-06-07 16:41:09', 2),\n",
       " ('2016-04-20 17:04:53', 2),\n",
       " ('2016-11-30 00:07:17', 2),\n",
       " ('2017-01-07 09:16:57', 2),\n",
       " ('2017-05-30 23:39:13', 2),\n",
       " ('2016-12-21 08:16:36', 2),\n",
       " ('2017-04-03 23:03:12', 2),\n",
       " ('2017-04-04 00:46:13', 2),\n",
       " ('2017-07-25 14:25:15', 2),\n",
       " ('2017-03-23 00:12:41', 2),\n",
       " ('2017-03-29 00:41:07', 2),\n",
       " ('2017-06-26 17:32:25', 2),\n",
       " ('2017-11-10 23:23:29', 2),\n",
       " ('2017-11-07 08:00:28', 2),\n",
       " ('2018-11-08 23:17:10', 2),\n",
       " ('2016-04-25 18:57:46', 2),\n",
       " ('2017-06-02 23:21:32', 2),\n",
       " ('2017-06-09 23:12:54', 2),\n",
       " ('2017-01-27 09:41:18', 2),\n",
       " ('2017-09-23 13:41:22', 2),\n",
       " ('2017-08-14 16:54:59', 2),\n",
       " ('2017-08-17 20:55:07', 2),\n",
       " ('2018-04-03 00:25:14', 2),\n",
       " ('2018-10-24 13:03:26', 2),\n",
       " ('2018-07-23 07:39:42', 2),\n",
       " ('2018-03-22 10:33:36', 2),\n",
       " ('2012-12-07 22:06:52', 2),\n",
       " ('2012-04-22 22:42:16', 2),\n",
       " ('2012-12-04 13:08:29', 2),\n",
       " ('2013-05-26 15:07:12', 2),\n",
       " ('2013-05-25 18:45:44', 2),\n",
       " ('2014-03-27 15:48:01', 2),\n",
       " ('2014-03-31 19:12:46', 2),\n",
       " ('2014-04-25 09:13:52', 2),\n",
       " ('2017-05-14 20:52:54', 2),\n",
       " ('2017-05-15 06:11:02', 2),\n",
       " ('2014-06-01 01:30:03', 2),\n",
       " ('2017-03-10 15:05:09', 2),\n",
       " ('2017-08-17 14:09:05', 2),\n",
       " ('2017-08-17 14:00:28', 2),\n",
       " ('2012-09-29 18:53:49', 2),\n",
       " ('2017-12-05 20:07:16', 2),\n",
       " ('2017-09-12 05:45:58', 2),\n",
       " ('2017-12-27 03:00:08', 2),\n",
       " ('2012-04-27 09:45:58', 1),\n",
       " ('2012-05-07 14:34:44', 1),\n",
       " ('2012-05-08 13:50:52', 1),\n",
       " ('2012-04-25 09:14:04', 1),\n",
       " ('2012-04-29 18:12:49', 1),\n",
       " ('2012-05-04 22:10:32', 1),\n",
       " ('2012-05-19 14:20:15', 1),\n",
       " ('2012-04-24 00:09:57', 1),\n",
       " ('2012-04-30 08:15:08', 1),\n",
       " ('2012-04-30 14:12:53', 1),\n",
       " ('2012-04-30 19:16:44', 1),\n",
       " ('2012-05-07 12:27:34', 1),\n",
       " ('2012-04-26 08:55:39', 1),\n",
       " ('2012-04-30 20:30:09', 1),\n",
       " ('2012-05-12 19:48:42', 1),\n",
       " ('2012-12-26 01:22:39', 1),\n",
       " ('2012-04-18 22:13:14', 1),\n",
       " ('2012-04-27 22:11:43', 1),\n",
       " ('2012-05-08 11:16:52', 1),\n",
       " ('2012-05-14 20:08:07', 1),\n",
       " ('2012-04-27 10:24:41', 1),\n",
       " ('2012-04-30 08:37:21', 1),\n",
       " ('2012-04-30 09:07:37', 1),\n",
       " ('2012-05-08 18:01:27', 1),\n",
       " ('2013-05-10 13:54:14', 1),\n",
       " ('2013-05-27 12:18:18', 1),\n",
       " ('2013-05-31 16:46:17', 1),\n",
       " ('2012-05-19 14:29:38', 1),\n",
       " ('2012-05-26 14:21:22', 1),\n",
       " ('2012-05-28 19:05:00', 1),\n",
       " ('2012-05-08 20:07:04', 1),\n",
       " ('2012-05-13 14:16:22', 1),\n",
       " ('2012-05-14 19:56:04', 1),\n",
       " ('2012-05-20 00:37:07', 1),\n",
       " ('2012-05-27 16:03:24', 1),\n",
       " ('2012-05-07 20:18:44', 1),\n",
       " ('2012-05-13 14:36:03', 1),\n",
       " ('2012-05-14 21:30:19', 1),\n",
       " ('2012-05-27 21:22:04', 1),\n",
       " ('2012-05-18 09:56:29', 1),\n",
       " ('2012-05-19 08:45:37', 1),\n",
       " ('2012-05-30 14:24:37', 1),\n",
       " ('2013-03-24 20:22:41', 1),\n",
       " ('2013-05-10 13:13:41', 1),\n",
       " ('2013-06-20 14:43:20', 1),\n",
       " ('2013-06-20 17:19:21', 1),\n",
       " ('2013-07-03 14:42:36', 1),\n",
       " ('2013-08-29 16:01:03', 1),\n",
       " ('2012-05-27 17:30:32', 1),\n",
       " ('2012-05-27 20:10:29', 1),\n",
       " ('2012-05-28 13:25:28', 1),\n",
       " ('2013-01-04 23:19:27', 1),\n",
       " ('2012-05-28 15:09:59', 1),\n",
       " ('2012-05-29 16:36:26', 1),\n",
       " ('2013-03-17 20:50:16', 1),\n",
       " ('2012-05-14 20:21:42', 1),\n",
       " ('2012-05-19 19:20:07', 1),\n",
       " ('2012-05-22 22:07:39', 1),\n",
       " ('2012-05-28 17:06:28', 1),\n",
       " ('2012-06-02 20:40:54', 1),\n",
       " ('2013-08-16 17:39:19', 1),\n",
       " ('2013-08-20 23:59:15', 1),\n",
       " ('2013-08-23 20:35:27', 1),\n",
       " ('2013-08-23 20:43:25', 1),\n",
       " ('2013-08-25 17:20:37', 1),\n",
       " ('2013-09-03 17:49:36', 1),\n",
       " ('2013-09-15 00:54:47', 1),\n",
       " ('2013-03-19 12:20:15', 1),\n",
       " ('2013-05-29 11:44:06', 1),\n",
       " ('2013-08-15 18:01:14', 1),\n",
       " ('2013-08-25 02:03:42', 1),\n",
       " ('2013-08-27 16:18:18', 1),\n",
       " ('2013-08-30 15:43:23', 1),\n",
       " ('2013-05-27 08:14:01', 1),\n",
       " ('2013-05-27 08:37:28', 1),\n",
       " ('2013-08-16 03:18:35', 1),\n",
       " ('2013-08-16 17:46:28', 1),\n",
       " ('2012-06-03 14:29:08', 1),\n",
       " ('2013-02-19 10:29:49', 1),\n",
       " ('2013-02-22 21:02:56', 1),\n",
       " ('2013-05-10 14:55:26', 1),\n",
       " ('2013-05-27 12:11:51', 1),\n",
       " ('2013-05-29 09:25:22', 1),\n",
       " ('2013-05-31 16:37:19', 1),\n",
       " ('2013-08-10 13:39:57', 1),\n",
       " ('2013-09-21 20:31:45', 1),\n",
       " ('2013-09-22 02:17:53', 1),\n",
       " ('2013-09-22 02:30:47', 1),\n",
       " ('2013-09-24 05:18:11', 1),\n",
       " ('2013-09-24 15:32:24', 1),\n",
       " ('2013-08-20 16:32:16', 1),\n",
       " ('2013-08-21 00:32:56', 1),\n",
       " ('2013-08-22 14:42:23', 1),\n",
       " ('2013-08-24 22:10:18', 1),\n",
       " ('2013-08-27 02:05:06', 1),\n",
       " ('2013-09-25 15:05:03', 1),\n",
       " ('2013-09-29 06:17:55', 1),\n",
       " ('2013-09-30 04:18:38', 1),\n",
       " ('2013-10-01 20:41:44', 1),\n",
       " ('2013-09-11 16:02:26', 1),\n",
       " ('2013-09-15 07:48:08', 1),\n",
       " ('2013-09-23 18:30:14', 1),\n",
       " ('2013-05-10 13:18:37', 1),\n",
       " ('2013-06-18 08:56:50', 1),\n",
       " ('2013-07-03 20:33:55', 1),\n",
       " ('2013-08-15 17:01:12', 1),\n",
       " ('2013-08-21 16:53:28', 1),\n",
       " ('2013-08-27 03:41:44', 1),\n",
       " ('2013-08-15 18:31:06', 1),\n",
       " ('2013-08-16 15:56:15', 1),\n",
       " ('2013-08-16 16:44:04', 1),\n",
       " ('2013-08-20 13:59:05', 1),\n",
       " ('2013-08-25 03:32:21', 1),\n",
       " ('2013-09-03 05:41:05', 1),\n",
       " ('2013-09-09 15:07:20', 1),\n",
       " ('2013-09-10 16:37:11', 1),\n",
       " ('2013-09-13 18:46:35', 1),\n",
       " ('2013-10-02 06:31:06', 1),\n",
       " ('2013-10-07 15:27:50', 1),\n",
       " ('2013-10-08 19:51:50', 1),\n",
       " ('2013-10-21 21:37:37', 1),\n",
       " ('2013-09-12 01:18:57', 1),\n",
       " ('2013-09-12 15:37:15', 1),\n",
       " ('2013-09-12 16:13:37', 1),\n",
       " ('2013-09-13 00:56:49', 1),\n",
       " ('2013-09-15 01:52:16', 1),\n",
       " ('2013-09-15 02:59:55', 1),\n",
       " ('2013-09-19 09:44:07', 1),\n",
       " ('2013-09-24 16:28:36', 1),\n",
       " ('2013-10-10 17:05:29', 1),\n",
       " ('2013-08-27 19:30:55', 1),\n",
       " ('2013-09-04 04:01:57', 1),\n",
       " ('2013-09-12 19:51:59', 1),\n",
       " ('2013-09-17 04:22:36', 1),\n",
       " ('2013-09-17 10:41:44', 1),\n",
       " ('2013-09-22 16:32:56', 1),\n",
       " ('2013-08-21 17:21:37', 1),\n",
       " ('2013-08-23 23:54:12', 1),\n",
       " ('2013-08-24 22:19:07', 1),\n",
       " ('2013-08-25 21:54:48', 1),\n",
       " ('2013-09-24 05:15:19', 1),\n",
       " ('2013-09-27 04:11:33', 1),\n",
       " ('2013-09-29 07:15:38', 1),\n",
       " ('2013-10-22 20:57:08', 1),\n",
       " ('2013-10-28 14:35:01', 1),\n",
       " ('2013-11-11 13:31:27', 1),\n",
       " ('2013-11-28 01:51:31', 1),\n",
       " ('2013-12-06 06:29:22', 1),\n",
       " ('2013-09-25 16:21:37', 1),\n",
       " ('2013-10-01 16:02:23', 1),\n",
       " ('2013-10-04 16:32:19', 1),\n",
       " ('2013-10-09 12:20:32', 1),\n",
       " ('2013-09-13 01:59:05', 1),\n",
       " ('2013-09-19 15:58:33', 1),\n",
       " ('2013-09-23 15:44:29', 1),\n",
       " ('2013-10-11 02:34:21', 1),\n",
       " ('2013-10-14 20:04:58', 1),\n",
       " ('2013-10-14 22:11:13', 1),\n",
       " ('2013-10-17 12:38:45', 1),\n",
       " ('2013-10-20 17:52:49', 1),\n",
       " ('2013-10-21 22:56:08', 1),\n",
       " ('2013-12-07 03:10:14', 1),\n",
       " ('2013-12-07 15:03:37', 1),\n",
       " ('2013-09-13 16:37:38', 1),\n",
       " ('2013-09-13 20:24:26', 1),\n",
       " ('2013-09-25 15:28:01', 1),\n",
       " ('2013-09-29 21:02:06', 1),\n",
       " ('2013-10-01 15:15:32', 1),\n",
       " ('2013-12-15 23:07:34', 1),\n",
       " ('2013-12-30 02:21:11', 1),\n",
       " ('2013-12-30 02:38:10', 1),\n",
       " ('2014-01-16 02:55:21', 1),\n",
       " ('2014-02-08 07:11:05', 1),\n",
       " ('2013-10-15 12:55:18', 1),\n",
       " ('2013-10-24 04:57:36', 1),\n",
       " ('2013-10-25 02:59:10', 1),\n",
       " ('2013-10-28 14:30:49', 1),\n",
       " ('2013-11-11 19:45:31', 1),\n",
       " ('2013-11-18 21:18:34', 1),\n",
       " ('2013-10-22 16:08:59', 1),\n",
       " ('2013-10-25 16:46:21', 1),\n",
       " ('2013-10-25 21:24:32', 1),\n",
       " ('2013-10-02 20:04:19', 1),\n",
       " ('2013-10-08 11:47:45', 1),\n",
       " ('2013-10-08 18:33:30', 1),\n",
       " ('2013-10-09 16:46:59', 1),\n",
       " ('2013-09-29 06:27:03', 1),\n",
       " ('2013-09-30 04:38:55', 1),\n",
       " ('2013-10-03 23:08:42', 1),\n",
       " ('2013-10-15 12:25:46', 1),\n",
       " ('2014-02-09 18:52:10', 1),\n",
       " ('2014-02-18 18:36:53', 1),\n",
       " ('2014-02-26 02:26:55', 1),\n",
       " ('2013-10-27 23:20:14', 1),\n",
       " ('2013-10-31 11:20:34', 1),\n",
       " ('2013-11-15 16:43:12', 1),\n",
       " ('2013-11-27 19:36:10', 1),\n",
       " ('2013-10-01 20:54:33', 1),\n",
       " ('2013-10-03 14:19:18', 1),\n",
       " ('2013-10-06 22:27:43', 1),\n",
       " ('2013-10-11 02:52:10', 1),\n",
       " ('2013-10-13 21:40:03', 1),\n",
       " ('2013-10-18 04:01:56', 1),\n",
       " ('2013-11-27 20:18:18', 1),\n",
       " ('2013-10-15 03:02:38', 1),\n",
       " ('2013-10-18 17:30:38', 1),\n",
       " ('2013-10-22 14:44:42', 1),\n",
       " ('2014-02-27 17:30:06', 1),\n",
       " ('2014-03-06 05:46:00', 1),\n",
       " ('2014-03-07 22:30:20', 1),\n",
       " ('2013-11-21 21:11:47', 1),\n",
       " ('2013-11-24 04:05:17', 1),\n",
       " ('2013-10-21 21:09:03', 1),\n",
       " ('2013-10-29 22:21:58', 1),\n",
       " ('2013-11-17 03:07:11', 1),\n",
       " ('2013-10-15 15:49:28', 1),\n",
       " ('2013-10-18 21:39:10', 1),\n",
       " ('2013-10-25 00:25:25', 1),\n",
       " ('2014-03-08 17:52:24', 1),\n",
       " ('2014-03-10 18:18:22', 1),\n",
       " ('2014-03-13 16:07:56', 1),\n",
       " ('2014-03-14 23:57:11', 1),\n",
       " ('2014-03-23 23:43:59', 1),\n",
       " ('2013-11-29 21:38:51', 1),\n",
       " ('2013-12-07 02:02:44', 1),\n",
       " ('2013-12-27 17:27:45', 1),\n",
       " ('2014-01-02 21:19:02', 1),\n",
       " ('2013-10-24 19:12:18', 1),\n",
       " ('2013-10-25 17:56:28', 1),\n",
       " ('2013-10-28 21:48:11', 1),\n",
       " ('2013-11-18 04:15:30', 1),\n",
       " ('2014-03-24 03:15:31', 1),\n",
       " ('2014-03-28 01:33:44', 1),\n",
       " ('2014-03-28 18:44:26', 1),\n",
       " ('2014-03-30 18:03:34', 1),\n",
       " ('2014-03-30 20:45:13', 1),\n",
       " ('2014-01-09 02:45:24', 1),\n",
       " ('2014-01-16 04:45:34', 1),\n",
       " ('2014-02-07 18:40:15', 1),\n",
       " ('2014-02-09 04:35:29', 1),\n",
       " ('2013-11-25 17:31:35', 1),\n",
       " ('2013-12-22 06:04:36', 1),\n",
       " ('2013-10-28 15:12:22', 1),\n",
       " ('2013-11-13 12:00:41', 1),\n",
       " ('2013-11-14 03:52:00', 1),\n",
       " ('2013-11-14 11:44:40', 1),\n",
       " ('2014-04-07 23:50:46', 1),\n",
       " ('2014-04-08 01:02:20', 1),\n",
       " ('2014-04-08 16:17:51', 1),\n",
       " ('2014-04-09 00:25:46', 1),\n",
       " ('2013-12-30 17:08:33', 1),\n",
       " ('2014-01-05 04:13:44', 1),\n",
       " ('2014-02-08 18:54:20', 1),\n",
       " ('2014-01-09 02:36:11', 1),\n",
       " ('2014-01-15 02:04:57', 1),\n",
       " ('2014-02-07 17:00:36', 1),\n",
       " ('2014-02-09 05:23:03', 1),\n",
       " ('2014-02-09 20:10:57', 1),\n",
       " ('2014-02-10 05:59:07', 1),\n",
       " ('2013-11-17 07:12:33', 1),\n",
       " ('2013-11-18 18:54:04', 1),\n",
       " ('2013-11-24 16:49:19', 1),\n",
       " ('2014-04-09 15:11:02', 1),\n",
       " ('2014-04-12 23:52:47', 1),\n",
       " ('2014-02-09 17:04:52', 1),\n",
       " ('2014-02-12 05:03:08', 1),\n",
       " ('2014-02-17 00:50:14', 1),\n",
       " ('2014-02-18 17:39:20', 1),\n",
       " ('2014-02-21 01:46:37', 1),\n",
       " ('2014-02-21 02:16:16', 1),\n",
       " ('2014-02-21 03:43:18', 1),\n",
       " ('2014-02-10 20:19:37', 1),\n",
       " ('2014-02-12 19:26:31', 1),\n",
       " ('2014-02-14 05:22:59', 1),\n",
       " ('2014-02-14 16:46:10', 1),\n",
       " ('2014-02-16 06:12:40', 1),\n",
       " ('2014-04-15 18:16:26', 1),\n",
       " ('2014-04-16 16:07:23', 1),\n",
       " ('2014-04-16 16:29:45', 1),\n",
       " ('2014-04-17 15:18:25', 1),\n",
       " ('2014-02-09 04:19:30', 1),\n",
       " ('2014-02-09 12:36:13', 1),\n",
       " ('2014-02-09 20:56:53', 1),\n",
       " ('2014-02-09 21:49:41', 1),\n",
       " ('2014-02-11 04:19:47', 1),\n",
       " ('2013-11-28 16:30:34', 1),\n",
       " ('2013-12-27 17:55:02', 1),\n",
       " ('2013-12-29 23:07:14', 1),\n",
       " ('2014-01-02 21:41:07', 1),\n",
       " ('2013-11-16 22:21:05', 1),\n",
       " ('2013-11-19 16:20:30', 1),\n",
       " ('2014-01-02 23:34:20', 1),\n",
       " ('2014-01-09 04:10:24', 1),\n",
       " ('2014-04-19 04:30:34', 1),\n",
       " ('2014-04-22 21:06:52', 1),\n",
       " ('2014-04-23 17:51:04', 1),\n",
       " ('2014-04-24 17:25:00', 1),\n",
       " ('2014-04-26 03:00:43', 1),\n",
       " ('2014-04-30 16:15:01', 1),\n",
       " ('2014-02-23 23:25:36', 1),\n",
       " ('2014-02-24 01:25:38', 1),\n",
       " ('2014-02-26 03:51:42', 1),\n",
       " ('2014-03-03 03:37:01', 1),\n",
       " ('2014-02-21 03:07:59', 1),\n",
       " ('2014-02-21 16:51:56', 1),\n",
       " ('2014-03-08 00:36:01', 1),\n",
       " ('2014-03-12 14:24:19', 1),\n",
       " ('2014-03-13 03:59:54', 1),\n",
       " ('2014-02-09 20:16:25', 1),\n",
       " ('2014-02-12 19:47:05', 1),\n",
       " ('2014-02-12 22:03:11', 1),\n",
       " ('2014-02-13 18:06:18', 1),\n",
       " ('2014-05-07 17:27:38', 1),\n",
       " ('2014-05-14 23:12:22', 1),\n",
       " ('2014-06-04 20:31:35', 1),\n",
       " ('2014-06-09 16:24:45', 1),\n",
       " ('2014-02-12 02:59:39', 1),\n",
       " ('2014-02-13 21:23:06', 1),\n",
       " ('2014-02-18 00:55:58', 1),\n",
       " ('2014-02-19 04:13:26', 1),\n",
       " ('2014-01-10 02:07:29', 1),\n",
       " ('2014-01-16 22:28:14', 1),\n",
       " ('2014-01-17 12:34:15', 1),\n",
       " ('2014-03-12 14:40:42', 1),\n",
       " ('2014-03-13 02:49:58', 1),\n",
       " ('2014-03-24 19:44:00', 1),\n",
       " ('2014-03-13 23:50:25', 1),\n",
       " ('2014-03-14 19:16:55', 1),\n",
       " ('2014-02-14 05:45:32', 1),\n",
       " ('2014-02-14 16:26:28', 1),\n",
       " ('2014-02-15 18:45:02', 1),\n",
       " ('2014-02-23 22:23:16', 1),\n",
       " ('2014-06-11 02:02:47', 1),\n",
       " ('2014-06-11 21:22:19', 1),\n",
       " ('2014-06-12 16:39:47', 1),\n",
       " ('2014-06-14 03:20:38', 1),\n",
       " ('2014-06-21 00:22:19', 1),\n",
       " ('2014-06-23 04:42:55', 1),\n",
       " ('2014-06-23 15:14:01', 1),\n",
       " ('2014-06-24 02:44:44', 1),\n",
       " ('2014-03-16 00:49:10', 1),\n",
       " ('2014-03-16 01:26:12', 1),\n",
       " ('2014-03-18 06:48:18', 1),\n",
       " ('2014-02-08 09:31:22', 1),\n",
       " ('2014-02-09 18:01:40', 1),\n",
       " ('2014-02-12 21:53:14', 1),\n",
       " ('2014-02-15 04:54:54', 1),\n",
       " ('2014-02-18 07:10:37', 1),\n",
       " ('2014-02-20 03:40:08', 1),\n",
       " ('2014-02-26 18:18:24', 1),\n",
       " ('2014-03-05 22:17:53', 1),\n",
       " ('2014-03-07 02:45:50', 1),\n",
       " ('2014-02-19 17:51:14', 1),\n",
       " ('2014-02-23 02:18:52', 1),\n",
       " ('2014-02-21 01:49:27', 1),\n",
       " ('2014-02-27 16:50:54', 1),\n",
       " ('2014-02-27 17:03:49', 1),\n",
       " ('2014-03-13 02:43:21', 1),\n",
       " ('2014-03-14 23:44:03', 1),\n",
       " ('2014-03-17 17:34:24', 1),\n",
       " ('2014-03-22 01:07:42', 1),\n",
       " ('2014-03-22 01:22:54', 1),\n",
       " ('2014-03-26 14:57:12', 1),\n",
       " ('2014-04-01 01:25:05', 1),\n",
       " ('2014-04-03 16:45:01', 1),\n",
       " ('2014-04-03 17:54:55', 1),\n",
       " ('2014-03-30 17:19:14', 1),\n",
       " ('2014-03-30 22:05:00', 1),\n",
       " ('2014-03-31 16:13:40', 1),\n",
       " ('2014-03-31 17:30:32', 1),\n",
       " ('2014-03-31 20:43:48', 1),\n",
       " ('2014-04-02 00:16:56', 1),\n",
       " ('2014-04-03 03:38:36', 1),\n",
       " ('2014-06-24 15:29:18', 1),\n",
       " ('2014-06-24 15:52:45', 1),\n",
       " ('2014-06-28 00:52:33', 1),\n",
       " ('2014-03-07 02:51:48', 1),\n",
       " ('2014-03-10 17:30:17', 1),\n",
       " ('2014-03-14 02:50:10', 1),\n",
       " ('2014-03-14 19:32:59', 1),\n",
       " ('2014-02-23 16:34:50', 1),\n",
       " ('2014-02-23 23:32:13', 1),\n",
       " ('2014-03-05 18:24:23', 1),\n",
       " ('2014-03-06 00:14:24', 1),\n",
       " ('2014-03-07 04:34:19', 1),\n",
       " ('2014-04-03 20:12:53', 1),\n",
       " ('2014-04-04 16:13:36', 1),\n",
       " ('2014-04-07 15:14:27', 1),\n",
       " ('2014-04-07 17:02:57', 1),\n",
       " ('2014-04-08 00:43:05', 1),\n",
       " ('2014-04-08 16:42:13', 1),\n",
       " ('2014-04-10 14:07:37', 1),\n",
       " ('2014-04-13 00:07:15', 1),\n",
       " ('2014-06-30 18:45:23', 1),\n",
       " ('2014-07-03 00:40:25', 1),\n",
       " ('2014-07-03 18:17:14', 1),\n",
       " ('2014-07-06 15:46:48', 1),\n",
       " ('2014-03-07 15:57:10', 1),\n",
       " ('2014-03-08 18:19:29', 1),\n",
       " ('2014-03-08 18:38:06', 1),\n",
       " ('2014-03-19 16:06:49', 1),\n",
       " ('2014-03-26 03:56:25', 1),\n",
       " ('2014-03-26 05:03:25', 1),\n",
       " ('2014-04-02 00:08:40', 1),\n",
       " ('2014-04-04 01:20:29', 1),\n",
       " ('2014-04-05 20:36:31', 1),\n",
       " ('2014-04-14 16:15:35', 1),\n",
       " ('2014-04-18 17:39:44', 1),\n",
       " ('2014-04-18 19:43:22', 1),\n",
       " ('2014-04-23 16:30:15', 1),\n",
       " ('2014-03-16 18:33:20', 1),\n",
       " ('2014-03-18 06:34:16', 1),\n",
       " ('2014-03-19 17:52:50', 1),\n",
       " ('2014-03-20 04:42:30', 1),\n",
       " ('2014-04-04 20:38:07', 1),\n",
       " ('2014-04-08 00:11:54', 1),\n",
       " ('2014-04-11 02:18:02', 1),\n",
       " ('2014-04-12 00:57:39', 1),\n",
       " ('2014-07-07 20:42:11', 1),\n",
       " ('2014-07-09 03:41:13', 1),\n",
       " ('2014-07-11 16:51:49', 1),\n",
       " ('2014-04-12 01:14:11', 1),\n",
       " ('2014-04-12 02:02:27', 1),\n",
       " ('2014-04-14 18:24:18', 1),\n",
       " ('2014-04-23 16:40:11', 1),\n",
       " ('2014-04-27 02:06:47', 1),\n",
       " ('2014-04-29 16:30:01', 1),\n",
       " ('2014-05-03 18:10:05', 1),\n",
       " ('2014-05-06 00:46:19', 1),\n",
       " ('2014-03-20 21:01:55', 1),\n",
       " ('2014-03-25 15:14:08', 1),\n",
       " ('2014-03-27 16:24:36', 1),\n",
       " ('2014-04-05 21:15:53', 1),\n",
       " ('2014-04-09 00:08:49', 1),\n",
       " ('2014-04-10 01:00:42', 1),\n",
       " ('2014-03-19 20:31:44', 1),\n",
       " ('2014-03-29 00:47:57', 1),\n",
       " ('2014-04-19 04:18:06', 1),\n",
       " ('2014-04-20 00:44:32', 1),\n",
       " ('2014-05-06 15:19:40', 1),\n",
       " ('2014-05-20 04:08:38', 1),\n",
       " ('2014-05-30 14:49:21', 1),\n",
       " ('2014-04-11 16:37:45', 1),\n",
       " ('2014-04-17 14:50:17', 1),\n",
       " ('2014-04-18 17:18:19', 1),\n",
       " ('2014-03-28 19:38:08', 1),\n",
       " ('2014-03-29 17:34:13', 1),\n",
       " ('2014-04-01 23:28:51', 1),\n",
       " ('2014-07-25 14:36:36', 1),\n",
       " ('2014-08-01 18:05:10', 1),\n",
       " ('2014-08-05 16:39:44', 1),\n",
       " ('2014-04-22 03:45:46', 1),\n",
       " ('2014-04-27 01:38:03', 1),\n",
       " ('2014-06-01 19:51:40', 1),\n",
       " ('2014-06-06 16:17:43', 1),\n",
       " ('2014-06-08 04:43:56', 1),\n",
       " ('2014-03-29 15:59:39', 1),\n",
       " ('2014-03-29 18:47:20', 1),\n",
       " ('2014-04-02 00:22:24', 1),\n",
       " ('2014-04-05 19:34:16', 1),\n",
       " ('2014-04-23 00:52:54', 1),\n",
       " ('2014-04-23 01:17:18', 1),\n",
       " ('2014-04-23 17:13:50', 1),\n",
       " ('2014-05-01 21:17:06', 1),\n",
       " ('2014-05-01 22:53:49', 1),\n",
       " ('2014-08-08 16:39:54', 1),\n",
       " ('2014-08-11 03:08:41', 1),\n",
       " ('2014-04-27 06:50:55', 1),\n",
       " ('2014-05-01 01:45:01', 1),\n",
       " ('2014-05-01 03:18:52', 1),\n",
       " ('2014-05-01 15:42:50', 1),\n",
       " ('2014-08-20 02:22:02', 1),\n",
       " ('2014-09-06 16:05:05', 1),\n",
       " ('2014-09-12 15:15:36', 1),\n",
       " ('2014-05-03 03:20:10', 1),\n",
       " ('2014-05-12 19:42:32', 1),\n",
       " ('2014-04-06 01:27:00', 1),\n",
       " ('2014-04-07 21:28:27', 1),\n",
       " ('2014-04-08 23:44:48', 1),\n",
       " ('2014-04-11 01:42:36', 1),\n",
       " ('2014-04-11 01:59:31', 1),\n",
       " ('2014-05-05 03:42:29', 1),\n",
       " ('2014-05-10 00:22:56', 1),\n",
       " ('2014-05-10 01:33:59', 1),\n",
       " ('2014-05-11 02:33:30', 1),\n",
       " ('2014-06-08 15:15:52', 1),\n",
       " ('2014-06-09 16:53:22', 1),\n",
       " ('2014-06-11 21:01:29', 1),\n",
       " ('2014-06-14 23:25:54', 1),\n",
       " ('2014-06-15 21:29:38', 1),\n",
       " ('2014-04-07 17:45:54', 1),\n",
       " ('2014-04-12 01:06:57', 1),\n",
       " ('2014-04-12 01:41:31', 1),\n",
       " ('2014-04-15 15:38:17', 1),\n",
       " ('2014-05-15 01:19:42', 1),\n",
       " ('2014-05-21 21:16:26', 1),\n",
       " ('2014-06-05 15:23:35', 1),\n",
       " ('2014-04-12 01:25:10', 1),\n",
       " ('2014-04-12 03:51:58', 1),\n",
       " ('2014-04-14 01:42:49', 1),\n",
       " ('2014-04-17 04:34:12', 1),\n",
       " ('2014-04-18 17:58:36', 1),\n",
       " ('2014-05-13 21:37:37', 1),\n",
       " ('2014-05-23 16:19:14', 1),\n",
       " ('2014-09-20 18:48:56', 1),\n",
       " ('2014-09-30 05:55:35', 1),\n",
       " ('2014-10-16 20:41:05', 1),\n",
       " ('2014-06-06 19:15:28', 1),\n",
       " ('2014-06-06 19:52:11', 1),\n",
       " ('2014-06-07 03:53:08', 1),\n",
       " ('2014-06-22 05:02:55', 1),\n",
       " ('2014-06-25 15:07:12', 1),\n",
       " ('2014-06-26 02:27:09', 1),\n",
       " ('2014-04-15 22:40:33', 1),\n",
       " ('2014-04-15 22:43:49', 1),\n",
       " ('2014-04-17 02:22:00', 1),\n",
       " ('2014-04-22 17:19:18', 1),\n",
       " ('2014-04-22 23:01:07', 1),\n",
       " ('2014-05-30 03:22:29', 1),\n",
       " ('2014-06-08 04:31:49', 1),\n",
       " ('2014-06-10 00:17:46', 1),\n",
       " ('2014-06-11 16:52:45', 1),\n",
       " ('2014-06-10 00:25:01', 1),\n",
       " ('2014-06-10 00:30:52', 1),\n",
       " ('2014-06-10 02:14:11', 1),\n",
       " ('2014-06-11 01:52:15', 1),\n",
       " ('2014-04-19 17:22:10', 1),\n",
       " ('2014-04-19 19:45:50', 1),\n",
       " ('2014-04-21 15:44:04', 1),\n",
       " ('2014-04-25 15:46:17', 1),\n",
       " ('2014-04-28 19:19:24', 1),\n",
       " ('2014-06-14 04:07:18', 1),\n",
       " ('2014-06-21 03:09:22', 1),\n",
       " ('2014-06-24 23:01:01', 1),\n",
       " ('2014-06-11 21:13:58', 1),\n",
       " ('2014-06-12 14:58:09', 1),\n",
       " ('2014-06-15 06:47:35', 1),\n",
       " ('2014-06-15 14:12:47', 1),\n",
       " ('2014-06-16 15:22:11', 1),\n",
       " ('2014-06-27 05:24:01', 1),\n",
       " ('2014-06-27 14:34:54', 1),\n",
       " ('2014-06-28 18:58:17', 1),\n",
       " ('2014-07-02 01:42:22', 1),\n",
       " ('2014-06-23 19:03:44', 1),\n",
       " ('2014-06-24 06:16:08', 1),\n",
       " ('2014-06-24 23:30:23', 1),\n",
       " ('2014-06-25 20:34:04', 1),\n",
       " ('2014-11-11 17:32:18', 1),\n",
       " ('2015-03-09 02:04:00', 1),\n",
       " ('2014-07-02 02:36:19', 1),\n",
       " ('2014-07-03 03:17:32', 1),\n",
       " ('2014-07-03 22:40:34', 1),\n",
       " ('2014-07-07 02:07:23', 1),\n",
       " ('2014-06-27 14:49:33', 1),\n",
       " ('2014-04-28 02:36:51', 1),\n",
       " ('2014-04-29 07:32:17', 1),\n",
       " ('2014-05-01 01:52:48', 1),\n",
       " ('2014-05-02 16:23:34', 1),\n",
       " ('2014-05-06 04:05:17', 1),\n",
       " ('2014-05-10 04:23:23', 1),\n",
       " ('2014-05-16 21:34:59', 1),\n",
       " ('2014-06-28 16:40:13', 1),\n",
       " ('2014-07-05 02:05:14', 1),\n",
       " ('2014-07-09 04:07:25', 1),\n",
       " ('2014-07-11 07:46:19', 1),\n",
       " ('2014-07-15 03:56:20', 1),\n",
       " ('2014-07-07 21:29:39', 1),\n",
       " ('2014-07-08 21:40:35', 1),\n",
       " ('2014-07-09 15:47:25', 1),\n",
       " ('2014-07-12 01:08:14', 1),\n",
       " ('2014-05-04 23:18:15', 1),\n",
       " ('2014-05-05 18:37:00', 1),\n",
       " ('2014-05-10 16:48:55', 1),\n",
       " ('2015-03-09 19:11:45', 1),\n",
       " ('2015-03-12 03:01:36', 1),\n",
       " ('2015-03-20 19:41:08', 1),\n",
       " ('2014-06-27 17:11:35', 1),\n",
       " ('2014-06-29 17:29:18', 1),\n",
       " ('2014-06-30 03:31:40', 1),\n",
       " ('2014-05-17 00:31:45', 1),\n",
       " ('2014-06-04 21:25:13', 1),\n",
       " ('2014-05-12 20:31:02', 1),\n",
       " ('2014-05-20 18:37:50', 1),\n",
       " ('2014-05-27 17:18:23', 1),\n",
       " ('2014-07-01 05:13:51', 1),\n",
       " ('2014-07-03 00:53:20', 1),\n",
       " ('2014-07-03 21:13:38', 1),\n",
       " ('2014-07-04 17:51:38', 1),\n",
       " ('2014-07-16 16:10:04', 1),\n",
       " ('2014-08-02 00:00:44', 1),\n",
       " ('2014-07-06 05:53:01', 1),\n",
       " ('2014-07-07 21:12:29', 1),\n",
       " ('2015-03-31 19:33:58', 1),\n",
       " ('2015-04-02 16:18:05', 1),\n",
       " ('2015-04-03 01:07:22', 1),\n",
       " ('2014-08-06 15:42:54', 1),\n",
       " ('2014-08-06 20:28:41', 1),\n",
       " ('2014-07-16 15:19:15', 1),\n",
       " ('2014-07-16 16:25:03', 1),\n",
       " ('2014-07-29 18:51:05', 1),\n",
       " ('2014-07-30 18:56:55', 1),\n",
       " ('2014-07-09 01:06:54', 1),\n",
       " ('2014-07-09 16:08:14', 1),\n",
       " ('2014-05-30 19:50:30', 1),\n",
       " ('2014-06-01 04:02:19', 1),\n",
       " ('2014-06-14 00:54:28', 1),\n",
       " ('2014-06-18 19:07:16', 1),\n",
       " ('2014-08-07 19:11:43', 1),\n",
       " ('2014-08-08 15:12:21', 1),\n",
       " ('2014-08-08 15:41:12', 1),\n",
       " ('2014-08-08 15:48:05', 1),\n",
       " ('2015-04-03 22:46:49', 1),\n",
       " ('2015-04-07 14:11:22', 1),\n",
       " ('2015-04-15 08:41:53', 1),\n",
       " ('2014-06-05 17:48:01', 1),\n",
       " ('2014-06-07 02:07:04', 1),\n",
       " ('2014-06-08 05:18:43', 1),\n",
       " ('2014-07-11 04:31:07', 1),\n",
       " ('2014-07-17 18:41:54', 1),\n",
       " ('2014-08-07 00:07:15', 1),\n",
       " ('2014-08-05 14:57:39', 1),\n",
       " ('2014-08-11 17:07:08', 1),\n",
       " ('2014-08-09 18:58:59', 1),\n",
       " ('2014-08-10 02:36:42', 1),\n",
       " ('2014-06-08 19:09:02', 1),\n",
       " ('2014-06-09 21:21:33', 1),\n",
       " ('2014-06-11 04:39:16', 1),\n",
       " ('2014-08-13 14:58:13', 1),\n",
       " ('2014-08-14 18:26:07', 1),\n",
       " ('2014-08-16 21:02:49', 1),\n",
       " ('2015-04-15 17:29:15', 1),\n",
       " ('2015-04-21 17:52:12', 1),\n",
       " ('2014-08-09 15:32:29', 1),\n",
       " ('2014-08-17 03:21:24', 1),\n",
       " ('2014-09-26 20:52:36', 1),\n",
       " ('2014-06-13 04:17:12', 1),\n",
       " ('2014-06-19 21:21:00', 1),\n",
       " ('2014-08-15 21:46:57', 1),\n",
       " ('2014-08-16 23:10:18', 1),\n",
       " ('2014-08-20 12:54:04', 1),\n",
       " ('2014-08-21 17:35:17', 1),\n",
       " ('2014-06-21 01:58:32', 1),\n",
       " ('2014-06-24 01:52:17', 1),\n",
       " ('2014-06-24 09:12:34', 1),\n",
       " ('2014-08-19 16:11:01', 1),\n",
       " ('2014-08-21 11:45:27', 1),\n",
       " ('2014-08-22 14:12:25', 1),\n",
       " ('2014-09-10 18:54:44', 1),\n",
       " ('2014-10-16 00:22:23', 1),\n",
       " ('2014-10-16 17:06:30', 1),\n",
       " ('2014-11-10 17:35:31', 1),\n",
       " ('2014-11-10 17:43:20', 1),\n",
       " ('2014-06-20 01:51:46', 1),\n",
       " ('2014-06-25 16:33:07', 1),\n",
       " ('2014-06-25 23:12:06', 1),\n",
       " ('2015-04-30 14:22:09', 1),\n",
       " ('2015-04-30 22:27:12', 1),\n",
       " ('2015-05-05 20:21:28', 1),\n",
       " ('2015-05-06 20:44:28', 1),\n",
       " ('2015-05-09 13:38:05', 1),\n",
       " ('2014-09-15 20:19:07', 1),\n",
       " ('2014-09-20 16:31:24', 1),\n",
       " ('2014-08-23 18:32:28', 1),\n",
       " ('2014-09-09 22:36:27', 1),\n",
       " ('2014-09-16 14:30:27', 1),\n",
       " ('2014-09-22 15:36:39', 1),\n",
       " ('2014-09-30 14:52:45', 1),\n",
       " ('2014-10-15 17:36:08', 1),\n",
       " ('2014-10-15 17:47:06', 1),\n",
       " ('2014-12-04 01:07:35', 1),\n",
       " ('2015-01-06 19:19:51', 1),\n",
       " ('2015-03-11 15:33:03', 1),\n",
       " ('2014-06-26 04:07:34', 1),\n",
       " ('2014-06-26 23:25:01', 1),\n",
       " ('2014-06-28 16:52:11', 1),\n",
       " ('2014-06-28 17:03:13', 1),\n",
       " ('2014-06-24 16:38:48', 1),\n",
       " ('2014-06-30 03:14:22', 1),\n",
       " ('2014-07-04 15:41:01', 1),\n",
       " ('2014-07-04 16:34:41', 1),\n",
       " ('2015-03-24 20:27:23', 1),\n",
       " ('2015-03-29 13:20:48', 1),\n",
       " ('2014-07-05 01:26:54', 1),\n",
       " ('2014-07-11 15:43:41', 1),\n",
       " ('2014-09-26 16:46:57', 1),\n",
       " ('2014-10-04 03:18:48', 1),\n",
       " ('2014-10-15 16:56:26', 1),\n",
       " ('2014-11-10 16:36:43', 1),\n",
       " ('2015-03-31 15:33:42', 1),\n",
       " ('2014-07-01 05:24:16', 1),\n",
       " ('2014-07-01 05:27:44', 1),\n",
       " ('2014-07-01 06:29:45', 1),\n",
       " ('2014-07-01 06:55:34', 1),\n",
       " ('2014-07-07 22:53:35', 1),\n",
       " ('2014-10-29 19:33:14', 1),\n",
       " ('2015-03-10 05:15:41', 1),\n",
       " ('2015-05-09 20:54:00', 1),\n",
       " ('2015-05-09 21:55:42', 1),\n",
       " ('2015-05-11 13:54:26', 1),\n",
       " ('2015-05-12 15:08:35', 1),\n",
       " ('2015-04-08 19:20:15', 1),\n",
       " ('2015-04-11 17:20:18', 1),\n",
       " ('2014-07-08 21:24:18', 1),\n",
       " ('2014-07-09 00:00:19', 1),\n",
       " ('2014-07-29 15:50:10', 1),\n",
       " ('2014-08-14 01:22:44', 1),\n",
       " ('2014-07-12 01:46:09', 1),\n",
       " ('2014-07-17 18:29:05', 1),\n",
       " ('2014-07-30 14:47:01', 1),\n",
       " ('2015-05-14 18:30:14', 1),\n",
       " ('2015-05-15 12:36:39', 1),\n",
       " ('2015-05-15 15:28:56', 1),\n",
       " ('2015-05-17 06:07:03', 1),\n",
       " ('2014-11-10 21:08:55', 1),\n",
       " ('2014-11-11 17:39:07', 1),\n",
       " ('2015-03-04 22:31:23', 1),\n",
       " ('2014-08-07 18:56:28', 1),\n",
       " ('2014-08-08 20:09:45', 1),\n",
       " ('2014-08-09 00:14:53', 1),\n",
       " ('2014-08-09 15:41:09', 1),\n",
       " ('2015-03-10 19:17:12', 1),\n",
       " ('2015-03-12 02:31:27', 1),\n",
       " ('2015-03-17 03:21:02', 1),\n",
       " ('2015-05-17 14:33:37', 1),\n",
       " ('2015-05-19 22:44:11', 1),\n",
       " ('2015-05-20 19:13:23', 1),\n",
       " ('2014-08-15 17:49:11', 1),\n",
       " ('2014-08-21 03:03:17', 1),\n",
       " ('2015-04-13 15:13:27', 1),\n",
       " ('2015-04-13 16:47:56', 1),\n",
       " ('2015-04-14 14:43:12', 1),\n",
       " ('2015-04-21 02:22:57', 1),\n",
       " ('2015-03-11 21:33:45', 1),\n",
       " ('2015-03-25 01:58:03', 1),\n",
       " ('2015-04-02 15:12:37', 1),\n",
       " ('2015-03-28 16:45:48', 1),\n",
       " ('2015-04-01 20:06:55', 1),\n",
       " ('2014-08-11 04:04:27', 1),\n",
       " ('2014-08-11 21:01:31', 1),\n",
       " ('2014-08-12 17:16:31', 1),\n",
       " ('2015-04-25 15:58:01', 1),\n",
       " ('2015-04-27 02:59:01', 1),\n",
       " ('2015-04-30 22:34:24', 1),\n",
       " ('2015-05-06 19:16:00', 1),\n",
       " ('2015-04-03 21:22:14', 1),\n",
       " ('2015-04-11 22:33:31', 1),\n",
       " ('2015-04-18 15:13:18', 1),\n",
       " ('2015-04-03 14:59:36', 1),\n",
       " ('2015-04-06 01:51:04', 1),\n",
       " ('2015-04-23 17:07:05', 1),\n",
       " ('2015-05-05 17:44:35', 1),\n",
       " ('2015-05-07 18:58:28', 1),\n",
       " ('2015-05-09 10:37:03', 1),\n",
       " ('2015-05-11 19:19:11', 1),\n",
       " ('2014-09-17 17:18:52', 1),\n",
       " ('2014-09-18 14:32:06', 1),\n",
       " ('2015-05-21 07:39:12', 1),\n",
       " ('2015-05-21 16:22:37', 1),\n",
       " ('2015-05-22 08:51:07', 1),\n",
       " ('2015-05-22 10:54:20', 1),\n",
       " ('2015-05-22 14:52:43', 1),\n",
       " ('2015-05-23 10:40:32', 1),\n",
       " ('2015-04-29 12:10:47', 1),\n",
       " ('2015-04-30 13:24:31', 1),\n",
       " ('2015-05-12 06:56:12', 1),\n",
       " ('2015-05-12 09:13:42', 1),\n",
       " ('2015-05-12 15:20:47', 1),\n",
       " ('2015-05-13 15:25:36', 1),\n",
       " ('2015-05-14 06:15:17', 1),\n",
       " ('2014-08-13 14:30:15', 1),\n",
       " ('2014-08-15 20:55:23', 1),\n",
       " ('2014-08-16 23:08:51', 1),\n",
       " ('2014-08-20 17:20:56', 1),\n",
       " ('2015-05-05 18:29:36', 1),\n",
       " ('2015-05-07 11:53:55', 1),\n",
       " ('2015-05-08 11:41:43', 1),\n",
       " ('2015-05-24 10:50:10', 1),\n",
       " ('2015-05-24 17:08:35', 1),\n",
       " ('2015-05-25 15:19:16', 1),\n",
       " ('2015-05-26 21:55:44', 1),\n",
       " ('2014-09-18 19:55:24', 1),\n",
       " ('2014-09-26 20:01:00', 1),\n",
       " ('2014-10-02 15:33:15', 1),\n",
       " ('2015-05-08 18:56:23', 1),\n",
       " ('2015-05-09 15:11:40', 1),\n",
       " ('2015-05-10 12:44:12', 1),\n",
       " ('2015-05-14 11:52:07', 1),\n",
       " ('2015-05-14 22:15:16', 1),\n",
       " ('2015-05-15 07:28:55', 1),\n",
       " ('2015-04-30 21:45:31', 1),\n",
       " ('2015-05-06 11:11:10', 1),\n",
       " ('2015-05-07 19:29:19', 1),\n",
       " ('2015-05-13 12:33:00', 1),\n",
       " ('2015-05-13 18:10:50', 1),\n",
       " ('2014-10-06 19:06:39', 1),\n",
       " ('2014-10-07 15:20:23', 1),\n",
       " ('2014-10-17 00:18:32', 1),\n",
       " ('2015-05-27 18:05:32', 1),\n",
       " ('2015-05-27 20:13:27', 1),\n",
       " ('2015-05-27 20:50:46', 1),\n",
       " ('2015-05-28 09:55:15', 1),\n",
       " ('2015-05-15 11:45:28', 1),\n",
       " ('2015-05-15 14:09:33', 1),\n",
       " ('2015-05-15 16:53:17', 1),\n",
       " ('2015-05-16 08:03:41', 1),\n",
       " ('2015-05-07 21:37:11', 1),\n",
       " ('2015-05-08 01:02:25', 1),\n",
       " ('2015-05-08 17:29:47', 1),\n",
       " ('2015-05-09 11:55:38', 1),\n",
       " ('2015-05-11 17:31:28', 1),\n",
       " ('2014-08-22 14:48:28', 1),\n",
       " ('2014-08-22 15:08:53', 1),\n",
       " ('2014-09-11 17:34:25', 1),\n",
       " ('2014-09-18 15:50:53', 1),\n",
       " ('2014-10-17 14:55:24', 1),\n",
       " ('2014-10-29 21:11:49', 1),\n",
       " ('2014-11-11 18:05:45', 1),\n",
       " ('2015-05-17 16:46:48', 1),\n",
       " ('2015-05-18 10:38:12', 1),\n",
       " ('2015-05-18 13:54:53', 1),\n",
       " ('2015-05-18 17:31:34', 1),\n",
       " ('2015-05-14 10:39:22', 1),\n",
       " ('2015-05-16 10:51:14', 1),\n",
       " ('2015-05-16 16:21:44', 1),\n",
       " ('2015-05-28 11:42:09', 1),\n",
       " ('2015-05-29 22:01:29', 1),\n",
       " ('2015-05-30 12:32:04', 1),\n",
       " ('2015-05-30 13:11:08', 1),\n",
       " ('2015-05-30 13:13:47', 1),\n",
       " ('2015-05-18 21:02:10', 1),\n",
       " ('2015-05-19 04:54:21', 1),\n",
       " ('2015-05-19 21:12:22', 1),\n",
       " ('2015-05-19 21:18:50', 1),\n",
       " ('2015-05-19 22:07:49', 1),\n",
       " ('2014-09-25 12:52:22', 1),\n",
       " ('2014-09-26 17:02:32', 1),\n",
       " ('2014-09-30 05:41:57', 1),\n",
       " ('2014-09-30 06:11:56', 1),\n",
       " ('2014-09-30 15:25:46', 1),\n",
       " ('2015-02-01 19:45:26', 1),\n",
       " ('2015-03-04 18:06:47', 1),\n",
       " ('2015-03-06 20:40:37', 1),\n",
       " ('2015-03-16 03:43:59', 1),\n",
       " ('2015-05-30 21:38:49', 1),\n",
       " ('2015-05-30 22:17:42', 1),\n",
       " ('2015-05-31 16:15:54', 1),\n",
       " ('2014-10-02 17:04:56', 1),\n",
       " ('2014-10-16 17:38:57', 1),\n",
       " ('2014-10-16 20:43:29', 1),\n",
       " ('2015-05-16 07:16:37', 1),\n",
       " ('2015-05-16 10:02:23', 1),\n",
       " ('2015-05-18 12:36:36', 1),\n",
       " ('2015-05-20 14:08:47', 1),\n",
       " ('2015-05-21 10:18:25', 1),\n",
       " ('2015-05-17 02:34:10', 1),\n",
       " ('2015-05-17 15:58:26', 1),\n",
       " ('2015-05-19 22:18:34', 1),\n",
       " ('2015-05-20 16:49:15', 1),\n",
       " ('2015-05-20 17:55:54', 1),\n",
       " ('2015-05-21 11:02:26', 1),\n",
       " ('2015-05-24 10:54:54', 1),\n",
       " ('2015-05-24 18:33:57', 1),\n",
       " ('2015-05-22 12:29:28', 1),\n",
       " ('2015-05-23 00:14:23', 1),\n",
       " ('2015-05-23 00:42:45', 1),\n",
       " ('2015-05-18 13:16:03', 1),\n",
       " ('2015-05-18 14:02:09', 1),\n",
       " ('2015-05-19 04:44:42', 1),\n",
       " ('2015-05-19 21:04:29', 1),\n",
       " ('2015-03-18 13:12:00', 1),\n",
       " ('2015-03-22 16:44:56', 1),\n",
       " ('2015-03-24 21:07:05', 1),\n",
       " ('2015-05-25 06:14:03', 1),\n",
       " ('2015-05-25 07:03:58', 1),\n",
       " ('2015-05-25 14:47:56', 1),\n",
       " ('2015-05-26 23:12:47', 1),\n",
       " ('2015-05-26 23:47:13', 1),\n",
       " ('2015-05-24 07:20:52', 1),\n",
       " ('2015-05-25 06:28:43', 1),\n",
       " ('2015-05-26 14:51:51', 1),\n",
       " ('2015-05-31 19:21:56', 1),\n",
       " ('2015-05-31 23:42:51', 1),\n",
       " ('2015-06-01 17:38:18', 1),\n",
       " ('2015-06-02 06:27:46', 1),\n",
       " ...]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[pandas_header[\"Created\"]] for row in pandas_rows]).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e8d2139a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'': 35169, 'Accepted': 52072})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[pandas_header[\"Accepted\"]] for row in pandas_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cbf3dc6",
   "metadata": {},
   "source": [
    "Here, we have one possible illegal value (empty string) which we can suppose means \"Not Accepted\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba8e8115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'Use wxpython DrawText function to add text in a Bitmap': 1,\n",
       "         'Selenium grid 2 over cygwin': 1,\n",
       "         'Issue with concatenation with delimiter formula in Excel': 1,\n",
       "         'Concatenate or print list elements with a trailing comma in Python': 1,\n",
       "         'Python: Exploring data of GUI program': 1,\n",
       "         'Counting statements in Python source files': 1,\n",
       "         'Python unicode issue when iterating over list': 1,\n",
       "         'Firefox WebDriver (Selenium) And Security Warning Popup': 1,\n",
       "         'Elliptic curve and point cardinalities': 1,\n",
       "         'Slow image processing with Python and PIL': 1,\n",
       "         'Is there a handle leak detector which can be linked into an existing application?': 1,\n",
       "         'Writing unicode data in csv': 1,\n",
       "         'sum of small double numbers c++': 1,\n",
       "         'What are the reasons for not hosting a compiler on a live server?': 1,\n",
       "         'Match first instance of digit by Regex in python': 1,\n",
       "         'defining a list of functions in Python': 1,\n",
       "         'Selenium+python Reporting': 1,\n",
       "         'Python faster than compiled Haskell?': 1,\n",
       "         'Pytables - Delete rows from table by some criteria': 1,\n",
       "         'Dictionary searching with datetime keys': 1,\n",
       "         'Python, unicodedata name, and codepoint value, what am i missing?': 1,\n",
       "         'defaultlist design': 1,\n",
       "         \"xl.Workbook() (pyvot) doesn't open an excel workbook\": 1,\n",
       "         'Creating Multiple Excel worksheets in a workbook from an SQLserver 2008 table': 1,\n",
       "         'How to exchange multiple columns': 1,\n",
       "         'How can I parse sql file from within Python?': 1,\n",
       "         'missing attribute error when trying to set cell color in xlwt using python': 1,\n",
       "         'How to read an octet string from file': 1,\n",
       "         'How to know which vars are defined in a class': 1,\n",
       "         \"AttributeError: NoneType object has no attribute 'health'\": 1,\n",
       "         'Format a float in Python with a maximum number of decimal places and without extra zero padding': 1,\n",
       "         'Excel Cells auto filled with value numbers': 1,\n",
       "         'String formatting %02d not performing as expected? What would cause the following code to not print truncated two-digit form? ': 1,\n",
       "         'Pythonic way to insert character': 1,\n",
       "         'Finding SUM except some columns': 1,\n",
       "         'transform excel format': 1,\n",
       "         'Copy format from one column range to another column range in Excel': 1,\n",
       "         'Remove all occurrences of several chars from a string': 1,\n",
       "         'python converting number into dollar format': 1,\n",
       "         'Zipping together unicode strings in Python': 1,\n",
       "         'Why is the same SQLite query being 30 times slower when fetching only twice as many results?': 1,\n",
       "         'Cell to Cell comparison in Sql Server?': 1,\n",
       "         'python pandas OLS.predict, what is the correct signature?': 1,\n",
       "         'Convert a numpy array to a CSV string and a CSV string back to a numpy array': 1,\n",
       "         'Python Pandas: Boolean indexing on multiple columns': 2,\n",
       "         'Concatenating Columns Pandas': 2,\n",
       "         'using python pandas lookup another dataframe and return corresponding values': 1,\n",
       "         'pick random item from a list with constantly reapeating items, but not repeat one twice immediately after': 1,\n",
       "         'Finding starting and ending column name': 1,\n",
       "         'Which python module is used to read CPU temperature and processor Fan speed in WINDOWS..?': 1,\n",
       "         'How to normalize unicode encoding for iso-8859-15 conversion in python?': 1,\n",
       "         'Can I read the browser url using selenium webdriver?': 1,\n",
       "         'How to make nested Panel and Sizer work in wxpython': 1,\n",
       "         'How to save an Excel worksheet as CSV': 1,\n",
       "         'MultiIndexing in pandas based on column conditions': 1,\n",
       "         'Paste multiple formatted cells into excel 12': 1,\n",
       "         'Python Requests throwing SSLError': 1,\n",
       "         'Trouble with UTF-8 CSV input in Python': 1,\n",
       "         'Executing statements in a class definition: Which variables does the interpreter know about?': 1,\n",
       "         'Excel what method use to calculate time and age': 1,\n",
       "         'How to replace non-zero elements randomly with zero?': 1,\n",
       "         'How to deal with log of zero in R in image.plot?': 2,\n",
       "         'convert number to formatted string in python with prefix 0s': 2,\n",
       "         'import modules from folder (Python)': 1,\n",
       "         'Is the build-in probability density functions of `scipy.stat.distributions` slower than a user provided one?': 2,\n",
       "         'combining 2D arrays to 3D arrays': 1,\n",
       "         'How to find x to make sure sum(row[i])-sum(clo[i])=bi in python?': 1,\n",
       "         'Create iterable list from very large number of entries': 1,\n",
       "         'How to access ODB files in Python 2.7': 1,\n",
       "         'Allowing users to delete user input in python': 1,\n",
       "         'Python - vectorizing a sliding window': 1,\n",
       "         'How do I interate through a paired list when using map and lambda?': 1,\n",
       "         'Replacing index variables in Python': 1,\n",
       "         'pivot \"stacked\" data with multiple indexes?': 1,\n",
       "         'upgade python version using pip': 1,\n",
       "         'Fitting a Weibull distribution using Scipy': 1,\n",
       "         'Two sums from one list': 1,\n",
       "         'Using ord() to convert letters to ints (Very basic)': 1,\n",
       "         'How to identify the first occurence of duplicate rows in Python pandas Dataframe': 2,\n",
       "         'Convert time series into dataframe so that each row has consequtive time slices': 1,\n",
       "         'Is there an efficient way to merge two sorted dataframes in pandas, maintaing sortedness?': 1,\n",
       "         'Ordering Headers in a DataFrame using Python': 1,\n",
       "         'Is there in pandas operation complementary (opposite) to groupby?': 1,\n",
       "         'Grouping values inside a list': 1,\n",
       "         'Pandas data frame from dictionary': 2,\n",
       "         'Count distinct words from a Pandas Data Frame': 2,\n",
       "         'Python: Difference between filter(function, sequence) and map(function, sequence)': 2,\n",
       "         'Reverse a string without using reversed() or [::-1]?': 1,\n",
       "         'os.mkdir(path) returns OSError when directory does not exist': 1,\n",
       "         'error using L-BFGS-B in scipy': 1,\n",
       "         'Why is this while statement looping infinitely?': 1,\n",
       "         'Normalization to bring in the range of [0,1]': 1,\n",
       "         'create a cross tab (maybe/sort of, not sure what its called) with Python': 1,\n",
       "         'why the list is not changed when I modify it within the function in python': 1,\n",
       "         'How to set pivots on colorbar at customisable location in Matplotlib?': 1,\n",
       "         \"numpy/scipy analog of matlab's fminsearch\": 1,\n",
       "         'How can I change my python code avoid computed the maxnumber index': 1,\n",
       "         'Fast peak-finding and centroiding in python': 1,\n",
       "         'Variable number of parameters for scipy optimize with L-BFGS-B algorithm': 1,\n",
       "         'Edge detection of histogram - python': 1,\n",
       "         'One-sided Wilcoxon signed-rank test using scipy': 1,\n",
       "         'Exception from HRESULT: 0x800A03EC when trying to add formula r1c1 while creating workbook': 1,\n",
       "         'Is there some elegant way to manipulate my ndarray': 1,\n",
       "         'merging files based on column coordinates of two files in python': 1,\n",
       "         'Creating a numpy array of 3D coordinates from three 1D arrays': 1,\n",
       "         'iteratively calling pandas datareader': 1,\n",
       "         'python convert string to integer array': 1,\n",
       "         '-Python- Ordering lists based on a format': 1,\n",
       "         'Transposing a 3d list (Python)': 1,\n",
       "         'Store business days according to week in Python (2.7)?': 1,\n",
       "         'How to select subarrays in numpy 1D array with minimal, average and maximal area values?': 1,\n",
       "         \"How to set Python's default version to 3.x on OS X?\": 1,\n",
       "         'ValueError in python.': 1,\n",
       "         'printing sub-array in numpy as Matlab does': 1,\n",
       "         'Excel-like user input in python GUI applications': 1,\n",
       "         'Bootstrap method and confidence interval': 1,\n",
       "         'matplotlib: how to plot concentric circles at a given set of radii': 1,\n",
       "         'Find ordered vector in numpy array': 1,\n",
       "         'Use applymap on DataFrame but keep index/column info': 1,\n",
       "         'matplotlib pyplot table with non-ascii data?': 1,\n",
       "         'Fitting empirical distribution to theoretical ones with Scipy (Python)?': 1,\n",
       "         'How do I use a minimization function in scipy with constraints': 1,\n",
       "         'Python: first unique number in a list': 1,\n",
       "         'Replacing element in list without list comprehension, slicing or using [ ]s': 1,\n",
       "         'Generating random numbers in python': 1,\n",
       "         'Assigning to a new list just the last object of another list - Python': 1,\n",
       "         'Create dummies from column with multiple values in pandas': 1,\n",
       "         'Align matplotlib Text with colorbar': 1,\n",
       "         'How to create pivot with totals (margins) in Pandas?': 1,\n",
       "         'Efficient way to create strings from a list': 2,\n",
       "         'Seeking a fast filter() with removal': 1,\n",
       "         'Python 3+ - for in range loop, multiply, product, list NOT using mul or lambda or for in loop': 1,\n",
       "         'Python, convert all entries of list from string to float': 1,\n",
       "         'Pandas - link an aggregated row back to its original elements': 2,\n",
       "         'Modify tick label text': 1,\n",
       "         'python array intersection efficiently': 1,\n",
       "         'function to get number of columns in a NumPy array that returns 1 if it is a 1D array': 1,\n",
       "         'Python: how do I create a list of combinations from a series of ranges of numbers': 1,\n",
       "         'how to create similarity matrix in numpy python?': 1,\n",
       "         'Setting Different Bar color in matplotlib Python': 1,\n",
       "         \"How to enforce scipy.optimize.fmin_l_bfgs_b to use 'dtype=float32'\": 1,\n",
       "         'Efficiency with very large numpy arrays': 1,\n",
       "         'How do I use the \"survival\" package and Surv function in R with left-truncated data?': 1,\n",
       "         'Best way to exchange datetime values between client and server': 1,\n",
       "         'getting a default value from pandas dataframe when a key is not present': 1,\n",
       "         'Numpy cumulative sum for csv rows gives type error': 1,\n",
       "         'Python-How to create a list of N elements from a given list': 1,\n",
       "         'how to use scipy.stats.kstest/basic questions about Kolmogorov–Smirnov test': 1,\n",
       "         'scipy.optimize.leastsq fails to fit simple model': 1,\n",
       "         'Pythonic vs Unpythonic': 1,\n",
       "         'Python Pandas: Resolving \"List Object has no Attribute \\'Loc\\'\"': 2,\n",
       "         'Trying to extract data from a file, then calculate two different averages from the file, and it keeps turning up 0.0, what do i do?': 1,\n",
       "         'Python: How to get local maxima values from 1D-array or list': 1,\n",
       "         \"How to enter censored data into R's survival model?\": 1,\n",
       "         'Printing floating point in python': 1,\n",
       "         'How to add an extra row to a pandas dataframe': 2,\n",
       "         'Replace row in DataFrame dependant on a value': 1,\n",
       "         \"How to change the location the 'r' axis for matplotlib polar plot?\": 1,\n",
       "         'Use Python to Write VBA Script?': 1,\n",
       "         'Subset of an ndarray based on another array': 1,\n",
       "         'Append numpy ndarrays with different dimensions in loop': 1,\n",
       "         'how to assign random float number in python': 1,\n",
       "         'how to get the index of numpy.random.choice? - python': 1,\n",
       "         'boxplot: index out of range error': 1,\n",
       "         'Python 2.7 Trying to narrow a list using range and duplicate': 1,\n",
       "         'scipy.optimize.curvefit: Asymmetric error in fit': 1,\n",
       "         'pandas merge columns to a single time series': 1,\n",
       "         'numpy vstack throwing dimension error': 1,\n",
       "         'matplotlib scatter works in ipython notebook, but not console': 1,\n",
       "         'Capitalize a substring within a string': 1,\n",
       "         'Extraction of common element from multiple arrays to make a new array': 1,\n",
       "         'pandas - checking a condition for each group in a dataframe': 1,\n",
       "         'max of Corresponding Elements in a numpy.ndarray': 1,\n",
       "         'Matplotlib heatmap with changing y-values': 1,\n",
       "         'SQL error 1064 fail to create a table': 1,\n",
       "         \"Is there a Python equivalent to MATLAB's pearsrnd function?\": 1,\n",
       "         'How to remove gaps between subplots in matplotlib?': 1,\n",
       "         'About NumPy array in Python': 1,\n",
       "         'Should it be supported to store a pandas dataframe in another pandas dataframe? I can no longer display such data frames. Is this a bug?': 1,\n",
       "         'Get particular row as series from pandas dataframe': 2,\n",
       "         'Numpy array cannot index within a single []': 1,\n",
       "         'How to combine several similar .csv files into one dataframe with given structure': 1,\n",
       "         'How to convert occurence matrix to co-occurence matrix in Python': 1,\n",
       "         'Matplotlib: different font type rendering in axis labels': 1,\n",
       "         'IndexError: list index out of range // numpy?': 1,\n",
       "         'Error: [only length-1 arrays can be converted to Python scalars] when changing variable order': 1,\n",
       "         'Plotting data from csv using matplotlib.pyplot': 1,\n",
       "         'Transpose columns in python/pandas': 1,\n",
       "         'R language, count the Friday in a month': 1,\n",
       "         'pandas - determine the duration of an event': 1,\n",
       "         'Creating a Multi-Index / Hierarchical DataFrame from Dictionaries': 2,\n",
       "         'How to do expanding functions using Pandas on Time Series': 1,\n",
       "         'Why this date conversion with Pandas.to_datetime is much slower than some alternatives?': 2,\n",
       "         'rpy2 module is invisible from python 2.7 on Mac 10.6.8': 1,\n",
       "         'Fitting a distribution to data: how to penalize \"bad\" parameter estimates?': 1,\n",
       "         'Matplotlib pcolor/pcolormesh falls apart if the number of rows/cols are certain numbers (usually prime)': 1,\n",
       "         'Numpy extract submatrix': 1,\n",
       "         'How can I efficiently move from a Pandas dataframe to JSON': 1,\n",
       "         'saving multiple figures in python to create array plot': 1,\n",
       "         'Efficient feature reduction in a pandas data frame': 1,\n",
       "         'how to draw a nonlinear function using matplotlib?': 1,\n",
       "         'List of 3x1 numpy arrays in Python': 1,\n",
       "         'Matplotlib: How to adjust linewidth in colorbar for contour plot?': 1,\n",
       "         'Styling of Pandas groupby boxplots': 1,\n",
       "         'Extracting a range of data from a python list': 1,\n",
       "         'format phone number in csv using pandas': 2,\n",
       "         \"How does one test if a matrix in Python has only 1's and 0's?\": 2,\n",
       "         'Filter out pandas pivot table rows': 1,\n",
       "         'Using scipy stats, how can I implement a statistical test in my if else statement?': 1,\n",
       "         'scipy curve_fit error: divide by zero encountered': 1,\n",
       "         \"matplotlib list not showing 0 or Nan's\": 1,\n",
       "         'Pandas: How to create a datetime dataframe index from dictionary': 1,\n",
       "         'Having problems with NumPy matrices': 1,\n",
       "         'How to count number of rows per group (and other statistics) in pandas group by?': 1,\n",
       "         'How to convert a Numpy 2D array with object dtype to a regular 2D array of floats': 1,\n",
       "         'Faster way to do this using Python, Numpy?': 1,\n",
       "         'fitting an image with 2D equation in python': 1,\n",
       "         'Pandas - cumsum by month?': 2,\n",
       "         'Dealing with N by 1 matrices in Numpy': 1,\n",
       "         'Compact Matplotlib code( define x,y axis together)': 1,\n",
       "         'Build diagonal matrix without using for loop': 1,\n",
       "         'Numpy: Comparing two data sets for fitness': 1,\n",
       "         'Replacing items in a two dimensional list in python': 1,\n",
       "         'estimate scale parameter by using scipy.stats.gamma and scipy.optimize.minimize': 1,\n",
       "         'Matplotlib Errorbar behaviour with NaNs': 1,\n",
       "         'How to store a numpy arrays in a column of a Pandas dataframe?': 1,\n",
       "         'Changing the marker on the same set of data': 1,\n",
       "         'performing comparative calculation for classes within csv': 1,\n",
       "         'How to visualize 95% confidence interval in matplotlib?': 1,\n",
       "         'How to turn categorical data into relative counts with R': 1,\n",
       "         'Pandas: how to get a particular group after groupby?': 2,\n",
       "         'count occurence in ranges in R': 1,\n",
       "         'Looking for the motifs in sequence': 1,\n",
       "         'plotting pandas data frame with unequal data set': 1,\n",
       "         'Python .split list is callable but index is out of range?': 1,\n",
       "         'linear interpolation in scipy': 1,\n",
       "         'Python: separating a list by unique values': 1,\n",
       "         'How do I catch import errors from read_stata function in pandas?': 1,\n",
       "         'how to make argsort result to be random between equal values?': 1,\n",
       "         'R translation to Python': 1,\n",
       "         'Running an Excel macro via Python?': 1,\n",
       "         'How to use read_html to read a table with 2 class attrs?': 1,\n",
       "         'How to align the bar and line in matplotlib two y-axes chart?': 1,\n",
       "         \"pandas.series.copy doesn't create new object\": 1,\n",
       "         'How to make this pandas query?': 1,\n",
       "         'Add index name to a dataframe / serie': 1,\n",
       "         'R_user not defined , rpy2': 1,\n",
       "         'Adding labels to candlestick series': 1,\n",
       "         'In R why do you need two // sometimes to read in a directory': 1,\n",
       "         'to find if list have the negative sign': 1,\n",
       "         'Broadcasting a list in Pandas': 3,\n",
       "         'How can I export Python multidimensional numpy array to different files based on second value of each line?': 1,\n",
       "         'Finding unique items in list and add them to new list with list comprehension': 1,\n",
       "         'How to sort and select pandas data': 1,\n",
       "         'How can I avoid repeated indices in pandas DataFrame after concat?': 1,\n",
       "         'removing some special columns in large data set with R': 1,\n",
       "         'Deleting row from hierarchical Series in Pandas based on column value and position': 1,\n",
       "         'How to efficiently concatenate many arange calls in numpy?': 1,\n",
       "         'DNA sequence random sampling': 1,\n",
       "         'Weibull distribution and the data in the same figure (with numpy and scipy)': 1,\n",
       "         'How to change the colors of pylab legend?': 1,\n",
       "         'Overlaying actual data on a boxplot from a pandas dataframe': 1,\n",
       "         'store result of for loop in a vector in R': 1,\n",
       "         'Display Entire Dataset,Python': 1,\n",
       "         'Changing ordinal character data to numeric data with Pandas': 1,\n",
       "         'Python plot label': 1,\n",
       "         'Extracting single value from column in pandas': 2,\n",
       "         'Add minor gridlines to matplotlib plot using seaborn': 1,\n",
       "         'LinearRegression Predict- ValueError: matrices are not aligned': 1,\n",
       "         'How to install numpy in OSX properly?': 1,\n",
       "         'trying to installing rpy2 on Windows Vista': 1,\n",
       "         'Finding non-numeric rows in dataframe in pandas?': 2,\n",
       "         'Pandas - How to dynamically get min and max value of each session in the column': 1,\n",
       "         'Grouping a set of points by proximity': 1,\n",
       "         'matplotlib colorbar not working (due to garbage collection?)': 2,\n",
       "         'How to do curve_fit in python': 1,\n",
       "         'autopct cant be added to axis.pie :: error: too many values': 1,\n",
       "         'Pandas, bygroup operation': 1,\n",
       "         'Scatter plots in Pandas/Pyplot: How to plot by category': 1,\n",
       "         'Adding labels from a file to data points on a plot': 1,\n",
       "         'Python Time differencing in pandas': 1,\n",
       "         'python pandas : How to get the index of the values from a series that have matching values in other series?': 1,\n",
       "         'find number of occurance of non zero values in column of csv file using python': 1,\n",
       "         'Vectorised code for selecting elements of a 2D array': 1,\n",
       "         'Numpy arrays assignment operations indexed with arrays': 1,\n",
       "         'how to solve 3 nonlinear equations in python': 1,\n",
       "         'Numpy: Remove neighboring repeated subarrays in a 2x2 array?': 1,\n",
       "         'Changing colour scheme of python matplotlib python plots': 1,\n",
       "         'R using apply with indexes': 1,\n",
       "         'How to get the union of two lists using list comprehension?': 1,\n",
       "         'Fast column shuffle of each row numpy': 2,\n",
       "         'Random number function python that includes 1?': 1,\n",
       "         \"How to find parameters of gumbel's distribution using scipy.optimize\": 1,\n",
       "         'Numpy: reading multiple columns from multiple files': 1,\n",
       "         'Setting dash length when plotting arrow': 1,\n",
       "         'Convert a Pandas DataFrame to bin frequencies': 1,\n",
       "         'How to apply R function Assocstats of library VCD in Rpy python': 1,\n",
       "         'Input into specific position using an index in an array': 1,\n",
       "         'python, if/elif/else syntax split up by comments?': 1,\n",
       "         'Why does np.median() return multiple rows?': 1,\n",
       "         'python compute distance matrix from dictionary data': 1,\n",
       "         'Python: retrieve values after index in each column of table': 1,\n",
       "         'Solve highly non-linear equation for x in Python': 1,\n",
       "         'R: Getting rows from specific columns': 1,\n",
       "         'pandas: extract or split char from number string': 2,\n",
       "         'Cycle through a string using a for loop': 1,\n",
       "         'Subsetting of data frame in R with row selection': 1,\n",
       "         'read fastq file into dictionary': 1,\n",
       "         'Best format to pack data for correlation determination?': 1,\n",
       "         'elegant way to get Scatter plot in R or matplotlib': 1,\n",
       "         'numpy: find first index of value in each row of 2D array': 1,\n",
       "         'Matplotlib boxplot using precalculated (summary) statistics': 1,\n",
       "         'Create Contour Plot from Pandas Groupby Dataframe': 1,\n",
       "         'Python Pandas: Get row by median value': 1,\n",
       "         'Combine Columns Pandas': 1,\n",
       "         'Stacked histogram will not stack': 1,\n",
       "         'Pandas Dataframe to_csv format output': 1,\n",
       "         'Python: Divide values in cell by max in each column': 1,\n",
       "         'How can I generate more colors on pie chart matplotlib': 1,\n",
       "         'python - matplotlib - polar plots with angular labels in radians': 1,\n",
       "         'Pandas: concatenating conditioned on unique values': 3,\n",
       "         'Pythonic way of detecting outliers in one dimensional observation data': 1,\n",
       "         'Transforming pairwise string into tuples': 1,\n",
       "         'Scipy - optimize. Find ratio between two variables': 1,\n",
       "         'How can check the distribution of a variable in python?': 1,\n",
       "         'Python 2D plots as 3D (Matplotlib)': 1,\n",
       "         'How can I plot multiple lines using the same array and set disconnect points in python?': 1,\n",
       "         'setting an array element with a sequence': 1,\n",
       "         'Function to check whether a number is a Fibonacci number or not?': 1,\n",
       "         'pyplot equivalent for pl.cm.Spectral in matplotlib': 2,\n",
       "         'Converting an RPy2 ListVector to a Python dictionary': 1,\n",
       "         'Why does sum(DF) behave differently from DF.sum()?': 2,\n",
       "         'pandas- adding a series to a dataframe causes NaN values to appear': 1,\n",
       "         'Get a clean summary of nlme.lme() or lme4.lmer() in RPy': 1,\n",
       "         'Convert 2D numpy.ndarray to pandas.DataFrame': 2,\n",
       "         'Pandas DataFrame selection using text from another DataFrame': 1,\n",
       "         'R Two matrices, extract rows from m based on common column of r': 1,\n",
       "         'Download stocks data from google finance': 1,\n",
       "         'Python Pandas - replace values with NAN in multiple columns based on mutliple dates?': 2,\n",
       "         'Minimizing three variables in python using scipy': 1,\n",
       "         'multivariate gaussian probability density function python on Mac': 1,\n",
       "         'Pandas: how to apply function to only part of a dataframe and append result back to dataframe?': 1,\n",
       "         'dividing list in python based on variable': 1,\n",
       "         'Use of fmin in python': 1,\n",
       "         'For loop in an opposite direction': 1,\n",
       "         'Pandas: Use multiple columns of a dataframe as index of another': 2,\n",
       "         'Matplotlib in python on a mac opens a new window': 1,\n",
       "         'How can I ignore zeros when I take the median on columns of an array?': 1,\n",
       "         'Using passed axis objects in a matplotlib.pyplot figure?': 1,\n",
       "         'Removing part of string from Pandas DataFrame column': 1,\n",
       "         'Python Empirical distribution function (ecdf) implementation': 1,\n",
       "         'Comparing and replacing elements of a list': 1,\n",
       "         'correlation between arrays in python': 1,\n",
       "         'Rearranging entries of rows in numpy array': 1,\n",
       "         'Why Pandas doesn´t allow multiple index setting?': 1,\n",
       "         'Extending regressions beyond data in Matplotlib': 1,\n",
       "         'Adding a colorbar to a polar contourf multiplot': 1,\n",
       "         'Finding several regions of interest in an array': 1,\n",
       "         'all possibilities of a small list from a superset in python': 1,\n",
       "         'Pyplot issues in horizontal bar chart': 1,\n",
       "         'pandas do a \"true\" concat': 1,\n",
       "         'Cumulative sum of variable till a given percentile': 1,\n",
       "         'Last minor tick not drawn - Pyplot': 1,\n",
       "         'Creating Matplotlib Graph with common line across subplots in Python': 1,\n",
       "         'Python: How to iterate lists of dictionaries Fast': 1,\n",
       "         \"Why does 'rmagic' %R cause an error when reading a file, while %%R does not?\": 1,\n",
       "         't-tests on different groups by iteration in R': 1,\n",
       "         'Skip missing column header in pandas dataframe?': 2,\n",
       "         'change X ticks in matplotlib plot': 1,\n",
       "         'How can I save two plots on a single file in python?': 1,\n",
       "         'How to find a string after a specific set of text?': 1,\n",
       "         \"Is there a method to do arithmetic with SciPy's random variables?\": 1,\n",
       "         'Python Pandas merge samed name columns in a dataframe': 3,\n",
       "         \"Polar plot with a 'floating' radial axis\": 1,\n",
       "         'Make Python Lists Into Two-Dimensional Arrays': 1,\n",
       "         'Can I speed up this basic linear algebra code?': 1,\n",
       "         'How to create DataFrame in pandas python': 1,\n",
       "         'fsolve - mismatch between input and output': 1,\n",
       "         'Skipping specific indices in an array (or addressing intervals of indices)': 1,\n",
       "         'counting number of gaps in python': 1,\n",
       "         'Idiomatic way to add two pandas Series objects with different indices': 1,\n",
       "         'format x-axis values and show them only under the scatter plot value - matplotlib in Python': 1,\n",
       "         'How to normalize a histogram in python?': 1,\n",
       "         'turn off axis border for polar matplotlib plot': 1,\n",
       "         \"Putting arrowheads on vectors in matplotlib's 3d plot\": 1,\n",
       "         'in python pandas, how to unpack the lists in a column?': 1,\n",
       "         'Fastest Way to Drop Duplicated Index in a Pandas DataFrame': 2,\n",
       "         'Group pandas multiindex dataframe by all the indices': 1,\n",
       "         'Installing rpy2 -- Variable Error': 1,\n",
       "         'Running rpy2 from command prompt': 1,\n",
       "         'Change main plot legend label text': 1,\n",
       "         'compare two lists and return not matching items': 1,\n",
       "         'Using query to extract date ranges': 1,\n",
       "         'Aggregate a DataFrame column by counting special elements': 1,\n",
       "         'Matplotlib line collection and additional line': 1,\n",
       "         'Matplotlib bar plot width only changing width of last bar': 1,\n",
       "         'Inexpensive way to add time series intensity in python pandas dataframe': 1,\n",
       "         'Construct 2 time series random variables with fixed correlation': 1,\n",
       "         'Numpy Polyfit or any fitting to X and Y multidimensional arrays': 1,\n",
       "         'Pandas groupby cumulative sum': 1,\n",
       "         'Numpy Assigning with Indexing': 1,\n",
       "         'Keep finite entries only in Pandas': 1,\n",
       "         \"I'm having trouble plotting date and two pieces of data on same graph with matplotlib\": 1,\n",
       "         'python error - moving average on numpy array': 1,\n",
       "         'Slice Function with a MultiIndex Pandas Panel': 1,\n",
       "         'Python boxplot out of columns of different lengths': 1,\n",
       "         'plotting & formatting seaborn chart from pandas dataframe': 1,\n",
       "         'Reindex Pandas DataFrame with sum instead of bfill or ffill': 1,\n",
       "         \"How to pick a random element in an np array only if it isn't a certain value\": 1,\n",
       "         'How do I check if a numpy dtype is integral?': 1,\n",
       "         'Weird pdfs from Generalised Extreme Value (GEV) Maximum Likelihood fitted data': 1,\n",
       "         'Plot background and labels in the same graph-matplotlib': 1,\n",
       "         'Generating a heat map using 3D data in matplotlib': 1,\n",
       "         'Pandas MultiIndex Operations on Sub-Index': 1,\n",
       "         'Adding error bars to grouped bar plot in pandas': 1,\n",
       "         'How to customize axes in 3D hist python/matplotlib': 1,\n",
       "         'Matplotlib - setting tick positions on a specific subplot': 1,\n",
       "         'How to find the shared properties of any two elements in a CSV file': 1,\n",
       "         \"Using Python's Pandas to find average values by bins\": 1,\n",
       "         'Printing Pandas Columns With Unicode Characters': 1,\n",
       "         'Inconsistent behavior of apply with operator.itemgetter v.s. applymap operator.itemgetter': 3,\n",
       "         'Bin datetime range using pandas': 1,\n",
       "         \"in Pandas Dataframe, after I set an entire column, I can't update another column with times\": 1,\n",
       "         'NumPy: matrix by vector multiplication': 1,\n",
       "         'Controlling representation of datetime64[ns] in Pandas plots': 1,\n",
       "         'Pandas: Sum of first N non-missing values per row': 1,\n",
       "         'Easiest way to write similarities in two strings to a new dynamic array': 1,\n",
       "         'Updating the fontsize only of title, xlabel or ylabel': 2,\n",
       "         'Sort column having float values in python': 1,\n",
       "         'Migrating a logistic regression from R to rpy2': 1,\n",
       "         \"Numpy - correlation coefficient and related statistical functions don't give same results\": 1,\n",
       "         'Fitting a variable Sinc function in python': 1,\n",
       "         'Propagating down the result of groupby aggregations': 2,\n",
       "         'Numpy summarize one array by values of another': 1,\n",
       "         'Sort data frame by character and date columns in R': 1,\n",
       "         'Read CSV with multiple Headers': 1,\n",
       "         'Filtering Groups With Pandas': 1,\n",
       "         'Bar plot cutting off axis title': 1,\n",
       "         'dealing with data labels in 96 well plate with python': 1,\n",
       "         'Linear fit including all errors with NumPy/SciPy': 1,\n",
       "         'how to set alignment in pandas in python with non-ANSI characters': 1,\n",
       "         'Pandas DataFrame - convert months to datetime and iteratively select data from multiple columns for plotting': 1,\n",
       "         'creating a python pandas dataframe from a list of dictionaries when one entry of each dictionary is itself an array': 1,\n",
       "         '2-D numpy array with tuples: expanding into multiple arrays w/combinations of original tuples': 1,\n",
       "         'In R data.table multiplication by column name based on values of another column': 1,\n",
       "         'Group labels in matplotlib barchart using Pandas MultiIndex': 1,\n",
       "         'Mapping a Series with a NumPy array -- dimensionality issue?': 2,\n",
       "         'I want to parse an XML document into the desired format to create data-points for a graph': 1,\n",
       "         'Pandas : compute mean or std (standard deviation) over entire dataframe': 1,\n",
       "         'Accessing nested JSON data as dataframes in Pandas': 1,\n",
       "         'in numpy, delete a row where an entry in a column cannot be converted to a float': 1,\n",
       "         'how to set bounds for the x-axis in one figure containing multiple matplotlib histograms and create just one column of graphs?': 1,\n",
       "         'Matplotlib Line Overlap/Resolution': 1,\n",
       "         'Replacing end numbers in a string python': 1,\n",
       "         'Categorical variable in Rpy2 (factor function)': 1,\n",
       "         'How can I import R dataframes into Pandas?': 1,\n",
       "         'How to unpack a Series of tuples in Pandas?': 2,\n",
       "         'Adding Multiple Dict Value against a Single Key': 2,\n",
       "         'Python: Replace a cell value in Dataframe with if statement': 1,\n",
       "         'Boxplot stratified by column in python pandas': 1,\n",
       "         'How to merge 2 dictionaries in Python by single value': 1,\n",
       "         'matplotlib polar plot set_ylabel on the left hand side': 1,\n",
       "         'Label objects not found': 1,\n",
       "         'Function returns a vector, how to minimize in via NumPy': 1,\n",
       "         'Replace NaN or missing values with rolling mean or other interpolation': 1,\n",
       "         'Fastest way to find nearest triangle number?': 1,\n",
       "         'Numpy: efficient array of indices to \"bump\" array': 1,\n",
       "         'Hierarchical group sizes in Pandas': 1,\n",
       "         'Creating contour plots without using numpy.meshgrid method?': 1,\n",
       "         'scipy.optimize.fsolve convergence bug?': 1,\n",
       "         'Pandas cumsum with conditional product of lagged value?': 1,\n",
       "         'Setting values on a subset of rows (indexing, boolean setting)': 2,\n",
       "         'Interpolating np.nan values in scipy': 1,\n",
       "         'Matplotlib: How to make two histograms have the same bin width?': 1,\n",
       "         'Best way to split every nth string element and merge into array?': 1,\n",
       "         'matplotlib: enlarge axis-scale label': 1,\n",
       "         'Matplotlib - plotting transparent and overlapping time series': 1,\n",
       "         'cycling through list of linestyles when plotting columns of a matrix in matplotlib': 1,\n",
       "         'Can matplotlib errorbars have a linestyle set?': 1,\n",
       "         'Write all elements of string array to a single string': 1,\n",
       "         'Drawing over matplotlib axis/figs: get yticklabel individual position and use it for drawlines': 1,\n",
       "         'numpy multiply matrices preserve third axis': 1,\n",
       "         'Can the scipy.stats.binom library return a value of \"N\" for a particular \"k\" and \"p\"': 1,\n",
       "         'Python Pandas calucate Z score of groupby means': 1,\n",
       "         'Replace lowercase ASCII characters with X in Python': 1,\n",
       "         'Linear Fit python errors': 1,\n",
       "         'Repeating elements of a list n times': 1,\n",
       "         'pandas dataframe (Selection)': 2,\n",
       "         'Fitting a Gaussian to a set of x,y data': 1,\n",
       "         'In-Place Update of Values in Pandas Dataframe': 1,\n",
       "         'Formatting datetime xlabels in matplotlib (pandas df.plot() method)': 1,\n",
       "         'Drop all duplicate rows in Python Pandas': 2,\n",
       "         'More plotting options for pandas.DataFrame.plot(kind=\"bar\")': 1,\n",
       "         'Elegant and efficient way to repeatedly merge dataframes into single column of a dataframe': 1,\n",
       "         'How to plot specific rows and columns of pandas dataframe (based on name of row and name of column) in bar plot with error bars?': 1,\n",
       "         'Why does Rmagic reverse order of plots in cell?': 1,\n",
       "         'How to calculate the likelihood of curve-fitting in scipy?': 1,\n",
       "         'How can I get a pandas dataframe into CSV format with different formats per columns?': 1,\n",
       "         'Regression of a timeseries delta in pandas': 1,\n",
       "         'Linear interpolation within groups': 1,\n",
       "         'Fitting negative binomial in python': 1,\n",
       "         'Why is numba faster than numpy here?': 2,\n",
       "         \"How can I get the matplotlib rgb color, given the colormap name, BoundryNorm, and 'c='?\": 1,\n",
       "         'How to recover matplotlib defaults after setting stylesheet': 1,\n",
       "         'How to apply OLS from statsmodels to groupby': 1,\n",
       "         'Create dataframe row with positive numbers and other with negative': 2,\n",
       "         'Regular expression negative lookbehind of non-fixed length': 1,\n",
       "         'simple string manipulation in python': 1,\n",
       "         'pandas convert_to_r_dataframe does not work with numpy.bool_': 1,\n",
       "         'Using .mean() in numpy': 1,\n",
       "         'Identifying range of x array indices to set corresponding y array values to zero': 1,\n",
       "         'nested list self increment': 1,\n",
       "         'Change pandas plot background color': 1,\n",
       "         'How to combine and do group computations on Pandas datasets?': 1,\n",
       "         'solve linear equations given variables and uncertainties: scipy-optimize?': 1,\n",
       "         'Date sampling / averaging for plotting in Pandas': 1,\n",
       "         'matplotlib changing barplot xtick labels and sorting the order of bars': 1,\n",
       "         'Change string to integer in python / pandas': 1,\n",
       "         \"Stacking 3 bars on top of each other via Python's Matplotlib\": 1,\n",
       "         'how to print equation of line using scipy stats': 1,\n",
       "         'rpy2 installation error on Windows 8 (Anaconda)': 1,\n",
       "         'matplotlib scatter plot np.choose Value error': 1,\n",
       "         \"How to get output of pandas .plot(kind='kde')?\": 1,\n",
       "         \"numpy, recarray: Methods to convert a list of dict's to a np.recarray?\": 1,\n",
       "         'pandas slicing and vstack interleaving a dataframe': 1,\n",
       "         'Detect and exclude outliers in Pandas data frame': 3,\n",
       "         'How to speed up numpy code': 1,\n",
       "         'Understanding axis in Python': 1,\n",
       "         'Adding a new pandas column with mapped value from a dictionary': 1,\n",
       "         'Skip gcf().autofmt_xdate() at pandas plot creation': 1,\n",
       "         'Merge after groupby': 1,\n",
       "         'matplotlib remove axis label offset by default': 1,\n",
       "         'Pandas - Computing the mean vector of a window when the start is column specific': 2,\n",
       "         'Using an image for tick labels in matplotlib': 1,\n",
       "         'How to drop rows based on year on a based dataframe in a function using *args': 1,\n",
       "         'Adding row in Pandas DataFrame keeping index order': 1,\n",
       "         'Filling missing lines with \"nan\" with pandas reindex': 1,\n",
       "         'Standalone curve_fit methode': 1,\n",
       "         'How can I delete portion of data in python pandas dataframe imported from csv?': 1,\n",
       "         'how to display and input chinese (and other non-ASCII) character in r console?': 1,\n",
       "         'pandas dataframe with 2-rows header and export to csv': 3,\n",
       "         'Python sci-kit learn (metrics): difference between r2_score and explained_variance_score?': 1,\n",
       "         'KeyError after resampling': 1,\n",
       "         'Access single cell of pandas dataframe?': 1,\n",
       "         'IPython in Safari: websocket connection failure': 2,\n",
       "         'Calculate correlation between all columns of a DataFrame and all columns of another DataFrame?': 1,\n",
       "         \"how do i 'update' a df based on the values of another dataframe that shares a common key? python\": 1,\n",
       "         'Calling R script from python using rpy2': 1,\n",
       "         'Applying function to a single column of a grouped data in pandas': 1,\n",
       "         'converting timestamps into plot-able array of values': 1,\n",
       "         'Regression in pandas': 1,\n",
       "         'Update a dataframe in pandas while iterating row by row': 2,\n",
       "         'Python using xhtml2pdf to print webpage into PDF': 1,\n",
       "         'Unsupported Operand Type Error with scipy.optimize.curve_fit': 1,\n",
       "         'matplotlib, add common horizontal lines at the x axis across multiple subplots': 1,\n",
       "         'Font size in matplotlib': 1,\n",
       "         'Python Numpy matrix multiplication in high dimension': 1,\n",
       "         'Scipy strange results using curve fitting': 2,\n",
       "         'Set xlim for pandas/matplotlib where index is string': 1,\n",
       "         'reading tab-delimited data without header in pandas': 1,\n",
       "         'pandas dataframe columns scaling with sklearn': 1,\n",
       "         'Select elements from an array using another array as index': 1,\n",
       "         \"The color parameter for matplotlib's scatter is `c`, but `color` works too; prod. diff. results\": 1,\n",
       "         'Applying Transformation to Table (Pandas)': 2,\n",
       "         'add rows to groups in pandas dataframe': 1,\n",
       "         'How to define interdependance between variables in scipy optimization (python 2.7)?': 1,\n",
       "         'Turning a list of tuples with variable lengths into a dataframe': 1,\n",
       "         'Obtaining values used in boxplot, using python and matplotlib': 1,\n",
       "         'Dynamically writing the objective function and constraints for scipy.optimize.minimize from matrices': 1,\n",
       "         'Matplotlib Histogram Alignment': 1,\n",
       "         'How to be a faster Panda with groupbys': 1,\n",
       "         'Elementwise Subtraction(or addition) of columns from two dataframes based on the same values/matches in other columns': 1,\n",
       "         'Pivot and plot data': 1,\n",
       "         'missing column after pandas groupby': 1,\n",
       "         'Python - Intersection of 2D Numpy Arrays': 1,\n",
       "         'How to create numpy.ndarray from tuple iteration': 1,\n",
       "         'Merge a lot of DataFrames together, without loop and not using concat': 1,\n",
       "         'Matplotlib pcolormesh, separate datacolor and color brightness information': 1,\n",
       "         'Force square subplots when plotting a colorbar': 1,\n",
       "         'Setting colormap limits in pandas df.plot': 1,\n",
       "         'How do I adjust figtext line spacing for when I have special characters in the text?': 1,\n",
       "         'Python/matplotlib : getting rid of matplotlib.mpl warning': 1,\n",
       "         'Count nan in data string with python': 1,\n",
       "         'merge the describe method output in pandas': 1,\n",
       "         'How to resample data in a single dataframe within 3 distinct groups': 1,\n",
       "         'View R data examples via RPy (example: lmeSplines)': 1,\n",
       "         'Graph created inside of a <div> seems to be larger then the <div> itself and overlaps': 1,\n",
       "         'Pandas Dataframe Multicolor Line plot': 1,\n",
       "         'Calculating and plotting count ratios with Pandas': 1,\n",
       "         'Missing values replace by med/mean in conti var, by mode in categorical var in pandas dataframe -after grouping the data by a column)': 1,\n",
       "         'Pandas: Group by year and plot density': 1,\n",
       "         'Conditionally concatenate aggregated columns from different DataFrames into a new DataFrame': 1,\n",
       "         'Using scipy.stats library or another method to generate data follows a distribution in a specific boundary': 1,\n",
       "         'Do matplotlib.contourf levels depend on the amount of colors in the colormap?': 1,\n",
       "         'Convert R Matrix to Pandas Dataframe': 2,\n",
       "         \"R's pdIndent function in RPy\": 1,\n",
       "         'Get matches between two dataframes in python': 1,\n",
       "         'Issues creating a skew normal distribution by subclassing scipy.stats.rv_continuous': 1,\n",
       "         'list of partial matches in very large lists of tuples': 1,\n",
       "         'Groupby given percentiles of the values of the chosen DataFrame column': 1,\n",
       "         'matplotlib pyplot side-by-side graphics': 1,\n",
       "         'How to get non-singleton cluster ids in scipy hierachical clustering': 1,\n",
       "         \"How to use the 'any()' function to search for multiple substrings?\": 1,\n",
       "         'Python: Numpy, the data genfromtxt cannot do tranpose': 1,\n",
       "         'Non-reducing variant of the ANY() function that respects NaN': 2,\n",
       "         'Creating DataFrame from ElasticSearch Results': 1,\n",
       "         'Aggregate over an index in pandas?': 1,\n",
       "         'Pandas - Add column containing metadata about the row': 1,\n",
       "         'Optimizing pandas filter inside apply function': 1,\n",
       "         'How do I compute the agreement between two Pandas data frame columns?': 1,\n",
       "         'Not sure what to change return address to in buffer overflow attack': 1,\n",
       "         'Numpy equivalent of dot(A,B,3)': 1,\n",
       "         'pandas read_csv with final column containing commas': 1,\n",
       "         'Python Pandas: transforming - moving values from diagonal': 1,\n",
       "         'Time-series plotting inconsistencies in Pandas': 1,\n",
       "         'How do we pass two datasets in scipy.stats.anderson_ksamp?Can anyone explain with an example?': 1,\n",
       "         'Using a MultiIndex value in a boolean selection (while setting)': 2,\n",
       "         'How to truncate a numpy/scipy exponential distribution in an efficient way?': 1,\n",
       "         'Matlibplot and scipy Interpolate: Show and evenly disperse Dates': 1,\n",
       "         'Groupby/Transform much better in 14.1 but still way slower than workaround': 2,\n",
       "         'Filtering muliple items in a multi-index Python Panda dataframe': 1,\n",
       "         'numpy.genfromtxt imports tuples instead of arrays': 1,\n",
       "         'pandas read rotated csv files': 1,\n",
       "         'for loop to list comprehension or map in python': 2,\n",
       "         'scipy linregress: computing only scaling/slope parameter with intercept fixed at 0': 1,\n",
       "         'Definite integral over one variable in a function with two variables in Scipy': 1,\n",
       "         'consecutive use of fixed_quad error': 1,\n",
       "         'python pandas trying to reduce reliance on loops': 1,\n",
       "         'Aggregate all dataframe row pair combinations using pandas': 2,\n",
       "         'Python: Subtract a float from a function (in order to call scipy.newton())': 1,\n",
       "         'How do I count the number of objects in a numpy array about a specified threshold': 1,\n",
       "         'Cleanest way to perform pandas join involving an index and also columns': 1,\n",
       "         'Error when trying to convert a column with string in Python Pandas to Float': 2,\n",
       "         'Select rows around a value in Pandas': 1,\n",
       "         'missing_values not working with genfromtxt': 1,\n",
       "         'scale two matrices with scipy or sklearn': 1,\n",
       "         'How do I extract the labels from a MultiIndex?': 1,\n",
       "         'How to flush away the default suptitle of boxplot with subplots made by pandas package for python': 1,\n",
       "         'What is the meaning of numpy reduceat() in python?': 1,\n",
       "         'Python indexing issue (converting MATLAB code)': 1,\n",
       "         'Reusable d3.js Components And Inheritance': 1,\n",
       "         'Using shift() with unevenly spaced data': 2,\n",
       "         'Creating unevenly distributed events with Pandas': 1,\n",
       "         'how to animate matplotlib function optimization?': 1,\n",
       "         'n-dimensional table lookup: array, dataframe, or dictionary?': 2,\n",
       "         'Finding the best matching sequence': 1,\n",
       "         'Slicing a pandas series using a list of float slices': 1,\n",
       "         'How to plot correct dates with matplotlib?': 1,\n",
       "         'Python Pandas, aggregate multiple columns from one': 1,\n",
       "         'Legend format lost after using: ax.legend(handles, labels )': 1,\n",
       "         'calculate percentile of 2D array': 1,\n",
       "         'Pandas copying using iloc not working as expected': 1,\n",
       "         'Python - U.S. ZipCode Matching': 1,\n",
       "         'How to get a sum of the two lists?': 1,\n",
       "         'Apply function with pandas dataframe - POS tagger computation time': 1,\n",
       "         'How can I reason about big O for various functions?': 1,\n",
       "         'CRTP with a Cyclic Dependency': 1,\n",
       "         'Apply a mask to multiple lines (syntactic sugar?)': 1,\n",
       "         'Convert a user function to be a dataframe method (or equivalent)': 2,\n",
       "         'Deleting all content between brackets from a string using python': 1,\n",
       "         'Python: fastest way to write pandas DataFrame to Excel on multiple sheets': 1,\n",
       "         'Efficient way to reshape a pandas dataframe': 1,\n",
       "         'Slice pandas dataframe in groups of consecutive values': 2,\n",
       "         'Pandas - Delete cells based on ranking within column': 1,\n",
       "         'Strip time from an object date in pandas': 2,\n",
       "         'getting direct access to data stored in vector from one class to another': 1,\n",
       "         'Cumulative custom function over grouped data in Python': 1,\n",
       "         'Pandas Correlation Groupby': 1,\n",
       "         'Saving a plot in python creates only an empty pdf document': 1,\n",
       "         'How to create a list in Python with the unique values of a CSV file?': 1,\n",
       "         'How to convert all data in dataframe column to dates [Python/Pandas]': 1,\n",
       "         'Correlation coefficients and p values for all pairs of rows of a matrix': 1,\n",
       "         \"Scipy's curve_fit / leastsq become slower when given the Jacobian?\": 1,\n",
       "         'Divide terms between two list': 1,\n",
       "         'Trouble to impliment scipy interpolation': 1,\n",
       "         'Pandas Dynamic Row-Based Equity Calculation': 1,\n",
       "         'Unit Test with Pandas Dataframe to read *.csv files': 1,\n",
       "         'How to load data from np matrix to seaborn?': 1,\n",
       "         'count lines with same value in column in python': 1,\n",
       "         'How to set matplotlib font for title, axes,': 1,\n",
       "         \"title in italic font in matplotlib isn't working\": 1,\n",
       "         \"Prepend values to Panda's dataframe based on index level of another dataframe\": 1,\n",
       "         'Plotting a line plot with error bars and datapoints from a pandas DataFrame': 1,\n",
       "         'Test if value in a Pandas series is in an array': 1,\n",
       "         'pandas - transform data view': 1,\n",
       "         'Python output readable in Matlab': 1,\n",
       "         'How to get a boolean array from applying criteria to an array in Python?': 1,\n",
       "         'Iterating a file to find string and output information': 1,\n",
       "         'Change size of outlier labels on boxplot in R': 1,\n",
       "         'Values not loading when using pandas read_csv': 1,\n",
       "         'Decrease array size by averaging adjacent values with numpy': 1,\n",
       "         'Strange or inaccurate result with rolling sum (floating point precision)': 1,\n",
       "         'Pandas backfilling values based on a datetime index and a column': 2,\n",
       "         'Non linear classifier against a linearly separable training set': 1,\n",
       "         'Scikit Learn: How can I set the SVM Output range in regression?': 1,\n",
       "         'Formatting datetime variables give missing time values as 00:00:00. Using Python': 1,\n",
       "         'Binary Search Tree insert causing a stack overflow C++': 1,\n",
       "         'Identifying the run lengths of a MultiIndex': 3,\n",
       "         'Pandas: Pivoting and plotting workflow': 1,\n",
       "         'Update marker sizes of a scatter plot': 2,\n",
       "         'Adding a colorbar to a pcolormesh with polar projection': 1,\n",
       "         'Faster alternative to grouby/shift': 2,\n",
       "         'Unexpected result of Pandas.apply() due to having integer as column index': 2,\n",
       "         'Why does the Bartlett test from scipy.stats.bartlett gives nan as output?': 1,\n",
       "         \"Iframe cutting off most of it's content inside of a <div>\": 1,\n",
       "         'pandas groupby with two key': 1,\n",
       "         'How can I round values in Pandas DataFrame containing mixed datatypes for further data comparison?': 1,\n",
       "         'frequency histogram with matplotlib--how to get rid of the array display?': 1,\n",
       "         'Is there a visual profiler for Python?': 1,\n",
       "         'How can you check if a column in a DataFrame is stale?': 1,\n",
       "         'How can I get info on run time errors for Python modules built with f2py': 2,\n",
       "         'pandas dataframe select columns in multiindex': 1,\n",
       "         'Operations within DataFrameGroupBy': 1,\n",
       "         'using pd.DataFrame.apply to create multiple columns': 2,\n",
       "         'ipython-qtconsole: change size of displayed plot': 1,\n",
       "         'pandas - how to combine selected rows in a DataFrame': 1,\n",
       "         'Rolling Correlation with Groupby in Pandas': 1,\n",
       "         'append rows to a Pandas groupby object': 1,\n",
       "         'How do I remove a duplicate dict in list, ignoring a dict key?': 2,\n",
       "         'pandas: join two table and populate value when even there is not a match': 1,\n",
       "         \"pandas' memory usage for list of SparseSeries\": 1,\n",
       "         'how to select inverse of indexes of a numpy array': 1,\n",
       "         'Can a pandas series be a column rather than a row?': 1,\n",
       "         'Pandas indexing by both boolean `loc` and subsequent `iloc`': 2,\n",
       "         'Pivot table with Pandas float and int values': 1,\n",
       "         'pandas - count size and frequency of different groupby levels': 1,\n",
       "         'Python datetime transform': 1,\n",
       "         'pandas shift time series with missing values': 1,\n",
       "         'datetime: subtracting date from itself yields 3288 days': 1,\n",
       "         'Pandas, Computing total sum on each MultiIndex sublevel': 1,\n",
       "         'Make seperate pandas data frames per header': 1,\n",
       "         'Pandas: Divide two rows by each other': 2,\n",
       "         'Getting standard error associated with parameter estimates from scipy.optimize.curve_fit': 1,\n",
       "         'Pandas Dataframe Series To List - Suppress Float Scientific Notation': 1,\n",
       "         'Solving a bounded non-linear minimization with scipy in python': 1,\n",
       "         'How to reindex csv data efficiently?': 1,\n",
       "         'Distribution-type graphs (histogram/kde) with weighted data': 1,\n",
       "         'Could you please explain this piece of code?': 1,\n",
       "         'Traverse every node of a tree to a given depth': 1,\n",
       "         'Pandas: Elsement-wise average and standard deviation across multiple dataframes': 2,\n",
       "         'pandas: extract a subset of columns where the column name contains a string': 2,\n",
       "         'pandas/matplotlib datetime tick labels': 1,\n",
       "         'Pandas DataFrame filtering': 2,\n",
       "         'Pandas Categorical data type not behaving as expected': 2,\n",
       "         'quicksort: how do the choice of a pivot-element, input and the worst-case performance relate?': 1,\n",
       "         'How to assert if a std::mutex is locked?': 1,\n",
       "         'An algorithm that return true if a given string match a given pattern': 1,\n",
       "         'Algorithm to find isomorphic set of permutations': 1,\n",
       "         'Split a dataframe into correspondingly named arrays or series (then recombine)': 2,\n",
       "         'Fastest way to numerically process 2d-array: dataframe vs series vs array vs numba': 2,\n",
       "         'Are C++11 containers supported by Cython?': 1,\n",
       "         'Why is numpy and scipy exp() faster than log()?': 1,\n",
       "         'Minimizing a multivariable function with scipy. Derivative not known': 1,\n",
       "         'Numpy Array python dimension uniform': 1,\n",
       "         'Transpose Pandas Pivot Table': 1,\n",
       "         'C++: Problems inheriting std::basic_streambuf': 1,\n",
       "         'Modifying values of small slice of a column': 1,\n",
       "         \"Can I / Should I use std::exception's for regular error handling?\": 1,\n",
       "         'Why is JSON all uppercase': 1,\n",
       "         'Returned dtype of numpy vectorized function': 1,\n",
       "         'd3 \".call\" function doesn\\'t work for': 1,\n",
       "         'python threading: event vs global variable': 1,\n",
       "         'Dimensional Charting with Non-Exclusive Attributes': 2,\n",
       "         'Solve system of nonlinear equations in scipy, why it says \" mismatch between the input and output shape': 1,\n",
       "         'Remove all-zero rows in a 2D matrix': 1,\n",
       "         'pandas apply with inputs from multiple rows': 1,\n",
       "         'How to glue elements into a list when using groupby function?': 1,\n",
       "         'Modify a multi index pandas data frame in place multiplying by scalar': 1,\n",
       "         'Untouched shared resources in C++ threading': 1,\n",
       "         \"Why isn't std::condition_variable templated by lock type?\": 1,\n",
       "         \"Isn't every language regular, according to formal definition of it?\": 1,\n",
       "         'How to insert ascending numbers in a Red Black Tree': 1,\n",
       "         'How to count no of elements before a given element in std::set': 1,\n",
       "         'Thread-Safe Unordered Map that is being Continuously Iterated Over': 1,\n",
       "         'Numpy nanmean and dataframe (possible bug?)': 1,\n",
       "         'Remove phrases That Contain Duplicate Words from a DF (Pandas, Python3)': 1,\n",
       "         'curve fitting with a known function numpy': 1,\n",
       "         'Proper design setup for derived classes with common attributes but different values': 1,\n",
       "         'Strange results with groupby, transform, and NaNs': 2,\n",
       "         'Secure Shuffle techniques?': 1,\n",
       "         'pandas: concatenate dataframes, forward-fill and multiindex on column data': 1,\n",
       "         '[c++]Why is my class destructor called twice?': 1,\n",
       "         'Combining pandas data frames with overlapping columns / rows': 1,\n",
       "         'Best practices for measuring the run-time complexity of a piece of code': 1,\n",
       "         'Parallel processing a large number of tasks': 1,\n",
       "         'What is the difference between pandas.qcut and pandas.cut?': 1,\n",
       "         'Python pandas how to optimizes comparing rows of a dataframe?': 1,\n",
       "         'plotting a histogram of a numpy array by timestamp': 1,\n",
       "         'Calculate mannwhitneyu over specified numpy axis': 1,\n",
       "         'Read multiple *.txt files into Pandas Dataframe with filename as column header': 1,\n",
       "         'Pandas: how to merge two dataframes on offset dates?': 1,\n",
       "         'pandas: how do I select first row in each GROUP BY group?': 2,\n",
       "         'One-liner for partial string match in list lookup': 1,\n",
       "         'Refactoring Repeated if Statements': 1,\n",
       "         'Why downcast and then assign to base-class in C++?': 1,\n",
       "         'Slow Stochastic Implementation in Python Pandas': 1,\n",
       "         'Shared state in multiprocessing Processes': 1,\n",
       "         'How list of prefix works in prolog': 1,\n",
       "         'How do I append 3 lists efficiently in Prolog?': 1,\n",
       "         'Detecting incorrect nesting of XML tags in Python': 1,\n",
       "         'How to search for a directory or file in Sublime Text': 1,\n",
       "         'genfromtxt: how to disable caching': 1,\n",
       "         'groupby of a groupby to select values in pandas': 1,\n",
       "         \"Additionally restricting scipy's root solver\": 1,\n",
       "         'Filtering pandas data frame with tabulated function': 1,\n",
       "         'What is correct syntax to swap column values for selected rows in a pandas data frame using just one line?': 3,\n",
       "         'Fill data gaps with average of data from adjacent days': 1,\n",
       "         'Pandas DF Pivot/Transform/Vectorize Operation': 2,\n",
       "         'Python - Unpivot data': 2,\n",
       "         \"datetime index KeyError: 'the label [2000-01-03 00:00:00] is not in the [index]'\": 1,\n",
       "         'how can I search an element in Skip-List': 1,\n",
       "         'Using pickle to serialize a class object': 1,\n",
       "         'Are memory leaks possible with decltype(new any_type())?': 1,\n",
       "         \"Fisher's Exact in scipy as new column using pandas\": 1,\n",
       "         'python: plot unevenly distributed axis': 1,\n",
       "         'groupby on sparse matrix with scipy': 1,\n",
       "         'How to speed up pandas with cython (or numpy)': 2,\n",
       "         'OpenMP parallelization': 1,\n",
       "         'Python: Read several json files from a folder': 1,\n",
       "         'Two threads accessing same variables, but one thread rarely happens. Non-atomics?': 1,\n",
       "         'Function redeclaration on C++': 1,\n",
       "         'Set S of n numbers - have a subset with the probability of each element of S occuring in it equal': 1,\n",
       "         'Heapsort - building heap': 1,\n",
       "         'Repeating a dataframe value N times based on another column': 1,\n",
       "         'How to plot a multivariate function in Python?': 1,\n",
       "         'Looking for a nicer python syntax to pass multiple parameters to a single-parameterized lambda': 1,\n",
       "         'Merge multiple trees with a depth of one': 1,\n",
       "         'How do I extend a pandas DataFrame by repeating the last row?': 2,\n",
       "         'matplotlib fill between discrete points': 1,\n",
       "         'ValueError loading data for scipy.odr regression': 1,\n",
       "         'pandaslooping through grouped data for a plot': 1,\n",
       "         \"Pandas plot subplots of a 'group by' result\": 1,\n",
       "         'Create a default constructor when using const strings': 1,\n",
       "         'Python numpy/f2py linking libraries': 1,\n",
       "         'Pandas TimeGrouper: Drop \"non full groups\"': 1,\n",
       "         'Access protected function from derived class': 1,\n",
       "         'Data structure in Python for NFA (regex)?': 1,\n",
       "         'Comparison between one element and all the others of a DataFrame column': 2,\n",
       "         'How to test if all rows are distinct in numpy': 1,\n",
       "         'python pandas groupby optimisation': 1,\n",
       "         \"HighCharts Graphs don't load half the time\": 1,\n",
       "         \"Designing Data structures for Prim's implementation in c++\": 1,\n",
       "         \"Is there a C++ equivalent to python's functools.partial\": 1,\n",
       "         'Python parse mathematical text expression': 1,\n",
       "         'What are potential hashing algorithms for strings in pure Python?': 1,\n",
       "         'Efficiently applying a function to a grouped pandas DataFrame in parallel': 1,\n",
       "         'Proper way to do row correlations in a pandas dataframe': 2,\n",
       "         'Converting a string to vector in C++': 1,\n",
       "         'Build a ranking from a series of transitive relationships that can be noisy, inconsistent, or incomplete': 1,\n",
       "         'Prolog lists transforming': 1,\n",
       "         'python object that takes less memory and time and provides delivers the element when index is provided': 1,\n",
       "         'Make matrix computation faster in c': 1,\n",
       "         'Why does Entropy function not working in MATLAB?': 1,\n",
       "         'Nested if loop with DataFrame is very,very slow': 1,\n",
       "         'Python - iterating beginning with the middle of the list and then checking either side': 1,\n",
       "         'checking if key exist in map then updating value': 1,\n",
       "         'Passing unique dictionary addresses to class instances': 1,\n",
       "         'Which vector and map, uses less memory (large set of data and unknown size)': 1,\n",
       "         'Peculiar behaviour while erasing an element from from std::multimap': 1,\n",
       "         'Sum two series the STD way': 1,\n",
       "         'Excel like formulas with pandas': 1,\n",
       "         'Padding a Pandas Dataframe with Entries Based on Date': 1,\n",
       "         'Add multiple columns to a Pandas dataframe quickly': 1,\n",
       "         'Select multiple columns by labels (pandas)': 1,\n",
       "         'How to parse token beginning with \".\"': 1,\n",
       "         'Numpy Histogram | Use one dimension to match bin, and another for the actual frequency': 1,\n",
       "         'std::reverse on MFC CArray': 1,\n",
       "         'Python script to run a visual studio C++ program multiple time with different arguments': 1,\n",
       "         'Find duplicate element in a list of lists': 1,\n",
       "         'Python: parallelizing operations on a big array iteratively': 1,\n",
       "         'What are some common uses of immutable data structures?': 1,\n",
       "         'Write a user defined fillna function in pandas dataframe to fill np.nan different values with conditions': 1,\n",
       "         'Classifying Data in a New Column': 2,\n",
       "         'Using condition 3 of the pumping lemma to prove irregularity': 1,\n",
       "         'Wish for Better c++ Stack Implementation using Vectors': 1,\n",
       "         'Pandas: Backfilling a DataFrameGroupBy object': 1,\n",
       "         'Updating rows of the same index': 2,\n",
       "         'how to extract a required list of strings from a different string, using lambdas in python': 1,\n",
       "         'Pandas - FillNa with another column': 2,\n",
       "         'How to get Employee --> Manager without separate tables (pandas or SQL)?': 1,\n",
       "         'Sequential Random Numbers C++': 1,\n",
       "         'Pandas Efficient VWAP Calculation': 1,\n",
       "         'pandas, chained indexing, spaces in csv, and speed': 1,\n",
       "         'MFC How to notify the document is changed (and display the *)': 1,\n",
       "         'C++ - Get equality from class Compare': 1,\n",
       "         'Making IPython Tab-Complete Hierarchical pandas DataFrames': 2,\n",
       "         'Clean list or list of lists of all None or Empty lists in Python': 1,\n",
       "         'Increasing performance of nearest neighbors of rows in Pandas': 1,\n",
       "         'State Machine in Python: Run Code in Transition or State?': 1,\n",
       "         'scikit-learn: custom distance function with NN does a problematic initial computation': 1,\n",
       "         'Python: Efficiently calling subset variables of multiple returns function': 1,\n",
       "         'Python Pandas: rolling_kurt vs. scipy.stats.kurtosis': 1,\n",
       "         'Pandas date ranges and averaging the counts': 1,\n",
       "         'Reducing count value to repeat a loop cycle is not working. The for loop in python has an exception handler that has a continue statement': 1,\n",
       "         'Pandas dataframe first x columns': 1,\n",
       "         'Why are unsigned integers error prone?': 1,\n",
       "         'which string matching algorithm is used in stl?': 1,\n",
       "         'efficiently calculating double integral': 1,\n",
       "         'Linear Regression from Time Series Pandas': 1,\n",
       "         'Divide one column in array by another numpy': 1,\n",
       "         'Global Seed for Multiple Numpy Imports': 1,\n",
       "         'Pseudocode or C# algorithm that returns all possible combinations sets for a number of variables': 1,\n",
       "         'Interpolate (or extrapolate) only small gaps in pandas dataframe': 1,\n",
       "         'Avoiding pandas chained selection': 1,\n",
       "         'Testing subsequent values in a DataFrame': 2,\n",
       "         'Pandas: Weighted median of grouped observations': 2,\n",
       "         'Pandas: Get grouping-index in apply function': 1,\n",
       "         'Pandas: Printing the Names and Values in a Series': 3,\n",
       "         'Matplotlib specific axis plotting': 1,\n",
       "         'template class method in source file': 1,\n",
       "         'randomly subsampling lines in a file': 1,\n",
       "         'Making this C array processing code more python (and even numpy)': 2,\n",
       "         'Loss of strings when creates a Numpy Array from a Pandas Dataframe': 1,\n",
       "         'Recompose a table from an SQL with row and colum ID in Python': 1,\n",
       "         'Pandas Difference in DataFrames': 1,\n",
       "         'Fastest way to compare row and previous row in pandas dataframe with millions of rows': 2,\n",
       "         'C++ inheritance error': 1,\n",
       "         'Generator in Python only returning first element': 1,\n",
       "         'Algorithm to check if any of a set of strings matches the input text?': 1,\n",
       "         'Fraction class for finding square root 2 convegerence': 1,\n",
       "         'single list comprehension to unpack nested dictionary': 1,\n",
       "         'template return when Element is a struct': 1,\n",
       "         'Automatic input from text file in excel': 1,\n",
       "         'C++ polymorphic class pointer in vector': 1,\n",
       "         'Invalid operator type of std::istream?': 1,\n",
       "         'Random generation of tuple (A, B) so that A + B <= C?': 1,\n",
       "         'C++ extincting features': 1,\n",
       "         'Nested pointer instead of inheritance': 1,\n",
       "         'how to omit the less frequent words from a dictionary in python?': 1,\n",
       "         'Calculating the difference in dates in a Pandas GroupBy object': 1,\n",
       "         'Enforcing that inputs sum to 1 and are contained in the unit interval in scikit-learn': 1,\n",
       "         'Combining every two tuples within a list in Python': 1,\n",
       "         'How can I determine the length, in seconds, of a Vorbis stream?': 1,\n",
       "         'How to lock a data structure modified by 2 threads which belong to 2 different classes': 1,\n",
       "         'Pandas correlation': 1,\n",
       "         'Is there a reason to prefer multiple unordered_map of one variable over one of a struct?': 1,\n",
       "         'Exponential growth doubling processor speed': 1,\n",
       "         'iterator over multiple std containers subsequently': 1,\n",
       "         'Creating a partial SAS PROC SUMMARY replacement in Python/Pandas': 1,\n",
       "         'Complexity - determining the order of growth': 1,\n",
       "         'pandas multiindex dataframe, ND interpolation for missing values': 1,\n",
       "         'Relabeling one level of a Columns MultiIndex': 1,\n",
       "         'Numpy genfromtxt read cell with multiple values': 1,\n",
       "         'unary operator in std::transform': 1,\n",
       "         \"Can't use scipy stats function on nested list\": 1,\n",
       "         'Percentage diff b/t two strings of different lengths': 1,\n",
       "         'Python - Pool.map_async only runs every other element in iterable unless chunksize=1': 1,\n",
       "         'Insert function in a Binary Search Tree': 1,\n",
       "         'Use of self when creating objects in Python': 1,\n",
       "         'Is it possible to specify the order of levels in Pandas factorize method?': 2,\n",
       "         'Should I use different mutexes for different objects?': 1,\n",
       "         'how to load a long panel dataset in Pandas?': 1,\n",
       "         'IndexError: index 1 is out of bounds for axis 1 with size 1': 1,\n",
       "         'Giving unique IDs to all nodes?': 1,\n",
       "         'Cancelling boost thread from another': 1,\n",
       "         'Is the trunc function very slow?': 1,\n",
       "         'Why does returning in Interactive Python print to sys.stdout?': 1,\n",
       "         'Pandas isin() function for continuous intervals': 2,\n",
       "         'Python Pandas overlapping data with TimeGrouper': 1,\n",
       "         'python pandas conditional count across columns': 1,\n",
       "         'multithreading check membership in Queue and stop the threads': 1,\n",
       "         'Number of route in a list of lists': 1,\n",
       "         'Performing arithmetic on partially known columns names': 1,\n",
       "         'Template C++: How to access iterator value for both std::map and std::set?': 1,\n",
       "         'Is there a concise way to show all rows in pandas for just the current command?': 1,\n",
       "         'How to properly instantiate a MultiSet (created on my own) using Python': 1,\n",
       "         'Heuristic to find the maximum weight independent set in an arbritary graph': 1,\n",
       "         'Convert hhmmss to time using python pandas': 1,\n",
       "         'Write Pandas DataFrame to file using FORTRAN format string': 1,\n",
       "         'Iterate over a list of images and assign them as variables in python': 1,\n",
       "         'Does __ne__ use an overridden __eq__?': 1,\n",
       "         'c++ iterate through all neighbor permutations': 1,\n",
       "         'Polygons from line segments': 1,\n",
       "         'How to append data to pandas multi-index dataframe': 1,\n",
       "         'KeyError when using melt to restructure Dataframe': 1,\n",
       "         'Plotting multiple time series after a groupby in pandas': 1,\n",
       "         'Remove smallest non-unique value from vector': 2,\n",
       "         'what is the inverse function of Sinc': 1,\n",
       "         'How do I generate a 2D Gaussian with a mean 50 and standard deviation of 5': 1,\n",
       "         'distance between two points in prolog': 1,\n",
       "         'Select random row based on intervals': 1,\n",
       "         'Availability of static_assert c++11': 1,\n",
       "         'Hashtable for a system with memory constraints': 1,\n",
       "         'Pandas dataframe insert rows': 1,\n",
       "         'Python dictionary as html table in ipython notebook': 1,\n",
       "         'List with many dictionaries VS dictionary with few lists?': 1,\n",
       "         \"Avoiding Excel's Scientific Notation Rounding when Parsing with Pandas\": 1,\n",
       "         'Average of a groupby or a groupby of an average on a dataframe with duplicated indexes': 1,\n",
       "         'Graphing with matplotlib: hours shown as float': 1,\n",
       "         'Error using bootstrap_plot in pandas if values have NaN': 1,\n",
       "         'delegation in python oop': 1,\n",
       "         'efficient way to get index in sorted vector in c++': 1,\n",
       "         'STL pair comparion - first elements': 1,\n",
       "         'Can Bellman-Ford algorithm be used to find shorthest path on a graph with only positive edges?': 1,\n",
       "         'Using Cross-Validation on a Scikit-Learn Classifer': 1,\n",
       "         'Pandas: Difference between pivot and pivot_table. Why is only pivot_table working?': 1,\n",
       "         'Pandas: Group-by and Aggregate Column 1 with Condition from Column 2': 1,\n",
       "         'How to count distance to the previous zero in pandas series?': 3,\n",
       "         'Derive a class name from another class name in template': 1,\n",
       "         'Smallest set of multi-sets that together contains all numbers from 1 to N': 1,\n",
       "         ...})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([row[pandas_header[\"Title\"]] for row in pandas_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aeb133b",
   "metadata": {},
   "source": [
    "### Step 0) Clean and standardize the dataset\n",
    "\n",
    "We want to keep only posts where views are >= 1000 and in the meantime convert all the values to their respective type (float or datetime) and add the \"Not Accepted\" labels \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ef40160a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17968"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def legal_pandas_row(row):\n",
    "    if row[pandas_header[\"Views\"]] == \"\" or float(row[pandas_header[\"Views\"]]) < 1000:\n",
    "        return False\n",
    "        \n",
    "    if row[pandas_header[\"Score\"]] == \"\":\n",
    "        return False\n",
    "    \n",
    "    if row[pandas_header[\"Title\"]] == \"\":\n",
    "        return False\n",
    "    \n",
    "    if row[pandas_header[\"Created\"]] == \"\":\n",
    "        return Falselen()\n",
    "    \n",
    "    return True\n",
    "\n",
    "dt_format = '%Y-%m-%d %H:%M:%S'\n",
    "def standardize_pandas_row(row):\n",
    "    row[pandas_header[\"Accepted\"]] = \"Accepted\" if row[pandas_header[\"Accepted\"]] == \"Accepted\" else \"Not Accepted\"\n",
    "    row[pandas_header[\"Created\"]] = dt.strptime(row[pandas_header[\"Created\"]], dt_format).date()\n",
    "    return row\n",
    "\n",
    "cleaned_pandas_rows = [standardize_pandas_row(row) for row in pandas_rows if legal_pandas_row(row)]\n",
    "len(cleaned_pandas_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "09dfa8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pandas_avail_dates = [row[pandas_header[\"Created\"]] for row in cleaned_pandas_rows]\n",
    "pandas_min_date = min(pandas_avail_dates)\n",
    "pandas_max_date = max(pandas_avail_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7fe395de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: table;\"><div style=\"display: table-row;\"><div style=\"display: table-cell;\"><b title=\"bokeh.models.ranges.Range1d\">Range1d</b>(</div><div style=\"display: table-cell;\">id&nbsp;=&nbsp;'1131', <span id=\"1133\" style=\"cursor: pointer;\">&hellip;)</span></div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">bounds&nbsp;=&nbsp;None,</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">end&nbsp;=&nbsp;2531.0,</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_event_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_property_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_interval&nbsp;=&nbsp;None,</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_interval&nbsp;=&nbsp;None,</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">name&nbsp;=&nbsp;None,</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">reset_end&nbsp;=&nbsp;None,</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">reset_start&nbsp;=&nbsp;None,</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">start&nbsp;=&nbsp;-7.0,</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">subscribed_events&nbsp;=&nbsp;[],</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">syncable&nbsp;=&nbsp;True,</div></div><div class=\"1132\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">tags&nbsp;=&nbsp;[])</div></div></div>\n",
       "<script>\n",
       "(function() {\n",
       "  let expanded = false;\n",
       "  const ellipsis = document.getElementById(\"1133\");\n",
       "  ellipsis.addEventListener(\"click\", function() {\n",
       "    const rows = document.getElementsByClassName(\"1132\");\n",
       "    for (let i = 0; i < rows.length; i++) {\n",
       "      const el = rows[i];\n",
       "      el.style.display = expanded ? \"none\" : \"table-row\";\n",
       "    }\n",
       "    ellipsis.innerHTML = expanded ? \"&hellip;)\" : \"&lsaquo;&lsaquo;&lsaquo;\";\n",
       "    expanded = !expanded;\n",
       "  });\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "Range1d(id='1131', ...)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_score = [float(row[pandas_header[\"Score\"]]) for row in cleaned_pandas_rows]\n",
    "stats.describe(default_score)\n",
    "Range1d(min(default_score), max(default_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "26802550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"display: table;\"><div style=\"display: table-row;\"><div style=\"display: table-cell;\"><b title=\"bokeh.models.ranges.Range1d\">Range1d</b>(</div><div style=\"display: table-cell;\">id&nbsp;=&nbsp;'1134', <span id=\"1136\" style=\"cursor: pointer;\">&hellip;)</span></div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">bounds&nbsp;=&nbsp;None,</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">end&nbsp;=&nbsp;2105027.0,</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_event_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">js_property_callbacks&nbsp;=&nbsp;{},</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">max_interval&nbsp;=&nbsp;None,</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">min_interval&nbsp;=&nbsp;None,</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">name&nbsp;=&nbsp;None,</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">reset_end&nbsp;=&nbsp;None,</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">reset_start&nbsp;=&nbsp;None,</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">start&nbsp;=&nbsp;1000.0,</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">subscribed_events&nbsp;=&nbsp;[],</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">syncable&nbsp;=&nbsp;True,</div></div><div class=\"1135\" style=\"display: none;\"><div style=\"display: table-cell;\"></div><div style=\"display: table-cell;\">tags&nbsp;=&nbsp;[])</div></div></div>\n",
       "<script>\n",
       "(function() {\n",
       "  let expanded = false;\n",
       "  const ellipsis = document.getElementById(\"1136\");\n",
       "  ellipsis.addEventListener(\"click\", function() {\n",
       "    const rows = document.getElementsByClassName(\"1135\");\n",
       "    for (let i = 0; i < rows.length; i++) {\n",
       "      const el = rows[i];\n",
       "      el.style.display = expanded ? \"none\" : \"table-row\";\n",
       "    }\n",
       "    ellipsis.innerHTML = expanded ? \"&hellip;)\" : \"&lsaquo;&lsaquo;&lsaquo;\";\n",
       "    expanded = !expanded;\n",
       "  });\n",
       "})();\n",
       "</script>\n"
      ],
      "text/plain": [
       "Range1d(id='1134', ...)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_views = [float(row[pandas_header[\"Views\"]]) for row in cleaned_pandas_rows]\n",
    "stats.describe(default_views)\n",
    "Range1d(min(default_views), max(default_views))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bb1e72e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cleaned_pandas_rows_by_filter(min_date, max_date):\n",
    "    def valid_pandas_filtered_rows(row):\n",
    "        if row[pandas_header[\"Created\"]] < min_date or row[pandas_header[\"Created\"]] > max_date:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    return [row for row in cleaned_pandas_rows if valid_pandas_filtered_rows(row)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "359774f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['0',\n",
       "  '10348407',\n",
       "  'Answer',\n",
       "  'Use wxpython DrawText function to add text in a Bitmap',\n",
       "  \"When I run your code, I got an error as there is not Font defined, I don't know if you experience the same issue, but by adding any `SetFont` statement like `gc.SetFont(wx.Font(22, wx.SWISS, wx.NORMAL, wx.BOLD))` before `DrawText` your code is working fine.\\n\\n\\n\\nBy the way, I put `300, 300` as coordinates to draw your 'Hello' text as in `100, 100` it is a little black on black and unreadable.\\n\\n\",\n",
       "  '<python><bitmap><wxpython><draw><drawtext>',\n",
       "  datetime.date(2012, 4, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1312.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3',\n",
       "  '10312647',\n",
       "  'Answer',\n",
       "  'Concatenate or print list elements with a trailing comma in Python',\n",
       "  \"If you are in Python 3, you could leverage the print built-in function:\\n\\n\\n\\n    print(*l, sep=', ', end=',')\\n\\n\\n\\n - `*l` unpacks the list of elements to pass them as individual arguments to print\\n\\n - `sep` is an optional argument that is set to in between elements printed from the elements, here I set it to `', '` with a space as you require\\n\\n - `end` is an optional argument that will be pushed at the and of the resulting printed string. I set it to `','` without space to match your need\\n\\n\\n\\nYou can use it starting Python 2.6 by importing the print function\\n\\n\\n\\n    from __future__ import print_function\\n\\n\\n\\nHowever going this way has several caveats:\\n\\n\\n\\n - This is assuming you want to output the resulting string in stdout ; or you can redirect the output in a file with the `file` optional argument into a file\\n\\n - if you are in Python 2, the `__future__` import can break you code compatibility so you would need to isolate your code in a separate module if the rest of your code is not compatible.\\n\\n\\n\\nLong story short, either this method or the other proposed answers are a lot of efforts to try to avoid just adding a `+','` at the end of the `join` resulting string\\n\\n\",\n",
       "  '<python><string><list><join>',\n",
       "  datetime.date(2012, 4, 25),\n",
       "  '2012-04-25 11:44:45',\n",
       "  'Boud (624829)',\n",
       "  '6',\n",
       "  '',\n",
       "  '18242.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['7',\n",
       "  '10290054',\n",
       "  'Answer',\n",
       "  'Firefox WebDriver (Selenium) And Security Warning Popup',\n",
       "  \"I faced this issue without any chance to bypass it: It is a builtin Firefox security feature to display this warning message, and it cannot be bypassed through a configuration flag - that's why you keep getting the message manually or via WebDriver that won't provide a bypass mechanism by clicking on the alert box.\\n\\n\\n\\nIf you want to get rid of this bugging message, consider to switch you WebDriver automated browser to Google Chrome if Firefox is not the required browser for your implementation. Chrome will not bug you with a blocking message.\\n\\n\\n\\n\",\n",
       "  '<c#><firefox><selenium><webdriver>',\n",
       "  datetime.date(2012, 4, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '3689.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['8',\n",
       "  '10380452',\n",
       "  'Answer',\n",
       "  'Elliptic curve and point cardinalities',\n",
       "  'This is related to this [topic][1].\\n\\n\\n\\n[Sage][2] is a powerful math package with a Python front-end. With Sage you will be able to use [PARI/GP][3] package\\n\\n\\n\\nYou have also some information in their [FAQ][4] about elliptic curves\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/a/408645/624829\\n\\n  [2]: http://www.sagemath.org/\\n\\n  [3]: http://pari.math.u-bordeaux.fr/doc.html\\n\\n  [4]: http://www.math.u-bordeaux.fr/~belabas/pari/doc/faq.html#ell',\n",
       "  '<python><cryptography><elliptic-curve>',\n",
       "  datetime.date(2012, 4, 30),\n",
       "  '2017-05-23 12:05:40',\n",
       "  'James K Polk (238704), URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1051.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['9',\n",
       "  '10385158',\n",
       "  'Answer',\n",
       "  'Slow image processing with Python and PIL',\n",
       "  'Indentation in Python is key, otherwise you can run code you wouldn\\'t for each iteration.\\n\\n\\n\\nHere you are saving your file each time you loop as your image generation is indented. Instead, try:\\n\\n\\n\\n    from PIL import Image\\n\\n\\n\\n    source = Image.open(\"source.jpg\")\\n\\n    colors = source.getcolors(source.size[0] * source.size[1]) #maxcolor value =256, get   all colors with source.size[0] * source.size[1]\\n\\n    pixels = []\\n\\n    for i, color in colors:\\n\\n        pixels.extend(i * [color])\\n\\n    pixels.sort()\\n\\n    new = Image.new(\\'RGB\\', source.size)\\n\\n    new.putdata(pixels) \\n\\n    new.save(\"new.png\")',\n",
       "  '<python><image-processing><python-imaging-library>',\n",
       "  datetime.date(2012, 4, 30),\n",
       "  '2012-04-30 15:18:57',\n",
       "  'Boud (624829)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1890.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['10',\n",
       "  '10389288',\n",
       "  'Answer',\n",
       "  'Is there a handle leak detector which can be linked into an existing application?',\n",
       "  'Mark Russinovich explains in his series to *Pushing the Limits of Windows* how to better deal with [handles][1] and how to track handle leaks.\\n\\n\\n\\nHe is mentioning Windows Debugger and Application Verifier, and he is explaining how you can use that to track your handle leaks down.\\n\\n\\n\\nIn the same page, he mentions also a neat feature of his well-known Process Explorer that is flashing green and red for processes creating / closing handles.\\n\\n\\n\\n\\n\\n  [1]: http://blogs.technet.com/b/markrussinovich/archive/2009/09/29/3283844.aspx',\n",
       "  '<c++><winapi><memory-leaks>',\n",
       "  datetime.date(2012, 4, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '3258.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['11',\n",
       "  '10482083',\n",
       "  'Answer',\n",
       "  'Writing unicode data in csv',\n",
       "  \"You are writing a file in UTF-8 format, but you don't indicate that into your csv file.\\n\\n\\n\\nYou should write the UTF-8 header at the beginning of the file. Add this:\\n\\n\\n\\n    ff = open('a.csv', 'w')\\n\\n    ff.write(codecs.BOM_UTF8)\\n\\n\\n\\nAnd your csv file should open correctly after that with the program trying to read it.\",\n",
       "  '<python><csv><python-2.7>',\n",
       "  datetime.date(2012, 5, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '8178.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['12',\n",
       "  '10330180',\n",
       "  'Answer',\n",
       "  'sum of small double numbers c++',\n",
       "  'Consider to apply [Kahan summation algorithm][1] for both your entire set or each of your subsets.\\n\\n\\n\\nThere are other [questions][2] referencing this algorithm that can help you\\n\\n\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Kahan_summation_algorithm\\n\\n  [2]: https://stackoverflow.com/questions/4940072/kahan-summation',\n",
       "  '<c++><numbers><double><sum>',\n",
       "  datetime.date(2012, 4, 26),\n",
       "  '2017-05-23 12:02:23',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '7590.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['13',\n",
       "  '10390158',\n",
       "  'Answer',\n",
       "  'What are the reasons for not hosting a compiler on a live server?',\n",
       "  \"I would suggest to refer to this [serverfault post][1].\\n\\n\\n\\nIt makes sense to avoid exploits being compiled remotely\\n\\n\\n\\nIt makes sense also to me that in terms of security, it will only make the task harder for a hijacker without than with a compiler, but it's not perfect.\\n\\n\\n\\n  [1]: https://serverfault.com/questions/143364/remote-server-security-handling-compiler-tools\",\n",
       "  '<python><deployment><compiler-construction><pip><binaries>',\n",
       "  datetime.date(2012, 4, 30),\n",
       "  '2017-04-13 12:13:35',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1068.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['14',\n",
       "  '10566887',\n",
       "  'Answer',\n",
       "  'Match first instance of digit by Regex in python',\n",
       "  \"You can write this regular expression:\\n\\n\\n\\n    re.match('Chr(\\\\d+)_.*','Chr6_clust92082')\\n\\n\\n\\n\",\n",
       "  '<python><regex>',\n",
       "  datetime.date(2012, 5, 12),\n",
       "  '2012-05-12 19:59:09',\n",
       "  'Boud (624829)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1080.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['15',\n",
       "  '14035158',\n",
       "  'Answer',\n",
       "  'defining a list of functions in Python',\n",
       "  \"You loop to avoid writing something like this:\\n\\n\\n\\n    lis.append( lambda: 0 )\\n\\n    lis.append( lambda: 1 )\\n\\n    lis.append( lambda: 2 )\\n\\n\\n\\nYour intention is to write lambda functions that return constant integers. But you are defining and appending a function that is returning object `i`. Thus the 3 appended functions are the same.\\n\\n\\n\\nThe byte code behind the functions created returns `i`:\\n\\n\\n\\n    In [22]: import dis\\n\\n    In [25]: dis.dis(lis[0])\\n\\n      3           0 LOAD_GLOBAL              0 (i)\\n\\n                  3 RETURN_VALUE        \\n\\n    \\n\\n    In [26]: dis.dis(lis[1])\\n\\n      3           0 LOAD_GLOBAL              0 (i)\\n\\n                  3 RETURN_VALUE        \\n\\n    \\n\\n    In [27]: dis.dis(lis[2])\\n\\n      3           0 LOAD_GLOBAL              0 (i)\\n\\n                  3 RETURN_VALUE   \\n\\n\\n\\n\\n\\nCalling any of those functions returns the latest value of `i`that is `2` in your sample code:\\n\\n\\n\\n    In [28]: lis[0]()\\n\\n    Out[28]: 2\\n\\n\\n\\nif you delete `i` object, you get an error:\\n\\n\\n\\n    In [29]: del i\\n\\n    \\n\\n    In [30]: lis[0]()\\n\\n    ---------------------------------------------------------------------------\\n\\n    NameError                                 Traceback (most recent call last)\\n\\n    <ipython-input-30-c9e334d64652> in <module>()\\n\\n    ----> 1 lis[0]()\\n\\n    \\n\\n    <ipython-input-18-15df6d11323a> in <lambda>()\\n\\n          1 lis = []\\n\\n          2 for i in range(3):\\n\\n    ----> 3     lis.append( lambda: i )\\n\\n    \\n\\n    NameError: global name 'i' is not defined\\n\\n\\n\\n\\n\\n----------\\n\\nA solution could be to keep using the loop to write the code with the constants you need and actually run that code:\\n\\n\\n\\n    In [31]: lis = []\\n\\n        ...: for i in range(3):\\n\\n        ...:     exec 'lis.append( lambda: {} )'.format(i)\\n\\n        ...:  \\n\\nWith the following results:\\n\\n\\n\\n    In [44]: lis[0]()\\n\\n    Out[44]: 0\\n\\n    \\n\\n    In [45]: lis[1]()\\n\\n    Out[45]: 1\\n\\n    \\n\\n    In [46]: dis.dis(lis[0])\\n\\n      1           0 LOAD_CONST               1 (0)\\n\\n                  3 RETURN_VALUE        \\n\\n    \\n\\n    In [47]: dis.dis(lis[1])\\n\\n      1           0 LOAD_CONST               1 (1)\\n\\n                  3 RETURN_VALUE        \\n\\n    \\n\\n      \",\n",
       "  '<python><loops><lambda>',\n",
       "  datetime.date(2012, 12, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1138.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['16',\n",
       "  '10218849',\n",
       "  'Answer',\n",
       "  'Selenium+python Reporting',\n",
       "  'To start building test reports on top of Selenium+Python, I would leverage the python unittest module. You will get a basic sample in Selenium documentation [here][1].\\n\\n\\n\\nThen [HTMLTestRunner][2] module combined with unittest provides basic but robust HTML reports.\\n\\n\\n\\n\\n\\n  [1]: http://selenium-python.readthedocs.io/getting-started.html#using-selenium-to-write-tests\\n\\n  [2]: http://tungwaiyip.info/software/HTMLTestRunner.html',\n",
       "  '<python><selenium>',\n",
       "  datetime.date(2012, 4, 18),\n",
       "  '2016-06-04 03:27:43',\n",
       "  'Boud (624829)',\n",
       "  '7',\n",
       "  '',\n",
       "  '10560.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['17',\n",
       "  '10358540',\n",
       "  'Answer',\n",
       "  'Python faster than compiled Haskell?',\n",
       "  \"To follow up @kindall interesting answer, those timings are dependent from both the python / Haskell implementation you use, the hardware configuration on which you run the tests, and the algorithm implementation you right in both languages. \\n\\n\\n\\nNevertheless we can try to get some good hints of the relative performances of one language implementation compared to another, or from one language to another language. With well known alogrithms like qsort, it's a good beginning.\\n\\n\\n\\nTo illustrate a python/python comparison, I just tested your script on CPython 2.7.3 and PyPy 1.8 on the same machine:\\n\\n\\n\\n - CPython: ~8s\\n\\n - PyPy: ~2.5s\\n\\n\\n\\nThis shows there can be room for improvements in the language implementation, maybe compiled Haskell is not performing at best the interpretation and compilation of your corresponding code. If you are searching for speed in Python, consider also to switch to pypy if needed and if your covering code permits you to do so.\",\n",
       "  '<python><haskell><quicksort>',\n",
       "  datetime.date(2012, 4, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '11309.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['19',\n",
       "  '10590315',\n",
       "  'Answer',\n",
       "  'Dictionary searching with datetime keys',\n",
       "  'Create an index based on `bisect` module seems to be a valuable idea to dig into. However, by looking at the documentation, you will see that bisect functions take a sorted list as a first argument and not in second argument.\\n\\n\\n\\nTry:\\n\\n\\n\\n    keys=sorted(data.keys())\\n\\n    bisect.bisect_left(keys,time), bisect.bisect_right(keys,time)\\n\\n\\n\\nAlso, you can try to optimize your code by constructing the `keys` object outside of your `findTime` function. If you `data` dictionary is not modified through your sequence of `findTime` calls, you will pay the construction of the sorted list only once.',\n",
       "  '<python><dictionary><indexing><python-2.7>',\n",
       "  datetime.date(2012, 5, 14),\n",
       "  '2012-05-14 20:28:36',\n",
       "  'Boud (624829)',\n",
       "  '1',\n",
       "  '',\n",
       "  '8657.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['22',\n",
       "  '10381054',\n",
       "  'Answer',\n",
       "  \"xl.Workbook() (pyvot) doesn't open an excel workbook\",\n",
       "  'From [Pyvot][1] homepage:\\n\\n\\n\\n> Pyvot requires CPython 2.6 or 2.7 with the Python for Windows extensions (pywin32) installed, and Office 2010. If a clean Python session can import the win32com module, Pyvot is ready to be installed.\\n\\n\\n\\nSo from what it is described, no need of PTVS, Excel 2010 is required, and you need `win32com` module up and running in your current interpreter. I use this configuration without any issue.\\n\\n\\n\\n  [1]: http://pytools.codeplex.com/wikipage?title=Pyvot',\n",
       "  '<python><windows><excel>',\n",
       "  datetime.date(2012, 4, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1311.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['25',\n",
       "  '16773069',\n",
       "  'Answer',\n",
       "  'How can I parse sql file from within Python?',\n",
       "  'You can try out the [sqlparse][1] library that will ease your work by parsing SQL statements and give you the capapbility to query and work with tokens within a SQL statement. It can be a goos base to filter out statements containing a specific token like `tableB` in your case\\n\\n\\n\\n\\n\\n  [1]: https://code.google.com/p/python-sqlparse/',\n",
       "  '<python><sql><regex>',\n",
       "  datetime.date(2013, 5, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '4405.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['26',\n",
       "  '16862129',\n",
       "  'Answer',\n",
       "  'missing attribute error when trying to set cell color in xlwt using python',\n",
       "  \"According to the source code:\\n\\n\\n\\n    xlwt.Worksheet.write(self, r, c, label='', style=<xlwt.Style.XFStyle object at 0x053AEA10>)\\n\\n\\n\\n`write` should receive the `style` as argument, and not directly the `Pattern` object.\\n\\n\\n\\nYou should try to put your pattern object into `style` and pass `style` as argument:\\n\\n\\n\\n    style.pattern = pattern\\n\\n    newSheet.write(row, col, values[row - 1][col], style)\\n\\n\\n\\nSome [examples][1] I found useful how to use `xlwt`, including how to set the style\\n\\n\\n\\n\\n\\n  [1]: http://www.youlikeprogramming.com/2011/04/examples-generating-excel-documents-using-pythons-xlwt/\",\n",
       "  '<python><excel><runtime-error><xlwt>',\n",
       "  datetime.date(2013, 5, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1315.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['27',\n",
       "  '10666041',\n",
       "  'Answer',\n",
       "  'How to read an octet string from file',\n",
       "  'You may see the `repr()` version of your string displayed somewhere, which doesn\\'t mean your string is incorrectly loaded:\\n\\n\\n\\n    >>> s = \"\\\\x7D\\\\x1E\\\\x40\\\\xEE\"\\n\\n    >>> s\\n\\n    \\'}\\\\x1e@\\\\xee\\'\\n\\n    >>> repr(s)\\n\\n    \"\\'}\\\\\\\\x1e@\\\\\\\\xee\\'\"\\n\\n\\n\\n**Edit:**\\n\\n\\n\\nIf your string is already double-escaped and you want to remove those, use `replace`, and don\\'t forget to escape the `\\\\` also in the arguments:\\n\\n\\n\\n    sig_bytes.replace(\"\\\\\\\\\\\\\\\\x\",\"\\\\\\\\x\")\\n\\n\\n\\n**Edit 2:**\\n\\n\\n\\nOP is somehow getting the repr string loaded into its string variable instead of the correct string data.\\n\\n\\n\\n    s = \"\\\\\\\\x7D\\\\\\\\x1E\\\\\\\\x40\\\\\\\\xEE\"\\n\\n\\n\\nLet\\'s reinterpret this into the correct form OP should have loaded from the beginning. For this purpose we will construct a full string repr of the string by surrounding it with quotes, then ask `ast` module to `literal_eval` it:\\n\\n\\n\\n    import ast\\n\\n    s = \"\\\\\\\\x7D\\\\\\\\x1E\\\\\\\\x40\\\\\\\\xEE\"\\n\\n    s = \\'\"\\'+s+\\'\"\\'\\n\\n    s = ast.literal_eval(s)\\n\\n    >>> s\\n\\n    \\'}\\\\x1e@\\\\xee\\'',\n",
       "  '<python><string>',\n",
       "  datetime.date(2012, 5, 19),\n",
       "  '2012-05-19 15:29:36',\n",
       "  'Boud (624829)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1043.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['29',\n",
       "  '10789330',\n",
       "  'Answer',\n",
       "  \"AttributeError: NoneType object has no attribute 'health'\",\n",
       "  'Explanation of a common error or *how to better understand a simple traceback to debug my program*:\\n\\n\\n\\n      File \"<path>\\\\<filename>.py\", line 393, in <function>\\n\\n\\n\\nTraceback gives you the callstack and points to the error raises in which function. Nothin special here\\n\\n\\n\\n        self.target.health -= self.attack_damage\\n\\n\\n\\nFriendly interpreter gives the statement causing the error. This is important as it means the error raised is related to this line of code.\\n\\n\\n\\n    AttributeError: \\'NoneType\\' object has no attribute \\'health\\'\\n\\nHere we are. The error is [`AttributeError`][1] ; the doc is pretty clear about it: I\\'m trying to read the value of or I\\'m trying to assign to a variable that is not a member of the related object.\\n\\n\\n\\nWhat is my object ? It is a `\\'NoneType\\' object` ; we don\\'t have the name, but we have its type. The object we have an issue with is `NoneType`, thus it is the special object `None`.\\n\\n\\n\\nWhat is the error with this `None`object ? It doesn\\'t have a `\\'health\\'` attribute is saying the Traceback. So, we are accessing the attribute `health` somewhere in the line of code the error raises, and this is called on a `None` object.\\n\\n\\n\\nNow we are almost all set: `self.target.health` is the only location in the error line we use `health`, thus it means `self.target` is `None`. Now I look at the source code I try to understand why it could be, even if I put a check at the beginning at the function against it. It leads to the fact I must set it to `None` somehow after that line.\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n\\n\\nAll of this reasoning steps lead to the short answer:\\n\\n\\n\\nIt means `self.target` is `None`. The error comes from your code:\\n\\n\\n\\n    if self.target.health <= 0:\\n\\n        REDGOLD += self.target.reward\\n\\n        BLUECOMMAND += self.target.cmdback\\n\\n        self.target = None  # set to None\\n\\n    if not self.cooldown_ready(): return\\n\\n    self.target.health -= self.attack_damage  # self.target is None, .health failed\\n\\n\\n\\n\\n\\n  [1]: http://docs.python.org/library/exceptions.html#exceptions.AttributeError',\n",
       "  '<python><debugging><pygame>',\n",
       "  datetime.date(2012, 5, 28),\n",
       "  '2012-05-28 19:20:19',\n",
       "  'Boud (624829)',\n",
       "  '8',\n",
       "  '',\n",
       "  '1486.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['30',\n",
       "  '10505705',\n",
       "  'Answer',\n",
       "  'Format a float in Python with a maximum number of decimal places and without extra zero padding',\n",
       "  \"What you are asking for should be adressed by the rounding methods like the `round` function and let the float number being naturally displayed with its string representation.\\n\\n\\n\\n    >>> round(65.53, 4)\\n\\n    '65.53'\\n\\n    >>> round(40.355435, 4)\\n\\n    '40.3554'\\n\\n    >>> round(0, 4)\\n\\n    '0.0'\",\n",
       "  '<python><string-formatting><padding>',\n",
       "  datetime.date(2012, 5, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '17',\n",
       "  '',\n",
       "  '12452.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['32',\n",
       "  '10590155',\n",
       "  'Answer',\n",
       "  'String formatting %02d not performing as expected? What would cause the following code to not print truncated two-digit form? ',\n",
       "  \"You are trying to get the integer part and the fractional part of the float to print your result. It is a good practice to use operators and functions on numeric data directly instead of adding a heavy overhead by converting the float into str and back to number.\\n\\n\\n\\nUse the math module modf function for that. It will also simplify your algorithm.\\n\\n\\n\\n    import time\\n\\n    import math\\n\\n    \\n\\n    s_time = time.time()\\n\\n    def format_time():\\n\\n        t = time.time() - s_time\\n\\n        if t < 60:\\n\\n            f,i = math.modf(t)\\n\\n            print '00:00:%02d:%02d' % (i, f/0.041666666666666664)\\n\\n    \\n\\n    while True:\\n\\n        format_time()\\n\\n\\n\\nPS: for your code error, in your elif block, you are passing t as an integer with a huge value instead of passing the 0.xxxxx value of it. This error wouldn't occur if you keep using the math functions of floats.\",\n",
       "  '<python><string><time><formatting>',\n",
       "  datetime.date(2012, 5, 14),\n",
       "  '2012-05-14 20:22:47',\n",
       "  'Boud (624829)',\n",
       "  '2',\n",
       "  '',\n",
       "  '3771.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['34',\n",
       "  '10775344',\n",
       "  'Answer',\n",
       "  'Finding SUM except some columns',\n",
       "  'The common approach to address this problem in Excel is to apply the `SUM` function to the entire range and to substract the exception list out of it, again with the `SUM` function, this time with a discrete list of individual cells to sum:\\n\\n\\n\\n    =SUM(<range to sum>)-SUM(<exception cell 1>, <exception cell 2>, ...)\\n\\n\\n\\nThis applied to your current example of ~100 cells to sum with say 5 exceptions:\\n\\n\\n\\n    =SUM(A1:CV1)-SUM(E1,M1,Y1,AB1,BU1)\\n\\n\\n\\nIf you want to get your Excel spreadsheet more flexible to those columns exclusion, you may consider to define a named range on the discrete selection of your columns to exclude. If you use it in the substraction `SUM` formula, you will avoid to change the formula when additional columns need to be excluded : you will only need to change the named range specification.\\n\\n\\n\\n    =SUM(A1:J1)-SUM(A1:J1 Exclusion_List)\\n\\n\\n\\nwhere `Exclusion_List` is a named range on the distinct columns to exclude',\n",
       "  '<excel><sum><formula><excel-formula>',\n",
       "  datetime.date(2012, 5, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '11664.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['36',\n",
       "  '10572573',\n",
       "  'Answer',\n",
       "  'Copy format from one column range to another column range in Excel',\n",
       "  \"Use Excel's **conditional formatting** with the *Top 10 items...* feature\\n\\n\\n\\n - Select column A\\n\\n - Call the top 10 items feature\\n\\n - Set the top number to 4 and set your conditional formatting for the cell\\n\\n\\n\\nColumn A now highlights the top 4 values\\n\\nNow clone this with the Format Painter tool:\\n\\n\\n\\n - Select column A\\n\\n - Click on the Format Painter button\\n\\n - Click on column B to apply the formatting the same way as A\\n\\n - Repeat the operation for column C to H, one column after another\\n\\n\\n\\nNow your columns have all the correct formatting. If you select your table you will get a conditional formatting rule list like this:\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/3HxdC.png\",\n",
       "  '<excel><ms-office>',\n",
       "  datetime.date(2012, 5, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1000.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['37',\n",
       "  '10591372',\n",
       "  'Answer',\n",
       "  'Remove all occurrences of several chars from a string',\n",
       "  'Use the [`translate`][1] function to delete the unwanted characters:\\n\\n\\n\\n    >>> \\'::2012-05-14 18:10:20.856000::\\'.translate(None, \\' -.:\\')\\n\\n    \\'20120514181020856000\\'\\n\\n\\n\\n\\n\\nBe sure your string is of `str` type and not `unicode`, as the parameters of the function won\\'t be the same. For unicode, use the following syntax ; it consists in building the dict of unicode ordinals from the chars to delete and to map them to `None`:\\n\\n\\n\\n    >>> u\\'::2012-05-14 18:10:20.856000::\\'.translate({ord(k):None for k in u\\' -.:\\'})\\n\\n    u\\'20120514181020856000\\'\\n\\n\\n\\n\\n\\nSome timings for performance comparison with `re`:\\n\\n\\n\\n    >>> timeit.timeit(\"\"\"re.sub(r\"[ -.:]\", r\"\", \"\\'::2012-05-14 18:10:20.856000::\\'\")\"\"\",\"import re\")\\n\\n    7.352270301875713\\n\\n    >>> timeit.timeit(\"\"\"\\'::2012-05-14 18:10:20.856000::\\'.translate(None, \\' -.:\\')\"\"\")\\n\\n    0.5894893344550951\\n\\n\\n\\n  [1]: http://docs.python.org/library/stdtypes.html#str.translate',\n",
       "  '<python><string><string-formatting><built-in>',\n",
       "  datetime.date(2012, 5, 14),\n",
       "  '2012-05-14 21:48:15',\n",
       "  'Boud (624829)',\n",
       "  '22',\n",
       "  '',\n",
       "  '12442.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['38',\n",
       "  '10777383',\n",
       "  'Answer',\n",
       "  'python converting number into dollar format',\n",
       "  \"[Format][1] your float as a string with 2 decimals:\\n\\n\\n\\n    >>> '{:.2f}'.format(12.1)\\n\\n    '12.10'\\n\\n\\n\\nPS: you don't need to use the heavy `enumerate` function to skip the header of your `csv` file and rewrite it as such in the output. You can simply write the first line directly:\\n\\n\\n\\n    csvin = csv.reader(inf)\\n\\n    outf.writerow(csvin.next())  # header line 1\\n\\n    for line in csvin:  # iterator goes on from line 2\\n\\n        price = line[4]\\n\\n        #...\\n\\n    \\n\\n  [1]: http://docs.python.org/library/string.html#formatspec\",\n",
       "  '<python><csv><formatting>',\n",
       "  datetime.date(2012, 5, 27),\n",
       "  '2012-05-27 21:30:54',\n",
       "  'Boud (624829)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2970.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['39',\n",
       "  '10650469',\n",
       "  'Answer',\n",
       "  'Zipping together unicode strings in Python',\n",
       "  'In Python 2.x, strings are not unicode by default. When dealing with unicode data, you have to do the following:\\n\\n\\n\\n - **prefix string literals with `u`** character: `a = u\\'ÀÁÂÃÈÉÊËÌÍÎÏÒÓÔÕÖÙÚÛÜ\\'`, or\\n\\n\\n\\n - if you want to avoid the `u` prefix and if the modules you are working with are enough compatible, **use `from __future__ import unicode_literals` import** to make string literals interpreted as unicode by default\\n\\n\\n\\n - if you write unicode string literals directly in your Python code, **save your .py file in `utf-8` format** so that the literals are correctly interpreted. Python 2.3+ will interpret the [utf-8 BOM][1] ; a good practice is also to [add a specific comment line][2] at the beginning of the file to indicate the encoding like `# -*- coding: utf-8 -*-`, or\\n\\n\\n\\n - you can also keep saving the .py file in `ascii`, but you will need to **escape the unicode characters in the literals**, which can be less readable: `\\'ÀÁÂÃ\\'` should become `\\'\\\\xc0\\\\xc1\\\\xc2\\\\xc3\\'`\\n\\n\\n\\nOnce you fulfill those conditions, the rest is about applying algorithms on those unicode strings the same way you would work with the str version. Here is one possible solution for your problem with the `__future__` import:\\n\\n\\n\\n\\n\\n    from __future__ import unicode_literals\\n\\n    \\n\\n    from itertools import chain\\n\\n    a = \"ÀÁÂÃÈÉÊËÌÍÎÏÒÓÔÕÖÙÚÛÜ\"\\n\\n    b = \"àáâãäèéçêëìíîïòóôõöùúûüÿ\"\\n\\n    \\n\\n    print \\'\\'.join(chain(*zip(a,b)))\\n\\n\\n\\n    >>> ÀàÁáÂâÃãÈäÉèÊéËçÌêÍëÎìÏíÒîÓïÔòÕóÖôÙõÚöÛùÜú\\n\\n\\n\\n\\n\\nFurther references:\\n\\n\\n\\n - [PEP 263][3] defines the non-ascii encoding comments\\n\\n - [PEP 3120][4] defines utf-8 as the default encoding in Python 3\\n\\n\\n\\n\\n\\n  [1]: http://en.wikipedia.org/wiki/Byte_order_mark\\n\\n  [2]: http://docs.python.org/tutorial/interpreter.html#source-code-encoding\\n\\n  [3]: http://www.python.org/dev/peps/pep-0263/\\n\\n  [4]: http://www.python.org/dev/peps/pep-3120/',\n",
       "  '<python><string><unicode><permutation>',\n",
       "  datetime.date(2012, 5, 18),\n",
       "  '2012-05-18 15:11:03',\n",
       "  'Boud (624829)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1312.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['40',\n",
       "  '10663730',\n",
       "  'Answer',\n",
       "  'Why is the same SQLite query being 30 times slower when fetching only twice as many results?',\n",
       "  'Consider using [covering indices][1] on the tables involved in your query.\\n\\n\\n\\nYou are indeed fetching a limited amount of columns in your `select` statement and the corresponding `inner join` and `where` clauses. By using a covering index with the columns well ordered in it, you should get a very fast query, that is you will remove the `scan table` in favor of an [`search table` using a `covering index`][2].\\n\\n\\n\\nTry to use those indices on your tables:\\n\\n\\n\\n    CREATE INDEX `fk_covering_feature` ON `feature` (`msrun_msrun_id`,`mzMin`,`mzMax`,`rtMin`,`rtMax`,`feature_table_id`);\\n\\n    CREATE INDEX `fk_covering_spectrum` ON  `spectrum` (`msrun_msrun_id`,`scan_start_time`,`spectrum_id`);\\n\\n    CREATE INDEX `fk_covering_MSMS_precursor` ON  `MSMS_precursor` (`spectrum_spectrum_id`,`ion_mz`,`precursor_id`);\\n\\n\\n\\nWhen going for speed, you should also hint the query planner to understand msrun_msrun_id  is a constant to check for both `feature` and `spectrum` tables. Add the constant test in your query by putting this additional test at the end of the query (and pass `spectrumFeature_InputValues` twice):\\n\\n\\n\\n    \"AND spectrum.msrun_msrun_id = ?\"\\n\\n\\n\\n\\n\\n  [1]: http://www.sqlite.org/queryplanner.html#covidx\\n\\n  [2]: http://www.sqlite.org/eqp.html#section_1_1',\n",
       "  '<python><performance><sqlite><fetchall>',\n",
       "  datetime.date(2012, 5, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '6726.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['43',\n",
       "  '16483149',\n",
       "  'Answer',\n",
       "  'Convert a numpy array to a CSV string and a CSV string back to a numpy array',\n",
       "  \"First you should use `join` this way to avoid the last comma issue:\\n\\n\\n\\n    VIstring = ','.join(['%.5f' % num for num in VI])\\n\\n\\n\\nThen to read it back, use `numpy.fromstring`:\\n\\n\\n\\n    np.fromstring(VIstring, sep=',')\",\n",
       "  '<python><csv><python-2.7><numpy>',\n",
       "  datetime.date(2013, 5, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '21',\n",
       "  '',\n",
       "  '11717.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['44',\n",
       "  '17216674',\n",
       "  'Answer',\n",
       "  'Python Pandas: Boolean indexing on multiple columns',\n",
       "  \"It is a precedence operator issue.\\n\\n\\n\\nYou should add extra parenthesis to make your multi condition test working:\\n\\n\\n\\n    d[(d['x']>2) & (d['y']>7)]\\n\\n\\n\\n[This section][1] of the tutorial you mentioned shows an example with several boolean conditions and the parenthesis are used.\\n\\n\\n\\n\\n\\n  [1]: https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2013, 6, 20),\n",
       "  '2017-06-15 20:41:52',\n",
       "  'A-B-B (832230)',\n",
       "  '62',\n",
       "  '',\n",
       "  '31179.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['46',\n",
       "  '17451067',\n",
       "  'Answer',\n",
       "  'using python pandas lookup another dataframe and return corresponding values',\n",
       "  'Use [`merge`][1] function:\\n\\n\\n\\n    df2.merge(df1)\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2013, 7, 3),\n",
       "  '2013-07-03 14:47:38',\n",
       "  'Boud (624829)',\n",
       "  '8',\n",
       "  '',\n",
       "  '6642.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['49',\n",
       "  '10776889',\n",
       "  'Answer',\n",
       "  'Which python module is used to read CPU temperature and processor Fan speed in WINDOWS..?',\n",
       "  'As per [Microsoft\\'s MSDN][1]:\\n\\n\\n\\n> Most of the information that the Win32_TemperatureProbe WMI class\\n\\n> provides comes from SMBIOS. Real-time readings for the CurrentReading\\n\\n> property cannot be extracted from SMBIOS tables. **For this reason,\\n\\n> current implementations of WMI do not populate the CurrentReading\\n\\n> property**. The CurrentReading property\\'s presence is reserved for\\n\\n> future use.\\n\\n\\n\\nYou can use `MSAcpi_ThermalZoneTemperature` instead:\\n\\n\\n\\n    import wmi\\n\\n    \\n\\n    w = wmi.WMI(namespace=\"root\\\\\\\\wmi\")\\n\\n    print (w.MSAcpi_ThermalZoneTemperature()[0].CurrentTemperature/10.0)-273.15\\n\\n\\n\\n  [1]: http://msdn.microsoft.com/en-us/library/windows/desktop/aa394493%28v=vs.85%29.aspx',\n",
       "  '<python>',\n",
       "  datetime.date(2012, 5, 27),\n",
       "  '2012-05-27 20:29:30',\n",
       "  'Boud (624829)',\n",
       "  '2',\n",
       "  '',\n",
       "  '3560.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['50',\n",
       "  '10785422',\n",
       "  'Answer',\n",
       "  'How to normalize unicode encoding for iso-8859-15 conversion in python?',\n",
       "  'Use the unicode version of the [`translate`][1] function, assuming `s` is a unicode string:\\n\\n\\n\\n    s.translate({ord(u\"\\\\u2019\"):ord(u\"\\'\")})\\n\\n\\n\\nThe argument of the unicode version of `translate` is a dict mapping unicode ordinals to unicode ordinals. Add to this dict other characters you cannot encode in your target encoding.\\n\\n\\n\\nYou can build your mapping table in a little more readable form and create your mapping dict from it, for instance:\\n\\n\\n\\n    char_mappings = [(u\"\\\\u2019\", u\"\\'\"),\\n\\n                     (u\"`\", u\"\\'\")]\\n\\n    translate_mapping = {ord(k):ord(v) for k,v in char_mappings}\\n\\n\\n\\n----------\\n\\nFrom translate documentation:\\n\\n> For Unicode objects, the translate() method does not accept the\\n\\n> optional deletechars argument. Instead, it returns a copy of the s\\n\\n> where all characters have been mapped through the given translation\\n\\n> table which must be a mapping of Unicode ordinals to Unicode ordinals,\\n\\n> Unicode strings or None. Unmapped characters are left untouched.\\n\\n> Characters mapped to None are deleted. Note, a more flexible approach\\n\\n> is to create a custom character mapping codec using the codecs module\\n\\n> (see encodings.cp1251 for an example).\\n\\n\\n\\n  [1]: http://docs.python.org/library/stdtypes.html#str.translate\\n\\n',\n",
       "  '<python><unicode><encoding><utf-8><iso-8859-15>',\n",
       "  datetime.date(2012, 5, 28),\n",
       "  '2012-05-28 13:32:50',\n",
       "  'Boud (624829)',\n",
       "  '5',\n",
       "  '',\n",
       "  '3109.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['51',\n",
       "  '14166631',\n",
       "  'Answer',\n",
       "  'Can I read the browser url using selenium webdriver?',\n",
       "  \"You get `current_url` attribute on the driver:\\n\\n\\n\\n    from selenium import webdriver\\n\\n    browser = webdriver.Firefox()\\n\\n    browser.get('http://www.google.com')\\n\\n    print browser.current_url\",\n",
       "  '<python><selenium><beautifulsoup><selenium-webdriver>',\n",
       "  datetime.date(2013, 1, 4),\n",
       "  '2013-01-04 23:29:02',\n",
       "  'Boud (624829)',\n",
       "  '16',\n",
       "  '',\n",
       "  '8866.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['52',\n",
       "  '10786816',\n",
       "  'Answer',\n",
       "  'How to make nested Panel and Sizer work in wxpython',\n",
       "  'The logic here is to create a `Panel` to add controls inside, then a `BoxSizer` where you declare how each control you add in the `Panel` behaves with resizing, and eventually to set on the `Panel` what is the `BoxSizer` applied on it.\\n\\n\\n\\nYou have 2 issues.\\n\\n\\n\\n - First you are missing the last part of the above statement for the `posPnl`object. Add this:\\n\\n\\n\\n        posPnl.SetSizer(posPnlSzr)\\n\\nonce you have added the controls to `posPnlSzr`\\n\\n\\n\\n - The second issue is that you are adding `posSnlSzr` in your `mainSzr`. It is wrong to do so. Again the logic is to add controls to a sizer, and you can view a `Panel` as a compounded control. Thus the correct code is to add `posPnl` into the `mainSzr`:\\n\\n\\n\\n        mainSzr.Add(posPnl)\\n\\nAs far I as read, you are trying to get `posPnl` automatically resized with the main window. If to add the panel so that `mainSzr` will actually resize it:\\n\\n\\n\\n        mainSzr.Add(posPnl, 1, wx.GROW)\\n\\n\\n\\n\\n\\n----------\\n\\nThis gives the final source code:\\n\\n\\n\\n    #!/usr/bin/env python\\n\\n    \\n\\n    import wx\\n\\n    class MainWindow(wx.Frame):\\n\\n        def __init__(self, parent, title):\\n\\n            wx.Frame.__init__(self, parent)\\n\\n    \\n\\n            #add position panel\\n\\n            posPnl = wx.Panel(self)\\n\\n            lbl1 = wx.StaticText(posPnl, label=\"Position\")\\n\\n            lbl2 = wx.StaticText(posPnl, label=\"Size\")\\n\\n            sizeCtrl = wx.TextCtrl(posPnl)\\n\\n    \\n\\n            posPnlSzr = wx.BoxSizer(wx.HORIZONTAL)\\n\\n            posPnlSzr.Add(lbl1, 1, wx.GROW)\\n\\n            posPnlSzr.Add(sizeCtrl, 1, wx.GROW)\\n\\n            posPnlSzr.Add(lbl2, 1, wx.GROW)\\n\\n    \\n\\n            posPnl.SetSizer(posPnlSzr)\\n\\n    \\n\\n            #create a top leverl sizer to add to the frame itself\\n\\n            mainSzr = wx.BoxSizer(wx.VERTICAL)\\n\\n            mainSzr.Add(posPnl, 1, wx.GROW)\\n\\n    \\n\\n            self.SetSizerAndFit(mainSzr)\\n\\n            self.Show()\\n\\n    \\n\\n    \\n\\n    app = wx.App(False)\\n\\n    frame = MainWindow(None, \"Trading Client\")\\n\\n    app.MainLoop()',\n",
       "  '<python><wxpython>',\n",
       "  datetime.date(2012, 5, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '5079.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['53',\n",
       "  '10803229',\n",
       "  'Answer',\n",
       "  'How to save an Excel worksheet as CSV',\n",
       "  'The most basic examples using the two libraries described line by line:\\n\\n\\n\\n 1. Open the xls workbook\\n\\n 2. Reference the first spreadsheet\\n\\n 3. Open in binary write the target csv file\\n\\n 4. Create the default csv writer object\\n\\n 5. Loop over all the rows of the first spreadsheet\\n\\n 6. Dump the rows into the csv\\n\\n\\n\\n----------\\n\\n\\n\\n    import xlrd\\n\\n    import csv\\n\\n    \\n\\n    with xlrd.open_workbook(\\'a_file.xls\\') as wb:\\n\\n        sh = wb.sheet_by_index(0)  # or wb.sheet_by_name(\\'name_of_the_sheet_here\\')\\n\\n        with open(\\'a_file.csv\\', \\'wb\\') as f:   # open(\\'a_file.csv\\', \\'w\\', newline=\"\") for python 3\\n\\n            c = csv.writer(f)\\n\\n            for r in range(sh.nrows):\\n\\n                c.writerow(sh.row_values(r))\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n    import openpyxl\\n\\n    import csv\\n\\n    \\n\\n    wb = openpyxl.load_workbook(\\'test.xlsx\\')\\n\\n    sh = wb.get_active_sheet()\\n\\n    with open(\\'test.csv\\', \\'wb\\') as f:  # open(\\'test.csv\\', \\'w\\', newline=\"\") for python 3\\n\\n        c = csv.writer(f)\\n\\n        for r in sh.rows:\\n\\n            c.writerow([cell.value for cell in r])\\n\\n\\n\\n',\n",
       "  '<python><excel><csv>',\n",
       "  datetime.date(2012, 5, 29),\n",
       "  '2017-12-20 12:06:22',\n",
       "  'Jean-François Fabre (6451573)',\n",
       "  '33',\n",
       "  '',\n",
       "  '30295.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['56',\n",
       "  '10668173',\n",
       "  'Answer',\n",
       "  'Python Requests throwing SSLError',\n",
       "  \"From requests [documentation on SSL verification][1]:\\n\\n\\n\\n>Requests can verify SSL certificates for HTTPS requests, just like a web browser. To check a host’s SSL certificate, you can use the verify argument:\\n\\n\\n\\n    >>> requests.get('https://kennethreitz.com', verify=True)\\n\\n\\n\\nIf you don't want to verify your SSL certificate, make `verify=False` \\n\\n  [1]: http://docs.python-requests.org/en/latest/user/advanced/#ssl-cert-verification\\n\\n\",\n",
       "  '<python><ssl><python-requests><urllib3>',\n",
       "  datetime.date(2012, 5, 19),\n",
       "  '2015-11-18 21:37:36',\n",
       "  'Ahmed (2135035)',\n",
       "  '95',\n",
       "  '',\n",
       "  '409412.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['57',\n",
       "  '10710872',\n",
       "  'Answer',\n",
       "  'Trouble with UTF-8 CSV input in Python',\n",
       "  \"At it fails from the first char to read, you may have a BOM. Use `codecs.open('utf8file.csv', 'rU', encoding='utf-8-sig')` if your file is UTF8 and has a BOM at the beginning.\",\n",
       "  '<python><encoding><utf-8>',\n",
       "  datetime.date(2012, 5, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '12',\n",
       "  '',\n",
       "  '22026.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['58',\n",
       "  '10788182',\n",
       "  'Answer',\n",
       "  'Executing statements in a class definition: Which variables does the interpreter know about?',\n",
       "  \"Partial answer, as it is more to cut some wrong paths to follow.\\n\\n\\n\\nIf I take back your working sample and put a dict comprehension:\\n\\n\\n\\n    class x:\\n\\n        dat = 1\\n\\n        datlist = {i:dat for i in range(10)}\\n\\n\\n\\nI get also this:\\n\\n\\n\\n    >>> NameError: global name 'dat' is not defined\\n\\n\\n\\nSo it looks like dict comprehension is hiding the temporary __dict__ use during class statement execution, but the list comprehension doesn't.\\n\\n\\n\\nNo further information found by now in the docs about this...\\n\\n\\n\\nEdit based on @interjay comment:\\n\\nclass construction not fulfilling scope norm is addressed in this [post][1]. Short story is that list comprehension are buggy in 2.x and see class members but they shouldn't.\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/a/1773697/624829\",\n",
       "  '<python>',\n",
       "  datetime.date(2012, 5, 28),\n",
       "  '2017-05-23 10:30:33',\n",
       "  'Boud (624829), URL Rewriter Bot (n/a)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1076.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['61',\n",
       "  '18346815',\n",
       "  'Answer',\n",
       "  'How to deal with log of zero in R in image.plot?',\n",
       "  \"Can't paste multiple lines of code in a comment, but this example shows what I meant:\\n\\n\\n\\n    > m=cbind(c(0,0.88,0.99),c(1,2,1),c(3,4,5))\\n\\n    > m=as.matrix(m)\\n\\n    > log(m)\\n\\n                [,1]      [,2]     [,3]\\n\\n    [1,]        -Inf 0.0000000 1.098612\\n\\n    [2,] -0.12783337 0.6931472 1.386294\\n\\n    [3,] -0.01005034 0.0000000 1.609438\\n\\n    > m\\n\\n         [,1] [,2] [,3]\\n\\n    [1,] 0.00    1    3\\n\\n    [2,] 0.88    2    4\\n\\n    [3,] 0.99    1    5\\n\\n\\n\\n\\n\\n\",\n",
       "  '<r>',\n",
       "  datetime.date(2013, 8, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2343.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['64',\n",
       "  '18431629',\n",
       "  'Question',\n",
       "  'Is the build-in probability density functions of `scipy.stat.distributions` slower than a user provided one?',\n",
       "  'Suppose I have an array: `adata=array([0.5, 1.,2.,3.,6.,10.])` and I want to calculate log likelihood of Weibull distribution of this array, given the parameters `[5.,1.5]` and `[5.1,1.6]`. I have never thought I need to write my own Weibull probability density functions for this task, as it is already provided in `scipy.stat.distributions`. So, this ought to do it:\\n\\n\\n\\n    from scipy import stats\\n\\n    from numpy import *\\n\\n    adata=array([0.5, 1.,2.,3.,6.,10.])\\n\\n    def wb2LL(p, x): #log-likelihood of 2 parameter Weibull distribution\\n\\n    \\treturn sum(log(stats.weibull_min.pdf(x, p[1], 0., p[0])), axis=1)\\n\\n\\n\\nAnd: \\n\\n\\n\\n    >>> wb2LL(array([[5.,1.5],[5.1,1.6]]).T[...,newaxis], adata)\\n\\n    array([-14.43743911, -14.68835298])\\n\\n\\n\\nOr I reinvent the wheel and write a new Weibull pdf function, such as:\\n\\n\\n\\n    wbp=lambda p, x: p[1]/p[0]*((x/p[0])**(p[1]-1))*exp(-((x/p[0])**p[1]))\\n\\n    def wb2LL1(p, x): #log-likelihood of 2 paramter Weibull\\n\\n\\t    return sum(log(wbp(p,x)), axis=1)\\n\\n\\n\\nAnd: \\n\\n\\n\\n    >>> wb2LL1(array([[5.,1.5],[5.1,1.6]]).T[...,newaxis], adata)\\n\\n    array([-14.43743911, -14.68835298])\\n\\n\\n\\nAdmittedly, I always take it for granted that if something is already in `scipy`, it should be very well optimized and re-inventing the wheel is seldom going to make it faster. But here comes the surprise: if I `timeit`, 100000 calls of `wb2LL1(array([[5.,1.5],[5.1,1.6]])[...,newaxis], adata)` takes 2.156s while 100000 calls of `wb2LL(array([[5.,1.5],[5.1,1.6]])[...,newaxis], adata)` takes 5.219s, the build-in `stats.weibull_min.pdf()` is more than twice slower.\\n\\n\\n\\nChecking the source code `python_path/sitepackage/scipy/stat/distributions.py` didn\\'t provides an easy answer, at least for me. If anything, from it I would expect `stats.weibull_min.pdf()` to be almost as fast as `wbp()`.\\n\\n\\n\\nRelevant source code: line 2999-3033:\\n\\n\\n\\n    class frechet_r_gen(rv_continuous):\\n\\n        \"\"\"A Frechet right (or Weibull minimum) continuous random variable.\\n\\n    \\n\\n        %(before_notes)s\\n\\n    \\n\\n        See Also\\n\\n        --------\\n\\n        weibull_min : The same distribution as `frechet_r`.\\n\\n        frechet_l, weibull_max\\n\\n    \\n\\n        Notes\\n\\n        -----\\n\\n        The probability density function for `frechet_r` is::\\n\\n    \\n\\n            frechet_r.pdf(x, c) = c * x**(c-1) * exp(-x**c)\\n\\n    \\n\\n        for ``x > 0``, ``c > 0``.\\n\\n    \\n\\n        %(example)s\\n\\n    \\n\\n        \"\"\"\\n\\n        def _pdf(self, x, c):\\n\\n            return c*pow(x,c-1)*exp(-pow(x,c))\\n\\n        def _logpdf(self, x, c):\\n\\n            return log(c) + (c-1)*log(x) - pow(x,c)\\n\\n        def _cdf(self, x, c):\\n\\n            return -expm1(-pow(x,c))\\n\\n        def _ppf(self, q, c):\\n\\n            return pow(-log1p(-q),1.0/c)\\n\\n        def _munp(self, n, c):\\n\\n            return special.gamma(1.0+n*1.0/c)\\n\\n        def _entropy(self, c):\\n\\n            return -_EULER / c - log(c) + _EULER + 1\\n\\n    frechet_r = frechet_r_gen(a=0.0, name=\\'frechet_r\\', shapes=\\'c\\')\\n\\n    weibull_min = frechet_r_gen(a=0.0, name=\\'weibull_min\\', shapes=\\'c\\')\\n\\n\\n\\nSo, the question is: is `stats.weibull_min.pdf()` really slower? If so, how come?',\n",
       "  '<python><performance><numpy><scipy>',\n",
       "  datetime.date(2013, 8, 25),\n",
       "  '2013-08-26 03:15:06',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '2.0',\n",
       "  '1483.0',\n",
       "  '1.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['65',\n",
       "  '18598478',\n",
       "  'Answer',\n",
       "  'combining 2D arrays to 3D arrays',\n",
       "  'No need to use `vstack`, `hstack`. Just swap the axis using `np.swapaxes`:\\n\\n\\n\\n    >>> d=array([a, b, c])\\n\\n    >>> d\\n\\n    array([[[ 1,  0,  0],\\n\\n            [ 3,  0,  0],\\n\\n            [ 5,  2,  0],\\n\\n            [ 2,  0,  0],\\n\\n            [ 1,  2,  1]],\\n\\n    \\n\\n           [[ 5,  9,  9],\\n\\n            [37,  8,  9],\\n\\n            [49,  8,  3],\\n\\n            [ 3,  3,  1],\\n\\n            [ 4,  4,  5]],\\n\\n    \\n\\n           [[ 0,  0,  0],\\n\\n            [ 0,  6,  0],\\n\\n            [ 1,  4,  6],\\n\\n            [ 6,  2,  0],\\n\\n            [ 0,  5,  4]]])\\n\\n    >>> swapaxes(d, 0, 1)\\n\\n    array([[[ 1,  0,  0],\\n\\n            [ 5,  9,  9],\\n\\n            [ 0,  0,  0]],\\n\\n    \\n\\n           [[ 3,  0,  0],\\n\\n            [37,  8,  9],\\n\\n            [ 0,  6,  0]],\\n\\n    \\n\\n           [[ 5,  2,  0],\\n\\n            [49,  8,  3],\\n\\n            [ 1,  4,  6]],\\n\\n    \\n\\n           [[ 2,  0,  0],\\n\\n            [ 3,  3,  1],\\n\\n            [ 6,  2,  0]],\\n\\n    \\n\\n           [[ 1,  2,  1],\\n\\n            [ 4,  4,  5],\\n\\n            [ 0,  5,  4]]])\\n\\n    ',\n",
       "  '<python><arrays><numpy>',\n",
       "  datetime.date(2013, 9, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '4068.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['68',\n",
       "  '16812974',\n",
       "  'Answer',\n",
       "  'How to access ODB files in Python 2.7',\n",
       "  'You can consider [Python Uno][1] API that comes with OpenOffice. There are several [Python examples][2] to interact with the API, including one with the [sample database][3]\\n\\n\\n\\nThere is also this [SO question][4] where it is explained how to use uno with LibreOffice.\\n\\n\\n\\n\\n\\n  [1]: http://www.openoffice.org/udk/python/python-bridge.html\\n\\n  [2]: http://www.openoffice.org/udk/python/python-bridge.html#examples\\n\\n  [3]: http://www.openoffice.org/udk/python/samples/biblioaccess.py\\n\\n  [4]: https://stackoverflow.com/questions/7784438/how-do-you-install-or-activate-pyuno-in-libreoffice',\n",
       "  '<python><sqlite3><python-2.7><hsqldb><libreoffice-base>',\n",
       "  datetime.date(2013, 5, 29),\n",
       "  '2017-05-23 12:00:58',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '4',\n",
       "  '',\n",
       "  '5185.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['70',\n",
       "  '18424957',\n",
       "  'Answer',\n",
       "  'Python - vectorizing a sliding window',\n",
       "  \"The problem lies in `x[1,x[0,:]+1]`, the index for the 2nd axis: `x[0,:]+1` is `[1 2 3 4 5 6 7 8 9 10]`, in which index `10` is larger than the dimension of x.\\n\\n\\n\\nIn the case of `x[1,x[0,:]-1]`, the index of the 2nd axis is `[-1 0 1 2 3 4 5 6 7 8 9]`, you end  up getting `[9 0 1 2 3 4 5 6 7 8]`, as `9` is the last element and has an index of `-1`. The index of the second element from the end is -2 and so on.\\n\\n\\n\\nWith `np.where((x[0,:]<5)&(x[0,:]>0),x[1,x[0,:]-1],x[1,:])` and `x[0,:]=[0 1 2 3 4 5 6 7 8 9]`, what essentially is going on is that the first cell is taken form `x[1,:]` because `x[0,0]` is 0 and `x[0,:]<5)&(x[0,:]>0` is `False`. The next four elements are taken from `x[1,x[0,:]-1]`. The rest are from `x[1,:]`. Finally the result is `[0 0 1 2 3 4 5 6 7 8]`\\n\\n\\n\\nIt may appear to be OK for sliding-window of just 1 cell, but it's gonna surprise you with:\\n\\n\\n\\n    >>> np.where((x[0,:]<5)&(x[0,:]>0),x[1,x[0,:]-2],x[1,:])\\n\\n    array([0, 9, 0, 1, 2, 5, 6, 7, 8, 9])\\n\\nWhen you try to move it by a windows of two cells. \\n\\n\\n\\nFor this specific problem, if we want to keep every thing in one line, this, will do:\\n\\n\\n\\n    >>> for i in [1, 2, 3, 4, 5, 6]:\\n\\n    \\tprint hstack((np.where(x[1,x[0,:]-i]<x[0, -i], x[1,x[0,:]-i], 0)[:5], x[0,5:]))\\n\\n\\t\\n\\n    [0 0 1 2 3 5 6 7 8 9]\\n\\n    [0 0 0 1 2 5 6 7 8 9]\\n\\n    [0 0 0 0 1 5 6 7 8 9]\\n\\n    [0 0 0 0 0 5 6 7 8 9]\\n\\n    [0 0 0 0 0 5 6 7 8 9]\\n\\n    [0 0 0 0 0 5 6 7 8 9]\\n\\n\\n\\nEdit:\\n\\nNow I understand your original question better, basically you want to take a 2D array and calculate N*N cell average around each cell. That is quite common. First you probably want to limit N to odd numbers, otherwise such thing as 2*2 average around a cell is difficult to define. Suppose we want 3*3 average:\\n\\n\\n\\n    #In this example, the shape is (10,10)\\n\\n    >>> a1=\\\\\\n\\n    array([[3, 7, 0, 9, 0, 8, 1, 4, 3, 3],\\n\\n       [5, 6, 5, 2, 9, 2, 3, 5, 2, 9],\\n\\n       [0, 9, 8, 5, 3, 1, 8, 1, 9, 4],\\n\\n       [7, 4, 0, 0, 9, 3, 3, 3, 5, 4],\\n\\n       [3, 1, 2, 4, 8, 8, 2, 1, 9, 6],\\n\\n       [0, 0, 3, 9, 3, 0, 9, 1, 3, 3],\\n\\n       [1, 2, 7, 4, 6, 6, 2, 6, 2, 1],\\n\\n       [3, 9, 8, 5, 0, 3, 1, 4, 0, 5],\\n\\n       [0, 3, 1, 4, 9, 9, 7, 5, 4, 5],\\n\\n       [4, 3, 8, 7, 8, 6, 8, 1, 1, 8]])\\n\\n    #move your original array 'a1' around, use range(-2,2) for 5*5 average and so on\\n\\n    >>> movea1=[a1[np.clip(np.arange(10)+i, 0, 9)][:,np.clip(np.arange(10)+j, 0, 9)] for i, j in itertools.product(*[range(-1,2),]*2)]\\n\\n    #then just take the average\\n\\n    >>> averagea1=np.mean(np.array(movea1), axis=0)\\n\\n    #trim the result array, because the cells among the edges do not have 3*3 average\\n\\n    >>> averagea1[1:10-1, 1:10-1]\\n\\n    array([[ 4.77777778,  5.66666667,  4.55555556,  4.33333333,  3.88888889,\\n\\n         3.66666667,  4.        ,  4.44444444],\\n\\n       [ 4.88888889,  4.33333333,  4.55555556,  3.77777778,  4.55555556,\\n\\n         3.22222222,  4.33333333,  4.66666667],\\n\\n       [ 3.77777778,  3.66666667,  4.33333333,  4.55555556,  5.        ,\\n\\n         3.33333333,  4.55555556,  4.66666667],\\n\\n       [ 2.22222222,  2.55555556,  4.22222222,  4.88888889,  5.        ,\\n\\n         3.33333333,  4.        ,  3.88888889],\\n\\n       [ 2.11111111,  3.55555556,  5.11111111,  5.33333333,  4.88888889,\\n\\n         3.88888889,  3.88888889,  3.55555556],\\n\\n       [ 3.66666667,  5.22222222,  5.        ,  4.        ,  3.33333333,\\n\\n         3.55555556,  3.11111111,  2.77777778],\\n\\n       [ 3.77777778,  4.77777778,  4.88888889,  5.11111111,  4.77777778,\\n\\n         4.77777778,  3.44444444,  3.55555556],\\n\\n       [ 4.33333333,  5.33333333,  5.55555556,  5.66666667,  5.66666667,\\n\\n         4.88888889,  3.44444444,  3.66666667]])\\n\\n\\n\\nI think you don't need to flatten you 2D-array, that causes confusion. Also, if you want to handle the edge elements differently other than just trim them away, consider making masked arrays using `np.ma` in 'Move your original array around' step.\",\n",
       "  '<python><numpy><scipy>',\n",
       "  datetime.date(2013, 8, 25),\n",
       "  '2013-08-25 16:19:21',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '3414.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['73',\n",
       "  '16768946',\n",
       "  'Answer',\n",
       "  'pivot \"stacked\" data with multiple indexes?',\n",
       "  \"You can set your multilevel index and then [`unstack`][1] the level within that index back to columns:\\n\\n\\n\\n    pivoted = my_df.set_index(['recid', 'exam_num', 'code']).unstack('code')\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.unstack.html\",\n",
       "  '<pandas>',\n",
       "  datetime.date(2013, 5, 27),\n",
       "  '2013-05-27 19:13:46',\n",
       "  'Boud (624829)',\n",
       "  '2',\n",
       "  '',\n",
       "  '4030.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['74',\n",
       "  '16769296',\n",
       "  'Answer',\n",
       "  'upgade python version using pip',\n",
       "  \"`pip` is designed to upgrade python packages and not to upgrade python itself. `pip` shouldn't try to upgrade python when you ask it to do so.\\n\\n\\n\\nDon't type `pip install python` but use an installer instead.\",\n",
       "  '<python><upgrade><pip>',\n",
       "  datetime.date(2013, 5, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '12',\n",
       "  '',\n",
       "  '24832.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['75',\n",
       "  '18265280',\n",
       "  'Answer',\n",
       "  'Fitting a Weibull distribution using Scipy',\n",
       "  \"   It is easy to verify which result is the true MLE, just need a simple function to calculate log likelihood:\\n\\n\\n\\n    >>> def wb2LL(p, x): #log-likelihood\\n\\n    \\treturn sum(log(stats.weibull_min.pdf(x, p[1], 0., p[0])))\\n\\n    >>> adata=loadtxt('/home/user/stack_data.csv')\\n\\n    >>> wb2LL(array([6.8820748596850905, 1.8553346917584836]), adata)\\n\\n    -8290.1227946678173\\n\\n    >>> wb2LL(array([5.93030013, 1.57463497]), adata)\\n\\n    -8410.3327470347667\\n\\n\\n\\n   The result from `fit` method of `exponweib` and R `fitdistr` (@Warren) is better and has higher log likelihood. It is more likely to be the true MLE. It is not surprising that the result from GAMLSS is different. It is a complete different statistic model: Generalized Additive Model.\\n\\n\\n\\n   Still not convinced? We can draw a 2D confidence limit plot around MLE, see Meeker and Escobar's book for detail). ![Multi-dimensional Confidence Region][1]\\n\\n\\n\\n\\n\\nAgain this verifies that `array([6.8820748596850905, 1.8553346917584836])` is the right answer as loglikelihood is lower that any other point in the parameter space. Note:\\n\\n\\n\\n    >>> log(array([6.8820748596850905, 1.8553346917584836]))\\n\\n    array([ 1.92892018,  0.61806511])\\n\\n\\n\\n   BTW1, MLE fit may not appears to fit the distribution histogram tightly. An easy way to think about MLE is that MLE is the parameter estimate most probable given the observed data. It doesn't need to visually fit the histogram well, that will be something minimizing mean square error. \\n\\n\\n\\n   BTW2, your data appears to be leptokurtic and left-skewed, which means Weibull distribution may not fit your data well. Try, e.g. Gompertz-Logistic, which improves log-likelihood by another about 100.\\n\\n![enter image description here][2]\\n\\n![enter image description here][3]\\n\\n   Cheers!\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/ONDUN.png\\n\\n  [2]: http://i.stack.imgur.com/ZGf0C.png\\n\\n  [3]: http://i.stack.imgur.com/Xc9BM.png\",\n",
       "  '<python><numpy><scipy><distribution><weibull>',\n",
       "  datetime.date(2013, 8, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '20',\n",
       "  '',\n",
       "  '32007.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['77',\n",
       "  '10871143',\n",
       "  'Answer',\n",
       "  'Using ord() to convert letters to ints (Very basic)',\n",
       "  \"The purpose of the  `-65 +1` is mostly the result of a bad optimization try from the original developer. I usually use the following function to convert Excel columns into an integer value:\\n\\n\\n\\n    return reduce(lambda x,y: x*26+ord(y)-ord('A')+1, column.upper(), 0)\\n\\n\\n\\nThe interesting part is `ord(y)-ord('A')+1` that gives you the key of your question. Assuming the column variable contains a valid `A-Z` only Excel column string, the column number is actually the shift of a given char from `A` char plus one. `ord('A')` will give you `65` as a result. The developer replaces `ord('A')` by its final value.\\n\\n\\n\\nThat said, yes it seems to be here an *optimization* to avoid calling `ord`, but it is really obfuscating the code and removing readability  for I think few time gained. If this function is indeed a critical function that is called millions of times in the program, then this is not this code that has to be written to optimize the code - you would instead create a precalculated dictionary with all Excel column name entries in it mapped to their integer value, or something like this that would be very efficient.\\n\\n\\n\\nHere, what has been done is a bad choice in the trade-off of performance against readability and code maintenance ; at least you would have expected a comment explaining `# 65 = ord('A')`, and you wouldn't have asked a question about it here. \\n\\n\\n\\nKey point: keep the code logic, simple, readable, and easy to maintain and don't change it for bad wannabe optimizations.\",\n",
       "  '<python><excel><ord>',\n",
       "  datetime.date(2012, 6, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '11145.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['78',\n",
       "  '14955222',\n",
       "  'Answer',\n",
       "  'How to identify the first occurence of duplicate rows in Python pandas Dataframe',\n",
       "  \"Let's say your dataframe is stored in `df`.\\n\\n\\n\\nYou can use [groupby][1] to get non duplicated rows of your dataframe. Here we have to ignore Column1 that is not part of the data:\\n\\n\\n\\n    df_nodup = df.groupby(by=['Column2', 'Column3', 'Column4']).first()\\n\\n\\n\\nyou can then merge this new dataframe with the original one by using the [merge][2] function:\\n\\n\\n\\n\\n\\n    df = df.merge(df_nodup, left_on=['Column2', 'Column3', 'Column4'], right_index=True, suffixes=('', '_dupindex'))\\n\\n\\n\\nYou can eventually use the _dupindex column merged in the dataframe to make the simple math to add the columns needed:\\n\\n\\n\\n\\n\\n    df['Is_Duplicate'] = df['Column1']!=df['Column1_dupindex']\\n\\n    df['Dup_Index'] = None\\n\\n    df['Dup_Index'] = df['Dup_Index'].where(df['Column1_dupindex']==df['Column1'], df['Column1_dupindex'])\\n\\n    del df['Column1_dupindex']\\n\\n\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/groupby.html\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\\n\\n\",\n",
       "  '<python-2.7><dataframe><pandas>',\n",
       "  datetime.date(2013, 2, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '7948.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['80',\n",
       "  '16485133',\n",
       "  'Answer',\n",
       "  'Is there an efficient way to merge two sorted dataframes in pandas, maintaing sortedness?',\n",
       "  \"If we limit the problem to `a` and `b` having only one column, then I would go through this path:\\n\\n\\n\\n    s = a.merge(b, how='outer', left_index=True, right_index=True)\\n\\n    s.stack().reset_index(level=1, drop=True)\\n\\n\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2013, 5, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2241.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['84',\n",
       "  '18162440',\n",
       "  'Answer',\n",
       "  'Pandas data frame from dictionary',\n",
       "  \"I provide another possibility here using [`pd.stack`][1]:\\n\\n\\n\\n    df = pd.DataFrame(sample)\\n\\n    df = df.T.stack().reset_index()\\n\\n\\n\\n\\n\\n----------\\n\\n**Detailed explanations**\\n\\n\\n\\n\\n\\n    In [24]: df = pd.DataFrame(sample)\\n\\n    \\n\\n    In [25]: df\\n\\n    Out[25]: \\n\\n           user1  user2  user3\\n\\n    item1    2.5    2.5    NaN\\n\\n    item2    3.5    3.0    4.5\\n\\n    item3    3.0    3.5    NaN\\n\\n    item4    3.5    4.0    NaN\\n\\n    item5    2.5    NaN    1.0\\n\\n    item6    3.0    NaN    4.0\\n\\n\\n\\nApplying `stack` will pivot the column axis on a sublevel of the row axis already indexed by `item`. As you want `user` first, let's do the operation on the transposed DataFrame by using `.T`:\\n\\n\\n\\n    In [34]: df = df.T.stack()\\n\\n    \\n\\n    In [35]: df\\n\\n    Out[35]: \\n\\n    user1  item1    2.5\\n\\n           item2    3.5\\n\\n           item3    3.0\\n\\n           item4    3.5\\n\\n           item5    2.5\\n\\n           item6    3.0\\n\\n    user2  item1    2.5\\n\\n           item2    3.0\\n\\n           item3    3.5\\n\\n           item4    4.0\\n\\n    user3  item2    4.5\\n\\n           item5    1.0\\n\\n           item6    4.0\\n\\n    dtype: float64\\n\\n\\n\\nYou expect basic columns and not index, so just reset the index:\\n\\n\\n\\n    In [36]: df = df.reset_index()\\n\\n    \\n\\n    In [37]: df\\n\\n    Out[37]: \\n\\n       level_0 level_1    0\\n\\n    0    user1   item1  2.5\\n\\n    1    user1   item2  3.5\\n\\n    2    user1   item3  3.0\\n\\n    3    user1   item4  3.5\\n\\n    4    user1   item5  2.5\\n\\n    5    user1   item6  3.0\\n\\n    6    user2   item1  2.5\\n\\n    7    user2   item2  3.0\\n\\n    8    user2   item3  3.5\\n\\n    9    user2   item4  4.0\\n\\n    10   user3   item2  4.5\\n\\n    11   user3   item5  1.0\\n\\n    12   user3   item6  4.0\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2013, 8, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '29009.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['85',\n",
       "  '18937309',\n",
       "  'Answer',\n",
       "  'Count distinct words from a Pandas Data Frame',\n",
       "  \"Use a `set` to create the sequence of unique elements.\\n\\n\\n\\nDo some clean-up on `df` to get the strings in lower case and split:\\n\\n\\n\\n    df['text'].str.lower().str.split()\\n\\n    Out[43]: \\n\\n    0             [my, nickname, is, ft.jgt]\\n\\n    1    [someone, is, going, to, my, place]\\n\\n\\n\\nEach list in this column can be passed to `set.update` function to get unique values. Use [`apply`][1] to do so:\\n\\n\\n\\n    results = set()\\n\\n    df['text'].str.lower().str.split().apply(results.update)\\n\\n    print results\\n\\n    \\n\\n    set(['someone', 'ft.jgt', 'my', 'is', 'to', 'going', 'place', 'nickname'])\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html\",\n",
       "  '<python><text><pandas>',\n",
       "  datetime.date(2013, 9, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '39',\n",
       "  '',\n",
       "  '24372.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['86',\n",
       "  '18939630',\n",
       "  'Answer',\n",
       "  'Python: Difference between filter(function, sequence) and map(function, sequence)',\n",
       "  \"`filter()`, as its name suggests, filters the original iterable and retents the items that returns `True` for the function provided to `filter()`.\\n\\n\\n\\n`map()` on the other hand, apply the supplied function to each element of the iterable and return a list of results for each element.\\n\\n\\n\\nFollows the example that you gave, let's compare them:\\n\\n\\n\\n    >>> def f(x): return x % 2 != 0 and x % 3 != 0\\n\\n    >>> range(11)\\n\\n    [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\n\\n    >>> map(f, range(11))  # the ones that returns TRUE are 1, 5 and 7\\n\\n    [False, True, False, False, False, True, False, True, False, False, False]\\n\\n    >>> filter(f, range(11))  # So, filter returns 1, 5 and 7\\n\\n    [1, 5, 7]\",\n",
       "  '<python><functional-programming><map-function><filterfunction>',\n",
       "  datetime.date(2013, 9, 22),\n",
       "  '2018-06-09 19:56:02',\n",
       "  'CT Zhu (2487184)',\n",
       "  '19',\n",
       "  '',\n",
       "  '11021.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['87',\n",
       "  '18939690',\n",
       "  'Answer',\n",
       "  'Reverse a string without using reversed() or [::-1]?',\n",
       "  \"This is a very interesting question, I will like to offer a simple one\\n\\nliner answer:\\n\\n\\n\\n    >>> S='abcdefg'\\n\\n    >>> ''.join(item[1] for item in sorted(enumerate(S), reverse=True))\\n\\n    'gfedcba'\\n\\n\\n\\nBrief explanation:\\n\\n\\n\\n`enumerate()` returns `[(0, 'a'), (1, 'b'), (2, 'c'), (3, 'd'), (4, 'e'), (5, 'f'), (6, 'g')]`. The indices and the values.\\n\\nTo reverse the values, just reverse sort it by `sorted()`.\\n\\nFinally, just put it together back to a `str`\",\n",
       "  '<python><string><function><for-loop><reverse>',\n",
       "  datetime.date(2013, 9, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '156873.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['88',\n",
       "  '18973466',\n",
       "  'Answer',\n",
       "  'os.mkdir(path) returns OSError when directory does not exist',\n",
       "  \"You have a file there with the name `test`. You can't make a directory with that exact same name. \",\n",
       "  '<python><system><mkdir>',\n",
       "  datetime.date(2013, 9, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '34505.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['89',\n",
       "  '18985916',\n",
       "  'Answer',\n",
       "  'error using L-BFGS-B in scipy',\n",
       "  \"BFGS method is one of those method that relies on not only the function value, but also the gradient and Hessian (think of it as first and second derivative if you wish). In your `func1()`, once you have `round()` in it, the gradient is no longer continuous. BFGS method therefore fails right after the 1st iteration (think of as this: BFGS searched around the starting parameter and found the gradient is not changed, so it stopped). Similarly, I would expect other methods requiring gradient fail as BGFS.\\n\\n\\n\\nYou may be able to get it working by precondition or rescaling X. But better yet, you should try gradient free method such as 'Nelder-Mead' or 'Powell'\",\n",
       "  '<python><optimization><scipy>',\n",
       "  datetime.date(2013, 9, 24),\n",
       "  '2013-09-24 20:40:34',\n",
       "  'CT Zhu (2487184)',\n",
       "  '10',\n",
       "  '',\n",
       "  '3670.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['91',\n",
       "  '18347054',\n",
       "  'Answer',\n",
       "  'How to deal with log of zero in R in image.plot?',\n",
       "  \"The except is thrown by `seq()`, which can not take `-inf` as any one of its arguments. You can get exactly the same type of error with the following code:\\n\\n\\n\\n    > seq(-log(0), 0, 50)\\n\\n    Error in seq.default(-log(0), 0, 50) : invalid (to - from)/by in seq(.)\\n\\n\\n\\nTo avoid it, follow @Metrics 's trick. Although I will suggest instead of adding 1.0, add a very small value, such as 1e-22, since your matrix is a matrix of probabilities.\",\n",
       "  '<r>',\n",
       "  datetime.date(2013, 8, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2343.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['92',\n",
       "  '18383630',\n",
       "  'Answer',\n",
       "  'Normalization to bring in the range of [0,1]',\n",
       "  'Finding the range of an array is provided by `numpy` build-in function `numpy.ptp()`, your question can be addresses by:\\n\\n\\n\\n    #First we should filter input_array so that it does not contain NaN or Inf.\\n\\n    input_array=np.array(some_data)\\n\\n    if np.unique(input_array).shape[0]==1:\\n\\n        pass #do thing if the input_array is constant\\n\\n    else:\\n\\n        result_array=(input_array-np.min(input_array))/np.ptp(input_array)\\n\\n    #To extend it to higher dimension, add axis= kwarvg to np.min and np.ptp',\n",
       "  '<python>',\n",
       "  datetime.date(2013, 8, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '9154.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['96',\n",
       "  '19075085',\n",
       "  'Answer',\n",
       "  \"numpy/scipy analog of matlab's fminsearch\",\n",
       "  \"`fminsearch` implements the Nelder-Mead method, see `Matlab` document: http://www.mathworks.com/help/matlab/ref/fminsearch.html. In the reference section.\\n\\n\\n\\nTo find its equivalent in `scipy`, you just need to check the doc strings of the methods provided in `scipy.optimize`. See: http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin.html#scipy.optimize.fmin. `fmin` also implements Nelder-Mead method. \\n\\n\\n\\nThe names do not always translate directly from `matlab` to `scipy` and are sometimes even misleading. For example, Brent's method is implemented as `fminbnd` in `Matlab` but `optimize.brentq` in `scipy`. So, checking the doc strings are always a good idea.\",\n",
       "  '<python><matlab><numpy><scipy>',\n",
       "  datetime.date(2013, 9, 29),\n",
       "  '2017-10-28 20:52:49',\n",
       "  'Simon (470433)',\n",
       "  '8',\n",
       "  '',\n",
       "  '10222.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['98',\n",
       "  '19125498',\n",
       "  'Answer',\n",
       "  'Fast peak-finding and centroiding in python',\n",
       "  'An other approach is to avoid all `sum()`, `meshgrid()` and stuff. Replace everything with straight linear algebra.\\n\\n\\n\\n    >>> def centroid2(data):\\n\\n    \\th,w=data.shape\\n\\n    \\tx=np.arange(h)\\n\\n    \\ty=np.arange(w)\\n\\n    \\tx1=np.ones((1,h))\\n\\n    \\ty1=np.ones((w,1))\\n\\n    \\treturn ((np.dot(np.dot(x1, data), y))/(np.dot(np.dot(x1, data), y1)),\\n\\n        \\t\\t(np.dot(np.dot(x, data), y1))/(np.dot(np.dot(x1, data), y1)))\\n\\n    #be careful, it returns two arrays\\n\\n\\n\\nThis can be expended to higher dimension as well. 60% of speedup compares to `centroid()`\\n\\n',\n",
       "  '<python><image-processing><numpy><matplotlib><scipy>',\n",
       "  datetime.date(2013, 10, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '8869.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['99',\n",
       "  '18746058',\n",
       "  'Answer',\n",
       "  'Variable number of parameters for scipy optimize with L-BFGS-B algorithm',\n",
       "  'If I understand correctly, you are asking for a constrained best fit, such that rather than finding the best `[p0,p1,p2...p10]` for function `func()`, you want to find the best best `[p0, p1, ...p5]` for function `func()` under a condition that `p6=fixed6, p7=fixed7, p8=fixed8...` and so on. \\n\\n\\n\\nTranslate it into `python` code is straight forward if you use `args=(somthing)` in `scipy.optimize.fmin_l_bfgs_b`. Firstly, write a partially fixed function `func_fixed()`\\n\\n\\n\\n    def func_fixed(p_var, p_fixed):\\n\\n        return func(p_var+p_fixed) \\n\\n    # this will only work if both of them are lists. If they are numpy arrays, use hstack, append or similar\\n\\n    \\n\\n    solution = optimize.fmin_l_bfgs_b(func_fixed,x0=guess_parameters,\\\\\\n\\n                                      approx_grad=your_grad,\\\\\\n\\n                                      bounds=your_bounds,\\\\\\n\\n                                      args=(your_fixed_parameters), \\\\ #this is the deal\\n\\n                                      other_things)\\n\\n\\n\\nIt is not necessary to have `func_fixed()`, you can use `lambda`. But it reads much easier this way. ',\n",
       "  '<python><optimization>',\n",
       "  datetime.date(2013, 9, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1356.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['101',\n",
       "  '18966286',\n",
       "  'Answer',\n",
       "  'One-sided Wilcoxon signed-rank test using scipy',\n",
       "  \"P value returned by `scipy.stats.wilcoxon` has nothing to do with the distribution of `x` or `y`, nor the difference between them. It is determined by the Wilcoxon test statistic (W as it in http://en.wikipedia.org/wiki/Wilcoxon_signed-rank_test, or T as in `scipy`), which is assumed to follow a normal distribution. If you check the source (in ~python_directory\\\\site-packages\\\\scipy\\\\stats\\\\morestats.py), you will find the last few lines of `def wilcoxon()`:\\n\\n\\n\\n    se = sqrt(se / 24)\\n\\n    z = (T - mn) / se\\n\\n    prob = 2. * distributions.norm.sf(abs(z))\\n\\n    return T, prob\\n\\n\\n\\nand:\\n\\n\\n\\n    mn = count*(count + 1.) * 0.25\\n\\n    se = count*(count + 1.) * (2. * count + 1.)\\n\\n\\n\\nWhere `count` is the number of non-zero difference between `x` and `y`.\\n\\n\\n\\nSo, to get one-side p value, you just need `prob/2.` or `1-prob/2.`\\n\\n\\n\\nExamples:\\n\\nIn `Python`:\\n\\n\\n\\n    >>> y1=[125,115,130,140,140,115,140,125,140,135]\\n\\n    >>> y2=[110,122,125,120,140,124,123,137,135,145]\\n\\n    >>> ss.wilcoxon(y1, y2)\\n\\n    (18.0, 0.5936305914425295)\\n\\n\\n\\nIn `R`:\\n\\n\\n\\n\\n\\n    > wilcox.test(y1, y2, paired=TRUE, exact=FALSE, correct=FALSE)\\n\\n\\n\\n            Wilcoxon signed rank test\\n\\n    \\n\\n    data:  y1 and y2 \\n\\n    V = 27, p-value = 0.5936\\n\\n    alternative hypothesis: true location shift is not equal to 0 \\n\\n\\n\\n    > wilcox.test(y1, y2, paired=TRUE, exact=FALSE, correct=FALSE, alt='greater')\\n\\n    \\n\\n            Wilcoxon signed rank test\\n\\n\\n\\n    data:  y1 and y2 \\n\\n    V = 27, p-value = 0.2968\\n\\n    alternative hypothesis: true location shift is greater than 0\",\n",
       "  '<python><statistics><scipy>',\n",
       "  datetime.date(2013, 9, 23),\n",
       "  '2013-09-24 14:44:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '13',\n",
       "  '',\n",
       "  '8722.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['102',\n",
       "  '16483240',\n",
       "  'Answer',\n",
       "  'Exception from HRESULT: 0x800A03EC when trying to add formula r1c1 while creating workbook',\n",
       "  'Put the correct number of parenthesis in your Excel formula : you are missing one at the end.',\n",
       "  '<c#><.net><excel-formula><excel-interop><vlookup>',\n",
       "  datetime.date(2013, 5, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2949.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['105',\n",
       "  '18257722',\n",
       "  'Answer',\n",
       "  'Creating a numpy array of 3D coordinates from three 1D arrays',\n",
       "  'For those who had to stay with numpy <1.7.x, here is a simple two-liner solution:\\n\\n\\n\\n    g_p=np.zeros((x_p.size, y_p.size, z_p.size))\\n\\n    array_you_want=array(zip(*[item.flatten() for item in \\\\\\n\\n                                     [g_p+x_p[...,np.newaxis,np.newaxis],\\\\\\n\\n\\t\\t\\t\\t                      g_p+y_p[...,np.newaxis],\\\\\\n\\n\\t\\t\\t\\t\\t                  g_p+z_p]]))\\n\\nVery easy to expand to even higher dimenision as well.\\n\\nCheers!\\n\\n',\n",
       "  '<python><arrays><performance><numpy>',\n",
       "  datetime.date(2013, 8, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '14393.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['106',\n",
       "  '18363358',\n",
       "  'Answer',\n",
       "  'iteratively calling pandas datareader',\n",
       "  'stockName is a variable that loops on the stock ticker list. It contains the ticker string. When you assign the DataFrame to it, this DataFrame is lost at the next turn of the `for` loop.\\n\\n\\n\\nCreate another variable, a `dict` for instance, where you assign the stock data into:\\n\\n\\n\\n\\n\\n    stockdata = {}\\n\\n    for stockName in stocks:\\n\\n        stockdata[stockName] = DataReader(stockName,  \"yahoo\", datetime(2013,1,1), datetime(2013,8,1))',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2013, 8, 21),\n",
       "  '2013-08-21 16:54:14',\n",
       "  'Viktor Kerkez (2199958)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2536.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['107',\n",
       "  '18456530',\n",
       "  'Answer',\n",
       "  'python convert string to integer array',\n",
       "  '    >>> import time\\n\\n    >>> t1=\"00:39:59.946000\"\\n\\n    >>> t2=time.strptime(t1.split(\\'.\\')[0]+\\':2013\\', \\'%H:%M:%S:%Y\\') #You probably want year as well.\\n\\n    >>> time.mktime(t2) #Notice that the decimal parts are gone, we need to add it back\\n\\n    1357018799.0\\n\\n    >>> time.mktime(t2)+float(\\'.\\'+t1.split(\\'.\\')[1]) #(add ms)\\n\\n    1357018799.946\\n\\n\\n\\n    #put things together:\\n\\n    >>> def str_time_to_float(in_str):\\n\\n    \\treturn time.mktime(time.strptime(in_str.split(\\'.\\')[0]+\\':2013\\', \\'%H:%M:%S:%Y\\'))\\\\\\n\\n    \\t       ++float(\\'.\\'+in_str.split(\\'.\\')[1])\\n\\n    >>> str_time_to_float(\"01:39:59.892652\")\\n\\n    1357022399.892652',\n",
       "  '<python><arrays><numpy><string-conversion>',\n",
       "  datetime.date(2013, 8, 27),\n",
       "  '2013-08-27 03:52:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '3190.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['112',\n",
       "  '18425392',\n",
       "  'Answer',\n",
       "  \"How to set Python's default version to 3.x on OS X?\",\n",
       "  \"Go to 'Applications', enter 'Python' folder, there should be a bash script called 'Update Shell Profile.command' or similar. Run that script and it should do it.\\n\\n\\n\\nUpdate: It looks like you should not update it: <https://stackoverflow.com/questions/5846167/how-to-change-default-python-version>\",\n",
       "  '<python><python-3.x><macos><install>',\n",
       "  datetime.date(2013, 8, 25),\n",
       "  '2018-07-09 13:47:17',\n",
       "  'URL Rewriter Bot (n/a), user3071284 (3071284)',\n",
       "  '8',\n",
       "  '',\n",
       "  '288453.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['114',\n",
       "  '18701382',\n",
       "  'Answer',\n",
       "  'printing sub-array in numpy as Matlab does',\n",
       "  \"Does this give what you want?\\n\\n\\n\\n    >>> for item in a[:,0:20].T:\\n\\n    \\tprint '\\\\t'.join(map(str,item.tolist()))\\n\\n\\n\\nOr this?\\n\\n\\n\\n    >>> for item in a[:,0:20]:\\n\\n    \\tprint '\\\\t'.join(map(str,item.tolist()))\",\n",
       "  '<python><matlab><numpy>',\n",
       "  datetime.date(2013, 9, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1510.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['115',\n",
       "  '18724453',\n",
       "  'Answer',\n",
       "  'Excel-like user input in python GUI applications',\n",
       "  \"Yeah. Copy and paste, select and set color, adjust col and row width/height. You name it, you get it. Haven't done it in many years and can't remember all the tricks. But it is quite easy to do in Boa Constructor, http://boa-constructor.sourceforge.net/. Also see this very helpful blog: http://www.blog.pythonlibrary.org/2010/03/18/wxpython-an-introduction-to-grids/. A real example that I wrote a few years back: http://sourceforge.net/projects/deday/\",\n",
       "  '<python><user-interface><input><spreadsheet>',\n",
       "  datetime.date(2013, 9, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '3730.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['116',\n",
       "  '18793176',\n",
       "  'Answer',\n",
       "  'Bootstrap method and confidence interval',\n",
       "  'Your `curve_fit` already gave you the covariance matrix, that is the `pout`. The 95% confidence limit for your ith parameter is: `pc[i]-1.95596*sqrt(pout[i,i])` and `pc[i]+1.95596*sqrt(pout[i,i])`. 1.95596 is the x, such that cumulative distribution function of the standard normal distribution F(x)=0.975. You can get confidence interval of other levels by using `scipy.stats.norm.ppf`. See wiki: http://en.wikipedia.org/wiki/1.96\\n\\n\\n\\nBootstrap is not going to give you the same (or, sometimes, even close) answers each time you run it. For your specific function, the very few early data points have very big influence on the fit https://stackoverflow.com/questions/18619131/solve-equation-with-a-set-of-points. I am not sure whether bootstrap is the way to go as if the very few early data points are not sampled, the fit will be very different from the fit of your original data. That also explains why your bootstrap intervals are so difference form each other. ',\n",
       "  '<python><numpy><statistics>',\n",
       "  datetime.date(2013, 9, 13),\n",
       "  '2017-05-23 12:33:14',\n",
       "  'CT Zhu (2487184), URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1757.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['117',\n",
       "  '19131156',\n",
       "  'Answer',\n",
       "  'matplotlib: how to plot concentric circles at a given set of radii',\n",
       "  \"easy, use it to make shooting targets all the time.:\\n\\n\\n\\n    ax.plot(np.linspace(0, 2*np.pi, 100), np.ones(100)*5, color='r', linestyle='-')\\n\\n\\n\\nJust think of how you define a circle in a polar axis? Need two things, angle and radius. Those are `np.linspace(0, 2*np.pi, 100)` and `np.ones(100)*5` here. If you just need a arc, change the first argument to something less than 0 to 2pi. And change the 2nd argument accordingly. \\n\\n\\n\\n![enter image description here][1]\\n\\n  [1]: http://i.stack.imgur.com/WRDLQ.png\\n\\n\\n\\nThere are other ways to do this. `plot()` creates `.lines.Line2D object` objects, if you want `.collections.PathCollection object` instead of Line2D:\\n\\n\\n\\n    ax.scatter(1, 0, s=100000, facecolors='none')\\n\\n\\n\\nOr you want to make `patch`es:\\n\\n\\n\\n    ax.bar(0, 5, 2*np.pi, bottom=0.0, facecolor='None') #need to modified the edge lines or won't look right\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2013, 10, 2),\n",
       "  '2013-10-02 08:13:28',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '2520.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['118',\n",
       "  '19228714',\n",
       "  'Answer',\n",
       "  'Find ordered vector in numpy array',\n",
       "  \"Try:\\n\\n\\n\\n    e[np.all((e-np.array([1,2]))==0, axis=2)]\\n\\n\\n\\nBrief explanation. `e-np.array([1,2])` returns `[0,0]` where it is `[1,2]` in array `e`. `np.all(..., axis=2` returns the Boolean array: `True` if `[0,0]` `False` otherwise (so things such as `[1,1]` will become False). Finally, just slice it from e. \\n\\n\\n\\nTo get the index of `[1,2]`'s (there may be multiple sub vector `[1,2]`):\\n\\n\\n\\n    np.argwhere(np.all((e-array([1,2]))==0, axis=2))\",\n",
       "  '<python><arrays><vector><numpy>',\n",
       "  datetime.date(2013, 10, 7),\n",
       "  '2013-10-08 14:41:03',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1803.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['120',\n",
       "  '19505247',\n",
       "  'Answer',\n",
       "  'matplotlib pyplot table with non-ascii data?',\n",
       "  \"It may be (slightly) platform dependent, but I prefer to use `unicode` for Chinese and other languages. One other thing is that you need to make sure is `matplotlib` must get the necessary font. You can do it anywhere you need a `text` except sometimes not with `mathtext`.\\n\\n\\n\\n\\t# -*- coding: utf-8 -*-\\n\\n\\timport matplotlib.pyplot as plt\\n\\n\\timport matplotlib\\n\\n\\tzhfont1 = matplotlib.font_manager.FontProperties(fname='/Library/Fonts/Kai.dfont') #I am on OSX.\\n\\n\\ts=u'\\\\u54c8\\\\u54c8' #Need the unicode for your Chinese Char.\\n\\n\\tplt.text(0.5,0.5,s,fontproperties=zhfont1, size=50) #example: plt.text()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/DFdHI.png\",\n",
       "  '<encoding><utf-8><matplotlib>',\n",
       "  datetime.date(2013, 10, 21),\n",
       "  '2013-10-22 05:01:52',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1211.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['121',\n",
       "  '18753838',\n",
       "  'Answer',\n",
       "  'Fitting empirical distribution to theoretical ones with Scipy (Python)?',\n",
       "  '`fit()` method mentioned by @Saullo Castro provides maximum likelihood estimates (MLE).  The best distribution for your data is the one give you the highest can be determined by several different ways: such as\\n\\n\\n\\n1, the one that gives you the highest log likelihood.\\n\\n\\n\\n2, the one that gives you the smallest AIC, BIC or BICc values (see wiki: http://en.wikipedia.org/wiki/Akaike_information_criterion, basically can be viewed as log likelihood adjusted for number of parameters, as distribution with more parameters are expected to fit better)\\n\\n\\n\\n3, the one that maximize the Bayesian posterior probability. (see wiki: http://en.wikipedia.org/wiki/Posterior_probability)\\n\\n\\n\\nOf course, if you already have a distribution that should describe you data (based on the theories in your particular field) and want to stick to that, you will skip the step of identifying the best fit distribution.\\n\\n\\n\\n`scipy` does not come with a function to calculate log likelihood (although MLE method is provided), but hard code one is easy: see https://stackoverflow.com/questions/18431629/is-the-build-in-probability-density-functions-of-scipy-stat-distributions-slow',\n",
       "  '<python><numpy><statistics><scipy><distribution>',\n",
       "  datetime.date(2013, 9, 12),\n",
       "  '2017-05-23 10:31:09',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '9',\n",
       "  '',\n",
       "  '69268.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['122',\n",
       "  '18768602',\n",
       "  'Answer',\n",
       "  'How do I use a minimization function in scipy with constraints',\n",
       "  \"Check `.minimize` docstring:\\n\\n\\n\\n    scipy.optimize.minimize(fun, x0, args=(), method='BFGS', jac=None, hess=None, hessp=None, \\\\\\n\\n                  bounds=None, constraints=(), tol=None, callback=None, options=None)\\n\\n\\n\\nWhat matters the most in your case will be the `bounds`. When you want to constrain your parameter in [0,1] (or (0,1)?) You need to define it for each variable, such as:\\n\\n\\n\\n    bounds=((0,1), (0,1).....)\\n\\n\\n\\nNow, the other part, `sum(x)==1`. There may be more elegant ways to do it, but consider this: instead of minimizing `f(x)`, you minimize `h=lambda x: f(x)+g(x)`, a new function essential `f(x)+g(x)` where `g(x)` is a function reaches it minimum when `sum(x)=1`. Such as `g=lambda x: (sum(x)-1)**2`.\\n\\n\\n\\nThe minimum of `h(x)` is reached when both `f(x)` and `g(x)` are at their minimum. Sort of a case of Lagrange multiplier method http://en.wikipedia.org/wiki/Lagrange_multiplier\",\n",
       "  '<python><optimization><numpy><scipy>',\n",
       "  datetime.date(2013, 9, 12),\n",
       "  '2013-09-12 16:35:43',\n",
       "  'CT Zhu (2487184)',\n",
       "  '7',\n",
       "  '',\n",
       "  '28527.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['124',\n",
       "  '18776566',\n",
       "  'Answer',\n",
       "  'Replacing element in list without list comprehension, slicing or using [ ]s',\n",
       "  'If `enumerate` is not allowed, you can also use a `while` loop.\\n\\n\\n\\n    >>> def replace(L_in, old_v, new_v):\\n\\n        \\twhile old_v in L_in:\\n\\n\\t\\t        idx=L_in.index(old_v)\\n\\n\\t\\t        L_in.pop(idx)\\n\\n\\t\\t        L_in.insert(idx, new_v)\\n\\n\\n\\n\\t\\t\\n\\n    >>> L = [3, 1, 4, 1, 5, 9]\\n\\n    >>> replace(L, 1, 7)\\n\\n    >>> L\\n\\n    [3, 7, 4, 7, 5, 9]',\n",
       "  '<python><list><replace><list-comprehension>',\n",
       "  datetime.date(2013, 9, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '52269.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['127',\n",
       "  '18891334',\n",
       "  'Answer',\n",
       "  'Create dummies from column with multiple values in pandas',\n",
       "  \"You can generate the dummies dataframe with your raw data, isolate the columns that contains a given atom, and then store the result matches back to the atom column.\\n\\n\\n\\n\\n\\n    df\\n\\n    Out[28]: \\n\\n      label\\n\\n    0     A\\n\\n    1     B\\n\\n    2     C\\n\\n    3     D\\n\\n    4   A*C\\n\\n    5   C*D\\n\\n    \\n\\n    dummies = pd.get_dummies(df['label'])\\n\\n    \\n\\n    atom_col = [c for c in dummies.columns if '*' not in c]\\n\\n    \\n\\n    for col in atom_col:\\n\\n        ...:     df[col] = dummies[[c for c in dummies.columns if col in c]].sum(axis=1)\\n\\n        ...:     \\n\\n    \\n\\n    df\\n\\n    Out[32]: \\n\\n      label  A  B  C  D\\n\\n    0     A  1  0  0  0\\n\\n    1     B  0  1  0  0\\n\\n    2     C  0  0  1  0\\n\\n    3     D  0  0  0  1\\n\\n    4   A*C  1  0  1  0\\n\\n    5   C*D  0  0  1  1\\n\\n\",\n",
       "  '<python><pandas><dummy-data><categorical-data>',\n",
       "  datetime.date(2013, 9, 19),\n",
       "  '2013-09-19 17:46:45',\n",
       "  'Boud (624829)',\n",
       "  '4',\n",
       "  '',\n",
       "  '20546.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['129',\n",
       "  '19301969',\n",
       "  'Answer',\n",
       "  'How to create pivot with totals (margins) in Pandas?',\n",
       "  \"I can reproduce your issue. It sounds like a bug. At least I found that reassigning the column names workaround the issue:\\n\\n\\n\\n    df.columns = ['rows', 'columns', 'values']\\n\\n    \\n\\n    pd.pivot_table(\\n\\n        ...:     data=df,\\n\\n        ...:     rows='rows',\\n\\n        ...:     cols='columns',\\n\\n        ...:     values='values',\\n\\n        ...:     margins=True)\\n\\n    Out[34]: \\n\\n    columns                     a    b  All\\n\\n    rows                                   \\n\\n    2013-01-01 00:00:00  0.000000  NaN    0\\n\\n    2013-01-02 00:00:00       NaN  1.0    1\\n\\n    2013-01-03 00:00:00  2.000000  NaN    2\\n\\n    2013-01-04 00:00:00  3.000000  NaN    3\\n\\n    2013-01-05 00:00:00       NaN  4.0    4\\n\\n    All                  1.666667  2.5    2\\n\\n\\n\\n\",\n",
       "  '<python><pandas><pivot>',\n",
       "  datetime.date(2013, 10, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '3120.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['132',\n",
       "  '18773187',\n",
       "  'Answer',\n",
       "  'Python 3+ - for in range loop, multiply, product, list NOT using mul or lambda or for in loop',\n",
       "  'Using `math` library allowed? That will be easy (doing it in `numpy` would be sort-of cheating):\\n\\n\\n\\n    >>> from math import *\\n\\n    >>> def prod(L):\\n\\n            if 0 in L:\\n\\n                return 0.\\n\\n            #elif len(L)==0:\\n\\n            #    return 0.\\n\\n            elif sum([item<0 for item in L])%2==0\\n\\n                return exp(sum(map(log, L)))\\n\\n            else:\\n\\n                return -exp(sum(map(log, map(abs, L))))',\n",
       "  '<python><list><for-loop><product>',\n",
       "  datetime.date(2013, 9, 12),\n",
       "  '2013-09-12 20:14:15',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '2106.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['135',\n",
       "  '18946103',\n",
       "  'Answer',\n",
       "  'Modify tick label text',\n",
       "  \"In newer versions of `matplotlib`, if you do not set the tick labels with a bunch of `str` values, they are `''` by default (and when the plot is draw the labels are simply the ticks values). Knowing that, to get your desired output would require something like this:\\n\\n\\n\\n    >>> from pylab import *\\n\\n    >>> axes = figure().add_subplot(111)\\n\\n    >>> a=axes.get_xticks().tolist()\\n\\n    >>> a[1]='change'\\n\\n    >>> axes.set_xticklabels(a)\\n\\n    [<matplotlib.text.Text object at 0x539aa50>, <matplotlib.text.Text object at 0x53a0c90>, \\n\\n    <matplotlib.text.Text object at 0x53a73d0>, <matplotlib.text.Text object at 0x53a7a50>, \\n\\n    <matplotlib.text.Text object at 0x53aa110>, <matplotlib.text.Text object at 0x53aa790>]\\n\\n    >>> plt.show()\\n\\n\\n\\nand the result:\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/YiGb5.png\\n\\n\\n\\nand now if you check the `_xticklabels`, they are no longer a bunch of `''`.\\n\\n\\n\\n    >>> [item.get_text() for item in axes.get_xticklabels()]\\n\\n    ['0.0', 'change', '1.0', '1.5', '2.0']\\n\\n\\n\\nIt works in the versions from `1.1.1rc1` to the current version `2.0`.\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2013, 9, 22),\n",
       "  '2016-09-11 02:15:17',\n",
       "  'Vural Acar (1524622), CT Zhu (2487184)',\n",
       "  '73',\n",
       "  '',\n",
       "  '261743.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['136',\n",
       "  '18363875',\n",
       "  'Answer',\n",
       "  'python array intersection efficiently',\n",
       "  '`numpy` `serachsorted()`, `argsort()`, and `intersect1d()` are possible alternatives and can be quite fast. This example should also take care of non-unique first element issue.\\n\\n\\n\\n    >>> import numpy as np\\n\\n    >>> a=np.array([[125, 1], [193, 1], [288, 23]])\\n\\n    >>> b=np.array([[108, 1], [288, 1], [193, 11]])\\n\\n    >>> aT=a.T\\n\\n    >>> bT=b.T\\n\\n    >>> aS=aT[0][np.argsort(aT[0])]\\n\\n    >>> bS=bT[0][np.argsort(bT[0])]\\n\\n    >>> i=np.intersect1d(aT[0], bT[0])\\n\\n    >>> cT=np.hstack((aT[:,np.searchsorted(aS, i)], bT[:,np.searchsorted(bS, i)]))\\n\\n    >>> [[item,np.sum(cT[1,np.argwhere(cT[0]==item).flatten()])] for item in i]\\n\\n    [[193, 12], [288, 24]] #not quite happy about this, can someone comes up with a vectorized way of doing it?\\n\\n',\n",
       "  '<python>',\n",
       "  datetime.date(2013, 8, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1229.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['137',\n",
       "  '18413494',\n",
       "  'Answer',\n",
       "  'function to get number of columns in a NumPy array that returns 1 if it is a 1D array',\n",
       "  \"By default, to iterate a `np.array` means to iterate over the rows. If you have to iterate over columns, just iterate through the transposed array:\\n\\n\\n\\n    >>> a2=array(range(12)).reshape((3,4))\\n\\n    >>> for col in a2.T:\\n\\n    \\tprint col\\n\\n\\t\\n\\n    [0 4 8]\\n\\n    [1 5 9]\\n\\n    [ 2  6 10]\\n\\n    [ 3  7 11]\\n\\n\\n\\nWhat's the intended behavior of an array `array([1,2,3])`, it is treated as having one column or having 3 cols? It is confusing that you mentioned that the arrays are all 3XN arrays, which means this should be the intended behavior, as it should be treated as having just 1 column:\\n\\n\\n\\n    >>> a1=array(range(3))\\n\\n    >>> for col in a1.reshape((3,-1)).T:\\n\\n\\tprint col\\n\\n\\t\\n\\n    [0 1 2]\\n\\n\\n\\nSo, a general solution: `for col in your_array.reshape((3,-1)).T: #do something`\\n\\n\",\n",
       "  '<python><numpy><shape>',\n",
       "  datetime.date(2013, 8, 23),\n",
       "  '2013-08-24 00:00:13',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '3751.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['139',\n",
       "  '18434126',\n",
       "  'Answer',\n",
       "  'how to create similarity matrix in numpy python?',\n",
       "  'Technically, this is not a programming problem but a math problem. But I think you better off using variance-covariance matrix. Or correlation matrix, if the scale of the values are very different, say, instead of having:\\n\\n\\n\\n    >>> x\\n\\n    array([[5, 3, 0],\\n\\n           [3, 0, 5],\\n\\n           [5, 5, 0],\\n\\n           [1, 1, 7]])\\n\\n\\n\\nYou have:\\n\\n\\n\\n    >>> x\\n\\n    array([[5, 300, 0],\\n\\n           [3, 0, 5],\\n\\n           [5, 500, 0],\\n\\n           [1, 100, 7]])\\n\\n\\n\\nTo get a variance-cov matrix:\\n\\n\\n\\n    >>> np.cov(x)\\n\\n    array([[  6.33333333,  -3.16666667,   6.66666667,  -8.        ],\\n\\n           [ -3.16666667,   6.33333333,  -5.83333333,   7.        ],\\n\\n           [  6.66666667,  -5.83333333,   8.33333333, -10.        ],\\n\\n           [ -8.        ,   7.        , -10.        ,  12.        ]])\\n\\n\\n\\nOr the correlation matrix:\\n\\n\\n\\n    >>> np.corrcoef(x)\\n\\n    array([[ 1.        , -0.5       ,  0.91766294, -0.91766294],\\n\\n           [-0.5       ,  1.        , -0.80295507,  0.80295507],\\n\\n           [ 0.91766294, -0.80295507,  1.        , -1.        ],\\n\\n           [-0.91766294,  0.80295507, -1.        ,  1.        ]])\\n\\n\\n\\nThis is the way to look at it, the diagonal cell, i.e., `(0,0)` cell, is the correlation of your 1st vector in X to it self, so it is 1. The other cells, i.e, `(0,1)` cell, is the correlation between the 1st and 2nd vector in X. They are negatively correlated. Or similarly, the 1st and 3rd cell are positively correlated.\\n\\n\\n\\ncovariance matrix or correlation matrix avoid the zero problem pointed out by @Akavall.',\n",
       "  '<python><numpy><matrix><machine-learning>',\n",
       "  datetime.date(2013, 8, 25),\n",
       "  '2013-08-25 22:01:33',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '7142.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['140',\n",
       "  '18973430',\n",
       "  'Answer',\n",
       "  'Setting Different Bar color in matplotlib Python',\n",
       "  \"Simple, just use `.set_color`\\n\\n\\n\\n    >>> barlist=plt.bar([1,2,3,4], [1,2,3,4])\\n\\n    >>> barlist[0].set_color('r')\\n\\n    >>> plt.show()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/SYCFW.png\\n\\n\\n\\nFor your new question, not much harder either, just need to find the bar from your axis, an example:\\n\\n\\n\\n    >>> f=plt.figure()\\n\\n    >>> ax=f.add_subplot(1,1,1)\\n\\n    >>> ax.bar([1,2,3,4], [1,2,3,4])\\n\\n    <Container object of 4 artists>\\n\\n    >>> ax.get_children()\\n\\n    [<matplotlib.axis.XAxis object at 0x6529850>, \\n\\n     <matplotlib.axis.YAxis object at 0x78460d0>,  \\n\\n     <matplotlib.patches.Rectangle object at 0x733cc50>, \\n\\n     <matplotlib.patches.Rectangle object at 0x733cdd0>, \\n\\n     <matplotlib.patches.Rectangle object at 0x777f290>, \\n\\n     <matplotlib.patches.Rectangle object at 0x777f710>, \\n\\n     <matplotlib.text.Text object at 0x7836450>, \\n\\n     <matplotlib.patches.Rectangle object at 0x7836390>, \\n\\n     <matplotlib.spines.Spine object at 0x6529950>, \\n\\n     <matplotlib.spines.Spine object at 0x69aef50>,\\n\\n     <matplotlib.spines.Spine object at 0x69ae310>, \\n\\n     <matplotlib.spines.Spine object at 0x69aea50>]\\n\\n    >>> ax.get_children()[2].set_color('r') \\n\\n     #You can also try to locate the first patches.Rectangle object \\n\\n     #instead of direct calling the index.\\n\\n\\n\\nIf you have a complex plot and want to identify the bars first, add those:\\n\\n\\n\\n    >>> import matplotlib\\n\\n    >>> childrenLS=ax.get_children()\\n\\n    >>> barlist=filter(lambda x: isinstance(x, matplotlib.patches.Rectangle), childrenLS)\\n\\n    [<matplotlib.patches.Rectangle object at 0x3103650>, \\n\\n     <matplotlib.patches.Rectangle object at 0x3103810>, \\n\\n     <matplotlib.patches.Rectangle object at 0x3129850>, \\n\\n     <matplotlib.patches.Rectangle object at 0x3129cd0>, \\n\\n     <matplotlib.patches.Rectangle object at 0x3112ad0>]\\n\\n\\n\\n\",\n",
       "  '<python><matplotlib><pandas><bar-chart>',\n",
       "  datetime.date(2013, 9, 24),\n",
       "  '2016-03-11 18:02:12',\n",
       "  'CT Zhu (2487184)',\n",
       "  '76',\n",
       "  '',\n",
       "  '92256.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['142',\n",
       "  '19075470',\n",
       "  'Answer',\n",
       "  'Efficiency with very large numpy arrays',\n",
       "  \"It is good to know that the smallest positive number will never show up in the end of rows. \\n\\n\\n\\nIn `samplez` there are 1 million unique values but in `zfit`, each row can only have 500 unique values at most. The entire `zfit` can have as much as 50 million unique values. The algorithm can be greatly sped up, if the number of 'finding the smallest positive number > each_element_in_samplez' calculation can be greatly reduced. Doing all 5e13 comparisons are probably an overkill and careful planing will be able to get rid of a large proportion of it. That will heavy depend on your actual underlying mathematics. \\n\\n\\n\\nWithout knowing it, there are still some small things can be done. 1, there are not so many of possible `(e-d)` so that can be taken out of the loop. 2, The loop can be eliminated by `map`. These two small fix, on my machine, result in about 22% speed-up.\\n\\n\\n\\n    def function_map(samplez, zfit):\\n\\n    \\tdiff=zfit[:,:-1]-zfit[:,1:]\\n\\n    \\tdef _fuc1(x):\\n\\n    \\t\\ta = x-zfit\\n\\n    \\t\\tmask = np.ma.masked_array(a)\\n\\n    \\t\\tmask[a <= 0] = np.ma.masked\\n\\n    \\t\\tindex = mask.argmin(axis=1)\\n\\n    \\t\\td = zfit[:,index]\\n\\n    \\t\\tf = (x-d)/diff[:,index] #constrain: smallest value never at the very end.\\n\\n    \\t\\treturn (index, f)\\n\\n    \\tresult=map(_fuc1, samplez)\\n\\n    \\treturn (np.array([item[1] for item in result]),\\n\\n    \\t       np.array([item[0] for item in result]))\\n\\n\\n\\nNext: `masked_array` can be avoided completely (which should bring significant improvement). `samplez` needs to be sorted as well.\\n\\n\\n\\n    >>> x1=arange(50)\\n\\n    >>> x2=random.random(size=(20, 10))*120\\n\\n    >>> x2=sort(x2, axis=1) #just to make sure the last elements of each col > largest val in x1\\n\\n    >>> x3=x2*1\\n\\n    >>> f1=lambda: function_map2(x1,x3)\\n\\n    >>> f0=lambda: function_map(x1, x2)\\n\\n    >>> def function_map2(samplez, zfit):\\n\\n        _diff=diff(zfit, axis=1)\\n\\n        _zfit=zfit*1\\n\\n        def _fuc1(x):\\n\\n            _zfit[_zfit<x]=(+inf)\\n\\n            index = nanargmin(zfit, axis=1)\\n\\n            d = zfit[:,index]\\n\\n            f = (x-d)/_diff[:,index] #constrain: smallest value never at the very end.\\n\\n            return (index, f)\\n\\n        result=map(_fuc1, samplez)\\n\\n        return (np.array([item[1] for item in result]),\\n\\n               np.array([item[0] for item in result]))\\n\\n    \\n\\n    >>> import timeit\\n\\n    >>> t1=timeit.Timer('f1()', 'from __main__ import f1')\\n\\n    >>> t0=timeit.Timer('f0()', 'from __main__ import f0')\\n\\n    >>> t0.timeit(5)\\n\\n    0.09083795547485352\\n\\n    >>> t1.timeit(5)\\n\\n    0.05301499366760254\\n\\n    >>> t0.timeit(50)\\n\\n    0.8838210105895996\\n\\n    >>> t1.timeit(50)\\n\\n    0.5063929557800293\\n\\n    >>> t0.timeit(500)\\n\\n    8.900799036026001\\n\\n    >>> t1.timeit(500)\\n\\n    4.614129018783569\\n\\n\\n\\nSo, that is another 50% speed-up. \\n\\n\\n\\n`masked_array` is avoided and that saves some RAM. Can't think of anything else to reduce RAM usage. It may be necessary to process `samplez` in parts. And also, dependents on the data and the required precision, if you can use `float16` or `float32` instead of the default `float64` that can save you a lot of RAM. \",\n",
       "  '<python><arrays><numpy><processing-efficiency><large-data>',\n",
       "  datetime.date(2013, 9, 29),\n",
       "  '2013-10-01 04:36:08',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1178.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['143',\n",
       "  '19528211',\n",
       "  'Answer',\n",
       "  'How do I use the \"survival\" package and Surv function in R with left-truncated data?',\n",
       "  \"See if these make better sense:\\n\\n\\n\\n    > clps <- c(0,0,1,0,1,1,1,0) #censor flag\\n\\n    > surv.obj <- Surv(rep(0, length(clps)), Year1-Year0+1, clps)\\n\\n    > surv.obj #Is this what you want?\\n\\n    [1] (0, 2+] (0, 3+] (0, 4 ] (0, 5+] (0,12 ] (0, 6 ] (0, 7 ] (0, 9+]\\n\\n    > survRzt <- survfit(surv.obj~1)\\n\\n    > plot(survRzt)\\n\\n![enter image description here][1]\\n\\n\\n\\nMy understanding is that you are trying to analyze the duration between being classified as deficient to the eventual failure. For a 'left truncated' (see more of that in my reply) data, say the 1st bridge, even it only spend 2 year in that duration, it in fact may have stayed for more than 2 year (`2+`) as you aren't  able to back date prior to 1992. To make that reflected in the `surv` object, instead of putting a `1` flag to it, I put a `0`.\\n\\n\\n\\nFor the other data points, such as the 3rd bridge. The length of duration is 4 years and we known it is exactly 4 years. It should get a censor flag of `1`. \\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/5wok9.png\",\n",
       "  '<r><survival-analysis>',\n",
       "  datetime.date(2013, 10, 22),\n",
       "  '2013-10-23 03:16:14',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2128.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['145',\n",
       "  '19907632',\n",
       "  'Answer',\n",
       "  'getting a default value from pandas dataframe when a key is not present',\n",
       "  '`ix` index access and `mean` function handle this for you. Fetch the two tuples from `df.ix` and apply the mean function to it: non existing keys are returned as nan values, and mean ignores nan values by default:\\n\\n\\n\\n    In [102]: df\\n\\n    Out[102]: \\n\\n       (26, 22)  (10, 48)  (48, 42)  (48, 10)  (42, 48)\\n\\n    a       311       NaN       724       879        42\\n\\n    \\n\\n    In [103]: df.ix[:,[(10, 48), (48, 10)]].mean(axis=1)\\n\\n    Out[103]: \\n\\n    a    879\\n\\n    dtype: float64\\n\\n    \\n\\n    In [104]: df.ix[:,[(42, 48), (48, 42)]].mean(axis=1)\\n\\n    Out[104]: \\n\\n    a    383\\n\\n    dtype: float64\\n\\n    \\n\\n    In [105]: df.ix[:,[(26, 22), (22, 26)]].mean(axis=1)\\n\\n    Out[105]: \\n\\n    a    311\\n\\n    dtype: float64\\n\\n\\n\\n',\n",
       "  '<pandas>',\n",
       "  datetime.date(2013, 11, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1030.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['146',\n",
       "  '20256379',\n",
       "  'Answer',\n",
       "  'Numpy cumulative sum for csv rows gives type error',\n",
       "  'Looks like some of your data is not the right `dtype`, try this (converting it to `float`), does it work or gives another `TypeError`?\\n\\n\\n\\n    f_column = numpy.cumsum(numpy.asarray(row[1], dtype=float))',\n",
       "  '<python><csv><numpy><typeerror><cumsum>',\n",
       "  datetime.date(2013, 11, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1534.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['148',\n",
       "  '19010237',\n",
       "  'Answer',\n",
       "  'how to use scipy.stats.kstest/basic questions about Kolmogorov–Smirnov test',\n",
       "  \"Q2: look at this, I have a array called `x1`\\n\\n\\n\\n    >>> stats.kstest(x1, 'norm')\\n\\n    (0.50018855199491585, 0.0)\\n\\n    >>> stats.kstest(x1, stats.norm.cdf)\\n\\n    (0.50018855199491585, 0.0)\\n\\n    >>> stats.kstest(x1, stats.norm.cdf, args=(0,))\\n\\n    (0.50018855199491585, 0.0)\\n\\n    >>> stats.kstest(x1, stats.norm.cdf, args=(2,))\\n\\n    (0.84134903906580316, 0.0)\\n\\n    >>> stats.kstest(x1, 'norm', args=(2,))\\n\\n    (0.84134903906580316, 0.0)\\n\\n\\n\\nIf you pass the name of distribution, i.e., `'norm'`, what actually get passed to `kstest` is the standard distribution `cdf`. By standard, it means for normal distribution having mean==0 and sigma=1.\\n\\nIf you don't want the standard `cdf`, you can pass additional parameters to `cdf` using `args=()`. In this case I only passed the mean. That is, we testing the difference between `x1` and a normal distribution with mean==2 and sigma=1.\\n\\n\\n\\nQ3: The short answer is, yes. But, why reinventing the wheel? If you want to know how it is implemented, just check the source code. It is in `your_package_folder\\\\scipy\\\\stats\\\\stats.py`, line 3292.\",\n",
       "  '<python><statistics><scipy>',\n",
       "  datetime.date(2013, 9, 25),\n",
       "  '2013-09-25 16:29:32',\n",
       "  'CT Zhu (2487184)',\n",
       "  '7',\n",
       "  '',\n",
       "  '5536.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['151',\n",
       "  '19271847',\n",
       "  'Answer',\n",
       "  'Python Pandas: Resolving \"List Object has no Attribute \\'Loc\\'\"',\n",
       "  'The traceback indicates to you that df is a `list` and not a `DataFrame` as expected in your line of code.\\n\\n\\n\\nIt means that between `df = pd.read_csv(\"test.csv\")` and `df.loc[df.ID == 103, [\\'fname\\', \\'lname\\']] = \\'Michael\\', \\'Johnson\\'` you have other lines of codes that assigns a list object to `df`. Review that piece of code to find your bug',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2013, 10, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '8869.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['153',\n",
       "  '18899583',\n",
       "  'Answer',\n",
       "  'Python: How to get local maxima values from 1D-array or list',\n",
       "  'You are on the right track. The only extra thing you need is to slice the Werke array. \\n\\nbut I think, finding the local max can be simplified to:\\n\\n\\n\\n     werte[1:-1][(diff(werte)[:-1]>0)*(diff(werte)[1:]<0)]\\n\\n\\n\\n@Jamine was quite right, `&` instead of `*` makes it reads better.',\n",
       "  '<python><numpy><local>',\n",
       "  datetime.date(2013, 9, 19),\n",
       "  '2013-09-20 00:22:53',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '6598.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['154',\n",
       "  '18963476',\n",
       "  'Answer',\n",
       "  \"How to enter censored data into R's survival model?\",\n",
       "  'First I shall say I disagree with the previous answer. For a subscription still active today, it should not be considered as tenure up until today, nor NA. What do we know exactly about those subscriptions? We know they tenured up until today, that is equivalent to say  `tenure_in_months` for those observations, although we don\\'t know exactly how long they are, they are longer than their tenure duration up to today.\\n\\n\\n\\nThis is a situation known as right-censor in survival analysis. See: http://en.wikipedia.org/wiki/Censoring_%28statistics%29\\n\\n\\n\\nSo your data would need to translate from \\n\\n\\n\\n    id  start_date  end_date\\n\\n    1   2013-06-01  2013-08-25\\n\\n    2   2013-06-01  NA\\n\\n    3   2013-08-01  2013-09-12\\n\\n\\n\\nto:\\n\\n\\n\\n    id  t1   t2    status(3=interval_censored)\\n\\n    1   2    2           3\\n\\n    2   3    NA          3\\n\\n    3   1    1           3\\n\\n\\n\\nThen you will need to change your R `surv` object, from:\\n\\n\\n\\n    Surv(time=tenure_in_months, event=status, type=\"right\")\\n\\n\\n\\nto:\\n\\n\\n\\n    Surv(t1, t2, event=status, type=\"interval2\")\\n\\n\\n\\nSee http://stat.ethz.ch/R-manual/R-devel/library/survival/html/Surv.html for more syntax details. A very good summary of computational details can be found: http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_lifereg_sect018.htm\\n\\n\\n\\n> Interval censored data can be represented in two ways. For the first use type = interval and the codes shown above. In that usage the value of the time2 argument is ignored unless event=3. The second approach is to think of each observation as a time interval with (-infinity, t) for left censored, (t, infinity) for right censored, (t,t) for exact and (t1, t2) for an interval. This is the approach used for type = interval2, with NA taking the place of infinity. It has proven to be the more useful.',\n",
       "  '<r><survival-analysis>',\n",
       "  datetime.date(2013, 9, 23),\n",
       "  '2013-09-24 07:37:57',\n",
       "  'CT Zhu (2487184)',\n",
       "  '8',\n",
       "  '',\n",
       "  '6698.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['156',\n",
       "  '19368360',\n",
       "  'Answer',\n",
       "  'How to add an extra row to a pandas dataframe',\n",
       "  'Upcoming pandas 0.13 version will allow to add rows through `loc` on non existing index data.\\n\\n\\n\\nDescription is [here][1] and this new feature is called *Setting With Enlargement*.\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/indexing.html#setting-with-enlargement',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2013, 10, 14),\n",
       "  '2015-09-07 09:32:17',\n",
       "  'Boud (624829), naught101 (210945)',\n",
       "  '38',\n",
       "  '',\n",
       "  '109030.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['157',\n",
       "  '19370121',\n",
       "  'Answer',\n",
       "  'How to add an extra row to a pandas dataframe',\n",
       "  \"A different approach that I found ugly compared to the classic dict+append, but that works:\\n\\n\\n\\n    df = df.T\\n\\n    \\n\\n    df[0] = ['1/1/2013', 'Smith','test',123]\\n\\n    \\n\\n    df = df.T\\n\\n    \\n\\n    df\\n\\n    Out[6]: \\n\\n           Date   Name Action   ID\\n\\n    0  1/1/2013  Smith   test  123\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2013, 10, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '109030.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['158',\n",
       "  '19426977',\n",
       "  'Answer',\n",
       "  'Replace row in DataFrame dependant on a value',\n",
       "  \"Use [`loc`][1] to filter out the part of the DataFrame you want to zero, and then assign the value to it. Below, it selects all lines where `c` column value is `'M'` and it takes all columns from `b` to `c`, and set the value of this selection to `0`:\\n\\n\\n\\n    df = pd.DataFrame([['1', 0.0, 'P'],\\n\\n        ...: ['2', 0.0, 'S'],\\n\\n        ...: ['3', 64,  'M'],\\n\\n        ...: ['4', 70,  'M'],], columns=['a', 'b', 'c'])\\n\\n    \\n\\n    df.loc[df['c']=='M','b':'c'] = 0\\n\\n    \\n\\n    df\\n\\n    Out[54]: \\n\\n       a  b  c\\n\\n    0  1  0  P\\n\\n    1  2  0  S\\n\\n    2  3  0  0\\n\\n    3  4  0  0\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/indexing.html#selection-by-label\\n\\n\",\n",
       "  '<python><replace><pandas><dataframe>',\n",
       "  datetime.date(2013, 10, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '1178.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['159',\n",
       "  '19480775',\n",
       "  'Answer',\n",
       "  \"How to change the location the 'r' axis for matplotlib polar plot?\",\n",
       "  \"One way that I can think of is to use `.set_rgrids` method:\\n\\n\\n\\n    f=plt.figure()\\n\\n    ax = f.add_axes([0.1, 0.1, 0.8, 0.8], projection='polar')\\n\\n    ax.plot(np.radians(az), inc, 'o')\\n\\n    ax.set_rgrids([10,20,30,40,50,60,70,80,90], angle=345.)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Zi6zd.png\",\n",
       "  '<python><matplotlib><plot>',\n",
       "  datetime.date(2013, 10, 20),\n",
       "  '2013-10-20 18:07:27',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1873.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['160',\n",
       "  '19506287',\n",
       "  'Answer',\n",
       "  'Use Python to Write VBA Script?',\n",
       "  'Yes, it is possible. You can start looking at how you can generate a VBA macro from VB on [that Microsoft KB][1].\\n\\n\\n\\nThe Python code below is illustrating how you can do the same ; it is a basic port of the first half of the KB sample code:\\n\\n\\n\\n    import win32com.client as win32\\n\\n    \\n\\n    import comtypes, comtypes.client\\n\\n    \\n\\n    xl = win32.gencache.EnsureDispatch(\\'Excel.Application\\')\\n\\n    xl.Visible = True\\n\\n    ss = xl.Workbooks.Add()\\n\\n    sh = ss.ActiveSheet\\n\\n    \\n\\n    xlmodule = ss.VBProject.VBComponents.Add(1)  # vbext_ct_StdModule\\n\\n    \\n\\n    sCode = \\'\\'\\'sub VBAMacro()\\n\\n           msgbox \"VBA Macro called\"\\n\\n          end sub\\'\\'\\'\\n\\n    \\n\\n    xlmodule.CodeModule.AddFromString(sCode)\\n\\n\\n\\nYou can look at the visible automated Excel macros, and you will see the `VBAMacro` defined above.\\n\\n\\n\\n  [1]: http://support.microsoft.com/kb/303871\\n\\n',\n",
       "  '<python><excel-vba><pywin32><autosize><vba>',\n",
       "  datetime.date(2013, 10, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '8538.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['162',\n",
       "  '20442833',\n",
       "  'Answer',\n",
       "  'Append numpy ndarrays with different dimensions in loop',\n",
       "  'No, you can\\'t create a `n*4` 2d `array` if `n` for each column is different:\\n\\n\\n\\n    >>> np.vstack((np.arange(10),np.arange(1,11),np.arange(2,12)))\\n\\n    array([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\\n\\n           [ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10],\\n\\n           [ 2,  3,  4,  5,  6,  7,  8,  9, 10, 11]])\\n\\n    >>> np.vstack((np.arange(10),np.arange(0,11),np.arange(0,12)))\\n\\n    \\n\\n    Traceback (most recent call last):\\n\\n      File \"<pyshell#36>\", line 1, in <module>\\n\\n        np.vstack((np.arange(10),np.arange(0,11),np.arange(0,12)))\\n\\n      File \"C:\\\\Python27\\\\lib\\\\site-packages\\\\numpy\\\\core\\\\shape_base.py\", line 226, in vstack\\n\\n        return _nx.concatenate(map(atleast_2d,tup),0)\\n\\n    ValueError: all the input array dimensions except for the concatenation axis must match exactly\\n\\n\\n\\nSee the `ValueError`, when the dimension of each `array` is different.\\n\\n\\n\\nYou either has to stay with `list` for `list3` or fill each `list2` to equal length.\\n\\n\\n\\nFor higher dimension, the same rule applies: `np.vstack((np.ones((10,4)),np.ones((10,6)),np.ones((10,6))))` won\\'t work, but ` np.vstack((np.ones((10,4)),np.ones((11,4)),np.ones((12,4))))` will and create a 35*4 `array`.\\n\\n\\n\\nIn your case, if you `vstack` your `list2`s, you will get get a 9938*4 `array`, if that is what you want. (I don\\'t get the *different number of rows* part)\\n\\n\\n\\nEDIT:\\n\\n\\n\\nTo pad the shorter `arrays` so that every `array` has the same `shape`, you need:`np.lib.pad`\\n\\n\\n\\n    >>> b=np.random.randint(0,20, size=(12,4))\\n\\n    >>> np.lib.pad(b, ((0,3),(0,0)), \\'constant\\', constant_values=[0.])\\n\\n    array([[ 5,  2, 10,  7],\\n\\n           [ 7, 17,  8, 11],\\n\\n           [ 7,  7,  2, 10],\\n\\n           [16, 17, 15, 16],\\n\\n           [ 0, 19,  5,  6],\\n\\n           [18, 19, 18,  6],\\n\\n           [ 2,  8, 11, 19],\\n\\n           [ 3, 17, 18, 16],\\n\\n           [10,  1, 12, 11],\\n\\n           [ 0,  7,  1, 14],\\n\\n           [ 7, 17,  8, 16],\\n\\n           [12,  6,  3,  5],\\n\\n           [ 0,  0,  0,  0],\\n\\n           [ 0,  0,  0,  0],\\n\\n           [ 0,  0,  0,  0]])\\n\\n\\n\\n`((0,3),(0,0))` means to pad 3 elements in the end of the first axis and pad 0 elements in the beginning of it. Also, it means to pad nothing in the 2nd axis. In your case you need to  `((0,max_length-length_of_current_array),(0,0))`.\\n\\n\\n\\nThen you just stack them all up using `np.hstack`.\\n\\n\\n\\nBut in my opinion you may want to pad `nan` instead of `0.`. `0.` may be meaningful data value.',\n",
       "  '<python><arrays><numpy><append><multidimensional-array>',\n",
       "  datetime.date(2013, 12, 7),\n",
       "  '2013-12-09 04:44:14',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '4820.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['164',\n",
       "  '18794650',\n",
       "  'Answer',\n",
       "  'how to get the index of numpy.random.choice? - python',\n",
       "  \"Regarding your first question, you can work the other way  around, randomly choose from the index of the array `a` and then fetch the value.\\n\\n\\n\\n    >>> a = [1,4,1,3,3,2,1,4]\\n\\n    >>> a = np.array(a)\\n\\n    >>> random.choice(arange(a.size))\\n\\n    6\\n\\n    >>> a[6]\\n\\n\\n\\nBut if you just need random sample without replacement, `replace=False` will do. Can't remember when it was firstly added to `random.choice`, might be 1.7.0. So if you are running very old `numpy` it may not work. Keep in mind the default is `replace=True`\",\n",
       "  '<python><random><numpy>',\n",
       "  datetime.date(2013, 9, 13),\n",
       "  '2013-09-13 20:46:07',\n",
       "  'CT Zhu (2487184)',\n",
       "  '6',\n",
       "  '',\n",
       "  '16615.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['165',\n",
       "  '19009053',\n",
       "  'Answer',\n",
       "  'boxplot: index out of range error',\n",
       "  'It appears the only case you may have `IndexError` is in `boxplot`, when you try to plot an array whose length is not equal to that of your `positions` array. i.e.:\\n\\n\\n\\n    >>> plt.boxplot([1,2,3], positions=[1.05, 1.35, 1.65, 1.95], widths = 0.15)\\n\\n\\n\\n    Traceback (most recent call last):\\n\\n      File \"<pyshell#66>\", line 1, in <module>\\n\\n        plt.boxplot([1,2,3], positions=[1.05, 1.35, 1.65, 1.95], widths = 0.15)\\n\\n      File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib    /pyplot.py\", line 2442, in boxplot\\n\\n        usermedians=usermedians, conf_intervals=conf_intervals)\\n\\n      File \"/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/matplotlib/axes.py\", line 5815, in boxplot\\n\\n        d = np.ravel(x[i])\\n\\n    IndexError: list index out of range\\n\\n\\n\\nI suspect your `e1[::4,0]` is not long enough.',\n",
       "  '<python><matplotlib><boxplot><ipython-notebook>',\n",
       "  datetime.date(2013, 9, 25),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1078.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['167',\n",
       "  '19119926',\n",
       "  'Answer',\n",
       "  'scipy.optimize.curvefit: Asymmetric error in fit',\n",
       "  'In the current version, I am afraid it is not doable. `curve_fit` is a wrap around the popular Fortran library `minipack`. Check the source code of `\\\\scipy_install_path\\\\optimize\\\\minipack.py`, you will see: (line 498-509):\\n\\n\\n\\n    if sigma is None:\\n\\n        func = _general_function\\n\\n    else:\\n\\n        func = _weighted_general_function\\n\\n        args += (1.0/asarray(sigma),)\\n\\n\\n\\nBasically what it means is that of `sigma` is not provided, then the unweighted Levenberg-Marquardt method in `minipack` will be called. If `sigma` is provided, then the weighted LM will be called. That implies, if `sigma` is to be provided, it must be provided as a array of the same length of `X` or `Y`. \\n\\n\\n\\nThat means if you want to have asymmetric error residue on `Y`, you have to come up with some modification to your target function, as @Jaime suggested.',\n",
       "  '<python><scipy><curve-fitting>',\n",
       "  datetime.date(2013, 10, 1),\n",
       "  '2017-07-30 10:51:07',\n",
       "  'CT Zhu (2487184), Vladimir F (721644)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1863.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['169',\n",
       "  '20832451',\n",
       "  'Answer',\n",
       "  'numpy vstack throwing dimension error',\n",
       "  \"Has to be the same `dtype`:\\n\\n\\n\\n    >>> d2=asarray([(1.,2.,3.,4.,5.)],dtype=[('millis',float),('temperature_Celsius',float),('relative_humidity',float),('setpoint',float),('relay_status',float)])\\n\\n    >>> d2=asarray([(1.,2.,3.,4.,5.)],dtype=data.dtype) #or this\\n\\n    >>> d2\\n\\n    array([(1.0, 2.0, 3.0, 4.0, 5.0)], \\n\\n          dtype=[('millis', '<f8'), ('temperature_Celsius', '<f8'), ('relative_humidity', '<f8'), ('setpoint', '<f8'), ('relay_status', '<f8')])\\n\\n    >>> vstack((data,d2))\\n\\n    array([[(0.0, 0.0, 0.0, 0.0, 0.0)],\\n\\n           [(1.0, 2.0, 3.0, 4.0, 5.0)]], \\n\\n          dtype=[('millis', '<f8'), ('temperature_Celsius', '<f8'), ('relative_humidity', '<f8'), ('setpoint', '<f8'), ('relay_status', '<f8')])\\n\\n\\n\\nSidenote: is it for some construction project? Looks fun.\",\n",
       "  '<python><numpy>',\n",
       "  datetime.date(2013, 12, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '4768.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['175',\n",
       "  '19580356',\n",
       "  'Answer',\n",
       "  'Matplotlib heatmap with changing y-values',\n",
       "  'I guess, maybe 2d interpolation by using `griddata` will be what you want?\\n\\n\\n\\n\\tfrom matplotlib.mlab import griddata\\n\\n\\txi=linspace(1,5,100)\\n\\n\\tyi=linspace(-10.5, 10.5, 100)\\n\\n\\ty=array([linspace(-i, i, 51) for i in (linspace(5,10))[::-1]]) #make up some y vectors with different range\\n\\n\\tx=zeros((50,51))+linspace(1,6, 50)[...,newaxis]\\n\\n\\tz=zeros((50,51))-linspace(-5, 5,51)**2+10 #make up some z data\\n\\n\\tx=x.flatten()\\n\\n\\ty=y.flatten()\\n\\n\\tz=z.flatten()\\n\\n\\tzi=griddata(x, y, z, xi, yi)\\n\\n\\tplt.contourf(xi, yi, zi, levels=-linspace(-5, 5,51)**2+10)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/jaGTw.png',\n",
       "  '<python><matplotlib><heatmap>',\n",
       "  datetime.date(2013, 10, 25),\n",
       "  '2013-10-25 03:08:55',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '4457.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['178',\n",
       "  '20058124',\n",
       "  'Answer',\n",
       "  'How to remove gaps between subplots in matplotlib?',\n",
       "  \"Have you tried `plt.tight_layout()`?\\n\\n\\n\\nwith `plt.tight_layout()`\\n\\n![enter image description here][1]\\n\\nwithout it:\\n\\n![enter image description here][2]\\n\\n\\n\\nOr: something like this (use `add_axes`)\\n\\n\\n\\n    left=[0.1,0.3,0.5,0.7]\\n\\n    width=[0.2,0.2, 0.2, 0.2]\\n\\n    rectLS=[]\\n\\n    for x in left:\\n\\n       for y in left:\\n\\n           rectLS.append([x, y, 0.2, 0.2])\\n\\n    axLS=[]\\n\\n    fig=plt.figure()\\n\\n    axLS.append(fig.add_axes(rectLS[0]))\\n\\n    for i in [1,2,3]:\\n\\n         axLS.append(fig.add_axes(rectLS[i],sharey=axLS[-1]))    \\n\\n    axLS.append(fig.add_axes(rectLS[4]))\\n\\n    for i in [1,2,3]:\\n\\n         axLS.append(fig.add_axes(rectLS[i+4],sharex=axLS[i],sharey=axLS[-1]))\\n\\n    axLS.append(fig.add_axes(rectLS[8]))\\n\\n    for i in [5,6,7]:\\n\\n         axLS.append(fig.add_axes(rectLS[i+4],sharex=axLS[i],sharey=axLS[-1]))     \\n\\n    axLS.append(fig.add_axes(rectLS[12]))\\n\\n    for i in [9,10,11]:\\n\\n         axLS.append(fig.add_axes(rectLS[i+4],sharex=axLS[i],sharey=axLS[-1]))\\n\\n\\n\\nIf you don't need to share axes, then simply `axLS=map(fig.add_axes, rectLS)`\\n\\n![enter image description here][3]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/KA46o.png\\n\\n  [2]: http://i.stack.imgur.com/gsXtg.png\\n\\n  [3]: http://i.stack.imgur.com/VJ1gA.png\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2013, 11, 18),\n",
       "  '2013-11-18 21:25:20',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '54885.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['181',\n",
       "  '19599661',\n",
       "  'Answer',\n",
       "  'Get particular row as series from pandas dataframe',\n",
       "  'Use the [`squeeze`][1] function that will remove one dimension from the dataframe:\\n\\n\\n\\n    df[df[\"location\"] == \"c\"].squeeze()\\n\\n    Out[5]: \\n\\n    date        20130102\\n\\n    location           c\\n\\n    Name: 2, dtype: object\\n\\n\\n\\n`DataFrame.squeeze` method acts the same way of the `squeeze` argument of the `read_csv` function when set to `True`: if the resulting dataframe is a 1-len dataframe, i.e. it has only one dimension (a column or a row), then the object is squeezed down to the smaller dimension object. \\n\\n\\n\\nIn your case, you get a Series object from the DataFrame. The same logic applies if you squeeze a Panel down to a DataFrame. \\n\\n\\n\\nsqueeze is explicit in your code and shows clearly your intent to \"cast down\" the object in hands because its dimension can be projected to a smaller one.\\n\\n\\n\\nIf the dataframe has more than one column or row, squeeze has no effect.\\n\\n\\n\\n  [1]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.squeeze.html\\n\\n',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2013, 10, 25),\n",
       "  '2018-07-12 11:03:46',\n",
       "  'Boud (624829), Ivan (6331369), Sushovan Mandal (4661264)',\n",
       "  '54',\n",
       "  '',\n",
       "  '19687.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['183',\n",
       "  '19246905',\n",
       "  'Answer',\n",
       "  'How to combine several similar .csv files into one dataframe with given structure',\n",
       "  \"in Python you should use [`pandas`][1] to perform these operations:\\n\\n\\n\\n    import pandas as pd\\n\\n    \\n\\n    df1 = pd.read_csv('1.csv', sep='\\\\s+', index_col=0)\\n\\n    df2 = pd.read_csv('2.csv', sep='\\\\s+', index_col=0)\\n\\n\\n\\n    pd.concat([df1, df2], axis=1)\\n\\n    Out[16]: \\n\\n           n   n\\n\\n    Type        \\n\\n    A      1   2\\n\\n    B     20  15\\n\\n    C     34  16\\n\\n    D      5   5\\n\\n\\n\\n\\n\\nIf you expect more automated columns renaming:\\n\\n\\n\\n    pd.merge(df1, df2, left_index=True, right_index=True, suffixes=['1', '2'])\\n\\n    Out[20]: \\n\\n          n1  n2\\n\\n    Type        \\n\\n    A      1   2\\n\\n    B     20  15\\n\\n    C     34  16\\n\\n    D      5   5\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><r><csv><dataframe>',\n",
       "  datetime.date(2013, 10, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2504.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['185',\n",
       "  '19277981',\n",
       "  'Answer',\n",
       "  'Matplotlib: different font type rendering in axis labels',\n",
       "  \"A quick and dirty way in `matplotlib` version 1.3.0 and up, you can use `Latex` `\\\\textup` and `\\\\textbf`:\\n\\n\\n\\n    ax.set_ylabel('\\\\textup{Surface Area} (\\\\AA^2)', size =16)\\n\\n\\n\\nsee tutorials here: http://matplotlib.org/users/usetex.html, don't forget to enable `latex` by:\\n\\n\\n\\n    rc('text', usetex=True)\\n\\n\\n\\nThis should work for older versions as well as long as you have `Latex` installed. But if you want to stick to `matplotlib`'s `mathtext`, it can be done as well:\\n\\n\\n\\n    >>> matplotlib.rcParams['mathtext.fontset']='stixsans'\\n\\n    >>> for i in [1]:\\n\\n    \\tfig, ax = plt.subplots()\\n\\n    \\tax.plot(data)\\n\\n    \\tytx=ax.set_ylabel(r'Surface Area $(\\\\AA^2)$', size =16)\\n\\n\\n\\nNot every font will work with `mathtext`, `stixsans` is the one that will 'look like' `sans-serif`: http://matplotlib.org/users/mathtext.html#mathtext-tutorial. Honestly, the difference is very small. I am pretty sure Angstrom is not in `stixsans` so to render it `mathtext` falls back to the default `Computer Modern` font.\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/6SBKZ.png\",\n",
       "  '<python><matplotlib><font-face><axis-labels>',\n",
       "  datetime.date(2013, 10, 9),\n",
       "  '2013-10-09 17:31:35',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1185.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['186',\n",
       "  '19075138',\n",
       "  'Answer',\n",
       "  'IndexError: list index out of range // numpy?',\n",
       "  'Check your *.csv file, the last line does not have value for `Adj Close` column. The last line in these stock history data downloaded from Google finance represent the *first* day in the dataset. Based on your daily return formula, the first day does not have value.',\n",
       "  '<python><arrays><numpy>',\n",
       "  datetime.date(2013, 9, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1299.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['187',\n",
       "  '19086546',\n",
       "  'Answer',\n",
       "  'Error: [only length-1 arrays can be converted to Python scalars] when changing variable order',\n",
       "  \"I wonder whether you data shows an exponential decay of rate. The mathematical model may not be the most suitable one. \\n\\n\\n\\n![enter image description here][1]See the doc string of `curve_fit`\\n\\n\\n\\n> f : callable\\n\\nThe model function, f(x, ...). It must take the independent variable as the first argument and the parameters to fit as separate remaining arguments.\\n\\n\\n\\nsince your formula is essentially: `k=A*ma.exp(-E/(R*T))`, the right order of parameters in `func` should be `(T, A, E)` or `(T, E, A)`.\\n\\n\\n\\nRegarding the order of `A` and `E`, they don't really matter. If you flip them, the result will get flipped as well:\\n\\n\\n\\n    >>> def func(T, A, E):\\n\\n        return A*ma.exp(-E/(R*T))\\n\\n\\n\\n    >>> so.curve_fit(func, T, k)\\n\\n    (array([  8.21449078e+00,  -5.86499656e+04]), array([[  6.07720215e+09,   4.31864058e+12],\\n\\n           [  4.31864058e+12,   3.07102992e+15]]))\\n\\n    >>> def func(T, E, A):\\n\\n        return A*ma.exp(-E/(R*T))\\n\\n\\n\\n    >>> so.curve_fit(func, T, k)\\n\\n    (array([ -5.86499656e+04,   8.21449078e+00]), array([[  3.07102992e+15,   4.31864058e+12],\\n\\n           [  4.31864058e+12,   6.07720215e+09]]))\\n\\n\\n\\nI didn't get your `typeerror` at all. \\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/gSt4y.png\",\n",
       "  '<python><function><scipy>',\n",
       "  datetime.date(2013, 9, 30),\n",
       "  '2013-09-30 05:55:15',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '2288.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['188',\n",
       "  '19170627',\n",
       "  'Answer',\n",
       "  'Plotting data from csv using matplotlib.pyplot',\n",
       "  \"`csv.reader()` returns strings (technically, `.next()`method of reader object returns lists of strings). Without converting them to `float` or `int`, you won't be able to `plt.plot()` them.\\n\\n\\n\\nTo save the trouble of converting, I suggest using `genfromtxt()` from `numpy`. (http://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html)\\n\\n\\n\\nFor example, there are two files: \\n\\ndata1.csv:\\n\\n\\n\\n    data1\\n\\n    2\\n\\n    3\\n\\n    4\\n\\n    3\\n\\n    6\\n\\n    6\\n\\n    4\\n\\n\\n\\nand data2.csv:\\n\\n\\n\\n    data2\\n\\n    92\\n\\n    73\\n\\n    64\\n\\n    53\\n\\n    16\\n\\n    26\\n\\n    74\\n\\n\\n\\nBoth of them have one line of header. We can do:\\n\\n\\n\\n\\timport numpy as np\\n\\n\\tdata1=np.genfromtxt('data1.csv', skip_header=1) #suppose it is in the current working directory\\n\\n\\tdata2=np.genfromtxt('data2.csv', skip_header=1)\\n\\n\\tplt.plot(data1, data2,'o-')\\n\\n\\n\\nand the result:\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/FQ7Q6.png\",\n",
       "  '<python><csv><python-2.7><matplotlib>',\n",
       "  datetime.date(2013, 10, 3),\n",
       "  '2013-10-04 00:00:05',\n",
       "  'CT Zhu (2487184)',\n",
       "  '9',\n",
       "  '',\n",
       "  '8698.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['189',\n",
       "  '19381211',\n",
       "  'Answer',\n",
       "  'Transpose columns in python/pandas',\n",
       "  \"You want to use the `Period` as an index. `set_index` will do this for you. Then you can transpose your resulting table:\\n\\n\\n\\n    df.set_index('Period').T\\n\\n    Out[13]: \\n\\n    Period         2010         2011         2012\\n\\n    Name            foo          bar          nil\\n\\n    Value1  0,994369885   0,92930566  0,997754262\\n\\n    Value2  0,780942307  0,274566936  0,488064461\\n\\n    Value3  0,510782105  0,390724018  0,642086396\\n\\n    Value4  0,842522334  0,613705323  0,028703768\\n\\n    Value5  0,383279727  0,287280101  0,764773601\\n\\n\",\n",
       "  '<python><python-2.7><pandas><dataframe>',\n",
       "  datetime.date(2013, 10, 15),\n",
       "  '2018-09-23 02:44:31',\n",
       "  'A-B-B (832230)',\n",
       "  '6',\n",
       "  '',\n",
       "  '12100.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['194',\n",
       "  '19705109',\n",
       "  'Answer',\n",
       "  'Why this date conversion with Pandas.to_datetime is much slower than some alternatives?',\n",
       "  \"to_datetime is going to parse the timestamp argument in several ways to find out what is the timestamp inside. It is useful to convert strings representing datetime into Timestamp objects.\\n\\n\\n\\nIf the data you are manipulating is already a timestamp int, you can directly call the Timestamp object to build it:\\n\\n\\n\\n    pd.Timestamp(timestamp)\\n\\n    Out[51]: Timestamp('1989-10-02 00:00:00', tz=None)\\n\\n    \\n\\n    %timeit pd.Timestamp(timestamp)\\n\\n    100000 loops, best of 3: 1.96 µs per loop\\n\\n\\n\\nIt will also work with negative numbers:\\n\\n\\n\\n    pd.Timestamp(-445645400000000000L)\\n\\n    Out[54]: Timestamp('1955-11-18 01:36:40', tz=None)\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2013, 10, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '3187.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['196',\n",
       "  '20251523',\n",
       "  'Answer',\n",
       "  'Fitting a distribution to data: how to penalize \"bad\" parameter estimates?',\n",
       "  \"This should have been a comment but I run out of space.\\n\\n\\n\\nI think a Maximum Likelihood fit is probably the most appropriate approach here. ML method is already implemented for many distributions in `scipy.stats`. For example, you can find the MLE of normal distribution by calling `scipy.stats.norm.fit` and find the MLE of exponential distribution in a similar way. Combining these two resulting MLE parameters should give you a pretty good starting parameter for Ex-Gaussian ML fit. In fact I would imaging most of your data is quite nicely Normally distributed. If that is the case, the ML parameter estimates for Normal distribution alone should give you a pretty good starting parameter. \\n\\n\\n\\nSince Ex-Gaussian only has 3 parameters, I don't think a ML fit will be hard at all. If you could provide a dataset for which your current method doesn't work well, it will be easier to show a real example.\\n\\n\\n\\nAlright, here you go:\\n\\n\\n\\n    >>> import scipy.special as sse\\n\\n    >>> import scipy.stats as sss\\n\\n    >>> import scipy.optimize as so\\n\\n    >>> from numpy import *\\n\\n\\n\\n    >>> def eg_pdf(p, x): #defines the PDF\\n\\n    \\tm=p[0]\\n\\n    \\ts=p[1]\\n\\n    \\tl=p[2]\\n\\n    \\treturn 0.5*l*exp(0.5*l*(2*m+l*s*s-2*x))*sse.erfc((m+l*s*s-x)/(sqrt(2)*s))\\n\\n\\n\\n    >>> xo=array([ 450.,  560.,  692.,  730.,  758.,  723.,  486.,  596.,  716.,\\n\\n            695.,  757.,  522.,  535.,  419.,  478.,  666.,  637.,  569.,\\n\\n            859.,  883.,  551.,  652.,  378.,  801.,  718.,  479.,  544.])\\n\\n\\n\\n    >>> sss.norm.fit(xo) #get the starting parameter vector form the normal MLE\\n\\n    (624.22222222222217, 132.23977474531389)\\n\\n\\n\\n    >>> def llh(p, f, x): #defines the negative log-likelihood function\\n\\n    \\treturn -sum(log(f(p,x)))\\n\\n\\n\\n    >>> so.fmin(llh, array([624.22222222222217, 132.23977474531389, 1e-6]), (eg_pdf, xo)) #yeah, the data is not good\\n\\n    Warning: Maximum number of function evaluations has been exceeded.\\n\\n    array([  6.14003407e+02,   1.31843250e+02,   9.79425845e-02])\\n\\n\\n\\n    >>> przt=so.fmin(llh, array([624.22222222222217, 132.23977474531389, 1e-6]), (eg_pdf, xo), maxfun=1000) #so, we increase the number of function call uplimit\\n\\n    Optimization terminated successfully.\\n\\n             Current function value: 170.195924\\n\\n             Iterations: 376\\n\\n             Function evaluations: 681\\n\\n\\n\\n    >>> llh(array([624.22222222222217, 132.23977474531389, 1e-6]), eg_pdf, xo)\\n\\n    400.02921290185645\\n\\n    >>> llh(przt, eg_pdf, xo) #quite an improvement over the initial guess\\n\\n    170.19592431051217\\n\\n    >>> przt\\n\\n    array([  6.14007039e+02,   1.31844654e+02,   9.78934519e-02])\\n\\n\\n\\nThe optimizer used here (`fmin`, or Nelder-Mead simplex algorithm) does not use any information from gradient and usually works much slower than the optimizer that does. It appears that the derivative of the negative log-likelihood function of Exponential Gaussian may be written in a close form easily. If so, optimizers that utilize gradient/derivative will be better and more efficient choice (such as `fmin_bfgs`).\\n\\n\\n\\nThe other thing to consider is parameter constrains. By definition, sigma and lambda has to be positive for Exponential Gaussian. You can use a constrained optimizer (such as `fmin_l_bfgs_b`). Alternatively, you can optimize for:\\n\\n\\n\\n    >>> def eg_pdf2(p, x): #defines the PDF\\n\\n    \\tm=p[0]\\n\\n    \\ts=exp(p[1])\\n\\n    \\tl=exp(p[2])\\n\\n    \\treturn 0.5*l*exp(0.5*l*(2*m+l*s*s-2*x))*sse.erfc((m+l*s*s-x)/(sqrt(2)*s))\\n\\n\\n\\nDue to the functional invariance property of MLE, the MLE of this function should be the same as same as the original `eg_pdf`. There are other transformation that you can use, besides `exp()`, to project `(-inf, +inf)` to `(0, +inf)`.\\n\\n\\n\\nAnd you can also consider http://en.wikipedia.org/wiki/Lagrange_multiplier.\",\n",
       "  '<python><numpy><scipy><mathematical-optimization>',\n",
       "  datetime.date(2013, 11, 27),\n",
       "  '2013-12-05 02:53:39',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1387.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['197',\n",
       "  '19125748',\n",
       "  'Answer',\n",
       "  'Matplotlib pcolor/pcolormesh falls apart if the number of rows/cols are certain numbers (usually prime)',\n",
       "  'Just add:\\n\\n\\n\\n\\tplt.xlim(xmax=22) #or xl\\n\\n\\tplt.ylim(ymax=51) #or yl\\n\\n\\n\\nafter\\n\\n\\n\\n\\tplt.pcolormesh(X, cmap=matplotlib.cm.RdBu_r, vmin=-5, vmax=5)\\n\\n\\n\\nshould do it.\\n\\n',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2013, 10, 1),\n",
       "  '2013-10-01 20:59:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1039.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['198',\n",
       "  '19161705',\n",
       "  'Answer',\n",
       "  'Numpy extract submatrix',\n",
       "  'First of all, your `Y` only has 4 col and rows, so there is no col4 or row4, at most col3 or row3.\\n\\n\\n\\nTo get 0, 3 cols: `Y[[0,3],:]`\\n\\nTo get 0, 3 rows: `Y[:,[0,3]]`\\n\\n\\n\\nSo to get the array you request: `Y[[0,3],:][:,[0,3]]`\\n\\n\\n\\nNote that if you just `Y[[0,3],[0,3]]` it is equivalent to `[Y[0,0], Y[3,3]]` and the result will be of two elements: `array([ 0, 15])`\\n\\n\\n\\n ',\n",
       "  '<python><numpy>',\n",
       "  datetime.date(2013, 10, 3),\n",
       "  '2013-10-03 14:25:53',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '52792.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['199',\n",
       "  '19214708',\n",
       "  'Answer',\n",
       "  'How can I efficiently move from a Pandas dataframe to JSON',\n",
       "  'Transform your date index back into a simple data column with `reset_index`, and then generate your json object by using the `orient=\\'index\\'` property:\\n\\n\\n\\n    In [11]: aggregated_df.reset_index().to_json(orient=\\'index\\')\\n\\n    Out[11]: \\'{\"0\":{\"created\":\"05-16-13\",\"counter\":3},\"1\":{\"created\":\"05-17-13\",\"counter\":1},\"2\":{\"created\":\"05-18-13\",\"counter\":1}}\\'\\n\\n\\n\\n',\n",
       "  '<javascript><python><json><d3.js><pandas>',\n",
       "  datetime.date(2013, 10, 6),\n",
       "  '',\n",
       "  '',\n",
       "  '23',\n",
       "  '',\n",
       "  '13170.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['202',\n",
       "  '19441364',\n",
       "  'Answer',\n",
       "  'how to draw a nonlinear function using matplotlib?',\n",
       "  \"One obvious way to do this is to found the `(x,y)` pairs satisfy the relationship, by numerically solving the equation.\\n\\n\\n\\n    from scipy import optimize\\n\\n\\tf=lambda x, y: (x**3+y**3+y**2+2*x*y*y-0)**2\\n\\n\\ty_range=linspace(-1, 1, 100)\\n\\n\\tx_range=[optimize.fmin(f,0,args=(y,), disp=0) for y in y_range]\\n\\n\\txr=linspace(-1,1)\\n\\n\\tyr=linspace(-1,1)\\n\\n\\tX, Y=meshgrid(xr, yr)\\n\\n\\tZ=f(X, Y)\\n\\n\\tplt.plot(x_range, y_range, 'k')\\n\\n\\tplt.contourf(xr, yr, Z, levels=linspace(0,0.001,51), alpha=0.5)\\n\\n\\tplt.colorbar()\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/EIMtV.png\\n\\n\\n\\nThe black line is what you want. The contour is just to show how the function behaves around 0. `optimize.fmin()` is not the most efficient solver here, just keep it simple.\\n\\n\\n\\nWhen the absolute values of `x` or `y` are large, you are essentially plotting `x+0.4496y=0` and you don't need to do all these above.\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2013, 10, 18),\n",
       "  '2013-10-18 04:16:41',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1790.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['203',\n",
       "  '20252317',\n",
       "  'Answer',\n",
       "  'List of 3x1 numpy arrays in Python',\n",
       "  \"I am just going to show *why* your code is not working. @peter de Rivaz's answer should be what you should do in `numpy`\\n\\n\\n\\nWhen you `allPixels.append(pixel)`, you are appending an incidence of `pixel`. If you change the value of `pixel` later, all the incidence of `pixel` changes. See this example:\\n\\n\\n\\n    >>> def RGBtoLMS(rgbValues, rgbLength): #Passing in a list of rgbValues and an int representing the length of that list\\n\\n        pixel = numpy.empty((3, 1), int)\\n\\n        allPixels = list()\\n\\n        x = 0\\n\\n        for h in xrange(rgbLength/3):\\n\\n            for i in xrange(len(pixel)):\\n\\n                for j in xrange(len(pixel[i])):\\n\\n                    if x < rgbLength: #For some reason, x reaches rgbLength...\\n\\n                        pixel[i][j] = rgbValues[x]\\n\\n                        x+=1\\n\\n            allPixels.append(pixel)\\n\\n        for pixel in allPixels:\\n\\n            print pixel, id(pixel)\\n\\n    \\n\\n            \\n\\n    >>> RGBtoLMS([1,2,3,101,102,103,4,5,6,104,105,106],12)\\n\\n    [[104]\\n\\n     [105]\\n\\n     [106]] 38693552\\n\\n    [[104]\\n\\n     [105]\\n\\n     [106]] 38693552\\n\\n    [[104]\\n\\n     [105]\\n\\n     [106]] 38693552\\n\\n    [[104]\\n\\n     [105]\\n\\n     [106]] 38693552\\n\\n\\n\\nSo you can do a few things to get the intended behavior:\\n\\n\\n\\n    >>> def RGBtoLMS(rgbValues, rgbLength): #Passing in a list of rgbValues and an int representing the length of that list\\n\\n        pixel = numpy.empty((3, 1), int)\\n\\n        allPixels = list()\\n\\n        x = 0\\n\\n        for h in xrange(rgbLength/3):\\n\\n            for i in xrange(len(pixel)):\\n\\n                for j in xrange(len(pixel[i])):\\n\\n                    if x < rgbLength: #For some reason, x reaches rgbLength...\\n\\n                        pixel[i][j] = rgbValues[x]\\n\\n                        x+=1\\n\\n            allPixels.append(pixel*1)\\n\\n        for pixel in allPixels:\\n\\n            print pixel, id(pixel)\\n\\n    \\n\\n            \\n\\n    >>> RGBtoLMS([1,2,3,101,102,103,4,5,6,104,105,106],12)\\n\\n    [[1]\\n\\n     [2]\\n\\n     [3]] 38301608\\n\\n    [[101]\\n\\n     [102]\\n\\n     [103]] 37756712\\n\\n    [[4]\\n\\n     [5]\\n\\n     [6]] 37949360\\n\\n    [[104]\\n\\n     [105]\\n\\n     [106]] 38399424\\n\\n\\n\\nOr moving the definition of `pixel` into the loop.\\n\\n\\n\\n    >>> def RGBtoLMS(rgbValues, rgbLength): #Passing in a list of rgbValues and an int representing the length of that list\\n\\n        allPixels = list()\\n\\n        x = 0\\n\\n        for h in xrange(rgbLength):\\n\\n            if x < rgbLength: #For some reason, x reaches rgbLength...\\n\\n                pixel = numpy.empty((3, 1), int)\\n\\n                for i in xrange(len(pixel)):\\n\\n                    for j in xrange(len(pixel[i])):\\n\\n                        pixel[i][j] = rgbValues[x]\\n\\n                        x+=1\\n\\n                allPixels.append(pixel)\\n\\n            else:\\n\\n    \\t    break\\n\\n        for pixel in allPixels:\\n\\n            print pixel, id(pixel)\\n\\n    \\n\\n            \\n\\n    >>> RGBtoLMS([1,2,3,101,102,103,4,5,6,104,105,106],12)\\n\\n    [[1]\\n\\n     [2]\\n\\n     [3]] 38301608\\n\\n    [[101]\\n\\n     [102]\\n\\n     [103]] 37756712\\n\\n    [[4]\\n\\n     [5]\\n\\n     [6]] 37949360\\n\\n    [[104]\\n\\n     [105]\\n\\n     [106]] 38399424\",\n",
       "  '<python><arrays><list><numpy><multidimensional-array>',\n",
       "  datetime.date(2013, 11, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1050.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['204',\n",
       "  '19372610',\n",
       "  'Answer',\n",
       "  'Matplotlib: How to adjust linewidth in colorbar for contour plot?',\n",
       "  \"You just need to find out how to access those lines, let try:\\n\\n\\n\\n    >>> CB.ax.get_children()\\n\\n    [<matplotlib.axis.XAxis object at 0x026A74B0>, <matplotlib.axis.YAxis object at 0x026AF270>, <matplotlib.lines.Line2D object at 0x026AF190>, <matplotlib.patches.Polygon object at 0x027387F0>, <matplotlib.collections.LineCollection object at 0x02748BD0>, <matplotlib.text.Text object at 0x026C0D10>, <matplotlib.patches.Rectangle object at 0x026C0D50>, <matplotlib.spines.Spine object at 0x026A7410>, <matplotlib.spines.Spine object at 0x026A7290>, <matplotlib.spines.Spine object at 0x026A7350>, <matplotlib.spines.Spine object at 0x026A71B0>]\\n\\n\\n\\nAlright, take a guess, I bet the 5th item is a list of the divider lines. We are looking for some `.line` objects and there are two. The first one (3rd item) actually is the edge of the entire color bar (if I remember correctly). So I will go for the next `.line` object.\\n\\n\\n\\nNow let's try to modified it in a few ways:\\n\\n\\n\\n    >>> len(lines1[4].get_linewidths())\\n\\n    11 #how many item are there? 11 lines\\n\\n    >>> lines1[4].set_color(['r']*11) #set them all to red, in this example we actually want to have the color stay the same, this is just for a demonstration. \\n\\n    >>> lines1[4].set_linewidths([2]*11) #set them all to have linewidth of 2.\\n\\n\\n\\nthe result![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/80y2b.png\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2013, 10, 15),\n",
       "  '2013-10-15 03:09:22',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '4441.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['205',\n",
       "  '19455669',\n",
       "  'Answer',\n",
       "  'Styling of Pandas groupby boxplots',\n",
       "  \"I am afraid you have to hard code. Take the `pandas` example: http://pandas.pydata.org/pandas-docs/stable/visualization.html#box-plotting\\n\\n\\n\\n\\tfrom pandas import *\\n\\n\\timport matplotlib\\n\\n\\tfrom numpy.random import rand\\n\\n\\timport matplotlib.pyplot as plt\\n\\n\\tdf = DataFrame(rand(10,2), columns=['Col1', 'Col2'] )\\n\\n\\tdf['X'] = Series(['A','A','A','A','A','B','B','B','B','B'])\\n\\n\\tbp = df.boxplot(by='X')\\n\\n\\tcl=bp[0].get_children()\\n\\n\\tcl=[item for item in cl if isinstance(item, matplotlib.lines.Line2D)]\\n\\n\\n\\nNow lets identify which one is the boxes, median's, etc:\\n\\n\\n\\n    for i, item in enumerate(cl):\\n\\n    \\tif item.get_xdata().mean()>0:\\n\\n    \\t\\tbp[0].text(item.get_xdata().mean(), item.get_ydata().mean(), str(i), va='center', ha='center')\\n\\n\\n\\nAnd the plot looks like this:\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nEach bar consists of 8 items. e.g, The 5th item is the median. The 7th and 8th items are probably the fliers, which we don't have any here.\\n\\n\\n\\nKnowing these, to modify some part of the bar is easy. If we want to set the median to have `linewidth` of 2:\\n\\n\\n\\n    for i in range(_your_number_of_classes_2_in_this_case):\\n\\n        cl[5+i*8].set_linewidth(2.)\\n\\n  [1]: http://i.stack.imgur.com/YRwaW.png\\n\\n\\n\\n\",\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2013, 10, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '6484.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['206',\n",
       "  '19521084',\n",
       "  'Answer',\n",
       "  'Extracting a range of data from a python list',\n",
       "  \"You should do `list[44:49]` rather than `list[44:5]`.\\n\\nUsually when you want to fetch 5 items after (including) the a+1th item, you do `L[a, a+5]`.\\n\\n'Usually' implies there are more flexible ways to do so: see Extended Slices: http://docs.python.org/release/2.3/whatsnew/section-slices.html.\\n\\n\\n\\nAlso try not to use `list` as your list name. It overwrites `list()`.\",\n",
       "  '<python><list>',\n",
       "  datetime.date(2013, 10, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '36825.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['207',\n",
       "  '22075926',\n",
       "  'Answer',\n",
       "  'format phone number in csv using pandas',\n",
       "  'I think the problem is that the phone numbers are stored as `float64`, so, adding a few things will fix your inner loop:\\n\\n\\n\\n\\tIn [75]:\\n\\n\\t\\n\\n\\tdf[\\'Phone_no\\']\\n\\n\\tOut[75]:\\n\\n\\t0    5554443333\\n\\n\\t1    1114445555\\n\\n\\tName: Phone_no, dtype: float64\\n\\n\\tIn [76]:\\n\\n\\t\\n\\n\\tfor phone_no in df[\\'Phone_no\\']:\\n\\n\\t    contactphone = \"(%c%c%c)%c%c%c-%c%c%c%c\" % tuple(map(ord,list(str(phone_no)[:10])))\\n\\n\\t    print contactphone\\n\\n\\t(555)444-3333\\n\\n\\t(111)444-5555\\n\\n\\n\\nHowever, I think it is easier just to have the phone numbers as `string` (@Andy_Hayden made a good point on missing values, so I made up the following dataset:)\\n\\n\\n\\n\\tIn [121]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t     Phone_no   Name\\n\\n\\t0  5554443333   John\\n\\n\\t1  1114445555   Jane\\n\\n\\t2         NaN  Betty\\n\\n\\t\\n\\n\\t[3 rows x 2 columns]\\n\\n\\tIn [122]:\\n\\n\\t\\n\\n\\tdf.dtypes\\n\\n\\tOut[122]:\\n\\n\\tPhone_no    float64\\n\\n\\tName         object\\n\\n\\tdtype: object\\n\\n\\t#In [123]: You don\\'t need to convert the entire DataFrame, only the \\'Phone_no\\' needs to be converted.\\n\\n\\t#\\n\\n\\t#df=df.astype(\\'S4\\')\\n\\n\\tIn [124]:\\n\\n\\t\\n\\n\\tdf[\\'PhoneNumber\\']=df[\\'Phone_no\\'].astype(str).apply(lambda x: \\'(\\'+x[:3]+\\')\\'+x[3:6]+\\'-\\'+x[6:10])\\n\\n\\tIn [125]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t       Phone_no   Name    PhoneNumber\\n\\n\\t0  5554443333.0   John  (555)444-3333\\n\\n\\t1  1114445555.0   Jane  (111)444-5555\\n\\n\\t2           NaN  Betty         (nan)-\\n\\n\\t\\n\\n\\t[3 rows x 3 columns]\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n\\tIn [134]:\\n\\n\\timport numpy as np\\n\\n\\tdf[\\'PhoneNumber\\']=df[\\'Phone_no\\'].astype(str).apply(lambda x: np.where((len(x)>=10)&set(list(x)).issubset(list(\\'.0123456789\\')),\\n\\n\\t                                                                      \\'(\\'+x[:3]+\\')\\'+x[3:6]+\\'-\\'+x[6:10],\\n\\n\\t                                                                      \\'Phone number not in record\\'))\\n\\n\\tIn [135]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t     Phone_no   Name                 PhoneNumber\\n\\n\\t0  5554443333   John               (555)444-3333\\n\\n\\t1  1114445555   Jane               (111)444-5555\\n\\n\\t2         NaN  Betty  Phone number not in record\\n\\n\\t\\n\\n\\t[3 rows x 3 columns]',\n",
       "  '<python><csv><formatting><pandas><phone-number>',\n",
       "  datetime.date(2014, 2, 27),\n",
       "  '2014-02-28 16:56:29',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2136.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['209',\n",
       "  '22261962',\n",
       "  'Answer',\n",
       "  'Filter out pandas pivot table rows',\n",
       "  \"'Imps Type' not 'Imp Type' right? If the cut off is 3000000:\\n\\n\\n\\n\\tIn [9]:\\n\\n\\t\\n\\n\\tprint df[(df['Imps']>3000000).any(axis=1)]\\n\\n\\t              Imps                             Revenue                        \\n\\n\\t         10day avg  30day avg    3day avg    10day avg   30day avg    3day avg\\n\\n\\tKept    8893221.80  4597369.7  8581754.90  1116.528169  576.108280  791.959157\\n\\n\\tResold  2924189.35  1399503.1  3631253.55   390.274367  199.269661  384.588853\\n\\n\\t\\n\\n\\t[2 rows x 6 columns]\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 3, 7),\n",
       "  '2014-03-08 00:11:37',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1020.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['211',\n",
       "  '20170944',\n",
       "  'Answer',\n",
       "  'scipy curve_fit error: divide by zero encountered',\n",
       "  \"Alright, two helpful tricks.\\n\\n\\n\\n1st, replace `0` in your `x` with some really small number, such as `1e-8` (don't laugh, there is a core package in `R` actually does this, written by `his name shall not be spoken` and people use it all the time.)\\n\\nActually I didn't get your `RuntimeWarning` at all. I am running `scipy` `0.12.0` and `numpy` `1.7.1`. Maybe this is version dependent.\\n\\n\\n\\nBut we will get a very bad fit:\\n\\n\\n\\n    In [41]: popt, pcov\\n\\n    Out[41]: (array([  3.90107143e+01,  -3.08698757e+07,  -1.52971609e+02]), inf)\\n\\n\\n\\nSo, trick 2, instead of optimizing `f` function, we define a `g` function:\\n\\n\\n\\n    In [38]: def g(x, a, b, c):\\n\\n       ....:     return b/a*x**c+1/a\\n\\n       ....:\\n\\n\\n\\n    In [39]: curve_fit(g, x, 1/y) #Better fit\\n\\n    Out[39]:\\n\\n    (array([ 19.76748582,  -0.14499508,   0.44206688]),\\n\\n     array([[ 0.29043958,  0.00899521,  0.01650935],\\n\\n            [ 0.00899521,  0.00036082,  0.00070345],\\n\\n            [ 0.01650935,  0.00070345,  0.00140253]]))\\n\\n\\n\\nWe can now use the resulting parameter vector as starting vector to optimize `f()`. As `curve_fit` is a nonlinear least square method, parameter optimizes `g()` is not necessary the parameter optimizes `f()`, but hopefully it will be close. The covariance matrices are very different of course.\\n\\n\\n\\n    In [78]: curve_fit(f, x, y, p0=curve_fit(g, x, 1/y)[0]) #Alternative Fit\\n\\n    Out[78]:\\n\\n    (array([ 18.0480446 ,  -0.22881647,   0.31200106]),\\n\\n     array([[ 1.14928169,  0.03741604,  0.03897652],\\n\\n            [ 0.03741604,  0.00128511,  0.00136315],\\n\\n            [ 0.03897652,  0.00136315,  0.00145614]]))\\n\\n\\n\\nThe comparison of the results:\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nNow the result is pretty good.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/PjwNZ.png\",\n",
       "  '<python><scipy><runtime-error><curve-fitting>',\n",
       "  datetime.date(2013, 11, 24),\n",
       "  '2013-11-24 16:18:31',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '3536.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['212',\n",
       "  '19504773',\n",
       "  'Answer',\n",
       "  \"matplotlib list not showing 0 or Nan's\",\n",
       "  'Use `np.where` to set the data not to be plotted to `np.nan`.\\n\\n\\n\\n    from numpy import *\\n\\n    a=linspace(1, 50, 1000)\\n\\n    b=sin(a)\\n\\n    c=where(b>-0.7, b, nan) #In this example, we plot only the values larger than -0.7\\n\\n    #if you want to skip the 0, c=where(b!=0, b, nan)\\n\\n    plt.plot(c)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/zRxn2.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2013, 10, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1237.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['215',\n",
       "  '19385591',\n",
       "  'Answer',\n",
       "  'How to count number of rows per group (and other statistics) in pandas group by?',\n",
       "  \"On `groupby` object, the `agg` function can take a list to [apply several aggregation methods][1] at once. This should give you the result you need:\\n\\n\\n\\n    df[['col1', 'col2', 'col3', 'col4']].groupby(['col1', 'col2']).agg(['mean', 'count'])\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/groupby.html#applying-multiple-functions-at-once\",\n",
       "  '<python><group-by><pandas><distinct>',\n",
       "  datetime.date(2013, 10, 15),\n",
       "  '2015-05-17 03:55:35',\n",
       "  'Alexander (2411802)',\n",
       "  '263',\n",
       "  '',\n",
       "  '400866.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['216',\n",
       "  '19459450',\n",
       "  'Answer',\n",
       "  'How to convert a Numpy 2D array with object dtype to a regular 2D array of floats',\n",
       "  \"You may want to use structured array, so that when you need to access the names and the values independently you can easily do so. In this example, there are two data points:\\n\\n\\n\\n\\tx = zeros(2, dtype=[('name','S10'), ('value','f4',(3,))])\\n\\n\\tx[0][0]='item1'\\n\\n\\tx[1][0]='item2'\\n\\n\\ty1=x['name']\\n\\n\\ty2=x['value']\\n\\n\\n\\nthe result:\\n\\n\\n\\n    >>> y1\\n\\n    array(['item1', 'item2'], \\n\\n          dtype='|S10')\\n\\n    >>> y2\\n\\n    array([[ 0.,  0.,  0.],\\n\\n           [ 0.,  0.,  0.]], dtype=float32)\\n\\n\\n\\nSee more details: http://docs.scipy.org/doc/numpy/user/basics.rec.html\",\n",
       "  '<python><arrays><object><numpy><2d>',\n",
       "  datetime.date(2013, 10, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '22578.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['219',\n",
       "  '22308136',\n",
       "  'Answer',\n",
       "  'Pandas - cumsum by month?',\n",
       "  \"Use `.groupby()`, but don't just group by month, `groupby` year-month instead. Or else `2013-02` will be in the same group as `2014-02`, etc.\\n\\n\\n\\n\\tIn [96]:\\n\\n\\t\\n\\n\\tdf['Month']=df['Date'].apply(lambda x: x[:7])\\n\\n\\tIn [97]:\\n\\n\\t\\n\\n\\tdf['csn']=df.groupby(['Month'])['n'].cumsum()\\n\\n\\tIn [98]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t\\t\\t Date  n    Month  csn\\n\\n\\t0  2014-02-27  4  2014-02    4\\n\\n\\t1  2014-02-28  5  2014-02    9\\n\\n\\t2  2014-03-01  1  2014-03    1\\n\\n\\t3  2014-03-02  6  2014-03    7\\n\\n\\t4  2014-03-03  7  2014-03   14\\n\\n\\t\\n\\n\\t[5 rows x 4 columns]\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2014, 3, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '2719.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['222',\n",
       "  '22598676',\n",
       "  'Answer',\n",
       "  'Build diagonal matrix without using for loop',\n",
       "  'Notice that the number of `0` is always 3 (or a constant whenever you want to have a diagonal matrix like this):\\n\\n\\n\\n\\tIn [10]:\\t\\n\\n\\timport numpy as np\\n\\n\\tA1=[0.1, 0.2]\\n\\n\\tA2=[1,2,3]\\n\\n\\tA3=[4,5]\\n\\n\\tSPC=[0,0,0] #=or use np.zeros #spacing zeros\\n\\n\\tnp.hstack((A1,SPC,A2,SPC,A2,SPC,A2,SPC,A3)).reshape(5,5)\\n\\n\\tOut[10]:\\n\\n\\tarray([[ 0.1,  0.2,  0. ,  0. ,  0. ],\\n\\n\\t       [ 1. ,  2. ,  3. ,  0. ,  0. ],\\n\\n\\t       [ 0. ,  1. ,  2. ,  3. ,  0. ],\\n\\n\\t       [ 0. ,  0. ,  1. ,  2. ,  3. ],\\n\\n\\t       [ 0. ,  0. ,  0. ,  4. ,  5. ]])\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n\\tIn [11]:\\t\\n\\n\\timport itertools #A more general way of doing it\\n\\n\\tnp.hstack(list(itertools.chain(*[(item, SPC) for item in [A1, A2, A2, A2, A3]]))[:-1]).reshape(5,5)\\n\\n\\tOut[11]:\\n\\n\\tarray([[ 0.1,  0.2,  0. ,  0. ,  0. ],\\n\\n\\t       [ 1. ,  2. ,  3. ,  0. ,  0. ],\\n\\n\\t       [ 0. ,  1. ,  2. ,  3. ,  0. ],\\n\\n\\t       [ 0. ,  0. ,  1. ,  2. ,  3. ],\\n\\n\\t       [ 0. ,  0. ,  0. ,  4. ,  5. ]])',\n",
       "  '<python><numpy><scipy><diagonal>',\n",
       "  datetime.date(2014, 3, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1823.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['224',\n",
       "  '20436672',\n",
       "  'Answer',\n",
       "  'Replacing items in a two dimensional list in python',\n",
       "  \"Shouldn't it be just like this?\\n\\n\\n\\n    >>> def f():\\n\\n        values = [[10,0], [13, 0], [36, 0], [74,0], [22,0]]\\n\\n        user = int(input('Enter a whole number'))\\n\\n        for i in range(len(values)):\\n\\n       \\t    values[i][1]=values[i][0]+user\\n\\n                print(values[i])\\n\\n     \\n\\n             \\n\\n    >>> f()\\n\\n    Enter a whole number2\\n\\n    [10, 12]\\n\\n    [13, 15]\\n\\n    [36, 38]\\n\\n    [74, 76]\\n\\n    [22, 24]\",\n",
       "  '<python><list><dimensional>',\n",
       "  datetime.date(2013, 12, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1223.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['227',\n",
       "  '19574569',\n",
       "  'Answer',\n",
       "  'How to store a numpy arrays in a column of a Pandas dataframe?',\n",
       "  \"Store them as elements as you would do for any other data:\\n\\n\\n\\n    import numpy as np\\n\\n    import pandas as pd\\n\\n    a = np.arange(10).reshape(2,5)\\n\\n    b = np.arange(10, 20).reshape(2,5)\\n\\n    pd.DataFrame({'foo':[42,51], 'arr':[a,b]})\\n\\n    Out[10]: \\n\\n                                                arr  foo\\n\\n    0            [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]   42\\n\\n    1  [[10, 11, 12, 13, 14], [15, 16, 17, 18, 19]]   51\\n\\n\\n\\nNote that what you try to do sounds more to use a [`Panel`][1].\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/dsintro.html#panel\",\n",
       "  '<python><python-2.7><numpy><pandas>',\n",
       "  datetime.date(2013, 10, 24),\n",
       "  '2015-12-28 03:30:52',\n",
       "  'Back2Basics (1499803)',\n",
       "  '13',\n",
       "  '',\n",
       "  '12714.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['230',\n",
       "  '20039978',\n",
       "  'Answer',\n",
       "  'How to visualize 95% confidence interval in matplotlib?',\n",
       "  \"You don't need `.interval` method, to get the *size* of confidence interval, you just need the `.ppf` method. \\n\\n\\n\\n\\timport numpy as np\\n\\n\\timport scipy.stats as ss\\n\\n\\tdata_m=np.array([1,2,3,4])   #(Means of your data)\\n\\n\\tdata_df=np.array([5,6,7,8])   #(Degree-of-freedoms of your data)\\n\\n\\tdata_sd=np.array([11,12,12,14])   #(Standard Deviations of your data)\\n\\n\\timport matplotlib.pyplot as plt\\n\\n\\tplt.errorbar([0,1,2,3], data_m, yerr=ss.t.ppf(0.95, data_df)*data_sd)\\n\\n\\tplt.xlim((-1,4))\\n\\n\\n\\n`ss.t.ppf(0.95, data_df)*data_sd` is a fully vectorize way to get the (half) size of interval, given the degrees of freedom and standard deviation.\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/kbFwm.png\",\n",
       "  '<python><matplotlib><statistics>',\n",
       "  datetime.date(2013, 11, 18),\n",
       "  '2016-12-29 09:23:07',\n",
       "  'omerbp (3523490)',\n",
       "  '8',\n",
       "  '',\n",
       "  '21432.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['232',\n",
       "  '22702570',\n",
       "  'Answer',\n",
       "  'Pandas: how to get a particular group after groupby?',\n",
       "  \"Try: `grouped.get_group('foo')`, that is what you need. \",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 3, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2749.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['234',\n",
       "  '22747708',\n",
       "  'Answer',\n",
       "  'Looking for the motifs in sequence',\n",
       "  \"It there something already exist that can do it? `biopython`? But anyway, it is not that hard and you don't need a loop:\\n\\n\\n\\n\\timport re\\n\\n\\t\\n\\n\\tseq='aaattatagggatatata'\\n\\n\\t\\n\\n\\tmotif='ata'\\n\\n\\t\\n\\n\\tQ=re.compile(motif)\\n\\n\\t\\n\\n\\t[item.start(0) for item in Q.finditer(seq)] #or maybe item.start(0)+1 if you want it\\n\\n\\t#Out[23]: [5, 11, 15]\\n\\n\",\n",
       "  '<python><sequence><bioinformatics>',\n",
       "  datetime.date(2014, 3, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '5238.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['240',\n",
       "  '20199459',\n",
       "  'Answer',\n",
       "  'how to make argsort result to be random between equal values?',\n",
       "  'Use `lexsort`:\\n\\n`np.lexsort((b,a))` means Sort by `a`, then by `b`\\n\\n\\n\\n    >>> a\\n\\n    array([0, 3, 1, 1, 1])\\n\\n    >>> b=np.random.random(a.size)\\n\\n    >>> b\\n\\n    array([ 0.00673736,  0.90089115,  0.31407214,  0.24299867,  0.7223546 ])\\n\\n    >>> np.lexsort((b,a))\\n\\n    array([0, 3, 2, 4, 1])\\n\\n    >>> a.argsort()\\n\\n    array([0, 2, 3, 4, 1])\\n\\n    >>> a[[0, 3, 2, 4, 1]]\\n\\n    array([0, 1, 1, 1, 3])\\n\\n    >>> a[[0, 2, 3, 4, 1]]\\n\\n    array([0, 1, 1, 1, 3])',\n",
       "  '<python><sorting><random><numpy>',\n",
       "  datetime.date(2013, 11, 25),\n",
       "  '',\n",
       "  '',\n",
       "  '12',\n",
       "  '',\n",
       "  '1211.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['241',\n",
       "  '20726729',\n",
       "  'Answer',\n",
       "  'R translation to Python',\n",
       "  \"First, your last line of code:\\n\\n\\n\\n    hdf = pd.DataFrame(prtns)/(pd.DataFrame(h).cummax()[1:len(h)]-1))[1:len(h)]]\\n\\n\\n\\ncan't be right. May be this according to your `R` code:\\n\\n\\n\\n    hdf = (pd.DataFrame(prtns)/(pd.DataFrame(h).cummax()[1:len(h)])-1)[1:len(h)]\\n\\n\\n\\nSecond, `c(1,p.rtns)` can be replaced with `np.hstack(1, prtns)` instead of converting the `np.array` to `list`.\\n\\n\\n\\nThird, looks like you are using `pandas` just for the `cummax()`. It is not hard to implement one, like this:\\n\\n\\n\\n    def cummax(a):\\n\\n    \\tac=a.copy()\\n\\n    \\tif a.size>0:\\n\\n    \\t\\tmax_idx=np.argmax(a)\\n\\n    \\t\\tac[max_idx:]=np.max(ac)\\n\\n    \\t\\tac[:max_idx]=cummax(ac[:max_idx])\\n\\n    \\telse:\\n\\n    \\t\\tpass\\n\\n    \\treturn ac\\n\\n\\n\\nAnd:\\n\\n\\n\\n    >>> a=np.random.randint(0,20,size=10)\\n\\n    >>> a\\n\\n    array([15, 15, 15,  8,  5, 14,  6, 18,  9,  1])\\n\\n    >>> cummax(a)\\n\\n    array([15, 15, 15, 15, 15, 15, 15, 18, 18, 18])\\n\\n\\n\\nTake these all together we get:\\n\\n\\n\\n    def run_simulation(mu, sigma, days, n):\\n\\n    \\tresult=[]\\n\\n    \\tfor i in range(n):\\n\\n    \\t\\trtns = np.random.normal(loc=1.*mu/days,\\n\\n    \\t\\t\\t\\t\\tscale=(((1./days)**0.5)*sigma),\\n\\n    \\t\\t\\t\\t\\tsize=days)\\n\\n    \\t\\tp_rtns = (rtns+1).cumprod()\\n\\n    \\t\\ttot_rtn = p_rtns[-1]-1 \\n\\n    \\t\\t#looks like you want the last element, rather than the 2nd form the last as you did\\n\\n    \\t\\tp_rtns_md =(p_rtns/cummax(np.hstack((0.,p_rtns)))[1:]-1).min() \\n\\n    \\t\\t#looks like you want to skip the first element, python is different from R for that.\\n\\n    \\t\\tresult.append((tot_rtn, p_rtns_md))\\n\\n    \\treturn result\\n\\n\\n\\nAnd:\\n\\n\\n\\n    >>> run_simulation(0.06, 0.2, 250,10)\\n\\n    [(0.096077511394818016, -0.16621830496112056), (0.73729333554192, -0.13566124517484235), (0.087761655465907973, -0.17862916081223446), (0.07434851091082928, -0.15972961033789046), (-0.094464694393288307, -0.2317397117033817), (-0.090720761054686627, -0.1454002204893271), (0.02221364097529932, -0.15606214341947877), (-0.12362835704696629, -0.24323096421682033), (0.023089144896788261, -0.16916790589553599), (0.39777037782177493, -0.10524624505023494)]\\n\\n\\n\\nThe loop is actually not necessary as we can work in two dimension by generating 2D `array` of Guassian random variable (change `size=days` to `size=(days, n)`). Avoiding the loop will most likely be faster. However, that will require a different `cummax()` function as this one show here is restricted to 1D. But `cummax()` in `R` is restricted to 1D as well (not exactly, if you pass a 2D to `cummax()`, it will be flattened). So to keep things simple and comparable between `Python` and `R`, let's settle for the loop version.\",\n",
       "  '<python><r><numpy>',\n",
       "  datetime.date(2013, 12, 22),\n",
       "  '2013-12-22 06:14:53',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1298.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['242',\n",
       "  '19638459',\n",
       "  'Answer',\n",
       "  'Running an Excel macro via Python?',\n",
       "  \"I suspect you haven't authorize your Excel installation to run macro from an automated Excel. It is a security protection by default at installation. To change this:\\n\\n\\n\\n 1. File > Options > Trust Center\\n\\n 2. Click on Trust Center Settings... button\\n\\n 3. Macro Settings > Check Enable all macros\\n\\n\\n\\n\",\n",
       "  '<python><excel><vba><python-2.7>',\n",
       "  datetime.date(2013, 10, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '49266.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['244',\n",
       "  '19969224',\n",
       "  'Answer',\n",
       "  'How to align the bar and line in matplotlib two y-axes chart?',\n",
       "  \"Just change the final line to:\\n\\n\\n\\n    ax2.plot(ax.get_xticks(),\\n\\n             df[['sales_gr','net_pft_gr']].values,\\n\\n             linestyle='-',\\n\\n             marker='o', linewidth=2.0)\\n\\n\\n\\nYou will be all set.\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nI don't quite get your second question. The 1st and 2nd y axis are of different scale, what do you mean by aligning them to the same line? They can't be aligned to the same grid line (yes you can but the right axis will look ugly, having values like 0.687 and alike). Anyway, you can do:\\n\\n\\n\\n    ax.set_ylim((-10, 80.))\\n\\n\\n\\nto align them, and the plot now looks ugly:\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/uxkFB.png\\n\\n  [2]: http://i.stack.imgur.com/3HwNd.png\",\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2013, 11, 14),\n",
       "  '2018-09-13 22:59:35',\n",
       "  'CT Zhu (2487184)',\n",
       "  '9',\n",
       "  '',\n",
       "  '11553.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['245',\n",
       "  '19976801',\n",
       "  'Answer',\n",
       "  \"pandas.series.copy doesn't create new object\",\n",
       "  '`copy` is defined as a helper to do a copy of the underlying arrays, and the function does not copy the index. See the source code:\\n\\n\\n\\n    Definition: series.copy(self, order=\\'C\\')\\n\\n    Source:\\n\\n        def copy(self, order=\\'C\\'):\\n\\n            \"\"\"\\n\\n            Return new Series with copy of underlying values\\n\\n    \\n\\n            Returns\\n\\n            -------\\n\\n            cp : Series\\n\\n            \"\"\"\\n\\n            return Series(self.values.copy(order), index=self.index,\\n\\n                          name=self.name)\\n\\n\\n\\nThe `index` remains shared by construction. If you want a deeper copy, then just use directly the `Series` constructor:\\n\\n\\n\\n\\n\\n    series = pd.Series(range(3))\\n\\n        ...: series_copy = pd.Series(series.values.copy(), index=series.index.copy(),\\n\\n        ...:                           name=series.name)\\n\\n        ...: series_copy.index += 1\\n\\n    \\n\\n    series\\n\\n    Out[72]: \\n\\n    0    0\\n\\n    1    1\\n\\n    2    2\\n\\n    dtype: int64\\n\\n    \\n\\n    series_copy\\n\\n    Out[73]: \\n\\n    1    0\\n\\n    2    1\\n\\n    3    2\\n\\n    dtype: int64\\n\\n\\n\\nIn 0.13, `copy(deep=True)` is the default interface for copy that will solve your issue. ([Fix is here][1])\\n\\n\\n\\n\\n\\n  [1]: https://github.com/pydata/pandas/commit/d7d9a6cce515b29b0f5ae1371dd22d30393c729b',\n",
       "  '<python><pandas><series>',\n",
       "  datetime.date(2013, 11, 14),\n",
       "  '2013-11-14 11:55:44',\n",
       "  'Boud (624829)',\n",
       "  '5',\n",
       "  '',\n",
       "  '3224.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['248',\n",
       "  '22942693',\n",
       "  'Answer',\n",
       "  'R_user not defined , rpy2',\n",
       "  \"Just create a new system variable `R_USER` in `Environment Variables`, with its value being the current user name, and the problem should goes away. \\n\\n\\n\\n***Note, this is clearly for windows platform only.***\\n\\n\\n\\nOtherwise you won't get `R_USER` exception in the first place.\",\n",
       "  '<django><r><python-2.7><rpy2>',\n",
       "  datetime.date(2014, 4, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1151.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['261',\n",
       "  '20177132',\n",
       "  'Answer',\n",
       "  'Weibull distribution and the data in the same figure (with numpy and scipy)',\n",
       "  'First, I think you want to fix `location` but not `scale`. (So both `scale` and `shape` can change).\\n\\nSecond, I think (not 100% sure) that you can\\'t have `0` in your data for Weibull (unless you hardcode a Weibull class yourself), so I changed your `0` to a small value `1e-8`.\\n\\n\\n\\n    >>> xdata=array([1e-8,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,3,3,3,3,3,3,4,4,4,5,4,4,5,5,6,6,6,6,7,7,8,8,8,9,9,10,11,12,13,13,14,14,13,17,14,15,17,18,18,19,22,23,22,23,24,26,28,32,33,32,31,33,34,37,36,40,40,41,44,41,44,45,47,52,53,51,52,52,53,55,56,59,61,62,65,63,68,69,80,71,71,72,71,69,70,70,71,72,73,75,74,74,75,76,74,79,77,77,77,84,92,88,79,81,81,83,84,88,87,84,84,85,85,85,94,95,91,89,90,87,89,89,90,93,92,93,96,95,98,99,100,99,100,98,94,89,87,86,85,85,84,85,83,83,84,83,81,85,83,83,81,84,93,91,78,79,80,80,80,80,80,78,79,78,79,80,78,78,78,78,79,77,77,77,78,80,82,83,82,80,82,82,83,87,82,82,80,80,79,77,77,77,77,75,75,73,71,73,73,70,72,69,70,70,78,81,69,68,68,68,65,64,66,65,64,62,62,62,62,67,65,61,61,59,58,59,59,59,59,59,59,59,59,59,59,59,58,56,55,52,50,50,48,48,47,46,46,45,44,44,43,43,43,41,41,41,46,47,40,39,39,38,37,37,38,36,35,35,35,35,36,35,33,33,32,31,31,31,29,29,28,28,28,28,30,30,30,28,27,26,25,23,22,23,22,21,20,19,19,18,18,18,17,17,17,14,14,13,13,14,13,12,12,11,11,10,10,9,9,9,8,8,8,8,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,5,5,5,5,5,5,5,5,5,5,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,3,3,3,3,3,3,3,3,3,3,3,3,3,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,3,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2])\\n\\n    >>> stats.exponweib.fit(xdata, floc=0, f0=1)\\n\\n    (1, 0.87120924706137459, 0, 35.884593247790207)\\n\\n    >>> stats.weibull_min.fit(xdata, floc=0)\\n\\n    (0.87120924706137459, 0, 35.884593247790036)\\n\\n    >>> p0, p1, p2=stats.weibull_min.fit(xdata, floc=0)\\n\\n    >>> ydata=stats.weibull_min.pdf(linspace(0, 120, 100), p0, p1, p2)\\n\\n    >>> plt.hist(xdata, 25, normed=True)\\n\\n    >>> plt.plot(linspace(0, 120, 100), ydata, \\'-\\')\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nThe fit is actually correct. It looks ugly but it is due to a large proportion of your data is smallish.\\n\\n\\n\\nFinally, I actually suspect that your original data is already frequency data not raw data, is that the case? (Let\\'s assume your data is not interval-censored, that will require quite a bit of hardcode)\\n\\n\\n\\n    >>> import itertools\\n\\n    >>> x2data=list(itertools.chain(*[[i,]*val for i, val in enumerate(xdata)]))\\n\\n    >>> p0, p1, p2=stats.weibull_min.fit(x2data, floc=0)\\n\\n    >>> y2data=stats.weibull_min.pdf(linspace(0, 500, 100), p0, p1, p2)\\n\\n    >>> plt.plot(linspace(0, 500, 100), y2data, \\'-\\')\\n\\n    [<matplotlib.lines.Line2D object at 0x0360B6B0>]\\n\\n    >>> r1,r2,r3=plt.hist(x2data, bins=60, normed=True)\\n\\n\\n\\n![enter image description here][2]\\n\\nNow the result looks much more reasonable. Although it still does not appears to be very closely  Weibull distributed. More like http://en.wikipedia.org/wiki/Shifted_Gompertz_distribution.\\n\\n\\n\\nUpdate: yes, if you have `0` in your data, you will get this when you call `fit` methods (`scipy` 0.12.0):\\n\\n\\n\\n    Warning (from warnings module):\\n\\n      File \"C:\\\\Python27\\\\lib\\\\site-packages\\\\scipy\\\\optimize\\\\optimize.py\", line 438\\n\\n         and numpy.max(numpy.abs(fsim[0] - fsim[1:])) <= ftol):\\n\\n    RuntimeWarning: invalid value encountered in subtract\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/JA70m.png\\n\\n  [2]: http://i.stack.imgur.com/91z0S.png',\n",
       "  '<python><numpy><matplotlib><scipy><weibull>',\n",
       "  datetime.date(2013, 11, 24),\n",
       "  '2013-11-24 17:22:17',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '3195.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['263',\n",
       "  '23037543',\n",
       "  'Answer',\n",
       "  'Overlaying actual data on a boxplot from a pandas dataframe',\n",
       "  \"A general solution for the boxplot for the entire dataframe, which should work for both `seaborn` and `pandas` as their are all `matplotlib` based under the hood, I will use `pandas` plot as the example, assuming `import matplotlib.pyplot as plt` already in place. As you have already have the `ax`, it would make better sense to just use `ax.text(...)` instead of `plt.text(...)`.\\n\\n\\n\\n\\tIn [35]:\\t\\n\\n\\tprint df\\n\\n\\t         V1        V2        V3        V4        V5\\n\\n\\t0  0.895739  0.850580  0.307908  0.917853  0.047017\\n\\n\\t1  0.931968  0.284934  0.335696  0.153758  0.898149\\n\\n\\t2  0.405657  0.472525  0.958116  0.859716  0.067340\\n\\n\\t3  0.843003  0.224331  0.301219  0.000170  0.229840\\n\\n\\t4  0.634489  0.905062  0.857495  0.246697  0.983037\\n\\n\\t5  0.573692  0.951600  0.023633  0.292816  0.243963\\n\\n\\t\\n\\n\\t[6 rows x 5 columns]\\n\\n\\t\\n\\n\\tIn [34]:\\t\\n\\n\\tdf.boxplot()\\n\\n\\tfor x, y, s in zip(np.repeat(np.arange(df.shape[1])+1, df.shape[0]), \\n\\n\\t                   df.values.ravel(), df.values.astype('|S5').ravel()):\\n\\n\\t    plt.text(x,y,s,ha='center',va='center')\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nFor a single series in the dataframe, a few small changes is necessary:\\n\\n\\t    \\n\\n\\tIn [35]:\\t\\n\\n\\tsub_df=df.V1\\n\\n\\tpd.DataFrame(sub_df).boxplot()\\n\\n\\tfor x, y, s in zip(np.repeat(1, df.shape[0]), \\n\\n\\t                   sub_df.ravel(), sub_df.values.astype('|S5').ravel()):\\n\\n\\t    plt.text(x,y,s,ha='center',va='center')\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\nMaking scatter plots is also similar:\\n\\n\\n\\n\\t#for the whole thing\\n\\n\\tdf.boxplot()\\n\\n\\tplt.scatter(np.repeat(np.arange(df.shape[1])+1, df.shape[0]), df.values.ravel(), marker='+', alpha=0.5)\\n\\n\\t#for just one column\\n\\n\\tsub_df=df.V1\\n\\n\\tpd.DataFrame(sub_df).boxplot()\\n\\n\\tplt.scatter(np.repeat(1, df.shape[0]), sub_df.ravel(), marker='+', alpha=0.5)\\n\\n\\n\\n![enter image description here][3]\\n\\n![enter image description here][4]\\n\\n\\n\\nTo overlay stuff on `boxplot`, we need to first guess where each boxes are plotted at among `xaxis`. They appears to be at `1,2,3,4,....`. Therefore, for the values in the first column, we want them to be plot at x=1; the 2nd column at x=2 and so on. \\n\\n\\n\\nAny efficient way of doing it is to use `np.repeat`, repeat `1,2,3,4...`, each for `n` times, where `n` is the number of observations. Then we can make a plot, using those numbers as `x` coordinates. Since it is one-dimensional, for the `y` coordinates, we will need a flatten view of the data, provided by `df.ravel()`\\n\\n\\n\\nFor overlaying the text strings, we need a anther step (a loop). As we can only plot one x value, one y value and one text string at a time.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/UseTN.png\\n\\n  [2]: http://i.stack.imgur.com/d6TA8.png\\n\\n  [3]: http://i.stack.imgur.com/fltVW.png\\n\\n  [4]: http://i.stack.imgur.com/wEw0g.png\",\n",
       "  '<matplotlib><pandas><dataframe><boxplot><seaborn>',\n",
       "  datetime.date(2014, 4, 12),\n",
       "  '2014-04-13 01:19:59',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '5516.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['265',\n",
       "  '21719109',\n",
       "  'Answer',\n",
       "  'Display Entire Dataset,Python',\n",
       "  \"The default options are listed here\\n\\n\\n\\n    pd.describe_option('display')\\n\\n\\n\\naccording to the option descriptions, to change the default print behavior, I guess:\\n\\n\\n\\n    pd.set_option('display.height', some_value)\\n\\n    pd.set_option('display.max_rows', some_value)\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 2, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1713.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['267',\n",
       "  '21861332',\n",
       "  'Answer',\n",
       "  'Python plot label',\n",
       "  \"I think the most `matplotlib`ish way of doing it would be to issue a separate `legend()` once the plots were generated.\\n\\n\\n\\n\\tl_plot=[]\\n\\n\\tfor i in range(10):\\n\\n\\t    x=arange(10)\\n\\n\\t    y=random.random(10)\\n\\n\\t    l_plot.append(plt.plot(x, y, '+-'))\\n\\n\\tplt.xlim(0,12)\\n\\n\\tplt.legend([item[0] for item in l_plot], map(str, range(10))) #change it to the plot labels say ['Mass = %f'%item for item in range(10)].\\n\\n\\tplt.savefig('temp.png')\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/JxIMi.png\",\n",
       "  '<python><matplotlib><label>',\n",
       "  datetime.date(2014, 2, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '4692.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['268',\n",
       "  '21923587',\n",
       "  'Answer',\n",
       "  'Extracting single value from column in pandas',\n",
       "  \"Simply: `df['B'][df['A'] == 23]`\\n\\n\\n\\nThanks @Jeff.\\n\\n\\n\\nAnd the speed comparisons:\\n\\n\\n\\n    In [30]:\\n\\n\\n\\n    %timeit df['B'][df['A'] == 23].values\\n\\n    1000 loops, best of 3: 813 µs per loop\\n\\n    In [31]:\\n\\n    \\n\\n    %timeit df.loc[df['A'] == 23, 'B']\\n\\n    1000 loops, best of 3: 976 µs per loop\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 2, 21),\n",
       "  '2014-02-21 02:20:33',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '8004.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['269',\n",
       "  '21923939',\n",
       "  'Answer',\n",
       "  'Add minor gridlines to matplotlib plot using seaborn',\n",
       "  \"That's because the minor ticks are not yet defined, so we need to add for example:\\n\\n\\n\\n    ax.set_xticks(np.arange(0,8)-0.5, minor=True)\\n\\n    ax.set_yticks([-1.25, -0.75, -0.25,0.24,0.75,1.25], minor=True)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/jjK9y.png\",\n",
       "  '<python><matplotlib><seaborn>',\n",
       "  datetime.date(2014, 2, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '12385.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['270',\n",
       "  '21924851',\n",
       "  'Answer',\n",
       "  'LinearRegression Predict- ValueError: matrices are not aligned',\n",
       "  'Trying to predict stock price by simple linear regression? :^|. Anyway, this is what you need to change:\\n\\n\\n\\n\\tIn [19]:\\n\\n\\t\\n\\n\\tM=model.fit(close[0][:-1].reshape(-1,1), close[0][1:].reshape(-1,1))\\n\\n\\tIn [31]:\\n\\n\\t\\n\\n\\tM.predict(close[0][-20:].reshape(-1,1))\\n\\n\\tOut[31]:\\n\\n\\tarray([[ 90.92224274],\\n\\n\\t       [ 94.41875811],\\n\\n\\t       [ 93.19997275],\\n\\n\\t       [ 94.21895723],\\n\\n\\t       [ 94.31885767],\\n\\n\\t       [ 93.030142  ],\\n\\n\\t       [ 90.76240203],\\n\\n\\t       [ 91.29187436],\\n\\n\\t       [ 92.41075928],\\n\\n\\t       [ 89.0940647 ],\\n\\n\\t       [ 85.10803717],\\n\\n\\t       [ 86.90624508],\\n\\n\\t       [ 89.39376602],\\n\\n\\t       [ 90.59257129],\\n\\n\\t       [ 91.27189427],\\n\\n\\t       [ 91.02214318],\\n\\n\\t       [ 92.86031126],\\n\\n\\t       [ 94.25891741],\\n\\n\\t       [ 94.45871828],\\n\\n\\t       [ 92.65052033]])\\n\\n\\n\\nRemember, when you build a model, `X` and `y` for `.fit` method should have the shape of `[n_samples,n_features]`. The same applies to the `.predict` method.',\n",
       "  '<python><numpy><linear-regression><predict>',\n",
       "  datetime.date(2014, 2, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2724.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['273',\n",
       "  '21771438',\n",
       "  'Answer',\n",
       "  'Finding non-numeric rows in dataframe in pandas?',\n",
       "  \"Sorry about the confusion, this should be the correct approach. Do you want only to capture `'bad'` only, not things like `'good'`; Or just any non-numerical values?\\n\\n\\n\\n    In[15]:\\n\\n    np.where(np.any(np.isnan(df.convert_objects(convert_numeric=True)), axis=1))\\n\\n    Out[15]:\\n\\n    (array([3]),)\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2014, 2, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '47737.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['276',\n",
       "  '23091382',\n",
       "  'Answer',\n",
       "  'matplotlib colorbar not working (due to garbage collection?)',\n",
       "  'It is because you first example, you are using `ax.polormesh`, not `pyplot.polotmesh` (namespace imported by `pylab`), when you call `colorbar()` (actually `plt.colorbar()`), it lost track of which mappable and which ax it should make colorbar to.\\n\\n\\n\\nTherefore adding these lines will make it work:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    fct()\\n\\n    ax=plt.gca() #get the current axes\\n\\n    PCM=ax.get_children()[2] #get the mappable, the 1st and the 2nd are the x and y axes\\n\\n    plt.colorbar(PCM, ax=ax) \\n\\n![enter image description here][1]\\n\\n\\n\\nNow you mentioned that your actual plot is a much more complex one. You want to make sure it is the `ax.get_children()[2]` or you can pick the it by look for a `matplotlib.collections.QuadMesh` instance.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/YrwoB.png',\n",
       "  '<python><matplotlib><garbage-collection><ipython>',\n",
       "  datetime.date(2014, 4, 15),\n",
       "  '2014-04-16 05:20:15',\n",
       "  'CT Zhu (2487184)',\n",
       "  '11',\n",
       "  '',\n",
       "  '9161.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['278',\n",
       "  '23115098',\n",
       "  'Answer',\n",
       "  'autopct cant be added to axis.pie :: error: too many values',\n",
       "  \"If you want to use `autopct`, remember now you have 3 values to be unpacked instead of two, so change your code to `wedges, plt_labels, junk = ax.pie([20, 40, 60], labels=labels, autopct='%1.1f%%')` will solve your problem\\n\\n\\n\\n`juck` is going to be the `text.Text` objects of your percentage values.\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/pSo1z.png\",\n",
       "  '<python><matplotlib><pie-chart>',\n",
       "  datetime.date(2014, 4, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1061.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['280',\n",
       "  '21655221',\n",
       "  'Answer',\n",
       "  'Scatter plots in Pandas/Pyplot: How to plot by category',\n",
       "  'With `plt.scatter`, I can only think of one: to use a proxy artist:\\n\\n\\n\\n    df = pd.DataFrame(np.random.normal(10,1,30).reshape(10,3), index = pd.date_range(\\'2010-01-01\\', freq = \\'M\\', periods = 10), columns = (\\'one\\', \\'two\\', \\'three\\'))\\n\\n    df[\\'key1\\'] = (4,4,4,6,6,6,8,8,8,8)\\n\\n    fig1 = plt.figure(1)\\n\\n    ax1 = fig1.add_subplot(111)\\n\\n    x=ax1.scatter(df[\\'one\\'], df[\\'two\\'], marker = \\'o\\', c = df[\\'key1\\'], alpha = 0.8)\\n\\n\\n\\n    ccm=x.get_cmap()\\n\\n    circles=[Line2D(range(1), range(1), color=\\'w\\', marker=\\'o\\', markersize=10, markerfacecolor=item) for item in ccm((array([4,6,8])-4.0)/4)]\\n\\n    leg = plt.legend(circles, [\\'4\\',\\'6\\',\\'8\\'], loc = \"center left\", bbox_to_anchor = (1, 0.5), numpoints = 1)\\n\\n\\n\\nAnd the result is:\\n\\n\\n\\n![enter image description here][1]\\n\\n \\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/7KzM5.png',\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2014, 2, 9),\n",
       "  '2014-02-09 04:43:37',\n",
       "  'CT Zhu (2487184)',\n",
       "  '17',\n",
       "  '',\n",
       "  '88036.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['283',\n",
       "  '21665511',\n",
       "  'Answer',\n",
       "  'python pandas : How to get the index of the values from a series that have matching values in other series?',\n",
       "  \"I think `serie2.index[(array(serie2)=='A').flatten()]` may work. `'A'` is the value you want to find index for. \\n\\n\\n\\nOr this, which may be less readable: `serie2.index[(serie2=='A')[0]]`\",\n",
       "  '<python><pandas><series>',\n",
       "  datetime.date(2014, 2, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2950.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['287',\n",
       "  '20831107',\n",
       "  'Answer',\n",
       "  'how to solve 3 nonlinear equations in python',\n",
       "  \"@Christian, I don't think the equation system can be linearize easily, unlike the post you suggested.\\n\\n\\n\\nPowell's Hybrid method (`optimize.fsolve()`) is quite sensitive to initial conditions, so it is very useful if you can come up with a good initial parameter guess. In the following example, we firstly minimize the sum-of-squares of all three equations using Nelder-Mead method (`optimize.fmin()`, for small problem like OP, this is probably already enough). The resulting parameter vector is then used as the initial guess for `optimize.fsolve()` to get the final result.\\n\\n\\n\\n    >>> from numpy import *\\n\\n    >>> from scipy import stats\\n\\n    >>> from scipy import optimize\\n\\n    >>> HF, M1F, x=1000.,900.,10.\\n\\n    >>> def f(p):\\n\\n    \\treturn abs(sum(array(equations(p))**2)-0)\\n\\n    >>> optimize.fmin(f, (1.,1.,1.))\\n\\n    Optimization terminated successfully.\\n\\n             Current function value: 0.000000\\n\\n             Iterations: 131\\n\\n             Function evaluations: 239\\n\\n    array([ -8.95023217,   9.45274653, -11.1728963 ])\\n\\n    >>> optimize.fsolve(equations, (-8.95023217,   9.45274653, -11.1728963))\\n\\n    array([ -8.95022376,   9.45273632, -11.17290503])\\n\\n    >>> pr=optimize.fsolve(equations, (-8.95023217,   9.45274653, -11.1728963))\\n\\n    >>> equations(pr)\\n\\n    (-7.9580786405131221e-13, -1.2732925824820995e-10, -5.6843418860808015e-14)\\n\\n\\n\\nThe result is pretty good.\\n\\n\\n\\n\",\n",
       "  '<python-2.7><optimization><numpy><scipy><nonlinear-optimization>',\n",
       "  datetime.date(2013, 12, 29),\n",
       "  '2013-12-29 23:16:19',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1771.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['289',\n",
       "  '20024658',\n",
       "  'Answer',\n",
       "  'Changing colour scheme of python matplotlib python plots',\n",
       "  \"You can use some colormaps to color your lines, example (here I use the 'jet' colormap):\\n\\n\\n\\n    >>> from matplotlib import cm\\n\\n    >>> for i, val in enumerate(cm.jet(linspace(0,1,10))): #different lines\\n\\n    \\tplt.plot(arange(10), arange(10)+i, color=val, linestyle='-')\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nTo do this in your existing code with minimal changes, just add (And don't forget to change `10` to the number of plots that you have.):\\n\\n\\n\\n    for L, C in zip([item for item in ax.get_children() if isinstance(item, matplotlib.lines.Line2D)], cm.jet(linspace(0,1,10))):\\n\\n    \\tL.set_color(C)   \\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/u7eQA.png\",\n",
       "  '<python><colors><matplotlib><plot><color-mapping>',\n",
       "  datetime.date(2013, 11, 16),\n",
       "  '2013-11-17 15:39:45',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '4921.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['291',\n",
       "  '20893948',\n",
       "  'Answer',\n",
       "  'How to get the union of two lists using list comprehension?',\n",
       "  \"    >>> list(set(a).union(b))\\n\\n    ['Orange and Banana', 'Orange Banana', 'Grapes']\\n\\n\\n\\nThanks @abarnert\",\n",
       "  '<python><list-comprehension>',\n",
       "  datetime.date(2014, 1, 2),\n",
       "  '2014-01-03 00:08:02',\n",
       "  'CT Zhu (2487184)',\n",
       "  '7',\n",
       "  '',\n",
       "  '54557.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['292',\n",
       "  '21011445',\n",
       "  'Answer',\n",
       "  'Fast column shuffle of each row numpy',\n",
       "  'Here are some ideas:\\n\\n\\n\\n    In [10]: a=np.zeros(shape=(1000,3))\\n\\n    \\n\\n    In [12]: a[:,0]=1\\n\\n    \\n\\n    In [13]: a[:,1]=2\\n\\n    \\n\\n    In [14]: a[:,2]=3\\n\\n    \\n\\n    In [17]: %timeit map(np.random.shuffle, a)\\n\\n    100 loops, best of 3: 4.65 ms per loop\\n\\n    \\n\\n    In [21]: all_perm=np.array((list(itertools.permutations([0,1,2]))))\\n\\n    \\n\\n    In [22]: b=all_perm[np.random.randint(0,6,size=1000)]\\n\\n    \\n\\n    In [25]: %timeit (a.flatten()[(b+3*np.arange(1000)[...,np.newaxis]).flatten()]).reshape(a.shape)\\n\\n    1000 loops, best of 3: 393 us per loop\\n\\n\\n\\nIf there are only a few columns, then the number of all possible permutation is much smaller than the number of rows in the array (in this case, when there are only 3 columns, there are only 6 possible permutations). A way to make it faster is to make all the permutations at once first and then rearrange each row by randomly picking one permutation from all possible permutations.  \\n\\n\\n\\nIt still appears to be 10 times faster even with larger dimension:\\n\\n\\n\\n    #adjust a accordingly\\n\\n    In [32]: b=all_perm[np.random.randint(0,6,size=1000000)]\\n\\n\\n\\n    In [33]: %timeit (a.flatten()[(b+3*np.arange(1000000)[...,np.newaxis]).flatten()]).reshape(a.shape)\\n\\n    1 loops, best of 3: 348 ms per loop\\n\\n\\n\\n    In [34]: %timeit map(np.random.shuffle, a)\\n\\n    1 loops, best of 3: 4.64 s per loop',\n",
       "  '<python><random><numpy><vectorization>',\n",
       "  datetime.date(2014, 1, 9),\n",
       "  '2014-01-09 04:16:28',\n",
       "  'CT Zhu (2487184)',\n",
       "  '6',\n",
       "  '',\n",
       "  '2009.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['293',\n",
       "  '23165846',\n",
       "  'Answer',\n",
       "  'Random number function python that includes 1?',\n",
       "  'Would it be just:\\n\\n\\n\\n    list_rnd=[random.random() for i in range(_number_of_numbers_you_want)]\\n\\n    list_rnd=[item/max(list_rnd) for item in list_rnd]\\n\\n\\n\\nGenerate a list of random numbers and divide it by its max value. The resulting list still flows uniform distribution.',\n",
       "  '<python><random>',\n",
       "  datetime.date(2014, 4, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1929.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['294',\n",
       "  '23230204',\n",
       "  'Answer',\n",
       "  \"How to find parameters of gumbel's distribution using scipy.optimize\",\n",
       "  'First of all, the easiest way of optimizing a function using `scipy.optimize` is to construct the target function such that the first argument is a list of the parameters need to be optimized and the following arguments specify other things, such as data and fixed parameters.\\n\\n\\n\\nSecond, it will be very helpful to use the vectorization provided by `numpy`\\n\\n\\n\\nTherefore we have these:\\n\\n\\n\\n\\tIn [61]:\\n\\n\\t#modified pdf and cdf\\n\\n\\tdef gumbel_pdf(x, loc, scale):\\n\\n\\t\\t\"\"\"Returns the value of Gumbel\\'s pdf with parameters loc and scale at x .\\n\\n\\t\\t\"\"\"\\n\\n\\t\\t# substitute\\n\\n\\t\\tz = (x - loc)/scale\\n\\n\\t \\n\\n\\t\\treturn (1./scale) * (np.exp(-(z + (np.exp(-z)))))\\n\\n\\t \\n\\n\\tdef gumbel_cdf(x, loc, scale):\\n\\n\\t\\t\"\"\"Returns the value of Gumbel\\'s cdf with parameters loc and scale at x.\\n\\n\\t\\t\"\"\"\\n\\n\\t\\treturn np.exp(-np.exp(-(x-loc)/scale))\\n\\n\\tIn [62]:\\n\\n\\t\\n\\n\\tdef trunc_GBL(p, x):\\n\\n\\t\\tthreshold=p[0]\\n\\n\\t\\tloc=p[1]\\n\\n\\t\\tscale=p[2]\\n\\n\\t\\tx1=x[x<threshold]\\n\\n\\t\\tnx2=len(x[x>=threshold])\\n\\n\\t\\tL1=(-np.log((gumbel_pdf(x1, loc, scale)/scale))).sum()\\n\\n\\t\\tL2=(-np.log(1-gumbel_cdf(threshold, loc, scale)))*nx2\\n\\n\\t\\t#print x1, nx2, L1, L2\\n\\n\\t\\treturn L1+L2\\n\\n\\tIn [63]:\\n\\n\\t\\n\\n\\timport scipy.optimize as so\\n\\n\\tIn [64]:\\n\\n\\t#first we make a simple Gumbel fit\\n\\n\\tso.fmin(lambda p, x: (-np.log(gumbel_pdf(x, p[0], p[1]))).sum(), [0.5,0.5], args=(np.array(non_truncated_data),))\\n\\n\\tOptimization terminated successfully.\\n\\n\\t\\t\\t Current function value: 35.401255\\n\\n\\t\\t\\t Iterations: 70\\n\\n\\t\\t\\t Function evaluations: 133\\n\\n\\tOut[64]:\\n\\n\\tarray([ 16.47028986,   0.72449091])\\n\\n\\tIn [65]:\\n\\n\\t#then we use the result as starting value for your truncated Gumbel fit\\n\\n\\tso.fmin(trunc_GBL, [17, 16.47028986,   0.72449091],  args=(np.array(non_truncated_data),))\\n\\n\\tOptimization terminated successfully.\\n\\n\\t\\t\\t Current function value: 0.000000\\n\\n\\t\\t\\t Iterations: 25\\n\\n\\t\\t\\t Function evaluations: 94\\n\\n\\tOut[65]:\\n\\n\\tarray([ 13.41111111,  16.65329308,   0.79694   ])\\n\\n\\n\\nHere in the `trunc_GBL` function I substituted your pdf with a scaled pdf\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nSee rationales here, basically is it because your `L1` is pdf based and `L2` is cdf based: http://support.sas.com/documentation/cdl/en/statug/63033/HTML/default/viewer.htm#statug_lifereg_sect018.htm\\n\\n\\n\\nThen we notice a problem see: `Current function value: 0.000000` in the last output. The negative loglikelihood function is 0. \\n\\n\\n\\nThis is because:\\n\\n\\n\\n    In [66]:\\n\\n \\n\\n    gumbel_cdf(13.41111111,  16.47028986,   0.72449091)\\n\\n    Out[66]:\\n\\n    2.3923515777163676e-30\\n\\n\\n\\nEffectively 0. This means based on the model you just described, maximum is always reached when the threshold value is low enough such that `L1` is non-exsit (`x < threshold` is empty) and `L2` is 1 (`1-F(C)` is `1`, for all items in your data).\\n\\n\\n\\nFor this reason, your model does not look all that right to me. You may want to rethink about it. \\n\\n\\n\\n#Edit\\n\\nWe can further isolate `threshold` and treat it as a fixed parameter:\\n\\n\\n\\n    def trunc_GBL(p, x, threshold):\\n\\n        loc=p[0]\\n\\n        scale=p[1]\\n\\n        x1=x[x<threshold]\\n\\n        nx2=len(x[x>=threshold])\\n\\n        L1=(-np.log((gumbel_pdf(x1, loc, scale)/scale))).sum()\\n\\n        L2=(-np.log(1-gumbel_cdf(threshold, loc, scale)))*nx2\\n\\n        #print x1, nx2, L1, L2\\n\\n        return L1+L2\\n\\n\\n\\nAnd call the optimizer differently:\\n\\n\\n\\n    so.fmin(trunc_GBL, [0.5, 0.5], args=(X, np.percentile(X, 20)))\\n\\n    Optimization terminated successfully.\\n\\n             Current function value: 20.412818\\n\\n             Iterations: 72\\n\\n             Function evaluations: 136\\n\\n    Out[9]:\\n\\n    array([ 16.34594943,   0.45253201])\\n\\n\\n\\nThis way if you want 70% quantile, you can simply changed it to `np.percentile(X, 30)` and so on. `np.percentile()` is just another way of doing `.quantile(0.8)`\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Gs1Ej.png',\n",
       "  '<python><scipy><minimization>',\n",
       "  datetime.date(2014, 4, 22),\n",
       "  '2014-04-23 16:17:14',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1779.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['297',\n",
       "  '23306112',\n",
       "  'Answer',\n",
       "  'Convert a Pandas DataFrame to bin frequencies',\n",
       "  \"An alternative way of doing it if you want `'0-10'` etc, instead of `(20, 30]` provided by `pd.cut`.\\n\\n\\n\\n\\tIn [52]:\\n\\n\\t\\n\\n\\toutput = ['0-10','10-20','20-30','30-40','40-50','50-60','60-70','70-80','80-90','90-100']\\n\\n\\tdf2=(df*10).astype(int)\\n\\n\\tdf2=df2.applymap(lambda x: output[x])\\n\\n\\tprint df2\\n\\n\\t    Percentile1 Percentile2 Percentile3 Percentile4\\n\\n\\t395       10-20       20-30       20-30       10-20\\n\\n\\t424       20-30       20-30       10-20        0-10\\n\\n\\t511        0-10       10-20       10-20       30-40\\n\\n\\t540       10-20       10-20       30-40       60-70\\n\\n\\t570       10-20       30-40       60-70       70-80\\n\\n\\t\\n\\n\\t[5 rows x 4 columns]\\n\\n\\n\\n\\tIn [53]:\\n\\n\\tprint df2.apply(lambda x: x.value_counts()) #or /x.count()\\n\\n\\tlevel_1  Percentile1  Percentile2  Percentile3  Percentile4\\n\\n\\tclass                                                      \\n\\n\\t0-10               1          NaN          NaN            1\\n\\n\\t10-20              3            2            2            1\\n\\n\\t20-30              1            2            1          NaN\\n\\n\\t30-40            NaN            1            1            1\\n\\n\\t60-70            NaN          NaN            1            1\\n\\n\\t70-80            NaN          NaN          NaN            1\\n\\n\\t\\n\\n\\t[6 rows x 4 columns]\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 4, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1708.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['301',\n",
       "  '22031358',\n",
       "  'Answer',\n",
       "  'Why does np.median() return multiple rows?',\n",
       "  \"That's really strange, I think you should get `(16026,)`, are we missing something here:\\n\\n\\n\\n\\tIn [241]:\\n\\n\\t\\n\\n\\tX_train=np.random.random((1000,16026)) #1000 can be any int.\\n\\n\\tindices = np.random.randint(0, 60, 100) #60 can be any int.\\n\\n\\ttempArray = X_train[indices, ]\\n\\n\\tmedArray = np.median(tempArray, axis=0)\\n\\n\\tprint(medArray.shape)\\n\\n\\n\\n\\t(16026,)\\n\\n\\n\\nAnd the only way you can get a `2d array` result is:\\n\\n\\n\\n\\tIn [243]:\\n\\n\\t\\n\\n\\tX_train=np.random.random((100,2,16026))\\n\\n\\tindices = np.random.randint(0, 60, 100)\\n\\n\\ttempArray = X_train[indices, ]\\n\\n\\tmedArray = np.median(tempArray, axis=0)\\n\\n\\tprint(medArray.shape)\\n\\n\\n\\n\\n\\n\\t(2, 16026)\\n\\n\\n\\nWhen you have a `3d array` input.\\n\\n\\n\\nWhen it is a `sparse` `array`, a dumb way to get around this might be:\\n\\n\\n\\n\\tIn [319]:\\n\\n\\t\\n\\n\\tX_train = sparse.rand(112, 16026, 0.5, 'csr') #just make up a random sparse array\\n\\n\\tindices = np.random.randint(0, 60, 100)\\n\\n\\ttempArray = X_train[indices, ]\\n\\n\\tmedArray = np.median(tempArray.toarray(), axis=0)\\n\\n\\tprint(medArray.shape)\\n\\n\\t(16026,)\\n\\n\\n\\n`.toarray()` might also go to the 3rd line instead. But either way, this means the `0`'s are also counted as @zhangxaochen pointed out.\\n\\n\\n\\nOut of ideas, there may be better explanations for it.\",\n",
       "  '<python><arrays><numpy>',\n",
       "  datetime.date(2014, 2, 26),\n",
       "  '2014-02-26 05:09:14',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1007.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['306',\n",
       "  '22354537',\n",
       "  'Answer',\n",
       "  'pandas: extract or split char from number string',\n",
       "  \"Could use a conversion dictionary, also I am sure you didn't mean `624540000`: \\n\\n\\n\\n\\tIn [9]:\\n\\n\\t\\n\\n\\tD={'M':'*1e6', 'B':'*1e9'}\\n\\n\\tdf['float_value']=df.shares_float.apply(lambda x: eval(x[:-1]+D[x[-1]]))\\n\\n\\tIn [10]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t   ticker_id shares_float  float_value\\n\\n\\t0          1      621.76M   621760000\\n\\n\\t1          2        3.51B  3510000000\\n\\n\\t\\n\\n\\t[2 rows x 3 columns]\\n\\n\\tIn [11]:\\n\\n\\t\\n\\n\\tdf.dtypes\\n\\n\\tOut[11]:\\n\\n\\tticker_id         int64\\n\\n\\tshares_float     object\\n\\n\\tfloat_value     float64\\n\\n\\tdtype: object\",\n",
       "  '<python><string><pandas>',\n",
       "  datetime.date(2014, 3, 12),\n",
       "  '2014-03-12 16:56:34',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1299.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['309',\n",
       "  '21738095',\n",
       "  'Answer',\n",
       "  'read fastq file into dictionary',\n",
       "  \"I don't think use the reads as the key is good idea, what if you got exactly the same read. But any way if you want to do it:\\n\\n\\n\\n\\tIn [9]:\\n\\n\\twith open('temp.fastq') as f:\\n\\n\\t    lines=f.readlines()\\n\\n\\thead=[item[:-1] for item in lines[::4]] #get rid of '\\\\n'\\n\\n\\tread=[item[:-1] for item in lines[1::4]]\\n\\n\\tqual=[item[:-1] for item in lines[3::4]]\\n\\n\\tdict(zip(read, qual))\\n\\n\\n\\n\\tOut[9]:\\n\\n\\t\\n\\n\\t{'AAAACATCAGTATCCATCAGGATCAGTTTGGAAAGGGAGAGGCAATTTTTCCTAAACATGTGTTCAAATGGTCTGAGACAGACGTTAAAATGAAAAGGGG': '\\\\\\\\\\\\\\\\YYWX\\\\\\\\PX^YT[TVYaTY]^\\\\\\\\^H\\\\\\\\`^`a`\\\\\\\\UZU__TTbSbb^\\\\\\\\a^^^`[GOVVXLXMV[Y_^a^BBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBBB',\\n\\n\\t 'CTGAGTAAATCATATACTCAATGATTTTTTTATGTGTGTGCATGTGTGCTGTTGATATTCTTCAGTACCAAAACCCATCATCTTATTTGCATAGGGAAGT': 'fff^fd\\\\\\\\c^d^Ycac`dcdcded`effdfedb]beeeeecd^ddccdddddfff`eaeeeffdTecacaLV[QRPa\\\\\\\\\\\\\\\\a\\\\\\\\`]aY]ZZ[XYcccYcZ\\\\\\\\\\\\\\\\]Y',\\n\\n\\t 'CTGCCAGCACGCTGTCACCTCTCAATAACAGTGAGTGTAATGGCCATACTCTTGATTTGGTTTTTGCCTTATGAATCAGTGGCTAAAAATATTATTTAAT': 'deeee`bbcddddad\\\\\\\\bbbbeee\\\\\\\\ecYZcc^dd^ddd\\\\\\\\\\\\\\\\`]``L`ccabaVJ`MZ^aaYMbbb__PYWY]RWNUUab`Y`BBBBBBBBBBBBBBBBBBBB',\\n\\n\\t 'TTAGAAACTATGGGATTATTCACTCCCTAGGTACTGAGAATGGAAACTTTCTTTGCCTTAATCGTTGACATCCCCTCTTTTAGGTTCTTGCTTCCTAACA': 'ee^e^\\\\\\\\`ad`eeee\\\\\\\\dd\\\\\\\\ddddYeebdd\\\\\\\\ddaYbdcYc`\\\\\\\\bac^YX[V^\\\\\\\\Ybb]]^bdbaZ]ZZ\\\\\\\\^K\\\\\\\\^]VPNME][`_``Ubb_bYddZbbbYbbYT^_'}\",\n",
       "  '<python><python-2.7><fastq>',\n",
       "  datetime.date(2014, 2, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '3455.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['313',\n",
       "  '23666687',\n",
       "  'Answer',\n",
       "  'Matplotlib boxplot using precalculated (summary) statistics',\n",
       "  \"In the old versions, you have to manually do it by changing boxplot elements individually:\\n\\n\\n\\n    Mean=[3.4] #mean\\n\\n    IQR=[3.0,3.9] #inter quantile range\\n\\n    CL=[2.0,5.0] #confidence limit\\n\\n    A=np.random.random(50)\\n\\n    D=plt.boxplot(A) # a simple case with just one variable to boxplot\\n\\n    D['medians'][0].set_ydata(Mean)\\n\\n    D['boxes'][0]._xy[[0,1,4], 1]=IQR[0]\\n\\n    D['boxes'][0]._xy[[2,3],1]=IQR[1]\\n\\n    D['whiskers'][0].set_ydata(np.array([IQR[0], CL[0]]))\\n\\n    D['whiskers'][1].set_ydata(np.array([IQR[1], CL[1]]))\\n\\n    D['caps'][0].set_ydata(np.array([CL[0], CL[0]]))\\n\\n    D['caps'][1].set_ydata(np.array([CL[1], CL[1]]))\\n\\n    _=plt.ylim(np.array(CL)+[-0.1*np.ptp(CL), 0.1*np.ptp(CL)]) #reset the limit\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/BpJpH.png\",\n",
       "  '<python><matplotlib><boxplot>',\n",
       "  datetime.date(2014, 5, 14),\n",
       "  '2018-05-28 20:23:11',\n",
       "  'Paul (103081)',\n",
       "  '8',\n",
       "  '',\n",
       "  '3254.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['314',\n",
       "  '24047026',\n",
       "  'Answer',\n",
       "  'Create Contour Plot from Pandas Groupby Dataframe',\n",
       "  \"Welcome to SO.\\n\\n\\n\\nIt looks quite clear that for each of your 'a' level, the numbers of 'b' levels are not the same, thus I will suggest the following solution:\\n\\n\\n\\n\\tIn [44]:\\n\\n\\t\\n\\n\\tprint df #an example, you can get your dataframe in to this by rest_index()\\n\\n\\t\\ta  b     value\\n\\n\\t0   0  1  0.336885\\n\\n\\t1   0  2  0.276750\\n\\n\\t2   0  3  0.796488\\n\\n\\t3   1  1  0.156050\\n\\n\\t4   1  2  0.401942\\n\\n\\t5   1  3  0.252651\\n\\n\\t6   2  1  0.861911\\n\\n\\t7   2  2  0.914803\\n\\n\\t8   2  3  0.869331\\n\\n\\t9   3  1  0.284757\\n\\n\\t10  3  2  0.488330\\n\\n\\t\\n\\n\\t[11 rows x 3 columns]\\n\\n\\tIn [45]:\\n\\n\\t#notice that you will have some 'NAN' values\\n\\n\\tdf=df.pivot('a', 'b', 'value')\\n\\n\\tIn [46]:\\n\\n\\t\\n\\n\\tX=df.columns.values\\n\\n\\tY=df.index.values\\n\\n\\tZ=df.values\\n\\n\\tx,y=np.meshgrid(X, Y)\\n\\n\\tplt.contourf(x, y, Z) #the NAN will be plotted as white spaces\\n\\n\\tOut[46]:\\n\\n\\t<matplotlib.contour.QuadContourSet instance at 0x1081385a8>\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/RH7Zm.png\",\n",
       "  '<python><matplotlib><pandas><group-by><contour>',\n",
       "  datetime.date(2014, 6, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '6987.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['315',\n",
       "  '24124523',\n",
       "  'Answer',\n",
       "  'Python Pandas: Get row by median value',\n",
       "  \"May be just : `data[data.performance==data.median()['performance']]`.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 6, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2056.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['318',\n",
       "  '21842304',\n",
       "  'Answer',\n",
       "  'Pandas Dataframe to_csv format output',\n",
       "  \"I think you can just convert the 1st to the 3rd `columns` to `int64` before writing the CSV file (if you check the `.types`, I am sure they are all `float64`):\\n\\n\\n\\n    df[['cycle', 'passs', 'ip']]=df[['cycle', 'passs', 'ip']].astype(int64)\",\n",
       "  '<python><pandas><format><dataframe>',\n",
       "  datetime.date(2014, 2, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1613.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['319',\n",
       "  '21870882',\n",
       "  'Answer',\n",
       "  'Python: Divide values in cell by max in each column',\n",
       "  'How do you handle `0`? Like the last column? It should be `nan` in theory. (`sum(elements) != 0`, what if it is -2 -1 0 1 2? That should be result in -1 -0.5 0 0.5 1, right?)\\n\\n\\n\\n\\tIn [138]:\\n\\n\\t\\n\\n\\tA*1./np.max(A, axis=0)\\n\\n\\tOut[138]:\\n\\n\\tarray([[ 0.33333333,  0.125     ,  1.        ,         nan],\\n\\n\\t       [ 0.5       ,  1.        ,  0.5       ,         nan],\\n\\n\\t       [ 0.33333333,  0.375     ,  0.        ,         nan],\\n\\n\\t       [ 0.83333333,  0.625     ,  0.75      ,         nan],\\n\\n\\t       [ 1.        ,  0.375     ,  0.75      ,         nan]])\\n\\n\\n\\nWe can leave the last column as it is.\\n\\n\\n\\n\\tIn [141]:\\n\\n\\t\\n\\n\\tnp.where(np.max(A, axis=0)==0, A, A*1./np.max(A, axis=0))\\n\\n\\tOut[141]:\\n\\n\\tarray([[ 0.33333333,  0.125     ,  1.        ,  0.        ],\\n\\n\\t       [ 0.5       ,  1.        ,  0.5       ,  0.        ],\\n\\n\\t       [ 0.33333333,  0.375     ,  0.        ,  0.        ],\\n\\n\\t       [ 0.83333333,  0.625     ,  0.75      ,  0.        ],\\n\\n\\t       [ 1.        ,  0.375     ,  0.75      ,  0.        ]])\\n\\n\\n\\nThe correct way of doing it with a loop is:\\n\\n\\n\\n\\tfor row in A.T:\\n\\n\\t    if max(row)>0:\\n\\n\\t        new_data.append([item*1./max(row) for item in row])\\n\\n\\t    else:\\n\\n\\t        new_data.append(row)',\n",
       "  '<python><numpy>',\n",
       "  datetime.date(2014, 2, 19),\n",
       "  '2014-02-19 04:34:03',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '3799.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['320',\n",
       "  '21034988',\n",
       "  'Answer',\n",
       "  'How can I generate more colors on pie chart matplotlib',\n",
       "  \"You need `colors` argument, beside that you can use some color maps from `cm`.\\n\\n\\n\\n    >>> import matplotlib.pyplot as plt\\n\\n    >>> from matplotlib import cm\\n\\n    >>> import numpy as np\\n\\n    >>> a=np.random.random(40)\\n\\n    >>> cs=cm.Set1(np.arange(40)/40.)\\n\\n    >>> f=plt.figure()\\n\\n    >>> ax=f.add_subplot(111, aspect='equal')\\n\\n    >>> p=plt.pie(a, colors=cs)\\n\\n    >>> plt.show()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/atLVp.png\\n\\n\\n\\nBeside using colormaps, also consider using `.set_color_cycle()` method. See this post: https://stackoverflow.com/questions/16006572/plotting-different-colors-in-matplotlib\",\n",
       "  '<python><matplotlib><pie-chart>',\n",
       "  datetime.date(2014, 1, 10),\n",
       "  '2017-05-23 10:31:09',\n",
       "  'CT Zhu (2487184), pfnuesel (1945981), URL Rewriter Bot (n/a)',\n",
       "  '25',\n",
       "  '',\n",
       "  '14111.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['321',\n",
       "  '21174240',\n",
       "  'Answer',\n",
       "  'python - matplotlib - polar plots with angular labels in radians',\n",
       "  \"This should do it:\\n\\n\\n\\n    >>> fig=plt.figure()\\n\\n    >>> axe=fig.gca(polar=True)\\n\\n    >>> thetas=linspace(0,2*pi,200)\\n\\n    >>> rhos=3+cos(5*thetas)\\n\\n    >>> axe.plot(thetas, rhos)\\n\\n    [<matplotlib.lines.Line2D object at 0x109a2b550>]\\n\\n    >>> xT=plt.xticks()[0]\\n\\n    >>> xL=['0',r'$\\\\frac{\\\\pi}{4}$',r'$\\\\frac{\\\\pi}{2}$',r'$\\\\frac{3\\\\pi}{4}$',\\\\\\n\\n        r'$\\\\pi$',r'$\\\\frac{5\\\\pi}{4}$',r'$\\\\frac{3\\\\pi}{2}$',r'$\\\\frac{7\\\\pi}{4}$']\\n\\n    >>> plt.xticks(xT, xL)\\n\\n    ([<matplotlib.axis.XTick object at 0x107bac490>, <matplotlib.axis.XTick object at 0x109a31310>, <matplotlib.axis.XTick object at 0x109a313d0>, <matplotlib.axis.XTick object at 0x109a31050>, <matplotlib.axis.XTick object at 0x1097a8690>, <matplotlib.axis.XTick object at 0x1097a8cd0>, <matplotlib.axis.XTick object at 0x1097a8150>, <matplotlib.axis.XTick object at 0x107bb8fd0>], <a list of 8 Text xticklabel objects>)\\n\\n    >>> plt.show()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/xEdw5.png\",\n",
       "  '<python><numpy><matplotlib><polar-coordinates>',\n",
       "  datetime.date(2014, 1, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '3795.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['322',\n",
       "  '21186178',\n",
       "  'Answer',\n",
       "  'Pandas: concatenating conditioned on unique values',\n",
       "  \"If you get a column with an `id`, then use it as an index. Perform manipulations with a real index will make things easier. Here you can use `combine_first` that does what you are searching for:\\n\\n\\n\\n    part1 = part1.set_index('id')\\n\\n    \\n\\n    part2 = part2.set_index('id')\\n\\n    \\n\\n    part1.combine_first(p2)\\n\\n    Out[38]: \\n\\n           amount\\n\\n    id           \\n\\n    100  1.685685\\n\\n    200 -1.895151\\n\\n    300 -0.804097\\n\\n    400  0.119948\\n\\n    500 -0.434062\\n\\n    700  0.215255\\n\\n    800 -0.031562\\n\\n\\n\\nIf you really need not to get that index, reset it after:\\n\\n\\n\\n    part1.combine_first(p2).reset_index()\\n\\n    Out[39]: \\n\\n        id    amount\\n\\n    0  100  1.685685\\n\\n    1  200 -1.895151\\n\\n    2  300 -0.804097\\n\\n    3  400  0.119948\\n\\n    4  500 -0.434062\\n\\n    5  700  0.215255\\n\\n    6  800 -0.031562\\n\\n\\n\\n\",\n",
       "  '<python><pandas><set><concatenation>',\n",
       "  datetime.date(2014, 1, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1609.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['323',\n",
       "  '22354945',\n",
       "  'Answer',\n",
       "  'Pythonic way of detecting outliers in one dimensional observation data',\n",
       "  'Use `np.percentile` as @Martin suggested:\\n\\n\\n\\n\\t\\n\\n\\tpercentiles = np.percentile(data, [2.5, 97.5])\\n\\n\\t\\n\\n    # or =>, <= for within 95%\\n\\n\\tdata[(percentiles[0]<data) & (percentiles[1]>data)]\\n\\n    \\n\\n    # set the outliners to np.nan\\n\\n\\tdata[(percentiles[0]>data) | (percentiles[1]<data)] = np.nan',\n",
       "  '<python><numpy><matplotlib><statistics><statsmodels>',\n",
       "  datetime.date(2014, 3, 12),\n",
       "  '2018-11-05 20:37:32',\n",
       "  'bugmenot123 (4828720)',\n",
       "  '3',\n",
       "  '',\n",
       "  '56721.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['326',\n",
       "  '22393440',\n",
       "  'Answer',\n",
       "  'How can check the distribution of a variable in python?',\n",
       "  'You can use Kolmogorove-Smirnov Test for continues *and* discrete distributions. This function is provided with `scipy.stats.kstest` http://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.kstest.html#scipy.stats.kstest.\\n\\n\\n\\n\\tIn [12]:\\n\\n\\t\\n\\n\\timport scipy.stats as ss\\n\\n\\timport numpy as np\\n\\n\\tIn [14]:\\n\\n\\t\\n\\n\\tA=np.random.randint(0,10,100)\\n\\n\\tIn [16]:\\n\\n\\t\\n\\n\\tss.kstest(A, ss.randint.cdf, args=(0,10))\\n\\n\\t#args is a tuple containing the extra parameter required by ss.randint.cdf, in this case, lower bound and upper bound\\n\\n\\tOut[16]:\\n\\n\\t(0.12, 0.10331653831438881)\\n\\n\\t#This a tuple of two values; KS test statistic, either D, D+ or D-. and p-value\\n\\n\\n\\nHere the resulting P value is 0.1033, we therefore conclude that the array `A` is not significantly different from a uniform distribution. The way to think about the P value is, it measures the probability of getting the test statistic as extreme as the one observed (here: the first number in the tuple) assuming the null hypothesis is true. In KS test, we actually has the null hypothesis that `A` is not different from a uniform distribution. A p value of 0.1033 is often not considered as extreme enough to reject the null hypothesis. Usually the P value has to be less than 0.05 or 0.01 in order to reject the null. If this p value in this example is less than 0.05, we will then say `A` is significantly different from a uniform distribution. \\n\\n\\n\\nThe alternative method of using `scipy.stats.chisquare()`:\\n\\n\\n\\n\\tIn [17]:\\n\\n\\t\\n\\n\\timport scipy.stats as ss\\n\\n\\timport numpy as np\\n\\n\\tIn [18]:\\n\\n\\t\\n\\n\\tA=np.random.randint(0, 10, 100)\\n\\n\\tIn [19]:\\n\\n\\t\\n\\n\\tFRQ=(A==np.arange(10)[...,np.newaxis]).sum(axis=1)*1./A.size #generate the expect frequecy table.\\n\\n\\tIn [20]:\\n\\n\\t\\n\\n\\tss.chisquare(FRQ) #If not specified, the default expected frequency is uniform across categories.\\n\\n\\tOut[20]:\\n\\n\\t(0.084000000000000019, 0.99999998822800984)\\n\\n\\n\\nThe first value is chisquare and the second value is P value.',\n",
       "  '<python><arrays><random><numpy><statistics>',\n",
       "  datetime.date(2014, 3, 13),\n",
       "  '2014-03-14 15:01:17',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '4578.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['327',\n",
       "  '22413882',\n",
       "  'Answer',\n",
       "  'Python 2D plots as 3D (Matplotlib)',\n",
       "  \"Would it be something like this?\\n\\n\\n\\n\\tfrom mpl_toolkits.mplot3d import Axes3D\\n\\n\\tfrom matplotlib.collections import PolyCollection\\n\\n\\tfrom matplotlib.colors import colorConverter\\n\\n\\timport matplotlib.pyplot as plt\\n\\n\\timport numpy as np\\n\\n\\t\\n\\n\\tfig = plt.figure()\\n\\n\\tax = fig.gca(projection='3d')\\n\\n\\t\\n\\n\\tzs = [0.0, 1.0, 2.0]\\n\\n\\tt  = np.arange(1024)*1e-6\\n\\n\\tones = np.ones(1024)\\n\\n\\ty1 = np.sin(t*2e3*np.pi) \\n\\n\\ty2 = 0.5*y1\\n\\n\\ty3 = 0.25*y1\\n\\n\\t\\n\\n\\tverts=[list(zip(t, y1)), list(zip(t, y2)), list(zip(t, y3))]\\n\\n\\t\\n\\n\\t\\n\\n\\tpoly = PolyCollection(verts, facecolors = ['r','g','b'])\\n\\n\\tpoly.set_alpha(0.7)\\n\\n\\tax.add_collection3d(poly, zs=zs, zdir='y')\\n\\n\\tax.set_xlabel('X')\\n\\n\\tax.set_xlim3d(0, 1024e-6)\\n\\n\\tax.set_ylabel('Y')\\n\\n\\tax.set_ylim3d(-1, 3)\\n\\n\\tax.set_zlabel('Z')\\n\\n\\tax.set_zlim3d(-1, 1)\\n\\n\\t\\n\\n\\tplt.show()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Fl8hC.png\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 3, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1627.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['328',\n",
       "  '21771713',\n",
       "  'Answer',\n",
       "  'How can I plot multiple lines using the same array and set disconnect points in python?',\n",
       "  \"Check your data, especially the last row, your code itself should work fine:\\n\\n\\n\\n\\ta=np.random.normal(size=(250,7))\\n\\n\\ta[:,0]=np.arange(250)*1.0\\n\\n\\tplt.plot(a[:,0], a[:,6], c = 'r', marker= '1')\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nAs expected, you have repeating values, so this should do the trick:\\n\\n\\n\\n\\ta=np.random.normal(size=(250,7))\\n\\n\\ta[:,0]=np.array(range(25)*10)\\n\\n\\tplots=[plt.plot(a[i*25:(i*25+25),0], a[i*25:(i*25+25),6], c = 'r', marker= '1') for i in range(10)]\\n\\n\\n\\nBasically, plot one column in 10 segments. \\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/8KjvP.png\\n\\n  [2]: http://i.stack.imgur.com/djhv5.png\",\n",
       "  '<python><matplotlib><plot><line>',\n",
       "  datetime.date(2014, 2, 14),\n",
       "  '2014-02-14 06:16:55',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1181.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['330',\n",
       "  '21801981',\n",
       "  'Answer',\n",
       "  'Function to check whether a number is a Fibonacci number or not?',\n",
       "  \"This is going to be a very efficient way of doing it.\\n\\n\\n\\n\\tIn [65]:\\n\\n\\timport scipy.optimize as so\\n\\n\\tfrom numpy import *\\n\\n\\t\\n\\n\\tIn [66]:\\n\\n\\tdef fib(n):\\n\\n\\t    s5=sqrt(5.)\\n\\n\\t    return sqrt(0.2)*(((1+s5)/2.)**n-((1-s5)/2.)**n)\\n\\n\\tdef apx_fib(n):\\n\\n\\t    s5=sqrt(5.)\\n\\n\\t    return sqrt(0.2)*(0.5*(1+s5))**n\\n\\n\\tdef solve_fib(f):\\n\\n\\t    _f=lambda x, y: (apx_fib(x)-y)**2\\n\\n\\t    return so.fmin_slsqp(_f,0., args=(f,),iprint=0)\\n\\n\\tdef test_fib(fibn):\\n\\n\\t    if fibn<1:\\n\\n\\t        print 'No, it is not'\\n\\n\\t    else:\\n\\n\\t        n=int(round(solve_fib(fibn)))\\n\\n\\t        if int(round(fib(n)))==int(fibn):\\n\\n\\t            print 'Yes, it is. (%d)'%n\\n\\n\\t        else:\\n\\n\\t            print 'No, it is not'\\n\\n\\t        \\n\\n\\tIn [67]:\\n\\n\\tasarray(fib(arange(1,20)), dtype='int64')\\n\\n\\tOut[67]:\\n\\n\\tarray([   1,    1,    2,    3,    5,    8,   13,   21,   34,   55,   89,\\n\\n\\t        144,  233,  377,  610,  987, 1597, 2584, 4181])\\n\\n\\t        \\n\\n\\tIn [68]:\\n\\n\\ttest_fib(34)\\n\\n\\tYes, it is. (9)\\n\\n\\t\\n\\n\\tIn [69]:\\n\\n\\ttest_fib(4181)\\n\\n\\tYes, it is. (19)\\n\\n\\t\\n\\n\\tIn [70]:\\n\\n\\ttest_fib(4444)\\n\\n\\tNo, it is not\",\n",
       "  '<python><python-3.x><fibonacci>',\n",
       "  datetime.date(2014, 2, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '4380.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['331',\n",
       "  '21975479',\n",
       "  'Answer',\n",
       "  'pyplot equivalent for pl.cm.Spectral in matplotlib',\n",
       "  'Should just be `cm.Spectral`\\n\\n\\n\\n\\tIn [123]:\\n\\n\\t\\n\\n\\timport matplotlib.cm as cm\\n\\n\\tcm.Spectral(np.linspace(0,1,10))\\n\\n\\tOut[123]:\\n\\n\\tarray([[ 0.61960787,  0.00392157,  0.25882354,  1.        ],\\n\\n\\t       [ 0.84721262,  0.26120723,  0.30519032,  1.        ],\\n\\n\\t       [ 0.96378316,  0.47743176,  0.28581316,  1.        ],\\n\\n\\t       [ 0.99346405,  0.74771243,  0.43529413,  1.        ],\\n\\n\\t       [ 0.99777009,  0.93087275,  0.63306423,  1.        ],\\n\\n\\t       [ 0.94425221,  0.97770089,  0.66205308,  1.        ],\\n\\n\\t       [ 0.74771243,  0.89803922,  0.627451  ,  1.        ],\\n\\n\\t       [ 0.45305653,  0.78154557,  0.64628991,  1.        ],\\n\\n\\t       [ 0.21607075,  0.55563248,  0.73194927,  1.        ],\\n\\n\\t       [ 0.36862746,  0.30980393,  0.63529414,  1.        ]])\\n\\n\\tIn [119]:\\n\\n\\t\\n\\n\\timport pylab as pl\\n\\n\\tpl.cm.Spectral(np.linspace(0, 1, 10))\\n\\n\\tOut[119]:\\n\\n\\tarray([[ 0.61960787,  0.00392157,  0.25882354,  1.        ],\\n\\n\\t       [ 0.84721262,  0.26120723,  0.30519032,  1.        ],\\n\\n\\t       [ 0.96378316,  0.47743176,  0.28581316,  1.        ],\\n\\n\\t       [ 0.99346405,  0.74771243,  0.43529413,  1.        ],\\n\\n\\t       [ 0.99777009,  0.93087275,  0.63306423,  1.        ],\\n\\n\\t       [ 0.94425221,  0.97770089,  0.66205308,  1.        ],\\n\\n\\t       [ 0.74771243,  0.89803922,  0.627451  ,  1.        ],\\n\\n\\t       [ 0.45305653,  0.78154557,  0.64628991,  1.        ],\\n\\n\\t       [ 0.21607075,  0.55563248,  0.73194927,  1.        ],\\n\\n\\t       [ 0.36862746,  0.30980393,  0.63529414,  1.        ]])',\n",
       "  '<python><matplotlib><scipy>',\n",
       "  datetime.date(2014, 2, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '6306.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['332',\n",
       "  '24153569',\n",
       "  'Answer',\n",
       "  'Converting an RPy2 ListVector to a Python dictionary',\n",
       "  \"I think to get a r vector into a `dictionary` does not have to be so involving, how about this:\\n\\n\\n\\n\\tIn [290]:\\n\\n\\t\\n\\n\\tdict(zip(a.names, list(a)))\\n\\n\\tOut[290]:\\n\\n\\t{'fizz': <FloatVector - Python:0x08AD50A8 / R:0x10A67DE8>\\n\\n\\t[123.000000],\\n\\n\\t 'foo': <StrVector - Python:0x08AD5030 / R:0x10B72458>\\n\\n\\t['barbat']}\\n\\n\\tIn [291]:\\n\\n\\t\\n\\n\\tdict(zip(a.names, map(list,list(a))))\\n\\n\\tOut[291]:\\n\\n\\t{'fizz': [123.0], 'foo': ['barbat']}\\n\\n\\n\\nAnd of course, if you don't mind using `pandas`, it is even easier. The result will have `numpy.array` instead of `list`, but that will be OK in most cases:\\n\\n\\n\\n\\tIn [294]:\\n\\n\\t\\n\\n\\timport pandas.rpy.common as com\\n\\n\\tcom.convert_robj(a)\\n\\n\\tOut[294]:\\n\\n\\t{'fizz': [123.0], 'foo': array(['barbat'], dtype=object)}\",\n",
       "  '<python><rpy2>',\n",
       "  datetime.date(2014, 6, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '20',\n",
       "  '',\n",
       "  '4923.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['334',\n",
       "  '24189653',\n",
       "  'Answer',\n",
       "  'pandas- adding a series to a dataframe causes NaN values to appear',\n",
       "  \"For your full dataset, change the last step from `df['concatenated'] = df_1` to `df['concatenated'] = df_1.values` will solve the issue, I think it a bug and I am very sure I have seen it in SO before.\\n\\n\\n\\nOr just: `df['concatenated'] = [':'.join(word for word in row if word is not np.nan) for row in rows]`\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2014, 6, 12),\n",
       "  '2014-06-12 16:47:01',\n",
       "  'CT Zhu (2487184)',\n",
       "  '9',\n",
       "  '',\n",
       "  '4300.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['336',\n",
       "  '24337174',\n",
       "  'Answer',\n",
       "  'Convert 2D numpy.ndarray to pandas.DataFrame',\n",
       "  \"I suspect your `ndarr`, if expressed as a 2d `np.array`, always has the shape of `n,m`, where `n` is the length of `cache1.id1` and `m` is the length of `cache2.id2`. And the last entry in cache2, should be `{'id2': 38472837}` instead of `{'id': 38472837}`. If so, the following simple solution may be all what is needed:\\n\\n\\n\\n\\tIn [30]:\\n\\n\\t\\n\\n\\tdf=pd.DataFrame(np.array(ndarr).ravel(),\\n\\n\\t             index=pd.MultiIndex.from_product([cache1.id1.values, cache2.id2.values],names=['idx1', 'idx2']),\\n\\n\\t             columns=['val'])\\n\\n\\tIn [33]:\\n\\n\\t\\n\\n\\tprint df.reset_index()\\n\\n\\t       idx1      idx2  val\\n\\n\\t0   ABC1234   3276827  4.3\\n\\n\\t1   ABC1234  98567498  5.6\\n\\n\\t2   ABC1234  38472837  6.7\\n\\n\\t3  NCMN7838   3276827  3.2\\n\\n\\t4  NCMN7838  98567498  4.5\\n\\n\\t5  NCMN7838  38472837  2.1\\n\\n\\t\\n\\n\\t[6 rows x 3 columns]\\n\\n\\t\\n\\nActually, I also think, that keep it having the `MultiIndex` may be a better idea.\",\n",
       "  '<python-2.7><pandas><multidimensional-array>',\n",
       "  datetime.date(2014, 6, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '3453.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['337',\n",
       "  '24358507',\n",
       "  'Answer',\n",
       "  'Pandas DataFrame selection using text from another DataFrame',\n",
       "  'I think `films[ films[\"director\"] == films.ix[0, \\'director\\' ]]` will suffice.\\n\\n\\n\\nThe reason `films.iloc[[1]][\"director\"]` won\\'t work is because it is a `Series`, not a `string`.\\n\\n\\n\\nIf you want to use `iloc`, do: `films.iloc[1][\"director\"]` instead of `films.iloc[[1]][\"director\"]`\\n\\n\\n\\nAlso:\\n\\n\\n\\n\\tIn [241]:\\n\\n\\t\\n\\n\\tstr(films.iloc[[1]][\"director\"])\\n\\n\\tOut[241]:\\n\\n\\t\\'1    Tarantino, Quentin\\\\nName: director, dtype: object\\'\\n\\n\\n\\nso, `films[ films[\"director\"] == str(director) ]` won\\'t match anything and will return a empty dataframe.',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 6, 23),\n",
       "  '2014-06-23 14:11:10',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1084.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['339',\n",
       "  '24377573',\n",
       "  'Answer',\n",
       "  'Download stocks data from google finance',\n",
       "  'It should work, but notice that the ticker should be: **BVMF:ABRE11**\\n\\n\\n\\n\\tIn [250]:\\n\\n\\t\\n\\n\\timport pandas.io.data as web\\n\\n\\timport datetime\\n\\n\\tstart = datetime.datetime(2010, 1, 1)\\n\\n\\tend = datetime.datetime(2013, 1, 27)\\n\\n\\tdf=web.DataReader(\"BVMF:ABRE11\", \\'google\\', start, end)\\n\\n\\tprint df.head(10)\\n\\n\\t             Open   High    Low  Close   Volume\\n\\n\\t?Date                                          \\n\\n\\t2011-07-26  19.79  19.79  18.30  18.50  1843700\\n\\n\\t2011-07-27  18.45  18.60  17.65  17.89  1475100\\n\\n\\t2011-07-28  18.00  18.50  18.00  18.30   441700\\n\\n\\t2011-07-29  18.30  18.84  18.20  18.70   392800\\n\\n\\t2011-08-01  18.29  19.50  18.29  18.86   217800\\n\\n\\t2011-08-02  18.86  18.86  18.60  18.80   154600\\n\\n\\t2011-08-03  18.90  18.90  18.00  18.00   168700\\n\\n\\t2011-08-04  17.50  17.85  16.50  16.90   238700\\n\\n\\t2011-08-05  17.00  17.00  15.63  16.00   253000\\n\\n\\t2011-08-08  15.50  15.96  14.35  14.50   224300\\n\\n\\t\\n\\n\\t[10 rows x 5 columns]\\n\\n\\n\\n\\tIn [251]:\\n\\n\\t\\n\\n\\tdf=web.DataReader(\"BVMF:BIOM3\", \\'google\\', start, end)\\n\\n\\tprint df.head(10)\\n\\n\\t            Open  High   Low  Close  Volume\\n\\n\\t?Date                                      \\n\\n\\t2010-01-04  2.90  2.90  2.90   2.90       0\\n\\n\\t2010-01-05  3.00  3.00  3.00   3.00       0\\n\\n\\t2010-01-06  3.01  3.01  3.01   3.01       0\\n\\n\\t2010-01-07  3.01  3.09  3.01   3.09    2000\\n\\n\\t2010-01-08  3.01  3.01  3.01   3.01       0\\n\\n\\t2010-01-11  3.00  3.00  3.00   3.00       0\\n\\n\\t2010-01-12  3.00  3.00  3.00   3.00       0\\n\\n\\t2010-01-13  3.00  3.10  3.00   3.00    7000\\n\\n\\t2010-01-14  3.00  3.00  3.00   3.00       0\\n\\n\\t2010-01-15  3.00  3.00  3.00   3.00    1000\\n\\n\\t\\n\\n\\t[10 rows x 5 columns]',\n",
       "  '<python><pandas><web-scraping>',\n",
       "  datetime.date(2014, 6, 24),\n",
       "  '2014-06-24 03:20:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '6944.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['341',\n",
       "  '22431897',\n",
       "  'Answer',\n",
       "  'Minimizing three variables in python using scipy',\n",
       "  'Change your last 4 lines to:\\n\\n\\n\\n\\tM=lambda p1, p2: mape(p2, p1)\\n\\n\\tresult = minimize(M, coeff, (x,), method =\"L-BFGS-B\", bounds =bnds)\\n\\n\\t\\n\\n\\topt = result[\\'x\\']\\n\\n\\tprint(\"opt : \", result[\\'x\\'])\\n\\n\\n\\nAnd it should work now, need explanations? I get the optimization result `(\\'opt : \\', array([ 0.45330204,  0.26761714,  0.        ]))`\\n\\n\\n\\nThe `lambda` function reverses the order of how the parameters are supplied to `mape`. As you are attempting to find the `coeff` that minimize the `mape()` given a fixed `x`, the target function should take `coeff` first and `x` second, which is not the case for `mape`.\\n\\n\\n\\nTo your comment question: I thought you are using `L-BFGS-B` in your code. The difference are explained here: http://docs.scipy.org/doc/scipy/reference/tutorial/optimize.html#tutorial-sqlsp. I have to admit I don\\'t too much detailed of `SLSQP` as that was long time ago in graduate school. `BFGS` is more common and every textbook explains it. `L-BFGS-B` supports bound constrained minimization. `SLSQP` supports bounds, as well as equality and inequality constraints. So, `SLSQP` can function when `L-BFGS-B` can\\'t. See, http://scipy-lectures.github.io/advanced/mathematical_optimization/index.html?utm_source=twitterfeed&utm_medium=twitter. ',\n",
       "  '<python><numpy><scipy><minimization><holtwinters>',\n",
       "  datetime.date(2014, 3, 16),\n",
       "  '2014-03-16 02:17:59',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1093.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['342',\n",
       "  '22472173',\n",
       "  'Answer',\n",
       "  'multivariate gaussian probability density function python on Mac',\n",
       "  \"It is provided by `scipy.stats.multivariate_normal.pdf()`: http://docs.scipy.org/doc/scipy-dev/reference/generated/scipy.stats.multivariate_normal.html. You will need to specify the `mean` and `variance-covariance matrix`.\\n\\n\\n\\nBefore that you need to install `scipy` if you have not done so. You can either download a `.dmg` installation package. Or try an 'distribution' such as `Anaconda Python`.\",\n",
       "  '<python><scipy><probability-density>',\n",
       "  datetime.date(2014, 3, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '5660.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['346',\n",
       "  '21793578',\n",
       "  'Answer',\n",
       "  'For loop in an opposite direction',\n",
       "  '    foo = [1a,2b,3c,4d]\\n\\n    for number in foo[::-1]:\\n\\n        print number',\n",
       "  '<python>',\n",
       "  datetime.date(2014, 2, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1941.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['347',\n",
       "  '21846599',\n",
       "  'Answer',\n",
       "  'Pandas: Use multiple columns of a dataframe as index of another',\n",
       "  \"change your first line to:\\n\\n\\n\\n    container.big_df.index=pd.MultiIndex.from_arrays(container.meta_df[['col1', 'col2']].values.T, names=['i1','i2'])\",\n",
       "  '<python><numpy><pandas><scipy><scikit-learn>',\n",
       "  datetime.date(2014, 2, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '12659.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['349',\n",
       "  '22049849',\n",
       "  'Answer',\n",
       "  'How can I ignore zeros when I take the median on columns of an array?',\n",
       "  '`Masked array` is always handy, but slooooooow:\\n\\n\\n\\n    In [14]:\\n\\n\\n\\n    %timeit np.ma.median(y, axis=0).filled(0)\\n\\n    1000 loops, best of 3: 1.73 ms per loop\\n\\n    In [15]:\\n\\n\\n\\n    %%timeit\\n\\n    ans=np.apply_along_axis(lambda v: np.median(v[v!=0]), 0, x)\\n\\n    ans[np.isnan(ans)]=0.\\n\\n    1000 loops, best of 3: 402 µs per loop\\n\\n\\n\\n    In [16]:\\n\\n\\n\\n    ans=np.apply_along_axis(lambda v: np.median(v[v!=0]), 0, x)\\n\\n    ans[np.isnan(ans)]=0.; ans\\n\\n    Out[16]:\\n\\n    array([ 9.,  9.,  9.,  0.])\\n\\n\\n\\n`np.nonzero` is even faster:\\n\\n\\n\\n    In [25]:\\n\\n\\n\\n    %%timeit\\n\\n    ans=np.apply_along_axis(lambda v: np.median(v[np.nonzero(v)]), 0, x)\\n\\n    ans[np.isnan(ans)]=0.\\n\\n    1000 loops, best of 3: 384 µs per loop',\n",
       "  '<python><arrays><numpy><zero><median>',\n",
       "  datetime.date(2014, 2, 26),\n",
       "  '2014-02-26 18:35:04',\n",
       "  'CT Zhu (2487184)',\n",
       "  '11',\n",
       "  '',\n",
       "  '7411.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['350',\n",
       "  '22210583',\n",
       "  'Answer',\n",
       "  'Using passed axis objects in a matplotlib.pyplot figure?',\n",
       "  'You are trying to make a plot first and then put that plot as a subplot in another plot (defined by `subplot2grid`). Unfortunately, that is not possible. Also see this post: https://stackoverflow.com/questions/16750333/how-do-i-include-a-matplotlib-figure-object-as-subplot.\\n\\n\\n\\nYou would have to make the subplot first and pass the axis of the subplot to your `drawfig_1()` function to plot it. Of course, `drawfig_1()` will need to be modified. e.g:\\n\\n\\n\\n\\tdef drawfig_1(ax1):\\n\\n\\t    ax1.plot(range(10))\\n\\n\\t    return ax1\\n\\n\\t    \\n\\n\\t# Setup plots for analysis\\n\\n\\tfig2 = plt.figure(figsize=(12, 8))\\n\\n\\t\\n\\n\\t# Set up 2 axes, one for a pixel map, the other for an image\\n\\n\\tax_map = plt.subplot2grid((3, 3), (0, 0), rowspan=3)\\n\\n\\tax_image = plt.subplot2grid((3, 3), (0, 1), colspan=2, rowspan=3)\\n\\n\\t\\n\\n\\t# Plot the image\\n\\n\\tax_image.imshow(image, vmin=0.00000001, vmax=0.000001, cmap=cm.gray)\\n\\n\\t# Plot the map:\\n\\n\\tdrawfig_1(ax_map)',\n",
       "  '<python><matplotlib><axes>',\n",
       "  datetime.date(2014, 3, 5),\n",
       "  '2017-05-23 12:07:37',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '1',\n",
       "  '',\n",
       "  '2409.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['352',\n",
       "  '21888413',\n",
       "  'Answer',\n",
       "  'Python Empirical distribution function (ecdf) implementation',\n",
       "  \"Since you are already using `pandas` I think it will be silly not to use some of its features:\\n\\n\\n\\n\\tIn [15]:\\n\\n\\timport numpy as np\\n\\n\\tfrom numpy import *\\n\\n\\tsq=ser.value_counts()\\n\\n\\tsq.sort_index().cumsum()*1./len(sq)\\n\\n\\tOut[15]:\\n\\n\\t2.083520e-12    0.058824\\n\\n\\t1.283440e-09    0.117647\\n\\n\\t8.517870e-09    0.176471\\n\\n\\t4.282550e-08    0.235294\\n\\n\\t1.121860e-07    0.294118\\n\\n\\t3.336140e-07    0.352941\\n\\n\\t4.276430e-07    0.411765\\n\\n\\t8.974670e-07    0.470588\\n\\n\\t2.018990e-06    0.529412\\n\\n\\t2.912570e-06    0.588235\\n\\n\\t9.761900e-06    0.647059\\n\\n\\t1.394780e-05    0.705882\\n\\n\\t1.937330e-05    0.764706\\n\\n\\t3.506300e-05    0.823529\\n\\n\\t1.209630e-04    0.882353\\n\\n\\t1.788900e-04    0.941176\\n\\n\\t1.732035e-02    1.000000\\n\\n\\tdtype: float64\\n\\n\\n\\nAnd speed comparison\\n\\n\\n\\n\\tIn [19]:\\n\\n\\t\\n\\n\\t%timeit sq.sort_index().cumsum()*1./len(sq)\\n\\n\\t1000 loops, best of 3: 344 µs per loop\\n\\n\\tIn [18]:\\n\\n\\t\\n\\n\\t%timeit ser.value_counts().sort_index().cumsum()*1./len(ser.value_counts())\\n\\n\\t1000 loops, best of 3: 1.58 ms per loop\\n\\n\\tIn [17]:\\n\\n\\t\\n\\n\\t%timeit [sum( ser <= x)/float(len(ser)) for x in ser]\\n\\n\\t100 loops, best of 3: 3.31 ms per loop\\n\\n\\n\\nIf the values are all unique, the `ser.value_counts()` is no longer needed. That part is slow (Fetching unique values). All you need in that case is just to sort `ser`.\\n\\n\\n\\n\\tIn [23]:\\n\\n\\t\\n\\n\\t%timeit np.arange(1, ser.size+1)/float(ser.size)\\n\\n\\t10000 loops, best of 3: 11.6 µs per loop\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\nThe fastest version that I can think of is to use get vectorized:\\n\\n\\n\\n\\tIn [35]:\\n\\n\\t\\n\\n\\tnp.sum(dfser['values'].values[...,newaxis]<=dfser['values'].values.reshape((1,-1)), axis=0)*1./dfser['values'].size\\n\\n\\tOut[35]:\\n\\n\\tarray([ 0.55555556,  0.33333333,  0.5       ,  0.61111111,  0.77777778,\\n\\n\\t        0.94444444,  0.88888889,  0.44444444,  0.38888889,  0.11111111,\\n\\n\\t        0.72222222,  0.27777778,  0.66666667,  0.22222222,  0.16666667,\\n\\n\\t        0.83333333,  1.        ,  0.11111111])\\n\\nAdd let see:\\n\\n\\n\\n\\tIn [37]:\\n\\n\\t\\n\\n\\t%timeit dfser['ecdf']=[sum( dfser['values'] <= x)/float(dfser['values'].size) for x in dfser['values']]\\n\\n\\t100 loops, best of 3: 6 ms per loop\\n\\n\\tIn [38]:\\n\\n\\t\\n\\n\\t%%timeit\\n\\n\\tdfser['rank'] = dfser['values'].rank(ascending = 0)\\n\\n\\tdfser['ecdf_r']=(len(dfser)-dfser['rank']+1)/len(dfser)\\n\\n\\t1000 loops, best of 3: 827 µs per loop\\n\\n\\tIn [39]:\\n\\n\\t\\n\\n\\t%timeit np.sum(dfser['values'].values[...,newaxis]<=dfser['values'].values.reshape((1,-1)), axis=0)*1./dfser['values'].size\\n\\n\\t10000 loops, best of 3: 91.1 µs per loop\",\n",
       "  '<python><numpy><pandas><ecdf>',\n",
       "  datetime.date(2014, 2, 19),\n",
       "  '2014-02-26 18:05:44',\n",
       "  'CT Zhu (2487184)',\n",
       "  '9',\n",
       "  '',\n",
       "  '9035.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['354',\n",
       "  '21923626',\n",
       "  'Answer',\n",
       "  'correlation between arrays in python',\n",
       "  'You need [`numpy.corrcoef`][1]:\\n\\n\\n\\n    In [8]:\\n\\n\\n\\n    np.corrcoef(a1,a2)\\n\\n    Out[8]:\\n\\n    array([[ 1.        ,  0.98198051],\\n\\n           [ 0.98198051,  1.        ]])\\n\\n\\n\\n\\n\\n  [1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html',\n",
       "  '<python><arrays><vector><correlation>',\n",
       "  datetime.date(2014, 2, 21),\n",
       "  '2014-02-21 02:05:23',\n",
       "  'Cristian Ciupitu (12892)',\n",
       "  '17',\n",
       "  '',\n",
       "  '16489.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['357',\n",
       "  '22368071',\n",
       "  'Answer',\n",
       "  'Extending regressions beyond data in Matplotlib',\n",
       "  \"If you want to extend the regression line beyond the data, for example, to cover the entire x range, you can do (just change the last 3 lines):\\n\\n\\n\\n\\timport numpy as np\\n\\n\\tX=np.arange(xmin, xmax, 50)\\n\\n\\tline=beta1*X**2+beta2*X+beta3\\n\\n\\tplt.plot(X, line, 'r-', lw=5.)\",\n",
       "  '<python><numpy><matplotlib>',\n",
       "  datetime.date(2014, 3, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1956.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['358',\n",
       "  '22417691',\n",
       "  'Answer',\n",
       "  'Adding a colorbar to a polar contourf multiplot',\n",
       "  \"Use `subplot2grid` and plot the `colorbar` in a different axis:\\n\\n\\n\\n\\tazimuths = np.radians(np.linspace(0, 360, 100))\\n\\n\\tzeniths = np.arange(0, 70, 10)\\n\\n\\tr, theta = np.meshgrid(zeniths, azimuths)\\n\\n\\tvalues1 = np.random.random((azimuths.size, zeniths.size))\\n\\n\\tvalues2 = np.random.random((azimuths.size, zeniths.size))\\n\\n\\t\\n\\n\\t#-- Plot... ------------------------------------------------\\n\\n\\t#fig, axs = plt.subplots(1, 2, subplot_kw=dict(projection='polar'))\\n\\n\\tfig = plt.figure()\\n\\n\\tax1=plt.subplot2grid((1,10), (0, 0), colspan=4, projection='polar')\\n\\n\\tax2=plt.subplot2grid((1,10), (0, 4), colspan=4, projection='polar')\\n\\n\\tax3=plt.subplot2grid((1,15), (0, 14), colspan=1)\\n\\n\\tp1 = ax1.contourf(theta, r, values1, 100)\\n\\n\\tp2 = ax2.contourf(theta, r, values2, 100)\\n\\n\\tcbar = plt.colorbar(p2, cax=ax3)\\n\\n\\t\\n\\n\\tplt.show()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/p1t43.png\",\n",
       "  '<python><matplotlib><contour>',\n",
       "  datetime.date(2014, 3, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2821.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['363',\n",
       "  '22775171',\n",
       "  'Answer',\n",
       "  'Cumulative sum of variable till a given percentile',\n",
       "  \"    A=np.array(a)\\n\\n    A[:(A<np.percentile(a, 90)).argmin()].sum() #461\\n\\n\\n\\n@JoshAdel's\\n\\n\\n\\n\\t%%timeit\\n\\n\\t    ...: b = np.cumsum(a)\\n\\n\\t    ...: p90 = np.percentile(a, 90)\\n\\n\\t    ...: b[b < p90][-1]\\n\\n\\t    ...: \\n\\n\\t1000 loops, best of 3: 217 µs per loop\\n\\n\\n\\nThis:\\n\\n\\t\\n\\n\\t%timeit A[:(A<np.percentile(a, 90)).argmin()].sum()\\n\\n\\t10000 loops, best of 3: 191 µs per loop\\n\\n\",\n",
       "  '<python><numpy><percentile>',\n",
       "  datetime.date(2014, 4, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1074.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['364',\n",
       "  '22843975',\n",
       "  'Answer',\n",
       "  'Last minor tick not drawn - Pyplot',\n",
       "  \"Haven't been able to find where the bug comes from yet, but version `1.3.1` has the same behavior.\\n\\n\\n\\nA work around would be to set the minor ticks manually, by adding a `ax2.xaxis.set_ticks(np.hstack((ax2.xaxis.get_ticklocs(minor=True), 6.4)), minor=True)`, where `6.4` is the last minor tick.\\n\\n![enter image description here][1]\\n\\n\\n\\nOr you can force the `xlim` to be slightly larger than the default and the last tick will come out. `ax2.set_xlim((0,6.6))`. The default is `(0.0, 6.5999999999999996)`.\\n\\n\\n\\nI guess it can be considered as a bug.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/OdOZx.png\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 4, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1157.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['368',\n",
       "  '22766721',\n",
       "  'Answer',\n",
       "  't-tests on different groups by iteration in R',\n",
       "  'Use a `list` for `patients` containing the actual vectors, rather than the names of the vectors:\\n\\n\\n\\n    > patients <- list(P1, P2, P3)\\n\\n    > for (i in patients){print(t.test(i,HC)$p.value)}\\n\\n    [1] 0.005015573\\n\\n    [1] 0.0002672035\\n\\n    [1] 0.00899473',\n",
       "  '<r><for-loop><statistics>',\n",
       "  datetime.date(2014, 3, 31),\n",
       "  '2014-03-31 16:20:19',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1510.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['370',\n",
       "  '22771795',\n",
       "  'Answer',\n",
       "  'change X ticks in matplotlib plot',\n",
       "  'Make everything 1/5 of the original?:\\n\\n\\n\\n    ax=plt.gca() \\n\\n    #ax.get_xticks() will get the current ticks\\n\\n    ax.set_xticklabels(map(str, ax.get_xticks()/5.0))',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 3, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1494.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['371',\n",
       "  '22799321',\n",
       "  'Answer',\n",
       "  'How can I save two plots on a single file in python?',\n",
       "  'No, look into `plt.subplot()`. You can plot two plots in one figure and save that into one file.',\n",
       "  '<python><image><matplotlib><plot><save>',\n",
       "  datetime.date(2014, 4, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '3827.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['374',\n",
       "  '24391139',\n",
       "  'Answer',\n",
       "  'Python Pandas merge samed name columns in a dataframe',\n",
       "  'Probably it is not a good idea to have duplicated column names, but it will work:\\n\\n\\n\\n\\tIn [72]:\\n\\n\\t\\n\\n\\tdf2=df[[\\'ID\\', \\'Name\\']]\\n\\n\\tdf2[\\'a\\']=\\'\"\\'+df.T[df.columns.values==\\'a\\'].apply(lambda x: \\';\\'.join([\"%i\"%item for item in x[x.notnull()]]))+\\'\"\\' #these columns are of float dtype\\n\\n\\tdf2[\\'b\\']=df.T[df.columns.values==\\'b\\'].apply(lambda x: \\';\\'.join([item for item in x[x.notnull()]])) #these columns are of objects dtype\\n\\n\\tprint df2\\n\\n\\t   ID   Name      a    b\\n\\n\\t0   1  test1    \"1\"  \"a\"\\n\\n\\t1   2  test2    \"2\"  \"a\"\\n\\n\\t2   3  test3  \"2;3\"  \"b\"\\n\\n\\t3   4  test4    \"4\"  \"b\"\\n\\n\\t\\n\\n\\t[4 rows x 4 columns]',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 6, 24),\n",
       "  '2014-06-24 16:21:34',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '3891.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['375',\n",
       "  '24462574',\n",
       "  'Answer',\n",
       "  \"Polar plot with a 'floating' radial axis\",\n",
       "  \"Maybe we can superimpose another plot on top:\\n\\n\\n\\n\\tfig, axes = plt.subplots(1, 4, figsize=(9, 2), subplot_kw=dict(polar=True))\\n\\n\\t\\n\\n\\tfor aa in axes.flat:\\n\\n\\t    aa.plot(theta, r, '-sb')\\n\\n\\t    aa.set_rlim(0, 1)\\n\\n\\t    aa.set_yticklabels([])\\n\\n\\t    \\n\\n\\tbox=axes[0].get_position()\\n\\n\\taxl=fig.add_axes([box.xmin/2, #put it half way between the edge of the 1st subplot and the left edge of the figure\\n\\n\\t                  0.5*(box.ymin+box.ymax), #put the origin at the same height of the origin of the polar plots\\n\\n\\t                  box.width/40, #Doesn't really matter, we will set everything invisible, except the y axis\\n\\n\\t                  box.height*0.4], #fig.subplots_adjust will not adjust this axis, so we will need to manually set the height to 0.4 (half of 0.9-0.1)\\n\\n\\t                 axisbg=None) #transparent background.\\n\\n\\taxl.spines['top'].set_visible(False)\\n\\n\\taxl.spines['right'].set_visible(False)\\n\\n\\taxl.spines['bottom'].set_visible(False)\\n\\n\\taxl.yaxis.set_ticks_position('both')\\n\\n\\taxl.xaxis.set_ticks_position('none')\\n\\n\\taxl.set_xticklabels([])\\n\\n\\taxl.set_ylim(0,1)\\n\\n\\taxl.set_ylabel('$R$\\\\t', rotation=0)\\n\\n\\t\\n\\n\\tfig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.5)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n#Edit\\n\\n\\n\\nIt turn out that the `subplots_adjust` also affects the superimposing axis. If we check the list of axes inside `fig`, the superimposing axis is right there (check **site-packages\\\\matplotlib\\\\figure.py** if you have doubt):\\n\\n \\n\\n\\tIn [27]:\\n\\n\\t\\n\\n\\tfig.axes\\n\\n\\tOut[27]:\\n\\n\\t[<matplotlib.axes.PolarAxesSubplot at 0x9714650>,\\n\\n\\t <matplotlib.axes.PolarAxesSubplot at 0x9152730>,\\n\\n\\t <matplotlib.axes.PolarAxesSubplot at 0x9195b90>,\\n\\n\\t <matplotlib.axes.PolarAxesSubplot at 0x91878b0>,\\n\\n\\t <matplotlib.axes.Axes at 0x9705a90>]\\n\\n\\n\\nThe real problem is that the `wspace=0.5` not only affects the width of the polar plot, but also affect the height (so the aspect stays the same). But for the non-polar superimposing axis, it only affect the width. Therefore, an additional width modification is required, and the solution is:\\n\\n\\n\\n    fig, axes = plt.subplots(1, 4, figsize=(10, 2), subplot_kw=dict(polar=True))\\n\\n\\n\\n    for aa in axes.flat:\\n\\n        aa.plot(theta, r, '-sb')\\n\\n        aa.set_rlim(0, 1)\\n\\n        aa.set_yticklabels([])\\n\\n\\n\\n    #fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.5)\\n\\n\\n\\n    box=axes[0].get_position()\\n\\n    axl=fig.add_axes([box.xmin/2, \\n\\n                      0.5*(box.ymin+box.ymax),\\n\\n                      box.width/40,\\n\\n                      box.height*0.5],\\n\\n                     axisbg=None)\\n\\n    #fig.add_axes([box.xmin, box.ymin, box.width, box.height])\\n\\n    axl.spines['top'].set_visible(False)\\n\\n    axl.spines['right'].set_visible(False)\\n\\n    axl.spines['bottom'].set_visible(False)\\n\\n    axl.yaxis.set_ticks_position('both')\\n\\n    axl.xaxis.set_ticks_position('none')\\n\\n    axl.set_xticklabels([])\\n\\n    axl.set_ylim(0,1)\\n\\n    axl.set_ylabel('$R$\\\\t', rotation=0)\\n\\n\\n\\n    w_pre_scl=box.width\\n\\n\\n\\n    fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9, wspace=0.5)\\n\\n    ratio=axes[0].get_position().width/w_pre_scl\\n\\n\\n\\n    axlb=axl.get_position()\\n\\n    axl.set_position([axlb.xmin, axlb.ymin, axlb.width, axlb.height*ratio])\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\nif there is no `wspace=0.5`, the last few lines has no net affect:\\n\\n\\n\\n    fig.subplots_adjust(left=0.1, right=0.9, bottom=0.1, top=0.9)\\n\\n    #ratio=axes[0].get_position().width/w_pre_scl\\n\\n\\n\\n    #axlb=axl.get_position()\\n\\n    #axl.set_position([axlb.xmin, axlb.ymin, axlb.width, axlb.height*ratio])\\n\\n\\n\\n![enter image description here][3]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Z1wB8.png\\n\\n  [2]: http://i.stack.imgur.com/RWa7U.png\\n\\n  [3]: http://i.stack.imgur.com/InPuQ.png\",\n",
       "  '<python><matplotlib><polar-coordinates>',\n",
       "  datetime.date(2014, 6, 28),\n",
       "  '2014-06-28 16:23:05',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1940.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['379',\n",
       "  '22414168',\n",
       "  'Answer',\n",
       "  'fsolve - mismatch between input and output',\n",
       "  'The exception means that the result from `fnz()` function call does not has the same dimension as the input `g`, which is a list of 3 elements, or can be seen as an `array` of shape `(3,)`.\\n\\n\\n\\nTo illustrate the problem, if we define:\\n\\n\\n\\n    def fnz(g):\\n\\n        return [2,3,5]\\n\\n    Anz = optimize.fsolve(fnz,g)\\n\\n\\n\\nThere will not be such an exception. But this will:\\n\\n\\n\\n    def fnz(g):\\n\\n        return [2,3,4,5]\\n\\n    Anz = optimize.fsolve(fnz,g)\\n\\n\\n\\nThe result from `fnz()` should have the same length as `t1`, which I am sure is longer than 3 elements. \\n\\n',\n",
       "  '<python><excel><numpy><scipy>',\n",
       "  datetime.date(2014, 3, 14),\n",
       "  '2014-03-17 00:06:53',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1218.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['384',\n",
       "  '22241514',\n",
       "  'Answer',\n",
       "  'How to normalize a histogram in python?',\n",
       "  'When you plot a normalized histogram, it is not the height that should sum up to one, but the area underneath the curve should sum up to one:\\n\\n\\n\\n\\tIn [44]:\\n\\n\\t\\n\\n\\timport matplotlib.pyplot as plt\\n\\n\\tk=(3,3,3,3)\\n\\n\\tx,bins,p=plt.hist(k, normed=1)\\n\\n\\tfrom numpy import *\\n\\n\\tplt.xticks( arange(10) ) # 10 ticks on x axis\\n\\n\\tplt.show()  \\n\\n\\tIn [45]:\\n\\n\\t\\n\\n\\tprint bins\\n\\n\\t[ 2.5  2.6  2.7  2.8  2.9  3.   3.1  3.2  3.3  3.4  3.5]\\n\\n\\n\\nHere, this example, the bin width is 0.1, the area underneath the curve sums up to one (0.1*10).\\n\\n\\n\\nTo have the sum of height to be 1, add the following before `plt.show()`:\\n\\n\\n\\n    for item in p:\\n\\n        item.set_height(item.get_height()/sum(x))\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/AIjXM.png',\n",
       "  '<python><matplotlib><normalization>',\n",
       "  datetime.date(2014, 3, 7),\n",
       "  '2014-03-07 04:46:08',\n",
       "  'CT Zhu (2487184)',\n",
       "  '10',\n",
       "  '',\n",
       "  '31588.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['385',\n",
       "  '22848030',\n",
       "  'Answer',\n",
       "  'turn off axis border for polar matplotlib plot',\n",
       "  \"Just add this line: `axes.spines['polar'].set_visible(False)` and it should go away!\\n\\n\\n\\neewh, all the anatomy terms.\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 4, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '19',\n",
       "  '',\n",
       "  '3590.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['386',\n",
       "  '22867877',\n",
       "  'Answer',\n",
       "  \"Putting arrowheads on vectors in matplotlib's 3d plot\",\n",
       "  'To add arrow patches to a 3D plot, the simple solution is to use `FancyArrowPatch` class defined in `/matplotlib/patches.py`.  However, it only works for 2D plot (at the time of writing), as its `posA` and `posB` are supposed to be tuples of length 2.\\n\\n\\n\\nTherefore we create a new arrow patch class, name it `Arrow3D`, which inherits from `FancyArrowPatch`.  The only thing we need to override its `posA` and `posB`.  To do that, we initiate `Arrow3d` with `posA` and `posB` of `(0,0)`s.  The 3D coordinates `xs, ys, zs` was then projected from 3D to 2D using `proj3d.proj_transform()`, and the resultant 2D coordinates get assigned to `posA` and `posB` using `.set_position()` method, replacing the `(0,0)`s. This way we get the 3D arrow to work.  \\n\\n\\n\\nThe projection steps go into the `.draw` method, which overrides the `.draw` method of the `FancyArrowPatch` object. \\n\\n\\n\\nThis might appear like a hack. However, the `mplot3d` currently only provides (again, only) simple 3D plotting capacity by supplying 3D-2D projections and essentially does all the plotting in 2D, which is not truly 3D.\\n\\n\\n\\n    import numpy as np\\n\\n    from numpy import *\\n\\n    from matplotlib import pyplot as plt\\n\\n    from mpl_toolkits.mplot3d import Axes3D\\n\\n    from matplotlib.patches import FancyArrowPatch\\n\\n    from mpl_toolkits.mplot3d import proj3d\\n\\n    \\n\\n    class Arrow3D(FancyArrowPatch):\\n\\n        def __init__(self, xs, ys, zs, *args, **kwargs):\\n\\n            FancyArrowPatch.__init__(self, (0,0), (0,0), *args, **kwargs)\\n\\n            self._verts3d = xs, ys, zs\\n\\n    \\n\\n        def draw(self, renderer):\\n\\n            xs3d, ys3d, zs3d = self._verts3d\\n\\n            xs, ys, zs = proj3d.proj_transform(xs3d, ys3d, zs3d, renderer.M)\\n\\n            self.set_positions((xs[0],ys[0]),(xs[1],ys[1]))\\n\\n            FancyArrowPatch.draw(self, renderer)\\n\\n    \\n\\n    ####################################################\\n\\n    # This part is just for reference if\\n\\n    # you are interested where the data is\\n\\n    # coming from\\n\\n    # The plot is at the bottom\\n\\n    #####################################################\\n\\n    \\n\\n    # Generate some example data\\n\\n    mu_vec1 = np.array([0,0,0])\\n\\n    cov_mat1 = np.array([[1,0,0],[0,1,0],[0,0,1]])\\n\\n    class1_sample = np.random.multivariate_normal(mu_vec1, cov_mat1, 20)\\n\\n    \\n\\n    mu_vec2 = np.array([1,1,1])\\n\\n    cov_mat2 = np.array([[1,0,0],[0,1,0],[0,0,1]])\\n\\n    class2_sample = np.random.multivariate_normal(mu_vec2, cov_mat2, 20)\\n\\n\\n\\nActual drawing. Note that we only need to change one line of your code, which add an new arrow artist:\\n\\n    \\n\\n    # concatenate data for PCA\\n\\n    samples = np.concatenate((class1_sample, class2_sample), axis=0)\\n\\n    \\n\\n    # mean values\\n\\n    mean_x = mean(samples[:,0])\\n\\n    mean_y = mean(samples[:,1])\\n\\n    mean_z = mean(samples[:,2])\\n\\n    \\n\\n    #eigenvectors and eigenvalues\\n\\n    eig_val, eig_vec = np.linalg.eig(cov_mat1)\\n\\n    \\n\\n    ################################\\n\\n    #plotting eigenvectors\\n\\n    ################################    \\n\\n    \\n\\n    fig = plt.figure(figsize=(15,15))\\n\\n    ax = fig.add_subplot(111, projection=\\'3d\\')\\n\\n    \\n\\n    ax.plot(samples[:,0], samples[:,1], samples[:,2], \\'o\\', markersize=10, color=\\'g\\', alpha=0.2)\\n\\n    ax.plot([mean_x], [mean_y], [mean_z], \\'o\\', markersize=10, color=\\'red\\', alpha=0.5)\\n\\n    for v in eig_vec:\\n\\n        #ax.plot([mean_x,v[0]], [mean_y,v[1]], [mean_z,v[2]], color=\\'red\\', alpha=0.8, lw=3)\\n\\n        #I will replace this line with:\\n\\n        a = Arrow3D([mean_x, v[0]], [mean_y, v[1]], \\n\\n                    [mean_z, v[2]], mutation_scale=20, \\n\\n                    lw=3, arrowstyle=\"-|>\", color=\"r\")\\n\\n        ax.add_artist(a)\\n\\n    ax.set_xlabel(\\'x_values\\')\\n\\n    ax.set_ylabel(\\'y_values\\')\\n\\n    ax.set_zlabel(\\'z_values\\')\\n\\n    \\n\\n    plt.title(\\'Eigenvectors\\')\\n\\n    \\n\\n    plt.draw()\\n\\n    plt.show()\\n\\n\\n\\n![final_output][1]\\n\\n\\n\\nPlease check [this post](https://stackoverflow.com/questions/11140163/python-matplotlib-plotting-a-3d-cube-a-sphere-and-a-vector), which inspired this question, for further details.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/ua9vW.png',\n",
       "  '<python><matplotlib><plot>',\n",
       "  datetime.date(2014, 4, 4),\n",
       "  '2017-05-23 12:09:15',\n",
       "  'CT Zhu (2487184), Seanny123 (1079075), URL Rewriter Bot (n/a)',\n",
       "  '37',\n",
       "  '',\n",
       "  '16386.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['387',\n",
       "  '22916441',\n",
       "  'Answer',\n",
       "  'in python pandas, how to unpack the lists in a column?',\n",
       "  \"This solution will change the order of you columns, which I think is fine in most cases. You can replace `dict` with `OrderedDict` if you want to preserve the column orders. \\n\\n\\n\\n\\tIn [31]:\\n\\n\\tprint DF\\n\\n\\t\\t   date        country  dollar\\n\\n\\t0  20140101  US|UK|Germany  123456\\n\\n\\t1  20140101  US|UK|Germany  123457\\n\\n\\t\\n\\n\\t[2 rows x 3 columns]\\n\\n\\tIn [32]:\\n\\n\\t\\n\\n\\tDF.country=DF.country.apply(lambda x: x.split('|'))\\n\\n\\tprint DF\\n\\n\\t\\t   date            country  dollar\\n\\n\\t0  20140101  [US, UK, Germany]  123456\\n\\n\\t1  20140101  [US, UK, Germany]  123457\\n\\n\\t\\n\\n\\t[2 rows x 3 columns]\\n\\n\\tIn [33]:\\n\\n\\t\\n\\n\\tprint pd.concat([pd.DataFrame(dict(zip(DF.columns,DF.ix[i]))) for i in range(len(DF))])\\n\\n\\t   country      date  dollar\\n\\n\\t0       US  20140101  123456\\n\\n\\t1       UK  20140101  123456\\n\\n\\t2  Germany  20140101  123456\\n\\n\\t0       US  20140101  123457\\n\\n\\t1       UK  20140101  123457\\n\\n\\t2  Germany  20140101  123457\\n\\n\\t\\n\\n\\t[6 rows x 3 columns]\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 4, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '2675.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['388',\n",
       "  '22918691',\n",
       "  'Answer',\n",
       "  'Fastest Way to Drop Duplicated Index in a Pandas DataFrame',\n",
       "  'Simply: `DF.groupby(DF.index).first()`',\n",
       "  '<python><pandas><duplicate-removal>',\n",
       "  datetime.date(2014, 4, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '26',\n",
       "  '',\n",
       "  '17433.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['390',\n",
       "  '22943223',\n",
       "  'Answer',\n",
       "  'Installing rpy2 -- Variable Error',\n",
       "  \"Are you [Gohlke's binaries][1]? \\n\\n\\n\\nAmong various combinations, R_HOME of \\n\\nc:\\\\Progra~1\\\\R\\\\R-3.0.3\\\\ \\n\\n\\n\\nand \\n\\n\\n\\nc:/program files/r/r-3.0.3 \\n\\n\\n\\nworked for me. Looks like either it doesn't like the ` ` or the `\\\\` or both.\\n\\n\\n\\n\\n\\n  [1]: http://www.lfd.uci.edu/~gohlke/pythonlibs/\",\n",
       "  '<python><r><rpy2>',\n",
       "  datetime.date(2014, 4, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1546.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['392',\n",
       "  '23037647',\n",
       "  'Answer',\n",
       "  'Change main plot legend label text',\n",
       "  \"You need to gain access of the `legend()` object and use `set_text()` to change the text values, a simple example:\\n\\n\\n\\n    plt.plot(range(10), label='Some very long label')\\n\\n    plt.plot(range(1,11), label='Short label')\\n\\n    L=plt.legend()\\n\\n    L.get_texts()[0].set_text('make it short')\\n\\n    plt.savefig('temp.png')\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nIn your case, you are changing the first item in the legend, I am quite sure the `0` index in `L.get_texts()[0]` applies to your problem too.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/qMuLc.png\",\n",
       "  '<python><matplotlib><pandas><label><legend>',\n",
       "  datetime.date(2014, 4, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '44',\n",
       "  '',\n",
       "  '24263.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['393',\n",
       "  '24496853',\n",
       "  'Answer',\n",
       "  'compare two lists and return not matching items',\n",
       "  \"Iteration though the entire thing is probably not a good idea for large problems, besides @JoranBeasley's suggestion, `pandas` is also an alternative:\\n\\n\\n\\n\\tIn [52]:\\n\\n\\timport pandas as pd\\n\\n\\tnodes = [['nodeID1', 'x1', 'y1', 'z1'],['nodeID2', 'x2', 'y2', 'z2'],['nodeIDn', 'xn', 'yn', 'zn']]\\n\\n\\tsubsetA_nodeID = [['nodeID1'], ['nodeID2']]\\n\\n\\tsubsetA_nodeIDa = ['nodeID1', 'nodeID2'] #use itertools.chain to get this\\n\\n\\tIn [53]:\\n\\n\\t\\n\\n\\tdf=pd.DataFrame(nodes)\\n\\n\\tprint df\\n\\n\\tdf.set_index(0, inplace=True)\\n\\n\\tprint df\\n\\n\\t\\t\\t 0   1   2   3\\n\\n\\t0  nodeID1  x1  y1  z1\\n\\n\\t1  nodeID2  x2  y2  z2\\n\\n\\t2  nodeIDn  xn  yn  zn\\n\\n\\t\\t\\t  1   2   3\\n\\n\\t0                  \\n\\n\\tnodeID1  x1  y1  z1\\n\\n\\tnodeID2  x2  y2  z2\\n\\n\\tnodeIDn  xn  yn  zn\\n\\n\\tIn [54]:\\n\\n\\t\\n\\n\\tprint df.ix[subsetA_nodeIDa]\\n\\n\\t\\t\\t  1   2   3\\n\\n\\tnodeID1  x1  y1  z1\\n\\n\\tnodeID2  x2  y2  z2\\n\\n\\tIn [55]:\\n\\n\\t\\n\\n\\tlist(map(list, df.ix[subsetA_nodeIDa].values))\\n\\n\\tOut[55]:\\n\\n\\t[['x1', 'y1', 'z1'], ['x2', 'y2', 'z2']]\",\n",
       "  '<python><list><compare>',\n",
       "  datetime.date(2014, 6, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2003.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['400',\n",
       "  '22511695',\n",
       "  'Answer',\n",
       "  'Numpy Polyfit or any fitting to X and Y multidimensional arrays',\n",
       "  \"It is possible to do so without iterate along the first axis. However, your second axis is rather short (being just 3), you can really  fit no more than 2 coefficients. \\n\\n\\n\\n\\tIn [67]:\\n\\n\\t\\n\\n\\timport numpy as np\\n\\n\\timport scipy.optimize as so\\n\\n\\n\\n\\tIn [68]:\\n\\n\\t\\n\\n\\tdef MD_ployError(p, x, y):\\n\\n\\t\\t'''if x has the shape of (n,m), y must be (n,m), p must be (n*p, ), where p is degree'''\\n\\n\\t\\t#d is no. of degree\\n\\n\\t\\tp_rshp=p.reshape((x.shape[0], -1))\\n\\n\\t\\tf=y*1.\\n\\n\\t\\tfor i in range(p_rshp.shape[1]):\\n\\n\\t\\t\\tf-=p_rshp[:,i][:,np.newaxis]*(x**i)\\n\\n\\t\\treturn (f**2).sum()\\n\\n\\n\\n\\tIn [69]:\\n\\n\\t\\n\\n\\tX=np.random.random((100, 6))\\n\\n\\tY=4+2*X+3*X*X\\n\\n\\tP=(np.zeros((100,3))+[1,1,1]).ravel()\\n\\n\\n\\n\\tIn [70]:\\n\\n\\t\\n\\n\\tMD_ployError(P, X, Y)\\n\\n\\n\\n\\tOut[70]:\\n\\n\\t11012.2067606684\\n\\n\\n\\n\\tIn [71]:\\n\\n\\t\\n\\n\\tR=so.fmin_slsqp(MD_ployError, P, args=(X, Y))\\n\\n\\tIteration limit exceeded    (Exit mode 9) #you can increase iteration limit, but the result is already good enough.\\n\\n\\t\\t\\t\\tCurrent function value: 0.00243784856039\\n\\n\\t\\t\\t\\tIterations: 101\\n\\n\\t\\t\\t\\tFunction evaluations: 30590\\n\\n\\t\\t\\t\\tGradient evaluations: 101\\n\\n\\n\\n\\tIn [72]:\\n\\n\\t\\n\\n\\tR.reshape((100, -1))\\n\\n\\n\\n\\tOut[72]:\\n\\n\\tarray([[ 3.94488512,  2.25402422,  2.74773571],\\n\\n\\t\\t   [ 4.00474864,  1.97966551,  3.02010015],\\n\\n\\t\\t   [ 3.99919559,  2.0032741 ,  2.99753804],\\n\\n\\t..............................................)\",\n",
       "  '<python><numpy><multidimensional-array><scipy><model-fitting>',\n",
       "  datetime.date(2014, 3, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2354.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['401',\n",
       "  '22651188',\n",
       "  'Answer',\n",
       "  'Pandas groupby cumulative sum',\n",
       "  \"This should do it, need `groupby()` twice.\\n\\n\\n\\n\\tIn [52]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t   name        day   no\\n\\n\\t0  Jack     Monday   10\\n\\n\\t1  Jack    Tuesday   20\\n\\n\\t2  Jack    Tuesday   10\\n\\n\\t3  Jack  Wednesday   50\\n\\n\\t4  Jill     Monday   40\\n\\n\\t5  Jill  Wednesday  110\\n\\n\\tIn [53]:\\n\\n\\t\\n\\n\\tprint df.groupby(by=['name','day']).sum().groupby(level=[0]).cumsum()\\n\\n\\t\\t\\t\\t\\t no\\n\\n\\tname day           \\n\\n\\tJack Monday      10\\n\\n\\t\\t Tuesday     40\\n\\n\\t\\t Wednesday   90\\n\\n\\tJill Monday      40\\n\\n\\t\\t Wednesday  150\\n\\n\\n\\nNote, the resulting `DataFrame` has `MultiIndex`.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 3, 26),\n",
       "  '2014-03-26 04:02:50',\n",
       "  'CT Zhu (2487184)',\n",
       "  '45',\n",
       "  '',\n",
       "  '32189.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['403',\n",
       "  '22799245',\n",
       "  'Answer',\n",
       "  'Keep finite entries only in Pandas',\n",
       "  'You can use `.dropna()` after a `DF[DF==np.inf]=np.nan`, (unless you still want to keep the `NAN`s and only drop the `inf`s)',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 4, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '6182.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['407',\n",
       "  '23159240',\n",
       "  'Answer',\n",
       "  'Python boxplot out of columns of different lengths',\n",
       "  \"The right way to do it, saving from reinventing the wheel, would be to use the `.boxplot()` in `pandas`, where the `nan` handled correctly:\\n\\n\\n\\n\\n\\n\\tIn [31]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t\\t  A     B     C     D     E     F\\n\\n\\t0  0.43  0.52  0.96  1.17  1.17  2.85\\n\\n\\t1  0.43  0.52  1.17  2.72  2.75  2.94\\n\\n\\t2  0.43  0.53  1.48  2.85  2.83   NaN\\n\\n\\t3  0.47  0.59  1.58   NaN  3.14   NaN\\n\\n\\t4  0.49  0.80   NaN   NaN   NaN   NaN\\n\\n\\t\\n\\n\\t[5 rows x 6 columns]\\n\\n\\tIn [32]:\\n\\n\\t\\n\\n\\t_=plt.boxplot(df.values)\\n\\n\\t_=plt.xticks(range(1,7),labels)\\n\\n\\tplt.savefig('1.png') #keeping the nan's and plot by plt\\n\\n![enter image description here][1]\\n\\n\\n\\n\\tIn [33]:\\n\\n\\t\\n\\n\\t_=df.boxplot()\\n\\n\\tplt.savefig('2.png') #keeping the nan's and plot by pandas\\n\\n![enter image description here][2]\\n\\n\\n\\n\\t\\n\\n\\tIn [34]:\\n\\n\\t\\n\\n\\t_=plt.boxplot(df.dropna().values)\\n\\n\\t_=plt.xticks(range(1,7),labels)\\n\\n\\tplt.savefig('3.png') #dropping the nan's and plot by plt\\n\\n![enter image description here][3]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/frVKM.png\\n\\n  [2]: http://i.stack.imgur.com/KUMNb.png\\n\\n  [3]: http://i.stack.imgur.com/lwvf0.png\",\n",
       "  '<python><pandas><boxplot><prettyplotlib>',\n",
       "  datetime.date(2014, 4, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '4111.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['408',\n",
       "  '23161111',\n",
       "  'Answer',\n",
       "  'plotting & formatting seaborn chart from pandas dataframe',\n",
       "  \"The `matplotlib` way would be to use `MutlipLocator`. The second one is also straight forward\\n\\n\\n\\n    from matplotlib.ticker import *\\n\\n    plt.plot(range(10))\\n\\n    ax=plt.gca()\\n\\n    ax.yaxis.set_major_locator(MultipleLocator(0.5))\\n\\n    plt.xticks(range(10), list('ABCDEFGHIJ'), rotation=90) #would be range(3xx), List_of_city_names, rotation=90\\n\\n    plt.savefig('temp.png')\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/24a2R.png\",\n",
       "  '<python><matplotlib><pandas><seaborn>',\n",
       "  datetime.date(2014, 4, 18),\n",
       "  '2014-04-18 19:51:05',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '7571.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['411',\n",
       "  '22471960',\n",
       "  'Answer',\n",
       "  'How do I check if a numpy dtype is integral?',\n",
       "  \"Do you mean line 17?\\n\\n\\n\\n\\tIn [13]:\\n\\n\\t\\n\\n\\timport numpy as np\\n\\n\\tA=np.array([1,2,3])\\n\\n\\tIn [14]:\\n\\n\\t\\n\\n\\tA.dtype\\n\\n\\tOut[14]:\\n\\n\\tdtype('int32')\\n\\n\\tIn [15]:\\n\\n\\t\\n\\n\\tisinstance(A, np.ndarray) #A is not an instance of int32, it is an instance of ndarray\\n\\n\\tOut[15]:\\n\\n\\tTrue\\n\\n\\tIn [16]:\\n\\n\\t\\n\\n\\tA.dtype==np.int32 #but its dtype is int32\\n\\n\\tOut[16]:\\n\\n\\tTrue\\n\\n\\tIn [17]:\\n\\n\\t\\n\\n\\tissubclass(np.int32, int) #and int32 is a subclass of int\\n\\n\\tOut[17]:\\n\\n\\tTrue\",\n",
       "  '<python><numpy><integral><abc>',\n",
       "  datetime.date(2014, 3, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '9589.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['412',\n",
       "  '22514175',\n",
       "  'Answer',\n",
       "  'Weird pdfs from Generalised Extreme Value (GEV) Maximum Likelihood fitted data',\n",
       "  'First, I think you may want to keep you location parameter fixed at `0`.\\n\\n\\n\\nSecond, you got zeros in your data, the resulting fit may have `+inf` `pdf` at `x=0` e.g. for the GEV fit or for the Weibull fit. \\n\\nTherefore, the fit is actually correct, but when you plot the `pdf` (including `x=0`), the resulting plot is distorted.\\n\\n\\n\\nThird, I really think `scipy` should drop the support for `x=0` for a bunch of distributions such as `Weibull`. For `x=0`, `R` gives a nice warning of `Error in fitdistr(data, \"weibull\") : Weibull values must be > 0`, that is helpful.\\n\\n\\n\\n\\tIn [103]:\\n\\n\\t\\n\\n\\tp=ss.genextreme.fit(data, floc=0)\\n\\n\\tss.genextreme.fit(data, floc=0)\\n\\n\\tOut[103]:\\n\\n\\t(-1.372872096699608, 0, 0.011680600795499299)\\n\\n\\tIn [104]:\\n\\n\\t\\n\\n\\tplt.hist(data, bins=20, normed=True, alpha=0.7, label=\\'Data\\')\\n\\n\\tplt.plot(np.linspace(5e-3, 1.6, 100),\\n\\n\\t\\t\\t ss.genextreme.pdf(np.linspace(5e-3, 1.6, 100), p[0], p[1], p[2]), \\'r--\\',\\n\\n\\t\\t\\t label=\\'GEV Fit\\')\\n\\n\\tplt.legend(loc=\\'upper right\\')\\n\\n\\tplt.savefig(\\'T1.png\\')\\n\\n\\t\\n\\n![enter image description here][1]\\n\\n\\n\\n\\tIn [105]:\\n\\n\\t\\n\\n\\tp=ss.expon.fit(data, floc=0)\\n\\n\\tss.expon.fit(data, floc=0)\\n\\n\\tOut[105]:\\n\\n\\t(0, 0.14838807003769505)\\n\\n\\tIn [106]:\\n\\n\\t\\n\\n\\tplt.hist(data, bins=20, normed=True, alpha=0.7, label=\\'Data\\')\\n\\n\\tplt.plot(np.linspace(0, 1.6, 100),\\n\\n\\t\\t\\t ss.expon.pdf(np.linspace(0, 1.6, 100), p[0], p[1]), \\'r--\\',\\n\\n\\t\\t\\t label=\\'Expon. Fit\\')\\n\\n\\tplt.legend(loc=\\'upper right\\')\\n\\n\\tplt.savefig(\\'T2.png\\')\\n\\n\\t\\n\\n![enter image description here][2]\\n\\n\\n\\n\\tIn [107]:\\n\\n\\t\\n\\n\\tp=ss.weibull_min.fit(data[data!=0], floc=0)\\n\\n\\tss.weibull_min.fit(data[data!=0], floc=0)\\n\\n\\tOut[107]:\\n\\n\\t(0.67366030738733995, 0, 0.10535422201164378)\\n\\n\\tIn [108]:\\n\\n\\t\\n\\n\\tplt.hist(data[data!=0], bins=20, normed=True, alpha=0.7, label=\\'Data\\')\\n\\n\\tplt.plot(np.linspace(5e-3, 1.6, 100),\\n\\n\\t\\t\\t ss.weibull_min.pdf(np.linspace(5e-3, 1.6, 100), p[0], p[1], p[2]), \\'r--\\',\\n\\n\\t\\t\\t label=\\'Weibull_Min Fit\\')\\n\\n\\tplt.legend(loc=\\'upper right\\')\\n\\n\\tplt.savefig(\\'T3.png\\')\\n\\n\\n\\n![enter image description here][3]\\n\\n\\n\\n\\n\\n----------\\n\\n#edit\\n\\n\\n\\nYour second data (which contains even more `0`\\'s )is a good example when MLE fit involving location parameter can become very challenging, especially potentially with a lot of float point overflow/underflow involved:\\n\\n\\n\\n\\tIn [122]:\\n\\n\\t#fit with location parameter fixed, scanning loc parameter from 1e-8 to 1e1\\n\\n\\tL=[] #stores the Log-likelihood\\n\\n\\tP=[] #stores the p value of K-S test\\n\\n\\tfor LC in np.linspace(-8, 1, 200):\\n\\n\\t    fit = gev.fit(data, floc=10**LC)\\n\\n\\t    L.append(np.log(gev.pdf(data, *fit)).sum())\\n\\n\\t    P.append(kstest(data, \\'genextreme\\', fit)[1])\\n\\n\\tL=np.array(L)\\n\\n\\tP=np.array(P)\\n\\n\\tIn [123]:\\n\\n\\t#plot log likelihood, a lot of overflow/underflow issues! (see the zigzag line?)\\n\\n\\tplt.plot(np.linspace(-8, 1, 200), L,\\'-\\')\\n\\n\\tplt.ylabel(\\'Log-Likelihood\\')\\n\\n\\tplt.xlabel(\\'$log_{10}($\\'+\\'location parameter\\'+\\'$)$\\')\\n\\n\\n\\n![enter image description here][4]\\n\\n\\t\\n\\n\\tIn [124]:\\n\\n\\t#plot p-value\\n\\n\\tplt.plot(np.linspace(-8, 1, 200), np.log10(P),\\'-\\')\\n\\n\\tplt.ylabel(\\'$log_{10}($\\'+\\'K-S test P value\\'+\\'$)$\\')\\n\\n\\tplt.xlabel(\\'$log_{10}($\\'+\\'location parameter\\'+\\'$)$\\')\\n\\n\\tOut[124]:\\n\\n\\t<matplotlib.text.Text at 0x107e3050>\\n\\n\\n\\n![enter image description here][5]\\n\\n\\t\\n\\n\\tIn [128]:\\n\\n\\t#The best fit between with location paramter between 1e-8 to 1e1 has the loglikelihood of 515.18\\n\\n\\tnp.linspace(-8, 1, 200)[L.argmax()]\\n\\n\\tfit = gev.fit(data, floc=10**(np.linspace(-8, 1, 200)[L.argmax()]))\\n\\n\\tnp.log(gev.pdf(data, *fit)).sum()\\n\\n\\tOut[128]:\\n\\n\\t515.17663678368604\\n\\n\\tIn [129]:\\n\\n\\t#The simple MLE fit is clearly bad, loglikelihood is -inf\\n\\n\\tfit0 = gev.fit(data)\\n\\n\\tnp.log(gev.pdf(data, *fit0)).sum()\\n\\n\\tOut[129]:\\n\\n\\t-inf\\n\\n\\t\\n\\n\\tIn [133]:\\n\\n\\t#plot the fit\\n\\n\\tx = np.linspace(0.005, 2, 200)\\n\\n\\tplt.plot(x, gev.pdf(x, *fit))\\n\\n\\tplt.hist(data,normed=True, alpha=0.6, bins=20)\\n\\n\\tOut[133]:\\n\\n\\t(array([ 8.91719745,  0.8492569 ,  0.        ,  1.27388535,  0.        ,\\n\\n\\t        0.42462845,  0.        ,  0.        ,  0.        ,  0.        ,\\n\\n\\t        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\\n\\n\\t        0.        ,  0.42462845,  0.        ,  0.        ,  0.8492569 ]),\\n\\n\\t array([ 0.    ,  0.0785,  0.157 ,  0.2355,  0.314 ,  0.3925,  0.471 ,\\n\\n\\t        0.5495,  0.628 ,  0.7065,  0.785 ,  0.8635,  0.942 ,  1.0205,\\n\\n\\t        1.099 ,  1.1775,  1.256 ,  1.3345,  1.413 ,  1.4915,  1.57  ]),\\n\\n\\t <a list of 20 Patch objects>)\\n\\n\\n\\n![enter image description here][6]\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n#Edit, goodness of fit test for GEV\\n\\n\\n\\nA side note on KS test. You are testing the goodness-of-fit to a GEV with its parameter ESTIMATED FROM THE DATA itself. In such a case, the p value is invalid, see: itl.nist.gov/div898/handbook/eda/section3/eda35g.htm\\n\\n\\n\\nThere seems to be a lot of studies on the topic of goodness-of-fit test for GEV, I haven\\'t found any available implementations for those yet.\\n\\n\\n\\nhttp://onlinelibrary.wiley.com/doi/10.1029/98WR02364/abstract\\n\\nhttp://onlinelibrary.wiley.com/doi/10.1029/91WR00077/abstract\\n\\nhttp://www.idrologia.polito.it/~laio/articoli/16-WRR%20EDFtest.pdf\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/6dU3y.png\\n\\n  [2]: http://i.stack.imgur.com/Ude8k.png\\n\\n  [3]: http://i.stack.imgur.com/CUZ3o.png\\n\\n  [4]: http://i.stack.imgur.com/3A4tg.png\\n\\n  [5]: http://i.stack.imgur.com/1YSXu.png\\n\\n  [6]: http://i.stack.imgur.com/4lXte.png',\n",
       "  '<python><statistics><scipy>',\n",
       "  datetime.date(2014, 3, 19),\n",
       "  '2014-03-23 14:52:19',\n",
       "  'CT Zhu (2487184)',\n",
       "  '8',\n",
       "  '',\n",
       "  '2213.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['414',\n",
       "  '22872581',\n",
       "  'Answer',\n",
       "  'Generating a heat map using 3D data in matplotlib',\n",
       "  'It looks like if reshape `x`, `y`, `z` to square matrix, you can do a `contourf` plot:\\n\\n\\n\\n\\tIn [7]:X\\n\\n\\tOut[7]:\\n\\n\\tarray([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n\\n\\t\\t   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n\\n\\t\\t   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n\\n\\t\\t   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n\\n\\t\\t   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n\\n\\t\\t   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n\\n\\t\\t   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n\\n\\t\\t   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n\\n\\t\\t   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\\n\\n\\t\\t   [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\\n\\n\\t\\t   \\n\\n\\tIn [8]:Y\\n\\n\\tOut[8]:\\n\\n\\tarray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\n\\n\\t\\t   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\\n\\n\\t\\t   [2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\\n\\n\\t\\t   [3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\\n\\n\\t\\t   [4, 4, 4, 4, 4, 4, 4, 4, 4, 4],\\n\\n\\t\\t   [5, 5, 5, 5, 5, 5, 5, 5, 5, 5],\\n\\n\\t\\t   [6, 6, 6, 6, 6, 6, 6, 6, 6, 6],\\n\\n\\t\\t   [7, 7, 7, 7, 7, 7, 7, 7, 7, 7],\\n\\n\\t\\t   [8, 8, 8, 8, 8, 8, 8, 8, 8, 8],\\n\\n\\t\\t   [9, 9, 9, 9, 9, 9, 9, 9, 9, 9]])\\n\\n\\n\\n    plt.contourf(X,Y,np.random.random((10,10))) #reshape Z too!\\n\\n    plt.colorbar()\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/zuYbx.png',\n",
       "  '<python><numpy><matplotlib><visualization>',\n",
       "  datetime.date(2014, 4, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '5494.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['416',\n",
       "  '23002359',\n",
       "  'Answer',\n",
       "  'Adding error bars to grouped bar plot in pandas',\n",
       "  \"Posting data as a picture is not a good idea. Here is a solution:\\n\\n\\n\\n\\tIn [16]:\\n\\n\\t#se column store the errorbar values\\n\\n\\tprint df\\n\\n\\t  class1 class2  se  val\\n\\n\\t0      A      R   1    1\\n\\n\\t1      A      G   1    2\\n\\n\\t2      B      R   1    3\\n\\n\\t3      B      G   1    4\\n\\n\\t\\n\\n\\t[4 rows x 4 columns]\\n\\n\\tIn [17]:\\n\\n\\t\\n\\n\\tdf.pivot(index='class1',columns='class2',values='val').plot(kind='bar', yerr=df.pivot(index='class1',columns='class2',values='se').values)\\n\\n    #or yerr=df.se.reshape((2,2))\\n\\n    #Where (2,2) is the shape of df.pivot(index='class1',columns='class2',values='val')\\n\\n    #which is less verbose but may not be general\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Kiygi.png\",\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2014, 4, 11),\n",
       "  '2014-04-11 04:40:58',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '2630.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['417',\n",
       "  '23024805',\n",
       "  'Answer',\n",
       "  'How to customize axes in 3D hist python/matplotlib',\n",
       "  'There are too many places you got it wrong, so I\\'d just post what it should be like:\\n\\n\\n\\n\\timport matplotlib.pyplot as plt\\n\\n\\tfrom mpl_toolkits.mplot3d import Axes3D\\n\\n\\t\\n\\n\\tdata = pd.DataFrame({\\'A\\': [1,1,2,2,2,3,1,1,1], \\'B\\': [2003,2003,2008,2007,2007,2004,2004,2004,2004] ,\\'freq\\': [2,2,1,2,2,1,3,3,3] })\\n\\n\\tfig = plt.figure()\\n\\n\\tax = fig.add_subplot(111, projection=\\'3d\\')\\n\\n\\t# put 0s on the y-axis, and put the y axis on the z-axis\\n\\n\\t\\n\\n\\t#ax.plot(data.A.values, data.B.values,data.freq.values, marker=\\'o\\', linestyle=\\'--\\', color=\"blue\", label=\\'ys=0, zdir=z\\')\\n\\n\\tPV = pd.pivot_table(data, values=\\'freq\\',rows=\\'A\\',cols=\\'B\\')\\n\\n\\txpos=np.arange(PV.shape[0])\\n\\n\\typos=np.arange(PV.shape[1])\\n\\n\\txpos, ypos = np.meshgrid(xpos+0.25, ypos+0.25)\\n\\n\\txpos = xpos.flatten()\\n\\n\\typos = ypos.flatten()\\n\\n\\tzpos=np.zeros(PV.shape).flatten()\\n\\n\\tdx=0.5 * np.ones_like(zpos)\\n\\n\\tdy=0.5 * np.ones_like(zpos)\\n\\n\\tdz=PV.values.ravel()\\n\\n\\tdz[np.isnan(dz)]=0.\\n\\n\\t\\n\\n\\tax.bar3d(xpos,ypos,zpos,dx,dy,dz,color=\\'b\\', alpha=0.5)\\n\\n\\tax.set_xticks([.5,1.5,2.5])\\n\\n\\tax.set_yticks([.5,1.5,2.5,3.5])\\n\\n\\tax.w_yaxis.set_ticklabels(PV.columns)\\n\\n\\tax.w_xaxis.set_ticklabels(PV.index)\\n\\n\\tax.set_xlabel(\\'A\\')\\n\\n\\tax.set_ylabel(\\'B\\')\\n\\n\\tax.set_zlabel(\\'Occurrence\\')\\n\\n\\t\\n\\n\\tplt.savefig(\"test.png\", dpi=300)\\n\\n\\tplt.show()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/4byXE.png',\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2014, 4, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1424.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['420',\n",
       "  '24702783',\n",
       "  'Answer',\n",
       "  \"Using Python's Pandas to find average values by bins\",\n",
       "  \"The most concise way is probably to convert this to a `timeseris` data and them downsample to get the means:\\n\\n\\n\\n\\tIn [75]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t\\t\\t\\t\\t\\t\\t ID  Level\\n\\n\\t1                                 \\n\\n\\t1980-04-17  485438103132901  -7.10\\n\\n\\t1980-05-06  485438103132901  -6.80\\n\\n\\t1979-09-10  483622101085001  -6.70\\n\\n\\t1979-07-31  485438103132901  -6.20\\n\\n\\t1980-11-11  483845101112801  -5.37\\n\\n\\t1980-11-11  484123101124601  -5.30\\n\\n\\t1977-07-06  485438103132901  -4.98\\n\\n\\tIn [76]:\\n\\n\\t\\n\\n\\tdf.Level.resample('60M', how='mean') \\n\\n    #also may consider different time alias: '5A', '5BA', '5AS', etc:\\n\\n    #see: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\\n\\n\\tOut[76]:\\n\\n\\t1\\n\\n\\t1977-07-31   -4.980\\n\\n\\t1982-07-31   -6.245\\n\\n\\tFreq: 60M, Name: Level, dtype: float64\\n\\n\\n\\nAlternatively, you may use `groupby` together with `cut`:\\n\\n\\n\\n\\n\\n\\tIn [99]:\\n\\n\\t\\n\\n\\tprint df.groupby(pd.cut(df.index.year, pd.date_range('1960', periods=5, freq='5A').year, include_lowest=True)).mean()\\n\\n\\t\\t\\t\\t\\t\\t\\tID     Level\\n\\n\\t[1960, 1965]           NaN       NaN\\n\\n\\t(1965, 1970]           NaN       NaN\\n\\n\\t(1970, 1975]           NaN       NaN\\n\\n\\t(1975, 1980]  4.847632e+14 -6.064286\\n\\n\\n\\nAnd by ID also:\\n\\n\\n\\n\\tIn [100]:\\n\\n\\t\\n\\n\\tprint df.groupby(['ID', \\n\\n\\t\\t\\t\\t\\t  pd.cut(df.index.year, pd.date_range('1960', periods=5, freq='5A').year, include_lowest=True)]).mean()\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t  Level\\n\\n\\tID                                 \\n\\n\\t483622101085001 (1975, 1980]  -6.70\\n\\n\\t483845101112801 (1975, 1980]  -5.37\\n\\n\\t484123101124601 (1975, 1980]  -5.30\\n\\n\\t485438103132901 (1975, 1980]  -6.27\",\n",
       "  '<python><pandas><bin>',\n",
       "  datetime.date(2014, 7, 11),\n",
       "  '2014-07-11 17:05:41',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '2826.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['421',\n",
       "  '23024906',\n",
       "  'Answer',\n",
       "  'Printing Pandas Columns With Unicode Characters',\n",
       "  \"There is a very useful function `u` in `pandas.compat`, to make your values in unicode.:\\n\\n\\n\\n\\tIn [26]:\\n\\n\\timport pandas as pd\\n\\n\\tfrom pandas.compat import u\\n\\n\\tno_unicode = pd.Series(['Steve', 'Jason', 'Jake'])\\n\\n\\t#yes_unicode = pd.Series(['tea', 'caf\\\\xe9', 'beer'])\\n\\n\\tyes_unicode = pd.Series(map(u,['tea', 'caf\\\\xe9', 'beer']))\\n\\n\\tvar_names = dict(no_unicode = no_unicode, yes_unicode = yes_unicode)\\n\\n\\tdf = pd.DataFrame(var_names)\\n\\n\\tprint(df)\\n\\n\\n\\n\\t  no_unicode yes_unicode\\n\\n\\t0      Steve         tea\\n\\n\\t1      Jason        café\\n\\n\\t2       Jake        beer\\n\\n\\t\\n\\n\\t[3 rows x 2 columns]\",\n",
       "  '<python><unicode><pandas>',\n",
       "  datetime.date(2014, 4, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '3671.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['423',\n",
       "  '23067406',\n",
       "  'Answer',\n",
       "  'Bin datetime range using pandas',\n",
       "  \"This is a quite verbose solution, the loop is hard to get rid of:\\n\\n\\n\\n\\tIn [3]:\\n\\n\\t\\n\\n\\tfrom collections import OrderedDict\\n\\n\\tIn [4]:\\n\\n\\t\\n\\n\\tdf['End_d']=pd.DatetimeIndex(df['End']).day\\n\\n\\tdf['Start_d']=pd.DatetimeIndex(df['Start']).day\\n\\n\\tIn [5]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t   OperID               Start                 End  End_d  Start_d\\n\\n\\t0     141 2014-03-04 19:28:39 2014-03-04 19:33:38      4        4\\n\\n\\t1   10502 2014-03-04 02:26:26 2014-03-08 20:09:21      8        4\\n\\n\\t2   10502 2014-03-15 00:03:45 2014-03-15 10:03:44     15       15\\n\\n\\t\\n\\n\\t[3 rows x 5 columns]\\n\\n\\tIn [6]:\\n\\n\\t\\n\\n\\tdf.dtypes\\n\\n\\tOut[6]:\\n\\n\\tOperID              int64\\n\\n\\tStart      datetime64[ns]\\n\\n\\tEnd        datetime64[ns]\\n\\n\\tEnd_d               int32\\n\\n\\tStart_d             int32\\n\\n\\tdtype: object\\n\\n\\tIn [7]:\\n\\n\\t\\n\\n\\tdf1=df[df.End_d==df.Start_d].loc[:,['OperID', 'Start','End']]  #the obs. of which the duration < 1day\\n\\n\\tdf2=df[df.End_d!=df.Start_d]                                   #the obs. of which the duration > 1day\\n\\n\\tfor i in df2.index:                                            #Expend it in to multiple rows.\\n\\n\\t\\tdays=df2.loc[i,:].End_d-df2.loc[i,:].Start_d+1\\n\\n\\t\\tstart_d_str=df2.loc[i,:].Start.strftime('%Y-%m-%d')\\n\\n\\t\\ttemp_df=pd.DataFrame(OrderedDict({'OperID': df2.loc[i,:].OperID,\\n\\n\\t\\t\\t\\t  'Start': pd.date_range('%s 00:00:00'%start_d_str, periods=days),\\n\\n\\t\\t\\t\\t  'End':   pd.date_range('%s 23:59:59'%start_d_str, periods=days)}))\\n\\n\\t\\ttemp_df.loc[0,'Start'] = df2.loc[i,'Start']\\n\\n\\t\\ttemp_df.loc[days-1, 'End'] = df2.loc[i,'End']\\n\\n\\t\\tdf1=df1.append(temp_df)\\n\\n\\tdf1['Bin']=pd.DatetimeIndex(df1.Start.apply(lambda x: x.strftime('%Y-%m-%d')))   #Get the YMD only\\n\\n\\tdf1['Seconds']=(df1['End']-df1['Start'])/np.timedelta64(1,'s')                   #Convert to seconds\\n\\n\\tdf1.sort(columns=['OperID', 'Start'], ascending=[-1,-1], inplace=True)\\n\\n\\tprint df1\\n\\n\\t\\t\\t\\t\\t  End  OperID               Start        Bin  Seconds\\n\\n\\t0 2014-03-04 19:33:38     141 2014-03-04 19:28:39 2014-03-04      299\\n\\n\\t0 2014-03-04 23:59:59   10502 2014-03-04 02:26:26 2014-03-04    77613\\n\\n\\t1 2014-03-05 23:59:59   10502 2014-03-05 00:00:00 2014-03-05    86399\\n\\n\\t2 2014-03-06 23:59:59   10502 2014-03-06 00:00:00 2014-03-06    86399\\n\\n\\t3 2014-03-07 23:59:59   10502 2014-03-07 00:00:00 2014-03-07    86399\\n\\n\\t4 2014-03-08 20:09:21   10502 2014-03-08 00:00:00 2014-03-08    72561\\n\\n\\t2 2014-03-15 10:03:44   10502 2014-03-15 00:03:45 2014-03-15    35999\\n\\n\\t\\n\\n\\t[7 rows x 5 columns]\\n\\n\\n\\nAlso if you count 1 days as 86400 seconds rather than 86299 seconds, aren't you count the last seconds twice (in both days)? Minor issue anyway.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 4, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1188.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['425',\n",
       "  '23318585',\n",
       "  'Answer',\n",
       "  'NumPy: matrix by vector multiplication',\n",
       "  'When you have a `matrix`, no matter it is a `matrix` and an `array` or two `matrixt`s. `*` is always seen as matrix multiplication, hence the `not aligned` error because it is simply not conducting a cell wise operation. You see the traceback goes to  `__mul__()` in `defmatrix.pyc`\\n\\n\\n\\nTo do the vector-wise operation you intended, use `np.multiply((A1 + offsets) / norms , priorita)`, it is the same thing as `np.matrix(np.array((A1 + offsets) / norms )* np.array(priorita))`, but slightly faster, and reads better I think. ',\n",
       "  '<python><numpy>',\n",
       "  datetime.date(2014, 4, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1117.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['426',\n",
       "  '23370279',\n",
       "  'Answer',\n",
       "  'Controlling representation of datetime64[ns] in Pandas plots',\n",
       "  \"Maybe you can just create a new column of the strings values you want:\\n\\n\\n\\n    df['date1']=df.date.apply(lambda x: x.strftime('%m-%y'))\\n\\n    ax=df.boxplot(column='value', by='date1')\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/psVeC.png\",\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2014, 4, 29),\n",
       "  '2014-04-29 22:38:07',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1074.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['427',\n",
       "  '23448125',\n",
       "  'Answer',\n",
       "  'Pandas: Sum of first N non-missing values per row',\n",
       "  'I think you can just do:\\n\\n\\n\\n    at_most=2\\n\\n    df.apply(lambda x: (x[np.isfinite(x)][:at_most]).sum(), axis=1)',\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2014, 5, 3),\n",
       "  '2014-05-04 03:08:34',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1324.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['432',\n",
       "  '22886919',\n",
       "  'Answer',\n",
       "  \"Numpy - correlation coefficient and related statistical functions don't give same results\",\n",
       "  'This is because, `np.var` default delta degrees of freedom is `0`, not `1`.\\n\\n\\n\\n\\tIn [57]:\\n\\n\\t\\n\\n\\tX = [0,0,1,1,0]\\n\\n\\tY = [1,1,0,1,1]\\n\\n\\tnp.corrcoef(X,Y) \\n\\n\\tOut[57]:\\n\\n\\tarray([[ 1.        , -0.61237244],\\n\\n\\t\\t   [-0.61237244,  1.        ]])\\n\\n\\tIn [58]:\\n\\n\\t\\n\\n\\tV = np.sqrt(np.array([np.var(X, ddof=1), np.var(Y, ddof=1)])).reshape(1,-1)\\n\\n\\tnp.matrix(np.cov(X,Y))\\n\\n\\tOut[58]:\\n\\n\\tmatrix([[ 0.3 , -0.15],\\n\\n\\t\\t\\t[-0.15,  0.2 ]])\\n\\n\\tIn [59]:\\n\\n\\t\\n\\n\\tnp.matrix(np.cov(X,Y))/(V*V.T)\\n\\n\\tOut[59]:\\n\\n\\tmatrix([[ 1.        , -0.61237244],\\n\\n\\t\\t\\t[-0.61237244,  1.        ]])\\n\\n\\n\\nOr looks it the otherway:\\n\\n\\n\\n\\n\\n\\tIn [70]:\\n\\n\\t\\n\\n\\tV=np.diag(np.cov(X,Y)).reshape(1,-1) #the diagonal elements\\n\\n\\tIn [71]:\\n\\n\\t\\n\\n\\tnp.matrix(np.cov(X,Y))/np.sqrt(V*V.T)\\n\\n\\tOut[71]:\\n\\n\\tmatrix([[ 1.        , -0.61237244],\\n\\n\\t\\t\\t[-0.61237244,  1.        ]])\\n\\n\\n\\nWhat is really going on, `np.cov(m, y=None, rowvar=1, bias=0, ddof=None)`, when `bias` and `ddof` both not provided, the default normalization is by `N-1`, N being the number of observation. So, that is equivalent to have delta degrees of freedom of `1`. Unfortunately, the default for `np.var(a, axis=None, dtype=None, out=None, ddof=0, keepdims=False)` has the default delta degrees of freedom of `0`.\\n\\n\\n\\nWhenever unsure, the safest way is to grab the diagonal elements of the covariance matrix rather than calculate `var` separately, to ensure consistent behavior.\\n\\n',\n",
       "  '<python><numpy><statistics><probability><correlation>',\n",
       "  datetime.date(2014, 4, 5),\n",
       "  '2014-04-05 21:40:30',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '12743.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['433',\n",
       "  '22950518',\n",
       "  'Answer',\n",
       "  'Fitting a variable Sinc function in python',\n",
       "  '\\n\\n\\n\\nYou need to make three changes, \\n\\n\\n\\n\\tdef gauss(x, A, mu, sigma):\\n\\n\\t    return A*exp(-1*(x[:]-mu)*(x[:]-mu)/sigma/sigma)\\n\\n\\t\\n\\n\\tdef sincSquare_mod(x, A, mu, sigma):\\n\\n\\t    x=np.array(x)\\n\\n\\t    return A * (np.sin(pi*(x[:]-mu)*sigma) / (pi*(x[:]-mu)*sigma))**2\\n\\n\\n\\n    fitdata = gauss(xpos,*p0)\\n\\n\\n\\n1, See [Documentation][1]\\n\\n\\n\\n2, replace `sin` by the `numpy` version for `array` broadcasting \\n\\n\\n\\n3, straight forward right? :P\\n\\n\\n\\nNote, i think you are looking for `p1, var_matrix = curve_fit(gauss,...` rather than the one in the OP, which appears do not have a solution.\\n\\n\\n\\n\\n\\n  [1]: http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.optimize.curve_fit.html',\n",
       "  '<python><curve-fitting>',\n",
       "  datetime.date(2014, 4, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1385.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['435',\n",
       "  '22517442',\n",
       "  'Answer',\n",
       "  'Numpy summarize one array by values of another',\n",
       "  \"Since `x` is `float`. I would do this:\\n\\n\\n\\n\\tIn [136]:\\n\\n\\t\\n\\n\\tnp.array([(x[y==0]==np.unique(x)[..., np.newaxis]).sum(axis=1),\\n\\n\\t\\t      (x[y==1]==np.unique(x)[..., np.newaxis]).sum(axis=1)]).T\\n\\n\\tOut[136]:\\n\\n\\tarray([[2, 1],\\n\\n\\t\\t   [2, 1],\\n\\n\\t\\t   [0, 1],\\n\\n\\t\\t   [1, 1],\\n\\n\\t\\t   [0, 1]])\\n\\n\\n\\nSpeed:\\n\\n\\n\\n\\tIn [152]:\\n\\n\\t\\n\\n\\t%%timeit\\n\\n\\tux=np.unique(x)[..., np.newaxis]\\n\\n\\tnp.array([(x[y==0]==ux).sum(axis=1),\\n\\n\\t          (x[y==1]==ux).sum(axis=1)]).T\\n\\n\\t10000 loops, best of 3: 92.7 µs per loop\\n\\n\\n\\nSolution @seikichi\\n\\n\\n\\n\\tIn [151]:\\n\\n\\t\\n\\n\\t%%timeit\\n\\n\\t>>> x = np.array([1.1, 1.1, 1.1, 3.3, 2.2, 2.2, 2.2, 5.5, 4.4, 4.4])\\n\\n\\t>>> y = np.array([0, 1, 0, 1, 0, 1, 0, 1, 0, 1])\\n\\n\\t>>> r = np.r_[np.unique(x), np.inf]\\n\\n\\t>>> np.concatenate([[np.histogram(x[y == v], r)[0]] for v in sorted(set(y))]).T\\n\\n\\t1000 loops, best of 3: 388 µs per loop\\n\\n\\n\\nFor more general cases when `y` is not just `{0,1}`, as @askewchan pointed out:\\n\\n\\n\\n\\tIn [155]:\\n\\n\\t\\n\\n\\t%%timeit\\n\\n\\tux=np.unique(x)[..., np.newaxis]\\n\\n\\tuy=np.unique(y)\\n\\n\\tnp.asanyarray([(x[y==v]==ux).sum(axis=1) for v in uy]).T\\n\\n\\t10000 loops, best of 3: 116 µs per loop\\n\\n\\n\\nTo explain the broadcasting further, see this example:\\n\\n\\n\\n\\tIn [5]:\\n\\n\\t\\n\\n\\tnp.unique(a)\\n\\n\\tOut[5]:\\n\\n\\tarray([ 0. ,  0.2,  0.4,  0.5,  0.6,  1.1,  1.5,  1.6,  1.7,  2. ])\\n\\n\\tIn [8]:\\n\\n\\t\\n\\n\\tnp.unique(a)[...,np.newaxis] #what [..., np.newaxis] will do:\\n\\n\\tOut[8]:\\n\\n\\tarray([[ 0. ],\\n\\n\\t       [ 0.2],\\n\\n\\t       [ 0.4],\\n\\n\\t       [ 0.5],\\n\\n\\t       [ 0.6],\\n\\n\\t       [ 1.1],\\n\\n\\t       [ 1.5],\\n\\n\\t       [ 1.6],\\n\\n\\t       [ 1.7],\\n\\n\\t       [ 2. ]])\\n\\n\\tIn [10]:\\n\\n\\t\\n\\n\\t(a==np.unique(a)[...,np.newaxis]).astype('int') #then we can boardcast (converted to int for readability)\\n\\n\\tOut[10]:\\n\\n\\tarray([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\\n\\n\\t       [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\\n\\n\\t       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\\n\\n\\t       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\\n\\n\\t       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\n\\n\\t       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\n\\n\\t       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\\n\\n\\t       [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\\n\\n\\t       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\\n\\n\\t       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0]])\\n\\n\\tIn [11]:\\n\\n\\t\\n\\n\\t(a==np.unique(a)[...,np.newaxis]).sum(axis=1) #getting the count of unique value becomes summing among the 2nd axis\\n\\n\\tOut[11]:\\n\\n\\tarray([1, 3, 1, 1, 2, 1, 1, 1, 1, 3])\",\n",
       "  '<python><numpy>',\n",
       "  datetime.date(2014, 3, 19),\n",
       "  '2014-03-20 03:29:29',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1134.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['436',\n",
       "  '22725574',\n",
       "  'Answer',\n",
       "  'Sort data frame by character and date columns in R',\n",
       "  \"`as.numeric` would be more understandable:\\n\\n\\n\\n    DF <- DF[order(DF$fx_code, - as.numeric(DF$date)), ] \\n\\n\\n\\nUnder the hood, this is what `xtfrm` actually does:\\n\\n\\n\\n\\t> xtfrm(as.Date('2000-04-29', format='%Y-%m-%d'))\\n\\n\\t[1] 11076\\n\\n\\t> as.numeric(as.Date('2000-04-29', format='%Y-%m-%d'))\\n\\n\\t[1] 11076\\n\\n\",\n",
       "  '<r><sorting><dataframe><order><dataset>',\n",
       "  datetime.date(2014, 3, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '3793.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['437',\n",
       "  '23165751',\n",
       "  'Answer',\n",
       "  'Read CSV with multiple Headers',\n",
       "  \"That data is surely not in a friendly shape, the following solution should work even if you have more than one rows of data in each section:\\n\\n\\n\\n\\tIn [67]:\\n\\n\\t\\n\\n\\t%%file temp.csv\\n\\n\\th1 h2 h3\\n\\n\\t11 12 13\\n\\n\\t10 10 10\\n\\n\\t \\n\\n\\th4 h5 h6\\n\\n\\t14 15 16\\n\\n\\t10 10 10\\n\\n\\tOverwriting temp.csv\\n\\n\\tIn [68]:\\n\\n\\t\\n\\n\\tdf=pd.read_csv('temp.csv', sep=' ', header=None)\\n\\n\\tdf=df.dropna()\\n\\n\\tdf.index=df[0].map(lambda x: not x.isdigit()).cumsum()\\n\\n\\tgp=df.groupby(df.index)\\n\\n\\tdf2=np.hstack([gp.get_group(i) for i in gp.groups])\\n\\n\\tIn [69]:\\n\\n\\t\\n\\n\\tprint pd.DataFrame(df2[1:].astype(float),columns=df2[0])\\n\\n\\t   h1  h2  h3  h4  h5  h6\\n\\n\\t0  11  12  13  14  15  16\\n\\n\\t1  10  10  10  10  10  10\\n\\n\\t\\n\\n\\t[2 rows x 6 columns]\\n\\n\\n\\nAnyone has better ideas, especially a solution of smaller memory footprint? Here I constructed a new `numpy` `array` `df2`, which certainly means more RAM usage.\",\n",
       "  '<csv><pandas>',\n",
       "  datetime.date(2014, 4, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2860.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['440',\n",
       "  '23750861',\n",
       "  'Answer',\n",
       "  'dealing with data labels in 96 well plate with python',\n",
       "  \"I will suggest just make a `DataFrame` with two columns, one stores the names, the other stores the readings.\\n\\n\\n\\n\\tIn [20]:\\n\\n\\t\\n\\n\\tprint data_df\\n\\n\\tprint name_df\\n\\n\\t     1    2    3    4\\n\\n\\tA  9.1  8.7  5.6  4.5\\n\\n\\tB  8.7  8.5  5.4  4.3\\n\\n\\tC  4.3  4.5  7.6  6.7\\n\\n\\tD  4.1  6.0  7.0  6.1\\n\\n\\t\\n\\n\\t[4 rows x 4 columns]\\n\\n\\t     1    2    3    4\\n\\n\\tA   l1   l2   l3   l4\\n\\n\\tB   l1   l2   l3   l4\\n\\n\\tC  ds1  ds2  ds3  ds4\\n\\n\\tD  ds1  ds2  ds3  ds4\\n\\n\\t\\n\\n\\t[4 rows x 4 columns]\\n\\n\\tIn [21]:\\n\\n\\t\\n\\n\\tfinal_df=pd.DataFrame({'Name':name_df.values.ravel(), 'Reading':data_df.values.ravel()})\\n\\n    #if you have additional readings, i.e. from a different assay,\\n\\n    #from a different wavelength, add them there, as:\\n\\n    #'OTHER_Reading':OTHER_data_df.values.ravel()\\n\\n\\tprint final_df\\n\\n\\t   Name  Reading\\n\\n\\t0    l1      9.1\\n\\n\\t1    l2      8.7\\n\\n\\t2    l3      5.6\\n\\n\\t3    l4      4.5\\n\\n\\t4    l1      8.7\\n\\n\\t5    l2      8.5\\n\\n\\t6    l3      5.4\\n\\n\\t7    l4      4.3\\n\\n\\t8   ds1      4.3\\n\\n\\t9   ds2      4.5\\n\\n\\t10  ds3      7.6\\n\\n\\t11  ds4      6.7\\n\\n\\t12  ds1      4.1\\n\\n\\t13  ds2      6.0\\n\\n\\t14  ds3      7.0\\n\\n\\t15  ds4      6.1\\n\\n\\t\\n\\n\\t[16 rows x 2 columns]\\n\\n\\n\\nThis way you can do some calculations rather easily, such as:\\n\\n\\n\\n\\tIn [22]:\\n\\n\\t\\n\\n\\tprint final_df.groupby('Name').mean()\\n\\n\\t      Reading\\n\\n\\tName         \\n\\n\\tds1      4.20\\n\\n\\tds2      5.25\\n\\n\\tds3      7.30\\n\\n\\tds4      6.40\\n\\n\\tl1       8.90\\n\\n\\tl2       8.60\\n\\n\\tl3       5.50\\n\\n\\tl4       4.40\\n\\n\\t\\n\\n\\t[8 rows x 1 columns]\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 5, 20),\n",
       "  '2014-05-20 04:26:20',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '6977.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['441',\n",
       "  '23957413',\n",
       "  'Answer',\n",
       "  'Linear fit including all errors with NumPy/SciPy',\n",
       "  \"Please note that, from the documentation of `curvefit`:\\n\\n\\n\\n> sigma : None or N-length sequence\\n\\n    If not None, this vector will be used as relative weights in the\\n\\n    least-squares problem.\\n\\n\\n\\nThe key point here is **as relative weights**, therefore, `yerr` in line 53 and `2*yerr` in 57 should give you similar, if not the same result.\\n\\n\\n\\nWhen you increase the **actually** residue error, you will see the values in the covariance matrix grow large. Say if we change the `y += random` to `y += 5*random` in function `generate_data()`:\\n\\n\\n\\n    Fit with scipy.optimize.curve_fit:\\n\\n    ('Parameters:', array([ 1.92810458,  3.97843448]))\\n\\n    ('Errors:    ', array([ 0.09617346,  0.64127574]))\\n\\n\\n\\nCompares to the original result:\\n\\n\\n\\n    Fit with scipy.optimize.curve_fit:\\n\\n    ('Parameters:', array([ 2.00760386,  2.97817514]))\\n\\n    ('Errors:    ', array([ 0.00782591,  0.02983339]))\\n\\n\\n\\nAlso notice that the parameter estimate is now further off from `(2,3)`, as we would expect from increased residue error and larger confidence interval of parameter estimates.\",\n",
       "  '<python><numpy><scipy>',\n",
       "  datetime.date(2014, 5, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2785.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['446',\n",
       "  '22734412',\n",
       "  'Answer',\n",
       "  'In R data.table multiplication by column name based on values of another column',\n",
       "  \"In this solution, you need to specify the number of different currency (in this case `2`) and the number of observations (in this case `10`), and it is also assumed that the currency values (`'aud','eur'` etc) are last a few columns.\\n\\n\\n\\n\\t> B_msk <- matrix(rep(DT$currency,2), ncol=2, byrow=TRUE)==matrix(rep(colnames(DT)[-(1:3)], 10), ncol=2)\\n\\n\\t> DF <- data.frame(DT)\\n\\n\\t> DF$in_USD <- rowSums(DF[colnames(DT)[-(1:3)]]*B_msk*DF$price)\\n\\n\\t> DF #or data.table(DF)\\n\\n\\t   day      price currency      aud      eur    in_USD\\n\\n\\t1    1 0.30776611      aud 1.624996 2.035811 0.5001188\\n\\n\\t2    2 0.25767250      eur 1.882166 2.210804 0.5696634\\n\\n\\t3    3 0.55232243      aud 1.280354 2.038349 0.7071681\\n\\n\\t4    4 0.05638315      eur 1.398488 2.248972 0.1268041\\n\\n\\t5    5 0.46854928      aud 1.762551 1.920101 0.8258420\\n\\n\\t6    6 0.48377074      eur 1.669022 1.671420 0.8085842\\n\\n\\t7    7 0.81240262      aud 1.204612 2.270302 0.9786301\\n\\n\\t8    8 0.37032054      eur 1.357525 2.381954 0.8820863\\n\\n\\t9    9 0.54655860      aud 1.359475 2.049097 0.7430328\\n\\n\\t10  10 0.17026205      eur 1.690291 1.777724 0.3026789\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n#Edit:\\n\\n\\n\\nHope this solution solves memory issue, (but still need to have the data in a `data.frame`)\\n\\n\\n\\n\\t> Idx=cbind(1:10,match(DT[,currency], colnames(DT))) #replace 10 with the actually np. of obs.\\n\\n\\t> DF=data.frame(DT)\\n\\n\\t> DF\\n\\n\\t   day      price currency      aud      eur\\n\\n\\t1    1 0.30776611      aud 1.624996 2.035811\\n\\n\\t2    2 0.25767250      eur 1.882166 2.210804\\n\\n\\t3    3 0.55232243      aud 1.280354 2.038349\\n\\n\\t4    4 0.05638315      eur 1.398488 2.248972\\n\\n\\t5    5 0.46854928      aud 1.762551 1.920101\\n\\n\\t6    6 0.48377074      eur 1.669022 1.671420\\n\\n\\t7    7 0.81240262      aud 1.204612 2.270302\\n\\n\\t8    8 0.37032054      eur 1.357525 2.381954\\n\\n\\t9    9 0.54655860      aud 1.359475 2.049097\\n\\n\\t10  10 0.17026205      eur 1.690291 1.777724\\n\\n\\t> DF$price*as.numeric(DF[Idx]) #assign it as 'DF$P_in_USD'\\n\\n\\t [1] 0.5001187 0.5696634 0.7071682 0.1268041 0.8258420 0.8085841 0.9786299 0.8820865 0.7430327 0.3026789\",\n",
       "  '<r><data.table>',\n",
       "  datetime.date(2014, 3, 29),\n",
       "  '2014-03-30 00:18:08',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1694.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['447',\n",
       "  '22798832',\n",
       "  'Answer',\n",
       "  'Group labels in matplotlib barchart using Pandas MultiIndex',\n",
       "  \"If you have just two levels in the `MultiIndex`, I believe the following will be easier:\\n\\n\\n\\n\\tplt.figure()\\n\\n\\tax = plt.gca()\\n\\n\\tDF.plot(kind='bar', ax=ax)\\n\\n\\tplt.grid(True, 'both')\\n\\n\\tminor_XT = ax.get_xaxis().get_majorticklocs()\\n\\n\\tDF['XT_V'] = minor_XT\\n\\n\\tmajor_XT = DF.groupby(by=DF.index.get_level_values(0)).first()['XT_V'].tolist()\\n\\n\\tDF.__delitem__('XT_V')\\n\\n\\tax.set_xticks(minor_XT, minor=True)\\n\\n\\tax.set_xticklabels(DF.index.get_level_values(1), minor=True)\\n\\n\\tax.tick_params(which='major', pad=15)\\n\\n\\t_ = plt.xticks(major_XT, (DF.index.get_level_values(0)).unique(), rotation=0)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nAnd a bit of involving, but more general solution (doesn't matter how many levels you have):\\n\\n\\n\\n    def cvt_MIdx_tcklab(df):\\n\\n        Midx_ar = np.array(df.index.tolist())\\n\\n        Blank_ar = Midx_ar.copy()\\n\\n        col_idx = np.arange(Midx_ar.shape[0])\\n\\n        for i in range(Midx_ar.shape[1]):\\n\\n            val,idx = np.unique(Midx_ar[:, i], return_index=True)\\n\\n            Blank_ar[idx, i] = val\\n\\n            idx=~np.in1d(col_idx, idx)\\n\\n            Blank_ar[idx, i]=''\\n\\n        return map('\\\\n'.join, np.fliplr(Blank_ar))\\n\\n\\n\\n    plt.figure()\\n\\n    ax = plt.gca()\\n\\n    DF.plot(kind='bar', ax=ax)\\n\\n    ax.set_xticklabels(cvt_MIdx_tcklab(DF), rotation=0)\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Xhc39.png\",\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2014, 4, 1),\n",
       "  '2018-03-18 16:26:45',\n",
       "  'CT Zhu (2487184), Max Ghenis (1840471)',\n",
       "  '5',\n",
       "  '',\n",
       "  '2063.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['450',\n",
       "  '25144059',\n",
       "  'Answer',\n",
       "  'Pandas : compute mean or std (standard deviation) over entire dataframe',\n",
       "  'You could convert the dataframe to be a single column with `stack` (this changes the shape from 5x3 to 15x1) and then take the standard deviation:\\n\\n\\n\\n    df.stack().std()         # pandas default degrees of freedom is one\\n\\n                          \\n\\nAlternatively, you can use `values` to convert from a pandas dataframe to a numpy array before taking the standard deviation:\\n\\n  \\n\\n    df.values.std(ddof=1)    # numpy default degrees of freedom is zero\\n\\n\\n\\nNote that (unlike pandas) numpy will give the standard deviation of the entire array by default, so there is no need to reshape before taking the standard deviation.\\n\\n\\n\\n',\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2014, 8, 5),\n",
       "  '2017-12-29 01:24:42',\n",
       "  'JohnE (3877338)',\n",
       "  '31',\n",
       "  '',\n",
       "  '45249.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['451',\n",
       "  '23210014',\n",
       "  'Answer',\n",
       "  'Accessing nested JSON data as dataframes in Pandas',\n",
       "  'Not sure how your multiple observations are organized in `json`. But it is clear that what is causing problem is you are having a nested structure for the `\"profilePicture\"` field. Therefore each observation is expressed as a nested dictionary. You need to convert each observation to a `dataframe` and `concat` them into the final `dataframe` as in this solution.\\n\\n\\n\\n\\tIn [3]:\\n\\n\\tprint df\\n\\n\\t                                             results\\n\\n\\t0  {u\\'linkedinAccount\\': u\\'\\', u\\'username\\': u\\'abc@g...\\n\\n\\t1  {u\\'linkedinAccount\\': u\\'\\', u\\'username\\': u\\'abc@g...\\n\\n\\t\\n\\n\\t[2 rows x 1 columns]\\n\\n\\tIn [4]:\\t\\n\\n\\tprint pd.concat([pd.DataFrame.from_dict(item, orient=\\'index\\').T for item in df.results])\\n\\n\\n\\n\\n\\n\\t  linkedinAccount       username registrationGate firstName title lastName  \\\\\\n\\n\\t0                  abc@gmail.com           normal       abc    AA      xyz   \\n\\n\\t0                  abc@gmail.com           normal       abc    AA      xyz   \\n\\n\\t\\n\\n\\t  company telephone                                     profilePicture  \\\\\\n\\n\\t0     XYZ            {u\\'url\\': u\\'url.url.com\\', u\\'__type\\': u\\'File\\', u...   \\n\\n\\t0     ABC            {u\\'url\\': u\\'url.url.com\\', u\\'__type\\': u\\'File\\', u...   \\n\\n\\t\\n\\n\\t  location                 updatedAt          email                 createdAt  \\\\\\n\\n\\t0           2014-03-27T23:24:20.220Z  abc@gmail.com  2014-03-27T23:21:48.758Z   \\n\\n\\t0           2014-03-27T23:24:20.220Z  abc@gmail.com  2014-03-27T23:21:48.758Z   \\n\\n\\t\\n\\n\\t  zipcode  \\n\\n\\t0   00000  \\n\\n\\t0   00000  \\n\\n\\t\\n\\n\\t[2 rows x 14 columns]\\n\\n\\n\\nThen you may want to think about how to deal the the `profilePicture` column. You can do what @U2EF1 suggested in the link. But I would probably just break that column into three columns `pfPIC_url`, `pfPIC_type`, `pfPIC_name`',\n",
       "  '<python><json><pandas>',\n",
       "  datetime.date(2014, 4, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '3924.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['453',\n",
       "  '23983834',\n",
       "  'Answer',\n",
       "  'how to set bounds for the x-axis in one figure containing multiple matplotlib histograms and create just one column of graphs?',\n",
       "  'There are two subplots, and you can access each of them and modify them seperately:\\n\\n\\n\\n    ax_list=df.hist()\\n\\n    ax_list[0][0].set_xlim((0,1))\\n\\n    ax_list[0][1].set_xlim((0.01, 0.07))\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\nWhat you are doing, by `plt.xlim`, changes the limit of the current working axis only. In this case, it is the second plot which is the most recently generated.\\n\\n\\n\\n\\n\\n----------\\n\\n#Edit:\\n\\n\\n\\nTo make the plots into 2 rows 1 column, use `layout` argument. To make the bin edges aligns, use `bins` argument. Set the x limit to `(-1, 1)` is probably not a good idea, you numbers are all smallish.\\n\\n\\n\\n    ax_list=df.hist(layout=(2,1),bins=np.histogram(df.values.ravel())[1])\\n\\n    ax_list[0][0].set_xlim((0.01, 0.07))\\n\\n    ax_list[1][0].set_xlim((0.01, 0.07))\\n\\n\\n\\n![enter image description here][3]\\n\\n\\n\\nOr specify exactly 10 bins between (-1,1):\\n\\n\\n\\n    ax_list=df.hist(layout=(2,1),bins=np.linspace(-1,1,10))\\n\\n    ax_list[0][0].set_xlim((-1,1))\\n\\n    ax_list[1][0].set_xlim((-1,1))\\n\\n\\n\\n![enter image description here][4]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/BD8ul.png\\n\\n  [2]: http://i.stack.imgur.com/BD8ul.png\\n\\n  [3]: http://i.stack.imgur.com/5v0k5.png\\n\\n  [4]: http://i.stack.imgur.com/IuQ4Q.png',\n",
       "  '<python><matplotlib><pandas><histogram>',\n",
       "  datetime.date(2014, 6, 1),\n",
       "  '2014-06-03 05:28:21',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '13779.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['454',\n",
       "  '24086439',\n",
       "  'Answer',\n",
       "  'Matplotlib Line Overlap/Resolution',\n",
       "  \"It has to do with the cap style of the `Line2D` object, the default style is 'projecting', which result in overlapping, see a zoom-in-ed PDF:\\n\\n![enter image description here][1]\\n\\n \\n\\nWe want to change it to 'butt' style:\\n\\n\\n\\n    L1=ax.plot((0, 10000000), (3, 3), linewidth = 2, markersize = 0, clip_on = True, aa = True)\\n\\n    L2=ax.plot((10000001, 200000001), (3, 3), linewidth = 1, markersize = 0, clip_on = True, aa = True)\\n\\n    for item in L1+L2:\\n\\n        item.set_solid_capstyle('butt')\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\nThe gap is very small, sure, since it is 1/10000000.\\n\\n\\n\\nOr if you want, a quick dirty solution is just to draw a small white circle marker of size=1 at `(10000000.5, 3)`.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/OEc8c.png\\n\\n  [2]: http://i.stack.imgur.com/1RPVj.png\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 6, 6),\n",
       "  '2014-06-06 16:23:28',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1294.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['457',\n",
       "  '22735332',\n",
       "  'Answer',\n",
       "  'How can I import R dataframes into Pandas?',\n",
       "  \"First: `array([(2.0, 1, 1), (3.0, 2, 0), (5.0, 3, 1)], dtype=[('a', '<f8'), ('b', '<i4'), ('c', '<i4')])`. That is a `numpy` structured `array`. http://docs.scipy.org/doc/numpy/user/basics.rec.html/. You can easily convert it to `pandas` DF by using `pd.DataFrame`:\\n\\n\\n\\n\\tIn [65]:\\n\\n\\t\\n\\n\\tfrom numpy import *\\n\\n\\tprint pd.DataFrame(array([(2.0, 1, 1), (3.0, 2, 0), (5.0, 3, 1)], dtype=[('a', '<f8'), ('b', '<i4'), ('c', '<i4')]))\\n\\n\\t   a  b  c\\n\\n\\t0  2  1  1\\n\\n\\t1  3  2  0\\n\\n\\t2  5  3  1\\n\\n\\n\\n`b` column is coded (as if `factor()`'ed in `R`), `c` column was converted from `boolean` to `int`. `a` was converted from `int` to `float` (`'<f8'`, actually I found that unexpected)\\n\\n\\n\\n2nd, I think `pandas.rpy.common` is the most convenient way of fetching data from `R`: http://pandas.pydata.org/pandas-docs/stable/r_interface.html (It is probably too brief, so I will add another example here):\\n\\n\\n\\n\\tIn [71]:\\n\\n\\t\\n\\n\\timport pandas.rpy.common as com\\n\\n\\tDF=pd.DataFrame({'val':[1,1,1,2,2,3,3]})\\n\\n\\tr_DF = com.convert_to_r_dataframe(DF)\\n\\n\\tprint pd.DataFrame(com.convert_robj(r_DF))\\n\\n\\t   val\\n\\n\\t0    1\\n\\n\\t1    1\\n\\n\\t2    1\\n\\n\\t3    2\\n\\n\\t4    2\\n\\n\\t5    3\\n\\n\\t6    3\\n\\n\\n\\nFinally, the `Unnamed: 0` column is the index column. You can avoid it by providing `index_col=0` to `pd.read_csv()`\",\n",
       "  '<python><r><import><pandas><dataframe>',\n",
       "  datetime.date(2014, 3, 29),\n",
       "  '2014-03-29 19:19:25',\n",
       "  'CT Zhu (2487184)',\n",
       "  '6',\n",
       "  '',\n",
       "  '1199.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['458',\n",
       "  '22799359',\n",
       "  'Answer',\n",
       "  'How to unpack a Series of tuples in Pandas?',\n",
       "  \"I believe you want this:\\n\\n\\n\\n    df=pd.DataFrame(out.tolist())\\n\\n    df.columns=['KS-stat', 'P-value']\\n\\n\\n\\nresult:\\n\\n\\n\\n               KS-stat   P-value\\n\\n    0   -2.12978778869  0.043643\\n\\n    1    3.50655433879  0.001813\\n\\n    2    -1.2221274198  0.233527\\n\\n    3  -0.977154419818  0.338240\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 4, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '12417.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['461',\n",
       "  '23233118',\n",
       "  'Answer',\n",
       "  'Boxplot stratified by column in python pandas',\n",
       "  \"`pandas.qcut` will give you the quantiles, but a histogram-like operation will require some `numpy` trickery which comes in handy here:\\n\\n\\n\\n    _, breaks = np.histogram(df.MAT, bins=5)\\n\\n    ax = df.boxplot(column='N0_YLDF', by='Class')\\n\\n    ax.xaxis.set_ticklabels(['%s'%val for i, val in enumerate(breaks) if i in df.Class])\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nThe dataframe now looks like this:\\n\\n\\n\\n\\t   N0_YLDF    MAT  Class\\n\\n\\t0     1.29  13.67      1\\n\\n\\t1     2.32  10.67      0\\n\\n\\t2     6.24  11.29      1\\n\\n\\t3     5.34  21.29      1\\n\\n\\t4     6.35  41.67      2\\n\\n\\t5     5.35  91.67      5\\n\\n\\t6     9.32  21.52      1\\n\\n\\t7     6.32  31.52      2\\n\\n\\t8     3.33  13.52      1\\n\\n\\t9     4.56  44.52      3\\n\\n\\t\\n\\n\\t[10 rows x 3 columns]\\n\\n\\n\\nIt can also be used to get the quartile plot:\\n\\n\\n\\n    breaks = np.asarray(np.percentile(df.MAT, [25,50,75,100]))\\n\\n    df['Class'] = (df.MAT.values > breaks[..., np.newaxis]).sum(0)\\n\\n    ax = df.boxplot(column='N0_YLDF', by='Class')\\n\\n    ax.xaxis.set_ticklabels(['%s'%val for val in breaks])\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/WEH6I.png\\n\\n  [2]: http://i.stack.imgur.com/pSqvB.png\",\n",
       "  '<python><matplotlib><pandas><boxplot>',\n",
       "  datetime.date(2014, 4, 23),\n",
       "  '2017-01-11 06:04:18',\n",
       "  'CT Zhu (2487184), Archie (5609221)',\n",
       "  '6',\n",
       "  '',\n",
       "  '14099.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['464',\n",
       "  '23418059',\n",
       "  'Answer',\n",
       "  'Label objects not found',\n",
       "  \"You must have plot the candle plots and the volume before plotting the SMA. The candle plot doesn't have any labeled object, when you call the `plt.legend()`, it tries to plot a label for every plot on the current axes. Therefore, you get this `UserWarning: No labeled objects found. Use label='...' kwarg on indivial plots.`\\n\\n\\n\\nTo solve it, I hope it is clear at this point, simply requires you to plot the SMA's very first, before the candle plot, and call the `legend()` right after that before any other plots being generated.\",\n",
       "  '<python><matplotlib><charts>',\n",
       "  datetime.date(2014, 5, 1),\n",
       "  '2014-05-02 15:51:26',\n",
       "  'CT Zhu (2487184)',\n",
       "  '9',\n",
       "  '',\n",
       "  '15170.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['465',\n",
       "  '25208202',\n",
       "  'Answer',\n",
       "  'Function returns a vector, how to minimize in via NumPy',\n",
       "  'If you want you resulting vector to be a vector containing only `0`s, you can use `fsolve` to do so. To do that will require modifying your objective function a little bit to get the input and output into the same shape:\\n\\n\\n\\n    import scipy.optimize as so\\n\\n    P = np.matrix([[0.3, 0.1, 0.2], [0.01, 0.4, 0.2], [0.0001, 0.3, 0.5]])  \\n\\n    Ps = np.array([10,14,5])\\n\\n\\n\\n    def objective(x):   \\n\\n        x = np.array([x])\\n\\n        res = np.square(Ps - np.dot(x, P)) \\n\\n        return np.array(res).ravel() \\n\\n    Root = so.fsolve(objective, x0=np.array([10, 11, 15]))\\n\\n    objective(Root)\\n\\n    #[  5.04870979e-29   1.13595970e-28   1.26217745e-29]\\n\\n\\n\\nResult: The solution is `np.array([ 31.95419775,  41.56815698, -19.40894189])`',\n",
       "  '<python><numpy><scipy><mathematical-optimization>',\n",
       "  datetime.date(2014, 8, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1675.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['466',\n",
       "  '25235377',\n",
       "  'Answer',\n",
       "  'Replace NaN or missing values with rolling mean or other interpolation',\n",
       "  'There are several ways to approach this, and the best way will depend on whether the January data is systematically different from other months.  Most real-world data is likely to be somewhat seasonal, so let\\'s use the average high temperature (Fahrenheit) of a random city in the northern hemisphere as an example.\\n\\n\\n\\n    df=pd.DataFrame({ \\'month\\' : [10,11,12,1,2,3],\\n\\n                      \\'temp\\'  : [65,50,45,np.nan,40,43] }).set_index(\\'month\\')\\n\\n\\n\\nYou could use a rolling mean as you suggest, but the issue is that you will get an average temperature over the entire year, which ignores the fact that January is the coldest month.  To correct for this, you could reduce the window to 3, which results in the January temp being the average of the December and February temps.  (I am also using `min_periods=1` as suggested in @user394430\\'s answer.)\\n\\n\\n\\n    df[\\'rollmean12\\'] = df[\\'temp\\'].rolling(12,center=True,min_periods=1).mean()\\n\\n    df[\\'rollmean3\\']  = df[\\'temp\\'].rolling( 3,center=True,min_periods=1).mean()\\n\\n\\n\\nThose are improvements but still have the problem of overwriting existing values with rolling means.  To avoid this you could combine with the `update()` method ([see documentation here][1]).\\n\\n\\n\\n    df[\\'update\\'] = df[\\'rollmean3\\']\\n\\n    df[\\'update\\'].update( df[\\'temp\\'] )  # note: this is an inplace operation\\n\\n\\n\\nThere are even simpler approaches that leave the existing values alone while filling the missing January temps with either the previous month, next month, or the mean of the previous and next month.\\n\\n\\n\\n    df[\\'ffill\\']   = df[\\'temp\\'].ffill()         # previous month \\n\\n    df[\\'bfill\\']   = df[\\'temp\\'].bfill()         # next month\\n\\n    df[\\'interp\\']  = df[\\'temp\\'].interpolate()   # mean of prev/next\\n\\n\\n\\nIn this case, `interpolate()` defaults to simple linear interpretation, but you have several other intepolation options also.  See [documentation on pandas interpolate][2] for more info.  Or this statck overflow question: \\n\\n https://stackoverflow.com/questions/10464738/interpolation-on-dataframe-in-pandas\\n\\n\\n\\nHere is the sample data with all the results:\\n\\n\\n\\n           temp  rollmean12  rollmean3  update  ffill  bfill  interp\\n\\n    month                                                           \\n\\n    10     65.0        48.6  57.500000    65.0   65.0   65.0    65.0\\n\\n    11     50.0        48.6  53.333333    50.0   50.0   50.0    50.0\\n\\n    12     45.0        48.6  47.500000    45.0   45.0   45.0    45.0\\n\\n    1       NaN        48.6  42.500000    42.5   45.0   40.0    42.5\\n\\n    2      40.0        48.6  41.500000    40.0   40.0   40.0    40.0\\n\\n    3      43.0        48.6  41.500000    43.0   43.0   43.0    43.0\\n\\n\\n\\nIn particular, note that \"update\" and \"interp\" give the same results in all months.  While it doesn\\'t matter which one you use here, in other cases one way or the other might be better.\\n\\n\\n\\n\\n\\n  [1]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.update.html?highlight=update\\n\\n  [2]: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.interpolate.html\\n\\n',\n",
       "  '<python><pandas><missing-data><moving-average>',\n",
       "  datetime.date(2014, 8, 11),\n",
       "  '2017-08-06 18:50:51',\n",
       "  'JohnE (3877338)',\n",
       "  '7',\n",
       "  '',\n",
       "  '9310.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['467',\n",
       "  '23320333',\n",
       "  'Answer',\n",
       "  'Fastest way to find nearest triangle number?',\n",
       "  'A little bit of math will bring a more analytical solution:\\n\\n\\n\\n    from math import sqrt\\n\\n    def close_triangle2(n):\\n\\n        m=int(0.5*(-1+sqrt(1+8*n))) #solve it for the explicit formula\\n\\n        tan0=(m*m+m)/2              #the closest one is either this\\n\\n        tan1=(m*m+3*m+2)/2          #or this\\n\\n        if (n-tan0)>(tan1-n):\\n\\n            return tan1-n\\n\\n        else:\\n\\n            return n-tan0\\n\\n\\n\\nIt will become slightly faster than the loop version from @perreal when the number is large:\\n\\n\\n\\n\\tIn [87]:\\n\\n\\t\\n\\n\\t%timeit close_triangle(111111111)\\n\\n\\t100000 loops, best of 3: 5 µs per loop\\n\\n\\tIn [86]:\\n\\n\\t\\n\\n\\t%timeit close_triangle2(111111111)\\n\\n\\t100000 loops, best of 3: 4.13 µs per loop\\n\\n\\n\\nIf you wonders:\\n\\n\\n\\n\\tIn [94]:\\n\\n\\t\\n\\n\\tclose_triangle2((30**10 * (30**10+1)) / 2 + 100)\\n\\n\\tOut[94]:\\n\\n\\t100L\\n\\n\\tIn [95]:\\n\\n\\t\\n\\n\\tclose_triangle((30**10 * (30**10+1)) / 2 + 100)\\n\\n\\tOut[95]:\\n\\n\\t100L\\n\\n\\n\\n\\tIn [102]:\\n\\n\\t\\n\\n\\t%timeit close_triangle((30**10 * (30**10+1)) / 2 + 100)\\n\\n\\t10000 loops, best of 3: 17.9 µs per loop\\n\\n\\tIn [103]:\\n\\n\\t\\n\\n\\t%timeit close_triangle2((30**10 * (30**10+1)) / 2 + 100)\\n\\n\\t100000 loops, best of 3: 12 µs per loop',\n",
       "  '<python>',\n",
       "  datetime.date(2014, 4, 27),\n",
       "  '2014-04-27 06:57:24',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1210.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['470',\n",
       "  '23411147',\n",
       "  'Answer',\n",
       "  'Creating contour plots without using numpy.meshgrid method?',\n",
       "  'I think you just need to reshape the `mu` by `mu=np.array(mu).reshape(100,100)`, and plot it by `plt.contourf(n_range,mass_range,mu.T)`\\n\\n\\n\\n    mu=np.array(mu).reshape(100,100)\\n\\n    plt.contourf(n_range,mass_range,mu.T)\\n\\n    plt.colorbar()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/WQzSM.png',\n",
       "  '<python><numpy><matplotlib>',\n",
       "  datetime.date(2014, 5, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1380.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['475',\n",
       "  '23617505',\n",
       "  'Answer',\n",
       "  'Matplotlib: How to make two histograms have the same bin width?',\n",
       "  'I think a consistent way that will easily work for most cases, without having to worry about what is the distribution range for each of your datasets, will be to put the datasets together into a big one, determine the bins edges and then plot:\\n\\n\\n\\n    a=np.random.random(100)*0.5 #a uniform distribution\\n\\n    b=1-np.random.normal(size=100)*0.1 #a normal distribution \\n\\n    bins=np.histogram(np.hstack((a,b)), bins=40)[1] #get the bin edges\\n\\n    plt.hist(a, bins)\\n\\n    plt.hist(b, bins)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/rsPG0.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 5, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '23',\n",
       "  '',\n",
       "  '7608.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['480',\n",
       "  '23002214',\n",
       "  'Answer',\n",
       "  'Can matplotlib errorbars have a linestyle set?',\n",
       "  \"It is trivial, changing the linestyle of the errorbars only require a simple `.set_linestyle` call:\\n\\n\\n\\n    eb1=plt.errorbar(x, y, yerr=yerr, lw=2, errorevery=2, ls='-.')\\n\\n    eb1[-1][0].set_linestyle('--') #eb1[-1][0] is the LineCollection objects of the errorbar lines\\n\\n    eb2=plt.errorbar(x, y2, yerr=yerr2, lw=2, errorevery=3)\\n\\n    eb2[-1][0].set_linestyle('-.')\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/ufxXW.png\",\n",
       "  '<python><matplotlib><plot>',\n",
       "  datetime.date(2014, 4, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '20',\n",
       "  '',\n",
       "  '8998.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['485',\n",
       "  '24107563',\n",
       "  'Answer',\n",
       "  'Python Pandas calucate Z score of groupby means',\n",
       "  \"There is a `zscore` functionality in `scipy`, but be careful the default delta-degree-of-freedom is 0 in `scipy.stats.zscore`:\\n\\n\\n\\n\\tIn [171]:\\n\\n\\timport scipy.stats as ss\\n\\n\\tS=(df[df.Year == '2010'].groupby(['Year', 'Name'])['Score'].mean())\\n\\n\\tpd.Series(ss.zscore(s, ddof=1), S.index)\\n\\n\\tOut[171]:\\n\\n\\tYear  Name\\n\\n\\t2010  Bill   -0.714286\\n\\n\\t      Bob     1.142857\\n\\n\\t      Joe    -0.428571\\n\\n\\tdtype: float64\",\n",
       "  '<python-2.7><pandas><group-by>',\n",
       "  datetime.date(2014, 6, 8),\n",
       "  '2014-06-08 15:32:11',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '2021.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['488',\n",
       "  '24225187',\n",
       "  'Answer',\n",
       "  'Repeating elements of a list n times',\n",
       "  'The ideal way is probably `numpy.repeat`:\\n\\n\\n\\n\\tIn [16]:\\n\\n\\t\\n\\n\\tx1=[1,2,3,4]\\n\\n\\tIn [17]:\\n\\n\\t\\n\\n\\tnp.repeat(x1,3)\\n\\n\\tOut[17]:\\n\\n\\tarray([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4])',\n",
       "  '<python>',\n",
       "  datetime.date(2014, 6, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '61',\n",
       "  '',\n",
       "  '36215.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['490',\n",
       "  '22919495',\n",
       "  'Answer',\n",
       "  'Fitting a Gaussian to a set of x,y data',\n",
       "  \"Looks like your data skews heavily to the left, why Gaussian? Not Boltzmann, Log-Normal, or anything else?\\n\\n\\n\\nMuch of these are already implemented in `scipy.stats`. See `scipy.stats.cauchy` for lorentzian and `scipy.stats.normal` gaussian. An example:\\n\\n\\n\\n    import scipy.stats as ss\\n\\n    A=ss.norm.rvs(0, 5, size=(100)) #Generate a random variable of 100 elements, with expected mean=0, std=5\\n\\n    ss.norm.fit_loc_scale(A) #fit both the mean and std\\n\\n    (-0.13053732553697531, 5.163322485150271) #your number will vary.\\n\\n\\n\\nAnd I think you don't need the `intensity0` parameter, it is just going to be `1/sigma/srqt(2*pi)`, because the density function has to sum up to 1.\",\n",
       "  '<python><numpy><matplotlib><curve>',\n",
       "  datetime.date(2014, 4, 7),\n",
       "  '2016-04-18 02:54:41',\n",
       "  'CT Zhu (2487184), Gabriel (1391441)',\n",
       "  '0',\n",
       "  '',\n",
       "  '3696.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['491',\n",
       "  '23024866',\n",
       "  'Answer',\n",
       "  'In-Place Update of Values in Pandas Dataframe',\n",
       "  \"You mean like this?\\n\\n\\n\\n\\tIn [16]:\\n\\n\\t\\n\\n\\timport operator\\n\\n\\tIn [17]:\\n\\n\\t\\n\\n\\tDF=pd.DataFrame({'Val':[[2013, 37722.322],[1998, 32323.232]]})\\n\\n\\tIn [18]:\\n\\n\\t\\n\\n\\tprint DF\\n\\n\\t                 Val\\n\\n\\t0  [2013, 37722.322]\\n\\n\\t1  [1998, 32323.232]\\n\\n\\t\\n\\n\\t[2 rows x 1 columns]\\n\\n\\tIn [19]:\\n\\n\\t\\n\\n\\tDF['Val2']=DF.Val.apply(operator.itemgetter(-1), axis=1)\\n\\n\\tIn [20]:\\n\\n\\t\\n\\n\\tprint DF\\n\\n\\t                 Val       Val2\\n\\n\\t0  [2013, 37722.322]  37722.322\\n\\n\\t1  [1998, 32323.232]  32323.232\\n\\n\\t\\n\\n\\t[2 rows x 2 columns]\\n\\n\\n\\nTo do this to all your columns:\\n\\n`DF.applymap(operator.itemgetter(-1))` per @DSM's suggestion.\",\n",
       "  '<python><python-2.7><pandas><dataframe>',\n",
       "  datetime.date(2014, 4, 12),\n",
       "  '2014-04-12 01:49:50',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1314.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['493',\n",
       "  '23088383',\n",
       "  'Answer',\n",
       "  'Formatting datetime xlabels in matplotlib (pandas df.plot() method)',\n",
       "  'Simply access the tick labels and change them:\\n\\n\\n\\n    xtl=[item.get_text()[:10] for item in ax.get_xticklabels()]\\n\\n    _=ax.set_xticklabels(xtl)\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/yX8yh.png',\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2014, 4, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '9',\n",
       "  '',\n",
       "  '6210.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['494',\n",
       "  '23667732',\n",
       "  'Answer',\n",
       "  'Drop all duplicate rows in Python Pandas',\n",
       "  \"Actually, drop rows 0 and 1 only requires (any observations containing matched A and C is kept.):\\n\\n\\n\\n\\tIn [335]:\\n\\n\\t\\n\\n\\tdf['AC']=df.A+df.C\\n\\n\\tIn [336]:\\n\\n\\t\\n\\n\\tprint df.drop_duplicates('C', take_last=True) #this dataset is a special case, in general, one may need to first drop_duplicates by 'c' and then by 'a'.\\n\\n\\t     A  B  C    AC\\n\\n\\t2  foo  1  B  fooB\\n\\n\\t3  bar  1  A  barA\\n\\n\\t\\n\\n\\t[2 rows x 4 columns]\\n\\n\\n\\nBut I suspect what you really want is this (one observation containing matched A and C is kept.):\\n\\n\\n\\n\\tIn [337]:\\n\\n\\t\\n\\n\\tprint df.drop_duplicates('AC')\\n\\n\\t     A  B  C    AC\\n\\n\\t0  foo  0  A  fooA\\n\\n\\t2  foo  1  B  fooB\\n\\n\\t3  bar  1  A  barA\\n\\n\\t\\n\\n\\t[3 rows x 4 columns]\\n\\n\\n\\n#Edit:\\n\\nNow it is much clearer, therefore:\\n\\n\\n\\n\\tIn [352]:\\n\\n    DG=df.groupby(['A', 'C'])\\t\\n\\n\\tprint pd.concat([DG.get_group(item) for item, value in DG.groups.items() if len(value)==1])\\n\\n\\t     A  B  C\\n\\n\\t2  foo  1  B\\n\\n\\t3  bar  1  A\\n\\n\\t\\n\\n\\t[2 rows x 3 columns]\",\n",
       "  '<python><pandas><duplicates>',\n",
       "  datetime.date(2014, 5, 15),\n",
       "  '2014-05-15 01:38:52',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '150205.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['497',\n",
       "  '23024959',\n",
       "  'Answer',\n",
       "  'How to plot specific rows and columns of pandas dataframe (based on name of row and name of column) in bar plot with error bars?',\n",
       "  \"It is quite easy, just pass your error data to `yerr` argument the same as you will do in `matplotlib`.\\n\\n\\n\\n    DF=pd.DataFrame({'a':[.5,.1],'b':[.1,.6]})\\n\\n    DF.index=['Result1','Result2']\\n\\n    DF.plot(kind='bar',yerr=DF.b)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/PXQxe.png\",\n",
       "  '<python><matplotlib><plot><pandas>',\n",
       "  datetime.date(2014, 4, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '4086.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['499',\n",
       "  '23050689',\n",
       "  'Answer',\n",
       "  'How to calculate the likelihood of curve-fitting in scipy?',\n",
       "  \"Your likelihood function\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nwhich is simply the sum of log of probability density function of Gaussian distribution. \\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\nis the likelihood of *fitting a mu and a sigma for your residue*, not the likelihood of *your model given your data*. In one word, your approach is *wrong*.\\n\\n\\n\\nSine you are doing non-linear least square, following what @usethedeathstar already mentioned, you should go straight for `F-test`. . Consider the following example, modified from http://www.walkingrandomly.com/?p=5254, and we conduct `F-test` using `R`. And we will discuss how to translate it into `python` in the end.\\n\\n\\n\\n\\t# construct the data vectors using c()\\n\\n\\t> xdata = c(-2,-1.64,-1.33,-0.7,0,0.45,1.2,1.64,2.32,2.9)\\n\\n\\t> ydata = c(0.699369,0.700462,0.695354,1.03905,1.97389,2.41143,1.91091,0.919576,-0.730975,-1.42001)\\n\\n\\t# some starting values\\n\\n\\t> p1 = 1\\n\\n\\t> p2 = 0.2\\n\\n\\t> p3 = 0.01\\n\\n\\t\\n\\n\\t# do the fit\\n\\n\\t> fit1 = nls(ydata ~ p1*cos(p2*xdata) + p2*sin(p1*xdata), start=list(p1=p1,p2=p2))\\n\\n\\t> fit2 = nls(ydata ~ p1*cos(p2*xdata) + p2*sin(p1*xdata)+p3*xdata, start=list(p1=p1,p2=p2,p3=p3))\\n\\n\\t\\n\\n\\t# summarise\\n\\n\\t> summary(fit1)\\n\\n\\t\\n\\n\\tFormula: ydata ~ p1 * cos(p2 * xdata) + p2 * sin(p1 * xdata)\\n\\n\\t\\n\\n\\tParameters:\\n\\n\\t   Estimate Std. Error t value Pr(>|t|)    \\n\\n\\tp1 1.881851   0.027430   68.61 2.27e-12 ***\\n\\n\\tp2 0.700230   0.009153   76.51 9.50e-13 ***\\n\\n\\t---\\n\\n\\tSignif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1\\n\\n\\t\\n\\n\\tResidual standard error: 0.08202 on 8 degrees of freedom\\n\\n\\t\\n\\n\\tNumber of iterations to convergence: 7 \\n\\n\\tAchieved convergence tolerance: 2.189e-06\\n\\n\\t\\n\\n\\t> summary(fit2)\\n\\n\\t\\n\\n\\tFormula: ydata ~ p1 * cos(p2 * xdata) + p2 * sin(p1 * xdata) + p3 * xdata\\n\\n\\t\\n\\n\\tParameters:\\n\\n\\t   Estimate Std. Error t value Pr(>|t|)    \\n\\n\\tp1  1.90108    0.03520  54.002 1.96e-10 ***\\n\\n\\tp2  0.70657    0.01167  60.528 8.82e-11 ***\\n\\n\\tp3  0.02029    0.02166   0.937     0.38    \\n\\n\\t---\\n\\n\\tSignif. codes:  0 ?**?0.001 ?*?0.01 ??0.05 ??0.1 ??1\\n\\n\\t\\n\\n\\tResidual standard error: 0.08243 on 7 degrees of freedom\\n\\n\\t\\n\\n\\tNumber of iterations to convergence: 9 \\n\\n\\tAchieved convergence tolerance: 2.476e-06\\n\\n\\t\\n\\n\\t> anova(fit2, fit1)\\n\\n\\tAnalysis of Variance Table\\n\\n\\t\\n\\n\\tModel 1: ydata ~ p1 * cos(p2 * xdata) + p2 * sin(p1 * xdata) + p3 * xdata\\n\\n\\tModel 2: ydata ~ p1 * cos(p2 * xdata) + p2 * sin(p1 * xdata)\\n\\n\\t  Res.Df Res.Sum Sq Df     Sum Sq F value Pr(>F)\\n\\n\\t1      7   0.047565                             \\n\\n\\t2      8   0.053813 -1 -0.0062473  0.9194 0.3696\\n\\n\\n\\nhere we have two model, `fit1` has 2 parameters, therefore the residue has 8 degrees-of-freedom; `fit2` has one additional parameter and the residue has 7 degrees of freedom. Is model 2 significantly better? No, the F value is 0.9194, on `(1,7)` degrees of freedom and it is not significant.\\n\\n\\n\\nTo get the ANOVA table: Residue DF is easy. Residue Sum of squares: `0.08202*0.08202*8=0.05381` and `0.08243*0.08243*7=0.04756293` (notice: *'Residual standard error: 0.08243 on 7 degrees of freedom'*, etc). In `python`, you can get it by `(y_observed-y_fitted)**2`, since `scipy.optimize.curve_fit()` doesn't return the residues. \\n\\n\\n\\nThe `F-ratio` is `0.0062473/0.047565*7` and to get P-value: `1-scipy.stats.f.cdf(0.9194, 1, 7)`.\\n\\n\\n\\nPut them together we have `python` equivalent:\\n\\n\\n\\n\\tIn [1]:\\n\\n\\t\\n\\n\\timport scipy.optimize as so\\n\\n\\timport scipy.stats as ss\\n\\n\\txdata = np.array([-2,-1.64,-1.33,-0.7,0,0.45,1.2,1.64,2.32,2.9])\\n\\n\\tydata = np.array([0.699369,0.700462,0.695354,1.03905,1.97389,2.41143,1.91091,0.919576,-0.730975,-1.42001])\\n\\n\\tdef model0(x,p1,p2):\\n\\n\\t    return p1*np.cos(p2*x) + p2*np.sin(p1*x)\\n\\n\\tdef model1(x,p1,p2,p3):\\n\\n\\t    return p1*np.cos(p2*x) + p2*np.sin(p1*x)+p3*x\\n\\n\\tp1, p2, p3 = 1, 0.2, 0.01\\n\\n\\tfit0=so.curve_fit(model0, xdata, ydata, p0=(p1,p2))[0]\\n\\n\\tfit1=so.curve_fit(model1, xdata, ydata, p0=(p1,p2,p3))[0]\\n\\n\\tyfit0=model0(xdata, fit0[0], fit0[1])\\n\\n\\tyfit1=model1(xdata, fit1[0], fit1[1], fit1[2])\\n\\n\\tssq0=((yfit0-ydata)**2).sum()\\n\\n\\tssq1=((yfit1-ydata)**2).sum()\\n\\n\\tdf=len(xdata)-3\\n\\n\\tf_ratio=(ssq0-ssq1)/(ssq1/df)\\n\\n\\tp=1-ss.f.cdf(f_ratio, 1, df)\\n\\n\\tIn [2]:\\n\\n\\t\\n\\n\\tprint f_ratio, p\\n\\n\\t0.919387419515 0.369574503394\\n\\n\\n\\nAs @usethedeathstar pointed out: when you the residue is normally distributed, nonlinear least square *IS* the maximum likelihood. Therefore F-test and likelihood ratio test is equivalent. Because, *F-ratio is a monotone transformation of the likelihood ratio λ*.\\n\\n\\n\\nOr in a descriptive way, see: http://www.stata.com/support/faqs/statistics/chi-squared-and-f-distributions/\\n\\n\\n\\n   [1]: http://i.stack.imgur.com/ogAML.png\\n\\n   [2]: https://upload.wikimedia.org/math/5/6/4/564914214ead956aceccd30b78d2f6ee.png\",\n",
       "  '<python><numpy><scipy><statsmodels>',\n",
       "  datetime.date(2014, 4, 14),\n",
       "  '2017-02-08 14:52:32',\n",
       "  'CT Zhu (2487184), URL Rewriter Bot (n/a)',\n",
       "  '4',\n",
       "  '',\n",
       "  '4613.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['501',\n",
       "  '23159519',\n",
       "  'Answer',\n",
       "  'Regression of a timeseries delta in pandas',\n",
       "  \"`pandas` and `statsmodel` work beautifully together for things like this, see this example:\\n\\n\\n\\n\\tIn [16]:\\n\\n\\t\\n\\n\\timport statsmodels.formula.api as smf\\n\\n\\tIn [17]:\\n\\n\\t\\n\\n\\tdf=pd.DataFrame(np.random.random((10,2)), columns=['A','B'])\\n\\n\\tIn [18]:\\n\\n\\t\\n\\n\\tdf.index=pd.date_range('1/1/2014', periods=10)\\n\\n\\tIn [19]:\\n\\n\\t\\n\\n\\tdfd=df.diff().dropna()\\n\\n\\tIn [20]:\\n\\n\\t\\n\\n\\tprint df\\n\\n\\t\\t\\t\\t\\t   A         B\\n\\n\\t2014-01-01  0.455924  0.375653\\n\\n\\t2014-01-02  0.585738  0.864693\\n\\n\\t2014-01-03  0.201121  0.640144\\n\\n\\t2014-01-04  0.685951  0.256225\\n\\n\\t2014-01-05  0.203623  0.007993\\n\\n\\t2014-01-06  0.626527  0.719438\\n\\n\\t2014-01-07  0.327197  0.324088\\n\\n\\t2014-01-08  0.115016  0.635999\\n\\n\\t2014-01-09  0.660070  0.246438\\n\\n\\t2014-01-10  0.141730  0.125918\\n\\n\\t\\n\\n\\t[10 rows x 2 columns]\\n\\n\\tIn [21]:\\n\\n\\t\\n\\n\\tprint dfd\\n\\n\\t\\t\\t\\t\\t   A         B\\n\\n\\t2014-01-02  0.129814  0.489041\\n\\n\\t2014-01-03 -0.384617 -0.224549\\n\\n\\t2014-01-04  0.484830 -0.383919\\n\\n\\t2014-01-05 -0.482328 -0.248233\\n\\n\\t2014-01-06  0.422905  0.711446\\n\\n\\t2014-01-07 -0.299330 -0.395351\\n\\n\\t2014-01-08 -0.212182  0.311911\\n\\n\\t2014-01-09  0.545054 -0.389561\\n\\n\\t2014-01-10 -0.518340 -0.120520\\n\\n\\t\\n\\n\\t[9 rows x 2 columns]\\n\\n\\tIn [22]:\\n\\n\\t\\n\\n\\tmod1 = smf.ols('A ~ B', data=dfd).fit()\\n\\n\\tIn [23]:\\n\\n\\t\\n\\n\\tprint mod1.summary()\\n\\n\\t\\t\\t\\t\\t\\t\\t\\tOLS Regression Results                            \\n\\n\\t==============================================================================\\n\\n\\tDep. Variable:                      A   R-squared:                       0.036\\n\\n\\tModel:                            OLS   Adj. R-squared:                 -0.101\\n\\n\\tMethod:                 Least Squares   F-statistic:                    0.2637\\n\\n\\tDate:                Fri, 18 Apr 2014   Prob (F-statistic):              0.623\\n\\n\\tTime:                        13:54:27   Log-Likelihood:                -4.5434\\n\\n\\tNo. Observations:                   9   AIC:                             13.09\\n\\n\\tDf Residuals:                       7   BIC:                             13.48\\n\\n\\tDf Model:                           1                                         \\n\\n\\t==============================================================================\\n\\n\\t\\t\\t\\t\\t coef    std err          t      P>|t|      [95.0% Conf. Int.]\\n\\n\\t------------------------------------------------------------------------------\\n\\n\\tIntercept     -0.0295      0.152     -0.194      0.852        -0.389     0.330\\n\\n\\tB              0.1960      0.382      0.513      0.623        -0.707     1.099\\n\\n\\t==============================================================================\\n\\n\\tOmnibus:                        1.832   Durbin-Watson:                   3.290\\n\\n\\tProb(Omnibus):                  0.400   Jarque-Bera (JB):                1.006\\n\\n\\tSkew:                           0.506   Prob(JB):                        0.605\\n\\n\\tKurtosis:                       1.711   Cond. No.                         2.52\\n\\n\\t==============================================================================\\n\\n\\n\\nFor the 2nd question you have to provide more detail or open a separate question. There are many different kinds of sliding windows, you need to be more specific.\\n\\n\\n\\n#Edit\\n\\n\\n\\nThe simple linear regression you just described, in case you only want to store the two coefficients, can be achieved as follows:\\n\\n\\n\\n    win_size=5\\n\\n    win_step=2\\n\\n    coef_ls=[]\\n\\n    for i in range(0,len(dfd)-win_size,2):\\n\\n        coef_ls.append(smf.ols('A ~ B', data=dfd.ix[i:i+win_size]).fit().params)\\n\\n    pd.concat(coef_ls, axis=1).T # The resulting DataFrame of coefficients. \",\n",
       "  '<python><pandas><regression><linear-regression>',\n",
       "  datetime.date(2014, 4, 18),\n",
       "  '2014-04-19 19:27:06',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1408.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['503',\n",
       "  '23834082',\n",
       "  'Answer',\n",
       "  'Fitting negative binomial in python',\n",
       "  \"Not only because it is discrete, also because maximum likelihood fit to negative binomial can be quite involving, especially with an additional location parameter. That would be the reason why `.fit()` method is not provided for it (and other discrete distributions in `Scipy`), here is an example:\\n\\n\\n\\n\\tIn [163]:\\n\\n\\t\\n\\n\\timport scipy.stats as ss\\n\\n\\timport scipy.optimize as so\\n\\n\\tIn [164]:\\n\\n\\t#define a likelihood function\\n\\n\\tdef likelihood_f(P, x, neg=1):\\n\\n\\t\\tn=np.round(P[0]) #by definition, it should be an integer \\n\\n\\t\\tp=P[1]\\n\\n\\t\\tloc=np.round(P[2])\\n\\n\\t\\treturn neg*(np.log(ss.nbinom.pmf(x, n, p, loc))).sum()\\n\\n\\tIn [165]:\\n\\n\\t#generate a random variable\\n\\n\\tX=ss.nbinom.rvs(n=100, p=0.4, loc=0, size=1000)\\n\\n\\tIn [166]:\\n\\n\\t#The likelihood\\n\\n\\tlikelihood_f([100,0.4,0], X)\\n\\n\\tOut[166]:\\n\\n\\t-4400.3696690513316\\n\\n\\tIn [167]:\\n\\n\\t#A simple fit, the fit is not good and the parameter estimate is way off\\n\\n\\tresult=so.fmin(likelihood_f, [50, 1, 1], args=(X,-1), full_output=True, disp=False)\\n\\n\\tP1=result[0]\\n\\n\\t(result[1], result[0])\\n\\n\\tOut[167]:\\n\\n\\t(4418.599495886474, array([ 59.61196161,   0.28650831,   1.15141838]))\\n\\n\\tIn [168]:\\n\\n\\t#Try a different set of start paramters, the fit is still not good and the parameter estimate is still way off\\n\\n\\tresult=so.fmin(likelihood_f, [50, 0.5, 0], args=(X,-1), full_output=True, disp=False)\\n\\n\\tP1=result[0]\\n\\n\\t(result[1], result[0])\\n\\n\\tOut[168]:\\n\\n\\t(4417.1495981801972,\\n\\n\\t array([  6.24809397e+01,   2.91877405e-01,   6.63343536e-04]))\\n\\n\\tIn [169]:\\n\\n\\t#In this case we need a loop to get it right\\n\\n\\tresult=[]\\n\\n\\tfor i in range(40, 120): #in fact (80, 120) should probably be enough\\n\\n\\t\\t_=so.fmin(likelihood_f, [i, 0.5, 0], args=(X,-1), full_output=True, disp=False)\\n\\n\\t\\tresult.append((_[1], _[0]))\\n\\n\\tIn [170]:\\n\\n\\t#get the MLE\\n\\n\\tP2=sorted(result, key=lambda x: x[0])[0][1]\\n\\n\\tsorted(result, key=lambda x: x[0])[0]\\n\\n\\tOut[170]:\\n\\n\\t(4399.780263084549,\\n\\n\\t array([  9.37289361e+01,   3.84587087e-01,   3.36856705e-04]))\\n\\n\\tIn [171]:\\n\\n\\t#Which one is visually better?\\n\\n\\tplt.hist(X, bins=20, normed=True)\\n\\n\\tplt.plot(range(260), ss.nbinom.pmf(range(260), np.round(P1[0]), P1[1], np.round(P1[2])), 'g-')\\n\\n\\tplt.plot(range(260), ss.nbinom.pmf(range(260), np.round(P2[0]), P2[1], np.round(P2[2])), 'r-')\\n\\n\\tOut[171]:\\n\\n\\t[<matplotlib.lines.Line2D at 0x109776c10>]\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/MN4L1.png\",\n",
       "  '<python><statistics><scipy><distribution><statsmodels>',\n",
       "  datetime.date(2014, 5, 23),\n",
       "  '2014-06-06 15:33:32',\n",
       "  'CT Zhu (2487184)',\n",
       "  '6',\n",
       "  '',\n",
       "  '3964.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['504',\n",
       "  '25952190',\n",
       "  'Answer',\n",
       "  'Why is numba faster than numpy here?',\n",
       "  \"Instead of cluttering the original question further, I'll add some more stuff here in response to Jeff, Jaime, Veedrac:\\n\\n\\n\\n    def proc_numpy2(x,y,z):\\n\\n       np.subtract( np.multiply(x,2), np.multiply(y,55),out=x)\\n\\n       np.add( x, np.multiply(y,2),out=y)\\n\\n       np.add(x,np.add(y,99),out=z) \\n\\n       np.multiply(z,np.subtract(z,.88),out=z)\\n\\n       return z\\n\\n\\n\\n    def proc_numpy3(x,y,z):\\n\\n       x *= 2\\n\\n       x -= y*55\\n\\n       y *= 2\\n\\n       y += x\\n\\n       z = x + y\\n\\n       z += 99\\n\\n       z *= (z-.88) \\n\\n       return z\\n\\n\\n\\nMy machine seems to be running a tad faster today than yesterday so here they are in comparison to proc_numpy (proc_numba is timing the same as before)\\n\\n\\n\\n    In [611]: %timeit proc_numpy(x,y,z)\\n\\n    10000 loops, best of 3: 103 µs per loop\\n\\n    \\n\\n    In [612]: %timeit proc_numpy2(x,y,z)\\n\\n    10000 loops, best of 3: 92.5 µs per loop\\n\\n    \\n\\n    In [613]: %timeit proc_numpy3(x,y,z)\\n\\n    10000 loops, best of 3: 85.1 µs per loop\\n\\n\\n\\nNote that as I was writing proc_numpy2/3 that I started seeing some side effects so I made copies of x,y,z and passed the copies instead of re-using x,y,z.  Also, the different functions sometimes had slight differences in precision, so some of the them didn't pass the equality tests but if you diff them, they are really close.  I assume that is due to creating or (not creating) temp variables.  E.g.:\\n\\n\\n\\n    In [458]: (res_numpy2 - res_numba)[:12]\\n\\n    Out[458]: \\n\\n    array([ -7.27595761e-12,   0.00000000e+00,   0.00000000e+00,\\n\\n             0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\\n\\n             0.00000000e+00,   0.00000000e+00,   0.00000000e+00,\\n\\n             0.00000000e+00,  -7.27595761e-12,   0.00000000e+00])\\n\\n\\n\\nAlso, it's pretty minor (about 10 µs) but using float literals (55. instead of 55) will also save a little time for numpy but doesn't help numba.\\n\\n\",\n",
       "  '<python><numpy><numba>',\n",
       "  datetime.date(2014, 9, 20),\n",
       "  '2014-09-21 16:37:08',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '7492.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['505',\n",
       "  '26113708',\n",
       "  'Answer',\n",
       "  \"How can I get the matplotlib rgb color, given the colormap name, BoundryNorm, and 'c='?\",\n",
       "  'This would suffice.\\n\\n\\n\\n    In [22]:\\n\\n\\n\\n    def cstm_autumn_r(x):\\n\\n        return plt.cm.autumn_r((np.clip(x,2,10)-2)/8.)\\n\\n    In [23]:\\n\\n\\n\\n    cstm_autumn_r(1.4)\\n\\n    Out[23]:\\n\\n    (1.0, 1.0, 0.0, 1.0) #rgba yellow\\n\\n    In [24]:\\n\\n\\n\\n    cstm_autumn_r(10.5) #rgba red\\n\\n    Out[24]:\\n\\n    (1.0, 0.0, 0.0, 1.0)\\n\\n    In [25]:\\n\\n\\n\\n    %matplotlib inline\\n\\n    x = np.linspace(0, 15)\\n\\n    plt.scatter(x,x, c=cstm_autumn_r(x))\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/J33ki.png',\n",
       "  '<python><matplotlib><rgb><colormap>',\n",
       "  datetime.date(2014, 9, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '8286.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['506',\n",
       "  '26413529',\n",
       "  'Answer',\n",
       "  'How to recover matplotlib defaults after setting stylesheet',\n",
       "  'You should be able to set it back to default by:\\n\\n\\n\\n    import matplotlib as mpl\\n\\n    mpl.rcParams.update(mpl.rcParamsDefault)\\n\\n\\n\\nIn `ipython`, things are a little different, especially with `inline` backend:\\n\\n\\n\\n    In [1]:\\n\\n\\n\\n    %matplotlib inline\\n\\n    In [2]:\\n\\n\\n\\n    import matplotlib as mpl\\n\\n    import matplotlib.pyplot as plt\\n\\n    In [3]:\\n\\n\\n\\n    inline_rc = dict(mpl.rcParams)\\n\\n    In [4]:\\n\\n\\n\\n    plt.plot(range(10))\\n\\n    Out[4]:\\n\\n    [<matplotlib.lines.Line2D at 0x72d2510>]\\n\\n![enter image description here][1]\\n\\n\\n\\n    In [5]:\\n\\n\\n\\n    mpl.rcParams.update(mpl.rcParamsDefault)\\n\\n    plt.plot(range(10))\\n\\n    Out[5]:\\n\\n    [<matplotlib.lines.Line2D at 0x7354730>]\\n\\n![enter image description here][2]\\n\\n\\n\\n    In [6]:\\n\\n\\n\\n    mpl.rcParams.update(inline_rc)\\n\\n    plt.plot(range(10))\\n\\n    Out[6]:\\n\\n    [<matplotlib.lines.Line2D at 0x75a8e10>] \\n\\n![enter image description here][3]\\n\\n\\n\\nBasically, `%matplotlib inline` uses its own `rcParams`. You can grab that from the source, but the arguably easier way is probably just save the `rcParams` as `inline_rc` after `%matplotlib inline` cell magic in this example, and reuse that later.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/TUIwA.png\\n\\n  [2]: http://i.stack.imgur.com/3tWfu.png\\n\\n  [3]: http://i.stack.imgur.com/TUIwA.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 10, 16),\n",
       "  '2014-10-16 22:41:01',\n",
       "  'CT Zhu (2487184)',\n",
       "  '45',\n",
       "  '',\n",
       "  '14759.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['507',\n",
       "  '24089112',\n",
       "  'Answer',\n",
       "  'How to apply OLS from statsmodels to groupby',\n",
       "  \"Use `get_group` to get each individual group and perform OLS model on each one:\\n\\n\\n\\n    for group in linear_regression_grouped.groups.keys():\\n\\n        df= linear_regression_grouped.get_group(group)\\n\\n        X = df['period_num'] \\n\\n        y = df['TOTALS']\\n\\n        model = sm.OLS(y, X)\\n\\n        results = model.fit()\\n\\n        print results.summary()\\n\\n\\n\\nBut in real case, you also want to have the intercept term so the model should be defined slightly differently:\\n\\n\\n\\n    for group in linear_regression_grouped.groups.keys():\\n\\n        df= linear_regression_grouped.get_group(group)\\n\\n        df['constant']=1\\n\\n        X = df[['period_num','constant']]\\n\\n        y = df['TOTALS']\\n\\n        model = sm.OLS(y,X)\\n\\n        results = model.fit()\\n\\n        print results.summary()\\n\\n\\n\\nThe results (with intercept and without) are, certainly, very different.\",\n",
       "  '<python><pandas><statsmodels>',\n",
       "  datetime.date(2014, 6, 6),\n",
       "  '2015-12-07 15:52:54',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1840.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['509',\n",
       "  '24093474',\n",
       "  'Question',\n",
       "  'Regular expression negative lookbehind of non-fixed length',\n",
       "  'As the [document][1] goes:\\n\\n\\n\\n> This is called a negative lookbehind assertion. Similar to positive lookbehind assertions, the contained pattern must only match strings of some **fixed length**. \\n\\n\\n\\nSo this will work, the intention is to match any `,` outside `{}`, but not inside `{}`:\\n\\n\\n\\n\\tIn [188]:\\n\\n\\t\\n\\n\\tre.compile(\"(?<!\\\\{)\\\\,.\").findall(\\'a1,a2,a3,a4,{,a6}\\')\\n\\n\\tOut[188]:\\n\\n\\t[\\',a\\', \\',a\\', \\',a\\', \\',{\\']\\n\\n\\n\\nthis will work, on a slightly different query:\\n\\n\\n\\n\\tIn [189]:\\n\\n\\t\\n\\n\\tre.compile(\"(?<!\\\\{a5)\\\\,.\").findall(\\'a1,a2,a3,a4,{a5,a6}\\')\\n\\n    #or this: re.compile(\"(?<!\\\\{..)\\\\,.\").findall(\\'a1,a2,a3,a4,{a5,a6}\\')\\n\\n\\tOut[189]:\\n\\n\\t[\\',a\\', \\',a\\', \\',a\\', \\',{\\']\\n\\n\\tIn [190]:\\n\\n\\n\\nBut if the query is `\\'a1,a2,a3,a4,{_some_length_not_known_in_advance,a6}\\'`, according to the document the following won\\'t work as intended:\\n\\n\\n\\n\\tIn [190]:\\n\\n\\t\\n\\n\\tre.compile(\"(?<![\\\\{.*])\\\\,.\").findall(\\'a1,a2,a3,a4,{a5,a6}\\')\\n\\n\\tOut[190]:\\n\\n\\t[\\',a\\', \\',a\\', \\',a\\', \\',{\\', \\',a\\']\\n\\n\\n\\nAny alternative to achieve this? Is negative lookbehind the wrong approach?\\n\\n\\n\\nAny reason this is how lookbehind was designed to do (only match strings of some fixed length) in the first place?\\n\\n\\n\\n  [1]: https://docs.python.org/2/library/re.html',\n",
       "  '<python><regex><python-2.7>',\n",
       "  datetime.date(2014, 6, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '2570.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['515',\n",
       "  '23123479',\n",
       "  'Answer',\n",
       "  'Change pandas plot background color',\n",
       "  \"`plot()` in `pandas` are build on `matplotlib`. The right way is just to access the plot axes object and change its bgcolor: change the lastlines to:\\n\\n\\n\\n    P=gs.plot()\\n\\n    P.set_axis_bgcolor('g') #or any HTML colorcode, (R,G,B) tuple, (R,G,B,A) tuple, etcetc.\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/k4XNf.png\",\n",
       "  '<python><colors><background><matplotlib><pandas>',\n",
       "  datetime.date(2014, 4, 17),\n",
       "  '2014-04-17 02:29:35',\n",
       "  'CT Zhu (2487184)',\n",
       "  '11',\n",
       "  '',\n",
       "  '5090.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['521',\n",
       "  '24168416',\n",
       "  'Answer',\n",
       "  \"Stacking 3 bars on top of each other via Python's Matplotlib\",\n",
       "  \"Should do: `ax1.bar(range(len(data3)), data3, bottom=np.array(data1)+np.array(data2), label='data 3', alpha=0.5, color='g')`:\\n\\n![enter image description here][1]\\n\\n\\n\\nAnd, may be a preferred way. it can be very elegantly handled by `pandas` in just a few lines:\\n\\n\\n\\n\\tIn [17]:\\n\\n\\t\\n\\n\\timport pandas as pd\\n\\n\\tdf=pd.DataFrame({'data1':data1, 'data2':data2, 'data3':data3})\\n\\n\\tdf.plot(kind='bar', stacked=True)\\n\\n\\tOut[17]:\\n\\n\\t<matplotlib.axes.AxesSubplot at 0x108f2b050> \\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/15dVr.png\\n\\n  [2]: http://i.stack.imgur.com/DeGWi.png\",\n",
       "  '<python><matplotlib><bar-chart>',\n",
       "  datetime.date(2014, 6, 11),\n",
       "  '2014-06-11 17:07:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '7',\n",
       "  '',\n",
       "  '7259.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['522',\n",
       "  '24131053',\n",
       "  'Answer',\n",
       "  'how to print equation of line using scipy stats',\n",
       "  \"Where do you want the equation to go? To put it on the title, for example: `plt.title('$y=%3.7sx+%3.7s$'%(slope, intercept))`. To put it inside the plot use `plot.text`.\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/EE6zm.png\",\n",
       "  '<python><numpy><matplotlib><scipy><regression>',\n",
       "  datetime.date(2014, 6, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '6334.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['523',\n",
       "  '24131095',\n",
       "  'Answer',\n",
       "  'rpy2 installation error on Windows 8 (Anaconda)',\n",
       "  \"Dr. Gohlke's binary is probably the easiest solution. But you need to change the `Python` installation path in your registry for this method to work. The relevant key is in **HKEY_LOCAL_MACHINE\\\\SOFTWARE\\\\Python\\\\PythonCore\\\\2.7\\\\InstallPath**. Change it so that the anaconda `Python` is the default `python` installation. You can always change it back if you want.\",\n",
       "  '<python><rpy2><anaconda>',\n",
       "  datetime.date(2014, 6, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '2052.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['525',\n",
       "  '24153492',\n",
       "  'Answer',\n",
       "  \"How to get output of pandas .plot(kind='kde')?\",\n",
       "  \"There are no output value from `.plot(kind='kde')`, it returns a `axes` object. \\n\\n\\n\\nThe raw values can be accessed by `_x` and `_y` method of the `matplotlib.lines.Line2D` object in the plot\\n\\n\\n\\n\\tIn [266]:\\n\\n\\t\\n\\n\\tser = pd.Series(np.random.randn(1000))\\n\\n\\tax=ser.plot(kind='kde')\\n\\n\\t\\n\\n\\tIn [265]:\\n\\n\\t\\n\\n\\tax.get_children() #it is the 3nd object\\n\\n\\tOut[265]:\\n\\n\\t[<matplotlib.axis.XAxis at 0x85ea370>,\\n\\n\\t <matplotlib.axis.YAxis at 0x8255750>,\\n\\n\\t <matplotlib.lines.Line2D at 0x87a5a10>,\\n\\n\\t <matplotlib.text.Text at 0x8796f30>,\\n\\n\\t <matplotlib.text.Text at 0x87a5850>,\\n\\n\\t <matplotlib.text.Text at 0x87a56d0>,\\n\\n\\t <matplotlib.patches.Rectangle at 0x87a56f0>,\\n\\n\\t <matplotlib.spines.Spine at 0x85ea5d0>,\\n\\n\\t <matplotlib.spines.Spine at 0x85eaed0>,\\n\\n\\t <matplotlib.spines.Spine at 0x85eab50>,\\n\\n\\t <matplotlib.spines.Spine at 0x85ea3b0>]\\n\\n\\tIn [264]:\\n\\n\\t#get the values\\n\\n\\tax.get_children()[2]._x\\n\\n\\tax.get_children()[2]._y\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 6, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '8',\n",
       "  '',\n",
       "  '3142.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['528',\n",
       "  '23200666',\n",
       "  'Answer',\n",
       "  'Detect and exclude outliers in Pandas data frame',\n",
       "  \"Use `boolean` indexing as you would do in `numpy.array`\\n\\n\\n\\n    df = pd.DataFrame({'Data':np.random.normal(size=200)})\\n\\n    # example dataset of normally distributed data. \\n\\n\\n\\n    df[np.abs(df.Data-df.Data.mean()) <= (3*df.Data.std())]\\n\\n    # keep only the ones that are within +3 to -3 standard deviations in the column 'Data'.\\n\\n\\n\\n    df[~(np.abs(df.Data-df.Data.mean()) > (3*df.Data.std()))]\\n\\n    # or if you prefer the other way around\\n\\n\\n\\nFor a series it is similar:\\n\\n\\n\\n    S = pd.Series(np.random.normal(size=200))\\n\\n    S[~((S-S.mean()).abs() > 3*S.std())]\",\n",
       "  '<python><pandas><filtering><dataframe><outliers>',\n",
       "  datetime.date(2014, 4, 21),\n",
       "  '2018-07-05 05:24:36',\n",
       "  'CT Zhu (2487184)',\n",
       "  '105',\n",
       "  '',\n",
       "  '113844.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['529',\n",
       "  '23297607',\n",
       "  'Answer',\n",
       "  'How to speed up numpy code',\n",
       "  'An almost fully vectorized version of your code is much faster (16.9%), suppose yours is named `f()`:\\n\\n\\n\\n\\tdef g():\\n\\n\\t\\t\\tn=6\\n\\n\\t\\t\\titers = 1000\\n\\n\\t\\t\\tS=np.repeat(list(itertools.product([-1,1], repeat = n+1)),iters, axis=0).reshape((-1,n+1))\\n\\n\\t\\t\\tF=np.random.choice(np.array([-1,0,0,1], dtype=np.int8), size = (iters*(2**(n+2)),n)) #oversampling\\n\\n\\t\\t\\tF=F[~(F==0).all(1)][:iters*(2**(n+1))]\\n\\n\\t\\t\\tFS=np.asanyarray(map(lambda x, y: np.convolve(x, y, \\'valid\\'), F, S))\\n\\n\\t\\t\\tfirstzero=(FS[:,0]==0).sum()\\n\\n\\t\\t\\tbothzero=(FS==0).all(1).sum()\\n\\n\\t\\t\\tprint \"firstzero\",    firstzero\\n\\n\\t\\t\\tprint \"bothzero\",  bothzero\\n\\n\\n\\nTiming result:\\n\\n\\n\\n\\tIn [164]:\\n\\n\\t\\n\\n\\t%timeit f()\\n\\n\\tfirstzero 27171\\n\\n\\tbothzero 12151\\n\\n\\tfirstzero 27206\\n\\n\\tbothzero 12024\\n\\n\\tfirstzero 27272\\n\\n\\tbothzero 12135\\n\\n\\tfirstzero 27173\\n\\n\\tbothzero 12079\\n\\n\\t1 loops, best of 3: 14.6 s per loop\\n\\n\\tIn [165]:\\n\\n\\t\\n\\n\\t%timeit g()\\n\\n\\tfirstzero 27182\\n\\n\\tbothzero 11952\\n\\n\\tfirstzero 27365\\n\\n\\tbothzero 12174\\n\\n\\tfirstzero 27318\\n\\n\\tbothzero 12173\\n\\n\\tfirstzero 27377\\n\\n\\tbothzero 12072\\n\\n\\t1 loops, best of 3: 2.47 s per loop',\n",
       "  '<python><performance><numpy><cython>',\n",
       "  datetime.date(2014, 4, 25),\n",
       "  '2014-04-25 19:18:54',\n",
       "  'CT Zhu (2487184)',\n",
       "  '8',\n",
       "  '',\n",
       "  '3165.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['530',\n",
       "  '23349458',\n",
       "  'Answer',\n",
       "  'Understanding axis in Python',\n",
       "  \"First, `data=numpy.array(a)` is already enough, no need to use `numpy.array([b for b in a])`.\\n\\n\\n\\n`data` is now a 3D [`ndarray`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html) with the shape `(2,2,3)`, and has 3 axes `0, 1, 2`. The first axis has a length of 2, the second axis's length is also 2 and the third axis's length is 3.\\n\\n\\n\\nTherefore both `numpy.apply_along_axis(my_func, 0, data)` and `numpy.apply_along_axis(my_func, 1, data)` will result in a 2D array of shape `(2,3)`. In both cases the shape is `(2,3)`, those of the remaining axes, 2nd and 3rd or 1st and 3rd.\\n\\n\\n\\n`numpy.apply_along_axis(my_func, 2, data)` returns the `(2,2)` shape array you showed, where `(2,2)` is the shape of the first 2 axes, as you `apply` along the 3rd axis (by giving index `2`).\\n\\n\\n\\nThe way to understand it is whichever axis you apply along will be 'collapsed' into the shape of your `my_func`, which in this case returns a single value. The order and shape of the remaining axis will remain unchanged.  \\n\\n\\n\\nThe alternative way to think of it is: `apply_along_axis` means apply that function to the values on that axis, for each combination of the remaining axis/axes. Fetch the result, and organize them back into the shape of the remaining axis/axes. So, if `my_func` returns a `tuple` of 4 values:\\n\\n\\n\\n    def my_func(x):\\n\\n        return (x[0] + x[-1]) * 2,1,1,1\\n\\n\\n\\nwe will expect `numpy.apply_along_axis(my_func, 0, data).shape` to be `(4,2,3)`.\\n\\n\\n\\n- See also [`numpy.apply_over_axes`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.apply_over_axes.html#numpy.apply_over_axes) for applying a function repeatedly over multiple axes\",\n",
       "  '<python><numpy><matrix><axis>',\n",
       "  datetime.date(2014, 4, 28),\n",
       "  '2018-02-25 18:28:09',\n",
       "  'CT Zhu (2487184), Louis Maddox (2668831)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1939.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['531',\n",
       "  '24216489',\n",
       "  'Answer',\n",
       "  'Adding a new pandas column with mapped value from a dictionary',\n",
       "  'The right way of doing it will be `df[\"B\"] = df[\"A\"].map(equiv)`.\\n\\n\\n\\n\\tIn [55]:\\n\\n\\t\\n\\n\\timport pandas as pd\\n\\n\\tequiv = {7001:1, 8001:2, 9001:3}\\n\\n\\tdf = pd.DataFrame( {\"A\": [7001, 8001, 9001]} )\\n\\n\\tdf[\"B\"] = df[\"A\"].map(equiv)\\n\\n\\tprint(df)\\n\\n\\t      A  B\\n\\n\\t0  7001  1\\n\\n\\t1  8001  2\\n\\n\\t2  9001  3\\n\\n\\t\\n\\n\\t[3 rows x 2 columns]\\n\\n\\n\\nAnd it will handle the situation when the key does not exist very nicely, considering the following example:\\n\\n\\n\\n\\tIn [56]:\\n\\n\\t\\n\\n\\timport pandas as pd\\n\\n\\tequiv = {7001:1, 8001:2, 9001:3}\\n\\n\\tdf = pd.DataFrame( {\"A\": [7001, 8001, 9001, 10000]} )\\n\\n\\tdf[\"B\"] = df[\"A\"].map(equiv)\\n\\n\\tprint(df)\\n\\n\\t       A   B\\n\\n\\t0   7001   1\\n\\n\\t1   8001   2\\n\\n\\t2   9001   3\\n\\n\\t3  10000 NaN\\n\\n\\t\\n\\n\\t[4 rows x 2 columns]',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 6, 14),\n",
       "  '2016-03-04 22:38:11',\n",
       "  'CT Zhu (2487184)',\n",
       "  '65',\n",
       "  '',\n",
       "  '34157.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['532',\n",
       "  '24337921',\n",
       "  'Answer',\n",
       "  'Skip gcf().autofmt_xdate() at pandas plot creation',\n",
       "  'I think you can just clear all the ticks before you make new ones:\\n\\n\\n\\n    df=pd.DataFrame({\\'A\\':np.random.random(20), \\'B\\':np.random.random(20)})\\n\\n    df.index=pd.date_range(\\'1/1/2014\\', periods=20, freq=\\'5H\\')\\n\\n    ax = plt.figure(figsize=(7,5), dpi=100).add_subplot(111)\\n\\n    df.plot(ax=ax)\\n\\n    ax.set_xticks([])\\n\\n    ax.set_xticks([], minor=True)\\n\\n    ax.xaxis.set_major_locator(mdates.HourLocator(interval=200))\\n\\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\\'%m/%d-%H:%S\\'))\\n\\n    ax.xaxis.grid(True, which=\"minor\")\\n\\n    plt.xticks(rotation=\\'vertical\\', fontsize = 8)\\n\\n    plt.subplots_adjust(bottom=.2)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n#Edit\\n\\n\\n\\nNow the labels are off. `ax.xaxis.set_major_formatter(mdates.DateFormatter(\\'%y/%m/%d-%H:%S\\'))` will show they are 1991/01/11 and so on. \\n\\n\\n\\n    df=pd.DataFrame({\\'A\\':np.random.random(20), \\'B\\':np.random.random(20)})\\n\\n    df.index=pd.date_range(\\'1/1/2014\\', periods=20, freq=\\'5H\\')\\n\\n    ax = plt.figure(figsize=(7,5), dpi=100).add_subplot(111)\\n\\n    ax.plot(df.index.to_pydatetime(), df.A, label=\\'A\\')\\n\\n    ax.plot(df.index.to_pydatetime(), df.B, label=\\'B\\')\\n\\n    ax.legend()\\n\\n    ax.xaxis.set_major_locator(mdates.HourLocator(interval=5))\\n\\n    ax.xaxis.set_major_formatter(mdates.DateFormatter(\\'%y/%m/%d-%H:%S\\'))\\n\\n    ax.xaxis.grid(True, which=\"major\")\\n\\n    ax.yaxis.grid(True, which=\"major\")\\n\\n    plt.xticks(rotation=\\'vertical\\', fontsize = 8)\\n\\n    plt.subplots_adjust(bottom=.2)\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/uqW80.png\\n\\n  [2]: http://i.stack.imgur.com/CfXsd.png',\n",
       "  '<python><matplotlib><plot><pandas>',\n",
       "  datetime.date(2014, 6, 21),\n",
       "  '2014-06-24 00:50:11',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '3641.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['533',\n",
       "  '24397728',\n",
       "  'Answer',\n",
       "  'Merge after groupby',\n",
       "  \"I think you should just do this:\\n\\n\\n\\n\\tIn [140]:\\n\\n\\t\\n\\n\\tdf2 = pd.merge(df2,\\n\\n                   pd.DataFrame(grouped, columns=['mean']),\\n\\n                   left_on='key', \\n\\n                   right_index=True)\\n\\n\\tprint df2\\n\\n\\t   key     var21     var22      mean\\n\\n\\t0    1  0.324476  0.701254  0.400313\\n\\n\\t1    2 -1.270500  0.055383 -0.293691\\n\\n\\t2    3  0.804864  0.566747  0.628787\\n\\n\\t\\n\\n\\t[3 rows x 4 columns]\\n\\n\\n\\nThe reason it didn't work is that `grouped` is a `Series` not a `DataFrame`\",\n",
       "  '<python><merge><pandas>',\n",
       "  datetime.date(2014, 6, 24),\n",
       "  '2016-09-23 23:59:37',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1715.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['534',\n",
       "  '24172676',\n",
       "  'Answer',\n",
       "  'matplotlib remove axis label offset by default',\n",
       "  'No, there is no way to do it. It is defined in the source file of `ticker.py`, line 353:\\n\\n\\n\\n    def __init__(self, useOffset=True, useMathText=None, useLocale=None):\\n\\n        # useOffset allows plotting small data ranges with large offsets: for\\n\\n        # example: [1+1e-9,1+2e-9,1+3e-9] useMathText will render the offset\\n\\n        # and scientific notation in mathtext\\n\\n\\n\\n        self.set_useOffset(useOffset)\\n\\n\\n\\nas a default parameter values. So the default is `True`. \\n\\n\\n\\nYou can modify the source, of course. ',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 6, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '3272.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['536',\n",
       "  '24227219',\n",
       "  'Answer',\n",
       "  'Using an image for tick labels in matplotlib',\n",
       "  \"Interesting question, and potentially has many possible solutions. Here is my approach, basically first calculate where the label `'0'` is, then draw a new axis there using absolute coordinates, and finally put the image there:\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    import matplotlib.image as mpimg\\n\\n    import pylab as pl\\n\\n\\n\\n    A = np.random.random(size=(5,5))\\n\\n    fig, ax = plt.subplots(1, 1)\\n\\n    \\n\\n    xl, yl, xh, yh=np.array(ax.get_position()).ravel()\\n\\n    w=xh-xl\\n\\n    h=yh-yl\\n\\n    xp=xl+w*0.1 #if replace '0' label, can also be calculated systematically using xlim()\\n\\n    size=0.05\\n\\n       \\n\\n    img=mpimg.imread('microblog.png')\\n\\n    ax.matshow(A)\\n\\n    ax1=fig.add_axes([xp-size*0.5, yh, size, size])\\n\\n    ax1.axison = False\\n\\n    imgplot = ax1.imshow(img,transform=ax.transAxes)\\n\\n    \\n\\n    plt.savefig('temp.png')\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Sg5vs.png\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 6, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1617.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['542',\n",
       "  '24521657',\n",
       "  'Answer',\n",
       "  'how to display and input chinese (and other non-ASCII) character in r console?',\n",
       "  'It is probably not very well documented, but you want to use `setlocale` in order to use Chinese. And the method applies to many other languages as well. The solution is not obvious as the official document of [`setlocale`][1] didn\\'t specifically mentioned it as a method to solve the display issues.\\n\\n\\n\\n\\t\\n\\n\\t> print(\\'ÊÔÊÔ\\') #试试, meaning let\\'s give it a shot in Chinese\\n\\n\\t[1] \"ÊÔÊÔ\" #won\\'t show up correctly\\n\\n\\t> Sys.getlocale()\\n\\n\\t[1] \"LC_COLLATE=English_United States.1252;LC_CTYPE=English_United States.1252;LC_MONETARY=English_United States.1252;LC_NUMERIC=C;LC_TIME=English_United States.1252\"\\n\\n\\t> Sys.setlocale(category = \"LC_ALL\", locale = \"chs\") #cht for traditional Chinese, etc.\\n\\n\\t[1] \"LC_COLLATE=Chinese_People\\'s Republic of China.936;LC_CTYPE=Chinese_People\\'s Republic of China.936;LC_MONETARY=Chinese_People\\'s Republic of China.936;LC_NUMERIC=C;LC_TIME=Chinese_People\\'s Republic of China.936\"\\n\\n\\t> print(\\'试试\\')\\n\\n\\t[1] \"试试\"\\n\\n\\t> read.table(\"c:/CHS.txt\",sep=\" \") #Chinese: the 1st record/observation\\n\\n      V1   V2  V3 V4  V5   V6\\n\\n    1 122 第一 122 条 122 记录 \\n\\n\\n\\nIf you just want to change the display encoding, without changing other aspects of locales, use `LC_CTYPE` instead of `LC_ALL`:\\n\\n\\n\\n    > Sys.setlocale(category = \"LC_CTYPE\", locale = \"chs\")\\n\\n    [1] \"Chinese_People\\'s Republic of China.936\"\\n\\n    > print(\\'试试\\')\\n\\n    [1] \"试试\"\\n\\n\\n\\nNow, of course this only applies to the official `R` console. If you use other IDE\\'s, such as the very popular `RStudio`, you don\\'t need to do this at all to be able to type and display Chinese, even if you didn\\'t have the Chinese locale loaded.\\n\\n\\n\\n#Migrate some useful stuff from the following comments:\\n\\nIf the data still fails to show up correctly, the we should also look into the issue of the file encoding. If the file is `UTF-8` encoded, tither `data <- read.table(\"you_file\", sep=\\',\\', fileEncoding=\"UTF-8-BOM\", header=TRUE)` or `fileEncoding=\"UTF-8\"` will do, depends on which encoding it really has. \\n\\n\\n\\nBut you may want to stay away from `UTF-BOM` as it is not recommended: https://stackoverflow.com/questions/2223882/whats-different-between-utf-8-and-utf-8-without-bom\\n\\n\\n\\n  [1]: http://stat.ethz.ch/R-manual/R-devel/library/base/html/locales.html',\n",
       "  '<r>',\n",
       "  datetime.date(2014, 7, 2),\n",
       "  '2017-05-23 12:02:30',\n",
       "  'CT Zhu (2487184), URL Rewriter Bot (n/a)',\n",
       "  '4',\n",
       "  '',\n",
       "  '6831.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['543',\n",
       "  '24373427',\n",
       "  'Answer',\n",
       "  'pandas dataframe with 2-rows header and export to csv',\n",
       "  'Use `df.to_csv(\"test.csv\", index = False,  tupleize_cols=True)` to get the resulting CSV to be:\\n\\n\\n\\n\\n\\n    \"(\\'AA\\', \\'DD\\')\",\"(\\'BB\\', \\'EE\\')\",\"(\\'CC\\', \\'FF\\')\"\\n\\n    a,b,c1\\n\\n    a,b,c2\\n\\n    a,b,c3\\n\\n\\n\\nTo read it back:\\n\\n\\n\\n    df2=pd.read_csv(\"test.csv\", tupleize_cols=True)\\n\\n    df2.columns=pd.MultiIndex.from_tuples(eval(\\',\\'.join(df2.columns)))\\n\\n\\n\\nTo get the exact output you wanted:\\n\\n\\n\\n    with open(\\'test.csv\\', \\'a\\') as f:\\n\\n        pd.DataFrame(np.asanyarray(df.columns.tolist())).T.to_csv(f, index = False, header=False)\\n\\n        df.to_csv(f, index = False, header=False)',\n",
       "  '<python><csv><pandas><dataframe>',\n",
       "  datetime.date(2014, 6, 23),\n",
       "  '2014-06-23 19:47:08',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '9351.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['544',\n",
       "  '24379542',\n",
       "  'Answer',\n",
       "  'Python sci-kit learn (metrics): difference between r2_score and explained_variance_score?',\n",
       "  'OK, look at this example:\\n\\n\\n\\n\\tIn [123]:\\n\\n\\t#data\\n\\n\\ty_true = [3, -0.5, 2, 7]\\n\\n\\ty_pred = [2.5, 0.0, 2, 8]\\n\\n\\tprint metrics.explained_variance_score(y_true, y_pred)\\n\\n\\tprint metrics.r2_score(y_true, y_pred)\\n\\n\\t0.957173447537\\n\\n\\t0.948608137045\\n\\n\\tIn [124]:\\n\\n\\t#what explained_variance_score really is\\n\\n\\t1-np.cov(np.array(y_true)-np.array(y_pred))/np.cov(y_true)\\n\\n\\tOut[124]:\\n\\n\\t0.95717344753747324\\n\\n\\tIn [125]:\\n\\n\\t#what r^2 really is\\n\\n\\t1-((np.array(y_true)-np.array(y_pred))**2).sum()/(4*np.array(y_true).std()**2)\\n\\n\\tOut[125]:\\n\\n\\t0.94860813704496794\\n\\n\\tIn [126]:\\n\\n\\t#Notice that the mean residue is not 0\\n\\n\\t(np.array(y_true)-np.array(y_pred)).mean()\\n\\n\\tOut[126]:\\n\\n\\t-0.25\\n\\n\\tIn [127]:\\n\\n\\t#if the predicted values are different, such that the mean residue IS 0:\\n\\n\\ty_pred=[2.5, 0.0, 2, 7]\\n\\n\\t(np.array(y_true)-np.array(y_pred)).mean()\\n\\n\\tOut[127]:\\n\\n\\t0.0\\n\\n\\tIn [128]:\\n\\n\\t#They become the same stuff\\n\\n\\tprint metrics.explained_variance_score(y_true, y_pred)\\n\\n\\tprint metrics.r2_score(y_true, y_pred)\\n\\n\\t0.982869379015\\n\\n\\t0.982869379015\\n\\n\\n\\nSo, when the mean residue is 0, they are the same. Which one to choose dependents on your needs, that is, is the mean residue **suppose** to be 0?',\n",
       "  '<python><scikit-learn>',\n",
       "  datetime.date(2014, 6, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '2378.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['547',\n",
       "  '26870873',\n",
       "  'Question',\n",
       "  'IPython in Safari: websocket connection failure',\n",
       "  'Despite being listed in the [ipython-notebook doc][1] as one of the supported browsers, `Safari` `5.1.10` running on `OSX 10.6.8(64bit)` can not run `IPython-notebook` fully functionally. `ipython` version `2.2.0`, in `anaconda` `python` environment.\\n\\n\\n\\nSafari displays the list of notebooks OK. Every aspects seem to be fine until one opens (or creates a new) notebook. A error message shows up:\\n\\n\\n\\n    \"A WebSocket connection could not be established. \\n\\n     You will NOT be able to run code.\\n\\n     Check your network connection or notebook server configuration.\"\\n\\n\\n\\nBasically the cell shows up alright, but you can\\'t run them.\\n\\n\\n\\n`Firefox 32.0.3` can, however, fully functions with `IPython notebook`. Therefore it is not a `IPython notebook` problem. Therefore there is no error message in the terminal, `i.e`:\\n\\n\\n\\n    6600sls-Mac-Pro:~ bio101sl$ ipython notebook --no-browser\\n\\n    2014-11-11 12:23:39.339 [NotebookApp] Using existing profile dir: u\\'/Users/bio101sl/.ipython/profile_default\\'\\n\\n    2014-11-11 12:23:39.369 [NotebookApp] Using MathJax from CDN: https://cdn.mathjax.org/mathjax/latest/MathJax.js\\n\\n    2014-11-11 12:23:39.541 [NotebookApp] Serving notebooks from local directory: /Users/bio101sl/IPython_NB\\n\\n    2014-11-11 12:23:39.542 [NotebookApp] 0 active kernels \\n\\n    2014-11-11 12:23:39.542 [NotebookApp] The IPython Notebook is running at: http://localhost:8888/\\n\\n    2014-11-11 12:23:39.542 [NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).\\n\\n\\n\\nSeems to be a `Safari` - `websocket` related problem. Is there any settings in `Safari` has to be changed to get it work? Unfortunately [ipython-notebook doc][1] doesn\\'t provide any hint on that.\\n\\n\\n\\n  [1]: http://ipython.org/ipython-doc/dev/install/install.html',\n",
       "  '<python><safari><websocket><ipython-notebook>',\n",
       "  datetime.date(2014, 11, 11),\n",
       "  '2017-10-20 17:45:06',\n",
       "  'CT Zhu (2487184), LW001 (8517948)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1367.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['548',\n",
       "  '28934228',\n",
       "  'Answer',\n",
       "  'Calculate correlation between all columns of a DataFrame and all columns of another DataFrame?',\n",
       "  \"(**Edit to add**:  Instead of this answer please check out @yt's answer which was added later but is clearly better.)\\n\\n\\n\\nYou could go with ```numpy.corrcoef()``` which is basically the same as ```corr``` in pandas, but the syntax may be more amenable to what you want.  \\n\\n\\n\\n    for s in ['s1','s2']:\\n\\n        for i in ['i1','i2']:\\n\\n            print( 'corrcoef',s,i,np.corrcoef(df1[s],df2[i])[0,1] )\\n\\n       \\n\\nThat prints:\\n\\n\\n\\n    corrcoef s1 i1 -0.00416977553597\\n\\n    corrcoef s1 i2 -0.0096393047035\\n\\n    corrcoef s2 i1 -0.026278689352\\n\\n    corrcoef s2 i2 -0.00402030582064\\n\\n    \\n\\nAlternatively you could load the results into a dataframe with appropriate labels:\\n\\n\\n\\n    cc = pd.DataFrame()     \\n\\n    for s in ['s1','s2']:\\n\\n        for i in ['i1','i2']:\\n\\n            cc = cc.append( pd.DataFrame(\\n\\n                 { 'corrcoef':np.corrcoef(df1[s],df2[i])[0,1] }, index=[s+'_'+i]))\\n\\n\\n\\nWhich looks like this:\\n\\n\\n\\n           corrcoef\\n\\n    s1_i1 -0.004170\\n\\n    s1_i2 -0.009639\\n\\n    s2_i1 -0.026279\\n\\n    s2_i2 -0.004020\\n\\n\\n\\n\",\n",
       "  '<python><python-3.x><pandas>',\n",
       "  datetime.date(2015, 3, 9),\n",
       "  '2018-10-05 12:32:20',\n",
       "  'JohnE (3877338)',\n",
       "  '4',\n",
       "  '',\n",
       "  '13903.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['550',\n",
       "  '24544362',\n",
       "  'Answer',\n",
       "  'Calling R script from python using rpy2',\n",
       "  '`source` is a `r` function, which runs a `r` source file. Therefore in `rpy2`, we have two ways to call it, either:\\n\\n\\n\\n    r[\\'source\\'](\"script.R\")\\n\\n\\n\\nor\\n\\n\\n\\n    r.source(\"script.R\")\\n\\n\\n\\n`r[r.source(\"script.R\")]` is a wrong way to do it. \\n\\n\\n\\nSame idea may apply to the next line.',\n",
       "  '<python><r><call><rpy2>',\n",
       "  datetime.date(2014, 7, 3),\n",
       "  '2014-07-03 03:35:12',\n",
       "  'CT Zhu (2487184)',\n",
       "  '24',\n",
       "  '',\n",
       "  '17499.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['554',\n",
       "  '23331659',\n",
       "  'Answer',\n",
       "  'Update a dataframe in pandas while iterating row by row',\n",
       "  \"You should assign value by `df.ix[i, 'exp']=X` or `df.loc[i, 'exp']=X` instead of `df.ix[i]['ifor'] = x`. \\n\\n\\n\\nOtherwise you are working on a view, and should get a warming:\\n\\n\\n\\n`-c:1: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame.\\n\\nTry using .loc[row_index,col_indexer] = value instead`\\n\\n\\n\\nBut certainly, loop probably should better be replaced by some vectorized algorithm to make the full use of `DataFrame` as @Phillip Cloud suggested.\",\n",
       "  '<python><pandas><updates><dataframe>',\n",
       "  datetime.date(2014, 4, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '16',\n",
       "  '',\n",
       "  '94807.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['555',\n",
       "  '23358348',\n",
       "  'Answer',\n",
       "  'Python using xhtml2pdf to print webpage into PDF',\n",
       "  \"`xhmlt2pdf` is not going to work with all the websites, for one, it is not working for `yahoo.com`. But the reason it is not working here is you are not providing the actual HTML file to `pisa` but rather providing the URL, you want to fetch the HTML first, for example using `urllib2`:\\n\\n\\n\\n    url=urllib2.urlopen('http://sheldonbrown.com/web_sample1.html')\\n\\n    srchtml=url.read()\\n\\n    pisa.showLogging()\\n\\n    convertHtmlToPdf(srchtml, outputFilename)\\n\\n\\n\\nAnd it will work. That is a very simple sample HTML.\",\n",
       "  '<python><url><pdf><pisa><xhtml2pdf>',\n",
       "  datetime.date(2014, 4, 29),\n",
       "  '2014-04-29 07:58:51',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2050.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['556',\n",
       "  '23400958',\n",
       "  'Answer',\n",
       "  'Unsupported Operand Type Error with scipy.optimize.curve_fit',\n",
       "  \"You need to convert you `list`s to `np.array`:\\n\\n\\n\\n    def my_func(x, alpha):\\n\\n        return np.array([10*np.log10(alpha*y*y) for y in x])\\n\\n    \\n\\n    known_x = np.array([1039.885254, 2256.833008, 6428.667969, 30602.62891]) #known x-values\\n\\n    known_y = np.array([31.87999916, 33.63000107, 35, 36.74000168])\\n\\n\\n\\nResult:\\n\\n\\n\\n    (array([ 0.00012562]), array([[  2.38452809e-08]]))\\n\\n\\n\\nThe reason is quite evident as indicated by this message:\\n\\n\\n\\n   `TypeError: unsupported operand type(s) for -: 'list' and 'list'`\\n\\n\\n\\nSure, `list` can not be subtracted by a `list`. In order to do so, we need them to be in `numpy.array`\",\n",
       "  '<python><scipy>',\n",
       "  datetime.date(2014, 5, 1),\n",
       "  '2014-05-01 01:58:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1030.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['559',\n",
       "  '23577072',\n",
       "  'Answer',\n",
       "  'Python Numpy matrix multiplication in high dimension',\n",
       "  \"Ha, it can be done in just one line: `np.einsum('nmk,nkj->mj',A,B)`.\\n\\n\\n\\nSee Einstein summation: http://docs.scipy.org/doc/numpy/reference/generated/numpy.einsum.html\\n\\n\\n\\nNot the same problem but the idea is quite much the same, see discussions and alternative methods in this topic we just discussed: https://stackoverflow.com/questions/23574806/numpy-multiply-matrices-preserve-third-axis\\n\\n\\n\\nDon't name your variable `sum`, you override the build-in `sum`.\\n\\n\\n\\nAs @Jaime pointed out, the loop is actually faster for dimensions of these size. In fact a solution based on `map` and `sum` is, albeit simpler, even slower:\\n\\n\\n\\n\\n\\n\\tIn [19]:\\n\\n\\t\\n\\n\\t%%timeit\\n\\n\\tSUM = np.zeros([20,5])\\n\\n\\tfor i in range(len(A)):\\n\\n\\t  SUM += np.dot(A[i],B[i])\\n\\n\\t10000 loops, best of 3: 115 µs per loop\\n\\n\\tIn [20]:\\n\\n\\t\\n\\n\\t%timeit np.array(map(np.dot, A,B)).sum(0)\\n\\n\\t1000 loops, best of 3: 445 µs per loop\\n\\n\\tIn [21]:\\n\\n\\t\\n\\n\\t%timeit np.einsum('nmk,nkj->mj',A,B)\\n\\n\\t1000 loops, best of 3: 259 µs per loop\\n\\n\\n\\nThing are different with larger dimension:\\n\\n\\n\\n    n_examples = 1000\\n\\n    A = np.random.randn(n_examples, 20,1000)\\n\\n    B = np.random.randn(n_examples, 1000,5)\\n\\n\\n\\nAnd:\\n\\n\\n\\n\\tIn [46]:\\n\\n\\t\\n\\n\\t%%timeit\\n\\n\\tSUM = np.zeros([20,5])\\n\\n\\tfor i in range(len(A)):\\n\\n\\t  SUM += np.dot(A[i],B[i])\\n\\n\\t1 loops, best of 3: 191 ms per loop\\n\\n\\tIn [47]:\\n\\n\\t\\n\\n\\t%timeit np.array(map(np.dot, A,B)).sum(0)\\n\\n\\t1 loops, best of 3: 164 ms per loop\\n\\n\\tIn [48]:\\n\\n\\t\\n\\n\\t%timeit np.einsum('nmk,nkj->mj',A,B)\\n\\n\\t1 loops, best of 3: 451 ms per loop\\n\\n\",\n",
       "  '<python><numpy><matrix><machine-learning><linear-algebra>',\n",
       "  datetime.date(2014, 5, 10),\n",
       "  '2017-05-23 12:23:32',\n",
       "  'CT Zhu (2487184), URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1466.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['561',\n",
       "  '24468911',\n",
       "  'Answer',\n",
       "  'Set xlim for pandas/matplotlib where index is string',\n",
       "  \"Use `set_xlim`, `+1` means moving 1 unit to the right and `-1` means the reverse. In the following example I expanded the plot 0.5 months each side:\\n\\n\\n\\n    df=pd.DataFrame({'A': range(10), 'B': range(1, 11), 'C': range(2,12)})\\n\\n    df.index=pd.date_range('2001/01/01', periods=10, freq='M')\\n\\n    ax=df.plot(kind='line')\\n\\n    ax.set_xlim(np.array([-0.5, 0.5])+ax.get_xlim())\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nEdit, to have `xticklabel` for every year, instead the default every two years in `pandas`:\\n\\n\\n\\n\\tax=df.plot(kind='line', xticks=df.index)\\n\\n\\tax.set_xticklabels(df.index.map(lambda x: datetime.datetime.strftime(x, '%Y')))\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/VeO30.png\\n\\n  [2]: http://i.stack.imgur.com/BDlX4.png\",\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2014, 6, 28),\n",
       "  '2014-06-29 00:45:51',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2927.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['562',\n",
       "  '24582348',\n",
       "  'Answer',\n",
       "  'reading tab-delimited data without header in pandas',\n",
       "  \"I think you are getting the it read correctly, but:\\n\\n\\n\\n 1. See: https://stackoverflow.com/questions/21482546/change-pandas-0-13-0-print-dataframe-to-print-dataframe-like-in-earlier-version, this is what pandas do in the older versions. So, update will solve it.\\n\\n 2. You can use `ipython notebook`, where `DataFrames` will show up as HTML tables.\\n\\n 3. You can use `df.head(5)` (similar to `r`'s `head`) to get the first a few rows just to make sure your `DataFrame` is correct.\",\n",
       "  '<python><pandas><dataframe><tab-delimited>',\n",
       "  datetime.date(2014, 7, 5),\n",
       "  '2017-05-23 12:32:56',\n",
       "  'CT Zhu (2487184), URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '8449.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['563',\n",
       "  '24645245',\n",
       "  'Answer',\n",
       "  'pandas dataframe columns scaling with sklearn',\n",
       "  \"You can do it using  `pandas` only:\\n\\n\\n\\n\\tIn [235]:\\n\\n\\tdfTest = pd.DataFrame({'A':[14.00,90.20,90.95,96.27,91.21],'B':[103.02,107.26,110.35,114.23,114.68], 'C':['big','small','big','small','small']})\\n\\n\\tdf = dfTest[['A', 'B']]\\n\\n\\tdf_norm = (df - df.min()) / (df.max() - df.min())\\n\\n\\tprint df_norm\\n\\n\\tprint pd.concat((df_norm, dfTest.C),1)\\n\\n\\t\\n\\n\\t          A         B\\n\\n\\t0  0.000000  0.000000\\n\\n\\t1  0.926219  0.363636\\n\\n\\t2  0.935335  0.628645\\n\\n\\t3  1.000000  0.961407\\n\\n\\t4  0.938495  1.000000\\n\\n\\t          A         B      C\\n\\n\\t0  0.000000  0.000000    big\\n\\n\\t1  0.926219  0.363636  small\\n\\n\\t2  0.935335  0.628645    big\\n\\n\\t3  1.000000  0.961407  small\\n\\n\\t4  0.938495  1.000000  small\\n\\n\",\n",
       "  '<python><pandas><scikit-learn><dataframe>',\n",
       "  datetime.date(2014, 7, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '63027.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['565',\n",
       "  '24749614',\n",
       "  'Answer',\n",
       "  \"The color parameter for matplotlib's scatter is `c`, but `color` works too; prod. diff. results\",\n",
       "  \"This is not a bug, but IMO, a little ambiguity of `matplotlib`'s documents.\\n\\n\\n\\nThe color of markers, can be defined by either `c`, `color`, `edgecolor` and `facecolor`.\\n\\n\\n\\n`c` is in the source code of `scatter()` in `axes.py`. That is equivalent to `facecolor`. When you use `c='r'`, `edgecolor` is left undefined and the default in `matplotlib.rcParams` come in to effect, which has a default value of `k` (black).\\n\\n\\n\\n`color`, `edgecolor` and `facecolor` are passed to the `collection.Collection` object `scatter()` returns. As you will see in the source code `collections.py` (`set_color()`, `set_edgecolor()` and `set_facecolor()` methods), `set_color()` basically calls `set_edgecolor` and `set_facecolor`, therefore set the two properties the same values.\\n\\n\\n\\nThese I hope should explain the behavior that your described in the OP. In the case of `c='red'` the edge is black and the face color is red. In the case of `color=red`, both the face color and the edge color are red.\\n\\n\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 7, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1673.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['567',\n",
       "  '24642029',\n",
       "  'Answer',\n",
       "  'add rows to groups in pandas dataframe',\n",
       "  \"Just `concat` the inserts that you want to insert in (and they will be appended in the rear, or `df.append(the_insert)`, which does the same thing) and `reset_index` the resultant to get things in the right order:\\n\\n\\n\\n    In [137]:\\n\\n\\n\\n    df2=pd.DataFrame({'b':[11,12,13], 'a':[0]*3})\\n\\n    In [138]:\\n\\n\\n\\n    df3=pd.concat((df, df2)).sort('a').reset_index(drop=True)\\n\\n    #pd.concat((df, df2, df3, df4...., all the others...))\\n\\n    In [139]:\\n\\n\\n\\n    print df3\\n\\n        a   b   c   d\\n\\n    0   0   1   1   1\\n\\n    1   0   2   2   2\\n\\n    2   0   3   3   3\\n\\n    3   0  11 NaN NaN\\n\\n    4   0  12 NaN NaN\\n\\n    5   0  13 NaN NaN\\n\\n    6   1   4   4   4\\n\\n    7   1   5   5   5\\n\\n    8   1   6   6   6\\n\\n    9   2   7   7   7\\n\\n    10  2   8   8   8\\n\\n    11  2   9   9   9\",\n",
       "  '<python><pandas><group-by><dataframe>',\n",
       "  datetime.date(2014, 7, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2273.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['570',\n",
       "  '23462659',\n",
       "  'Answer',\n",
       "  'Obtaining values used in boxplot, using python and matplotlib',\n",
       "  \"Why do you want to do so? what you are doing is already pretty direct.\\n\\n\\n\\nYeah, if you want to fetch them for the plot, when the plot is already made, simply use the `get_ydata()` method.\\n\\n\\n\\n    B=plt.boxplot(data)\\n\\n    [item.get_ydata() for item in B['whiskers']]\\n\\n\\n\\nIt returns an array of the shape (2,) for each whiskers, the second element is the value we want:\\n\\n\\n\\n    [item.get_ydata()[0] for item in B['whiskers']]\",\n",
       "  '<python><numpy><matplotlib><scipy>',\n",
       "  datetime.date(2014, 5, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '11',\n",
       "  '',\n",
       "  '10751.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['571',\n",
       "  '23479395',\n",
       "  'Answer',\n",
       "  'Dynamically writing the objective function and constraints for scipy.optimize.minimize from matrices',\n",
       "  \"There are two obvious problems:\\n\\n\\n\\n1, the Jacobians for both the inequality and equality constrains are both `array` not callables (functions), as they are defined as :\\n\\n\\n\\n    dgdx = eval_part_ineq_cons(x_k)\\n\\n    dhdx = eval_part_eq_cons(x_k)\\n\\n\\n\\nThey are function returns (therefore `array`s) not functions.\\n\\n\\n\\n2, The constrain function returns are not scalars, they are `array`s. e.g.:\\n\\n\\n\\n    g_k + np.dot(dgdx, (x_k1 - x_k)).squeeze()\\n\\n\\n\\nThe way to correct this would depended on your real work problems, there are alternatives such as:\\n\\n\\n\\n    (g_k + np.dot(dgdx, (x_k1 - x_k)).squeeze()).sum()\\n\\n\\n\\nor\\n\\n\\n\\n    ((g_k + np.dot(dgdx, (x_k1 - x_k)).squeeze())**2).sum() #sum of squares.\\n\\n\\n\\nSame problem for inequality constrain. \\n\\nI suspect this is because `fun` is passed to `fmin_slsqp` as `eqcons` not `f_eqcons`. If you want the all elements in the equality constrain function returns ==0, probably you should use `fmin_slsqp` directly and pass the constrain functions `f_eqcons`.\\n\\n\\n\\nTherefore, it can be as followes:\\n\\n\\n\\n\\n\\n    cons = ({'type': 'ineq',\\n\\n             'fun': lambda x_k1: (g_k + np.dot(dgdx, (x_k1 - x_k)).squeeze()).sum(),\\n\\n             'jac':  eval_part_ineq_cons},\\n\\n            {'type': 'eq',\\n\\n             'fun': lambda x_k1: (h_k + np.dot(dhdx, (x_k1 - x_k)).squeeze()).sum(),\\n\\n             'jac': eval_part_eq_cons})\\n\\n\\n\\nAnd the result is:\\n\\n\\n\\n\\tIn [31]:\\n\\n\\t\\n\\n\\tres\\n\\n\\tOut[31]:\\n\\n\\t  status: 9\\n\\n\\t success: False\\n\\n\\t\\tnjev: 101\\n\\n\\t\\tnfev: 1385\\n\\n\\t\\t fun: array([ 2.69798824])\\n\\n\\t\\t   x: array([ 1.89963627,  0.04972158])\\n\\n\\t message: 'Iteration limit exceeded'\\n\\n\\t\\t jac: array([ 4.,  2.,  0.])\\n\\n\\t\\t nit: 101\\n\\n\\n\\nActually, notice that the optimization failed. I suspect it is not going to guarantee to work when you have the Jacobian constrains. Removing the Jac's and it will work (although may not be the way that you want it).\\n\\n\\n\\n\\tIn [29]:\\n\\n\\t\\n\\n\\tres\\n\\n\\tOut[29]:\\n\\n\\t  status: 0\\n\\n\\t success: True\\n\\n\\t\\tnjev: 11\\n\\n\\t\\tnfev: 122\\n\\n\\t\\t fun: array([ 1.33333333])\\n\\n\\t\\t   x: array([ 0.5       ,  2.16666667])\\n\\n\\t message: 'Optimization terminated successfully.'\\n\\n\\t\\t jac: array([ 4.,  2.,  0.])\\n\\n\\t\\t nit: 15\\n\\n\\n\\n\",\n",
       "  '<python><matrix><scipy><constraints><mathematical-optimization>',\n",
       "  datetime.date(2014, 5, 5),\n",
       "  '2014-05-05 18:45:42',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '2048.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['576',\n",
       "  '24457374',\n",
       "  'Answer',\n",
       "  'missing column after pandas groupby',\n",
       "  \"You need to include `'name'` in `groupby` by groups:\\n\\n\\n\\n\\tIn [43]:\\n\\n\\t\\n\\n\\tgrouped = df.groupby(['desk_id', 'shift_id', 'shift_hour', 'name']).size()\\n\\n\\tgrouped = grouped.reset_index()\\n\\n\\tgrouped.columns=np.where(grouped.columns==0, 'count', grouped.columns) #replace the default 0 to 'count'\\n\\n\\tprint grouped\\n\\n        desk_id  shift_id  shift_hour        name  count\\n\\n    0  15557987  37423064           0  Adam Scott      1\\n\\n    1  15557987  37423064           2  Adam Scott      3\\n\\n    2  15557987  37423064           3  Adam Scott      1\\n\\n\\n\\nIf the name-to-id relationship is a many-to-one type, say we have a pete scott for the same set of data, the result will become:\\n\\n\\n\\n        desk_id  shift_id  shift_hour        name  count\\n\\n    0  15557987  37423064           0  Adam Scott      1\\n\\n    1  15557987  37423064           0  Pete Scott      1\\n\\n    2  15557987  37423064           2  Adam Scott      3\\n\\n    3  15557987  37423064           2  Pete Scott      3\\n\\n    4  15557987  37423064           3  Adam Scott      1\\n\\n    5  15557987  37423064           3  Pete Scott      1\",\n",
       "  '<python><pandas><group-by><dataframe>',\n",
       "  datetime.date(2014, 6, 27),\n",
       "  '2014-06-27 17:29:53',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '6265.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['577',\n",
       "  '24478521',\n",
       "  'Answer',\n",
       "  'Python - Intersection of 2D Numpy Arrays',\n",
       "  \"This should do it:\\n\\n\\n\\n\\tIn [11]:\\n\\n\\t\\n\\n\\tdef f(arrA, arrB):\\n\\n\\t    return not set(map(tuple, arrA)).isdisjoint(map(tuple, arrB))\\n\\n\\tIn [12]:\\n\\n\\t\\n\\n\\tf(A, B)\\n\\n\\tOut[12]:\\n\\n\\tTrue\\n\\n\\tIn [13]:\\n\\n\\t\\n\\n\\tf(A, C)\\n\\n\\tOut[13]:\\n\\n\\tFalse\\n\\n\\tIn [14]:\\n\\n\\t\\n\\n\\tf(B, C)\\n\\n\\tOut[14]:\\n\\n\\tFalse\\n\\n\\n\\nTo find intersection? OK, `set` sounds like a logical choice.\\n\\nBut `numpy.array` or `list` are not hashable? OK, convert them to `tuple`.\\n\\nThat is the idea.\\n\\n\\n\\nA `numpy` way of doing involves very unreadable boardcasting:\\n\\n\\n\\n\\tIn [34]:\\n\\n\\t\\n\\n\\t(A[...,np.newaxis]==B[...,np.newaxis].T).all(1)\\n\\n\\tOut[34]:\\n\\n\\tarray([[False, False],\\n\\n\\t       [ True, False],\\n\\n\\t       [False, False]], dtype=bool)\\n\\n\\tIn [36]:\\n\\n\\t\\n\\n\\t(A[...,np.newaxis]==B[...,np.newaxis].T).all(1).any()\\n\\n\\tOut[36]:\\n\\n\\tTrue\\n\\n\\n\\nSome timeit result:\\n\\n\\n\\n\\tIn [38]:\\n\\n\\t#Dan's method\\n\\n\\t%timeit set_comp(A,B)\\n\\n\\t10000 loops, best of 3: 34.1 µs per loop\\n\\n\\tIn [39]:\\n\\n\\t#Avoiding lambda will speed things up\\n\\n\\t%timeit f(A,B)\\n\\n\\t10000 loops, best of 3: 23.8 µs per loop\\n\\n\\tIn [40]:\\n\\n\\t#numpy way probably will be slow, unless the size of the array is very big (my guess)\\n\\n\\t%timeit (A[...,np.newaxis]==B[...,np.newaxis].T).all(1).any()\\n\\n\\t10000 loops, best of 3: 49.8 µs per loop\\n\\n\\n\\nAlso the `numpy` method will be RAM hungry, as `A[...,np.newaxis]==B[...,np.newaxis].T` step creates a 3D array.\",\n",
       "  '<python><arrays><numpy><3d>',\n",
       "  datetime.date(2014, 6, 29),\n",
       "  '2014-06-29 18:06:21',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '8002.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['581',\n",
       "  '23618288',\n",
       "  'Answer',\n",
       "  'Force square subplots when plotting a colorbar',\n",
       "  \"To expend my comment that one can make 3 plots, plot the `colorbar()` in the 3rd one, the data plots in the 1st and 2nd. This way, if necessary, we are free to do anything we want to the 1st and 2nd plots:\\n\\n\\n\\n    def rand_data(l, h):\\n\\n        return np.random.uniform(low=l, high=h, size=(100,))\\n\\n    \\n\\n    # Generate data.\\n\\n    x1, x2, y, z = rand_data(0., 1.), rand_data(100., 175.), \\\\\\n\\n    rand_data(150., 200.), rand_data(15., 33.)\\n\\n    \\n\\n    fig = plt.figure(figsize=(12,6))\\n\\n    gs=gridspec.GridSpec(1,3, width_ratios=[4,4,0.2])\\n\\n    ax1 = plt.subplot(gs[0])\\n\\n    ax2 = plt.subplot(gs[1])\\n\\n    ax3 = plt.subplot(gs[2])\\n\\n    cm = plt.cm.get_cmap('RdYlBu')\\n\\n    ax1.scatter(x1, y, c=z, cmap=cm)\\n\\n    SC=ax2.scatter(x2, y, c=z, cmap=cm)\\n\\n    plt.setp(ax2.get_yticklabels(), visible=False)\\n\\n    plt.colorbar(SC, cax=ax3)\\n\\n    plt.tight_layout()\\n\\n    plt.savefig('temp.png')\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/CvT1V.png\",\n",
       "  '<python><matplotlib><plot><subplot><colorbar>',\n",
       "  datetime.date(2014, 5, 12),\n",
       "  '2014-05-13 00:33:40',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1526.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['584',\n",
       "  '24502872',\n",
       "  'Answer',\n",
       "  'Python/matplotlib : getting rid of matplotlib.mpl warning',\n",
       "  'You can suppress that particular warning, which is probably the preferred way:\\n\\n\\n\\n    import warnings\\n\\n    import matplotlib.cbook\\n\\n    warnings.filterwarnings(\"ignore\",category=matplotlib.cbook.mplDeprecation)',\n",
       "  '<python><matplotlib><deprecation-warning>',\n",
       "  datetime.date(2014, 7, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '19',\n",
       "  '',\n",
       "  '9410.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['588',\n",
       "  '24785604',\n",
       "  'Answer',\n",
       "  'View R data examples via RPy (example: lmeSplines)',\n",
       "  \"Don't try to translate `R` code line by line, try to do it in `rpy2` way if we put it so. For example, to access the dataset `smsSplineEx`: use `ro.r.smSplineEx1`\\n\\n\\n\\n\\tIn [34]:\\n\\n\\t\\n\\n\\timport rpy2.robjects as ro\\n\\n\\timport pandas.rpy.common as com\\n\\n\\tmydata = ro.r['data.frame']\\n\\n\\tread = ro.r['read.csv']\\n\\n\\thead = ro.r['head']\\n\\n\\tsummary = ro.r['summary']\\n\\n\\tlibrary = ro.r['library']\\n\\n\\tIn [35]:\\n\\n\\t\\n\\n\\tformula = '~ time'\\n\\n\\tlibrary('lmeSplines')\\n\\n\\tro.reval('smSplineEx1$all <- rep(1,nrow(smSplineEx1))')\\n\\n\\tresult = ro.r.smspline(formula=ro.r(formula), data=ro.r.smSplineEx1) #notice: data=ro.r.smSplineEx1\\n\\n\\tIn [36]:\\n\\n\\t\\n\\n\\tprint com.convert_robj(result).head()\\n\\n\\t\\t\\t 0         1         2         3         4         5         6   \\\\\\n\\n\\t1  1.168560  2.071261  2.944953  3.782848  4.584037  5.348937  6.078121   \\n\\n\\t2  0.148786  1.072013  1.948857  2.789264  3.593423  4.361817  5.095016   \\n\\n\\t3 -0.054492  0.072766  0.952761  1.795679  2.602809  3.374698  4.111911   \\n\\n\\t4 -0.053646 -0.135912 -0.043334  0.802095  1.612194  2.387579  3.128806   \\n\\n\\t5 -0.052799 -0.133771 -0.250619 -0.191489  0.621580  1.400459  2.145701   \\n\\n\\t\\n\\n\\t\\t\\t 7         8         9     ...           88        89        90  \\\\\\n\\n\\t1  6.772184  7.431719  8.057321    ...     0.933947  0.769591  0.619420   \\n\\n\\t2  5.793601  6.458153  7.089255    ...     0.904395  0.745337  0.599976   \\n\\n\\t3  4.815018  5.484587  6.121190    ...     0.874843  0.721083  0.580531   \\n\\n\\t4  3.836434  4.511021  5.153124    ...     0.845291  0.696829  0.561086   \\n\\n\\t5  2.857851  3.537455  4.185059    ...     0.815739  0.672575  0.541641   \\n\\n\\t\\n\\n\\t\\t\\t 91       92        93        94        95        96        97  \\n\\n\\t1  0.484029  0.36401  0.259959  0.172468  0.102133  0.049547  0.015305  \\n\\n\\t2  0.468893  0.35267  0.251890  0.167135  0.098986  0.048026  0.014836  \\n\\n\\t3  0.453756  0.34133  0.243821  0.161801  0.095838  0.046504  0.014368  \\n\\n\\t4  0.438620  0.32999  0.235753  0.156467  0.092691  0.044982  0.013899  \\n\\n\\t5  0.423484  0.31865  0.227684  0.151134  0.089544  0.043461  0.013431  \\n\\n\\t\\n\\n\\t[5 rows x 98 columns]\",\n",
       "  '<python><r><rpy2><spline><nlme>',\n",
       "  datetime.date(2014, 7, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1542.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['591',\n",
       "  '24619962',\n",
       "  'Answer',\n",
       "  'Calculating and plotting count ratios with Pandas',\n",
       "  'Yep, it can be done in a very concise way using the build in `cut` function:\\n\\n\\n\\nIn [65]:\\n\\n\\n\\n    nrows=1000\\n\\n    df=pd.DataFrame([[random.random(), random.random()]+[random.randint(0, 1)] for _ in range(nrows)],\\n\\n                     columns=list(\"ABC\"))\\n\\n    In [66]:\\n\\n    #This does the trick.\\n\\n    pd.crosstab(np.array(pd.cut(df.A, 20)), np.array(pd.cut(df.B, 20))).values\\n\\n    Out[66]:\\n\\n    array([[2, 2, 2, 2, 7, 2, 3, 5, 1, 4, 2, 2, 1, 3, 2, 1, 7, 2, 4, 2],\\n\\n           [1, 2, 4, 2, 0, 3, 3, 3, 1, 1, 2, 1, 4, 3, 2, 1, 1, 2, 2, 1],\\n\\n           [0, 4, 1, 3, 1, 3, 2, 5, 2, 3, 1, 1, 1, 4, 2, 3, 6, 5, 2, 2],\\n\\n           [5, 2, 3, 2, 2, 1, 3, 2, 4, 0, 3, 2, 0, 4, 3, 2, 1, 3, 1, 3],\\n\\n           [2, 2, 4, 1, 3, 2, 2, 4, 1, 4, 3, 5, 5, 2, 3, 3, 0, 2, 4, 0],\\n\\n           [2, 3, 3, 5, 2, 0, 5, 3, 2, 3, 1, 2, 5, 4, 4, 3, 4, 3, 6, 4],\\n\\n           [3, 2, 2, 4, 3, 3, 2, 0, 0, 4, 3, 2, 2, 5, 4, 0, 1, 2, 2, 3],\\n\\n           [0, 0, 4, 4, 3, 2, 4, 6, 4, 2, 0, 5, 2, 2, 1, 3, 4, 4, 3, 2],\\n\\n           [3, 2, 2, 3, 4, 2, 1, 3, 1, 3, 4, 2, 4, 3, 2, 3, 2, 3, 4, 4],\\n\\n           [0, 1, 1, 4, 1, 4, 3, 0, 1, 1, 1, 2, 6, 4, 3, 5, 3, 3, 1, 4],\\n\\n           [2, 2, 4, 1, 3, 4, 1, 2, 1, 3, 3, 3, 1, 2, 1, 5, 2, 1, 4, 3],\\n\\n           [0, 0, 0, 4, 2, 0, 2, 3, 2, 2, 2, 4, 4, 2, 3, 2, 1, 2, 1, 0],\\n\\n           [3, 3, 0, 3, 1, 5, 1, 1, 2, 5, 6, 5, 0, 0, 3, 2, 1, 5, 7, 2],\\n\\n           [3, 3, 2, 1, 2, 2, 2, 2, 4, 0, 1, 3, 3, 1, 5, 6, 1, 3, 2, 2],\\n\\n           [3, 0, 3, 4, 3, 2, 1, 4, 2, 3, 4, 0, 5, 3, 2, 2, 4, 3, 0, 2],\\n\\n           [0, 3, 2, 2, 1, 5, 1, 4, 3, 1, 2, 2, 3, 5, 1, 2, 2, 2, 1, 2],\\n\\n           [1, 3, 2, 1, 1, 4, 4, 3, 2, 2, 5, 5, 1, 0, 1, 0, 4, 3, 3, 2],\\n\\n           [2, 2, 2, 1, 1, 3, 1, 6, 5, 2, 5, 2, 3, 4, 2, 2, 1, 1, 4, 0],\\n\\n           [3, 3, 4, 7, 0, 2, 6, 4, 1, 3, 4, 4, 1, 4, 1, 1, 2, 1, 3, 2],\\n\\n           [3, 6, 3, 4, 1, 3, 1, 3, 3, 1, 6, 2, 2, 2, 1, 1, 4, 4, 0, 4]])\\n\\n    In [67]:\\n\\n\\n\\n    abins=np.linspace(df.A.min(), df.A.max(), 21)\\n\\n    bbins=np.linspace(df.B.min(), df.B.max(), 21)\\n\\n    Z=pd.crosstab(np.array(pd.cut(df.ix[df.C==1, \\'A\\'], abins)), \\n\\n                np.array(pd.cut(df.ix[df.C==1, \\'B\\'], bbins)), aggfunc=np.mean).div(\\n\\n                pd.crosstab(np.array(pd.cut(df.A, abins)), \\n\\n                            np.array(pd.cut(df.B, bbins)), aggfunc=np.mean)).values\\n\\n    Z = np.ma.masked_where(np.isinf(Z),Z)\\n\\n    x=np.linspace(df.A.min(), df.A.max(), 20)\\n\\n    y=np.linspace(df.B.min(), df.B.max(), 20)\\n\\n    X,Y=np.meshgrid(x, y)\\n\\n    plt.contourf(X, Y, Z, vmin=0, vmax=1)\\n\\n    plt.colorbar()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n    plt.pcolormesh(X, Y, Z, vmin=0, vmax=1)\\n\\n    plt.colorbar()\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Q9Jzt.png\\n\\n  [2]: http://i.stack.imgur.com/D0QYb.png',\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2014, 7, 7),\n",
       "  '2014-07-08 20:43:03',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1068.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['592',\n",
       "  '29377008',\n",
       "  'Answer',\n",
       "  'Missing values replace by med/mean in conti var, by mode in categorical var in pandas dataframe -after grouping the data by a column)',\n",
       "  \"If I understand correctly, this is mostly in the [documentation][1], but probably not where you'd be looking if you're asking the question.  See note regarding ```mode``` at the bottom as it is slightly trickier than ```mean``` and ```median```.\\n\\n\\n\\n    df = pd.DataFrame({ 'v':[1,2,2,np.nan,3,4,4,np.nan] }, index=[1,1,1,1,2,2,2,2],)\\n\\n    \\n\\n    df['v_mean'] = df.groupby(level=0)['v'].transform( lambda x: x.fillna(x.mean()))\\n\\n    df['v_med' ] = df.groupby(level=0)['v'].transform( lambda x: x.fillna(x.median()))\\n\\n    df['v_mode'] = df.groupby(level=0)['v'].transform( lambda x: x.fillna(x.mode()[0]))\\n\\n    \\n\\n    df\\n\\n        v    v_mean  v_med  v_mode\\n\\n    1   1  1.000000      1       1\\n\\n    1   2  2.000000      2       2\\n\\n    1   2  2.000000      2       2\\n\\n    1 NaN  1.666667      2       2\\n\\n    2   3  3.000000      3       3\\n\\n    2   4  4.000000      4       4\\n\\n    2   4  4.000000      4       4\\n\\n    2 NaN  3.666667      4       4\\n\\n\\n\\nNote that ```mode()``` may not be unique, unlike ```mean``` and ```median``` and pandas returns it as a ```Series``` for that reason.  To deal with that, I just took the simplest route and added ```[0]``` in order to extract the first member of the series.\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/dev/groupby.html#transformation\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 3, 31),\n",
       "  '2015-03-31 20:26:08',\n",
       "  'JohnE (3877338)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2198.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['596',\n",
       "  '25169616',\n",
       "  'Answer',\n",
       "  'Do matplotlib.contourf levels depend on the amount of colors in the colormap?',\n",
       "  \"Yes, it is a discrete colormap, and if you want to have a continuos one you need to make a customized colormap.\\n\\n\\n\\n    #the colormap data can be found here: https://github.com/SciTools/iris/blob/master/lib/iris/etc/palette/sequential/Reds_09.txt\\n\\n\\n\\n\\tIn [22]:\\n\\n\\t\\n\\n\\t%%file temp.txt\\n\\n\\t1.000000 0.960784 0.941176\\n\\n\\t0.996078 0.878431 0.823529\\n\\n\\t0.988235 0.733333 0.631373\\n\\n\\t0.988235 0.572549 0.447059\\n\\n\\t0.984314 0.415686 0.290196\\n\\n\\t0.937255 0.231373 0.172549\\n\\n\\t0.796078 0.094118 0.113725\\n\\n\\t0.647059 0.058824 0.082353\\n\\n\\t0.403922 0.000000 0.050980\\n\\n\\tOverwriting temp.txt\\n\\n\\tIn [23]:\\n\\n\\t\\n\\n\\tc_array = np.genfromtxt('temp.txt')\\n\\n\\tfrom matplotlib.colors import LinearSegmentedColormap\\n\\n\\tplt.register_cmap(name='Test', data={key: tuple(zip(np.linspace(0,1,c_array.shape[0]), c_array[:,i], c_array[:,i])) \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t for key, i in zip(['red','green','blue'], (0,1,2))})\\n\\n\\tIn [24]:\\n\\n\\t\\n\\n\\tplt.contourf(X, Y, Z, 50, cmap=plt.get_cmap('Test'))\\n\\n\\tplt.colorbar()\\n\\n\\tOut[24]:\\n\\n\\t<matplotlib.colorbar.Colorbar instance at 0x108948320>\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/NyqT0.png\",\n",
       "  '<python><matplotlib><colorbrewer>',\n",
       "  datetime.date(2014, 8, 6),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '5654.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['597',\n",
       "  '24784519',\n",
       "  'Answer',\n",
       "  'Convert R Matrix to Pandas Dataframe',\n",
       "  \"I think you are getting confused about the difference between `convert_to_r_dataframe` and `convert_robj`. Use the former one for converting **TO** `R`, and the latter one for converting **BACK** from `R`:\\n\\n\\n\\n\\tIn [30]:\\n\\n\\tfrom rpy2 import robjects\\n\\n\\tm=robjects.r('matrix(1:6, nrow=2, ncol=3)')\\n\\n\\tIn [31]:\\n\\n\\t\\n\\n\\tprint com.convert_robj(m)\\n\\n\\t   0  1  2\\n\\n\\t1  1  3  5\\n\\n\\t2  2  4  6\\n\\n\\tIn [32]:\\n\\n\\t\\n\\n\\tm=robjects.r('as.data.frame(matrix(1:6, nrow=2, ncol=3, dimnames=list(1:2, 1:3)))')\\n\\n\\tIn [33]:\\n\\n\\t\\n\\n\\tprint com.convert_robj(m)\\n\\n\\t   1  2  3\\n\\n\\t1  1  3  5\\n\\n\\t2  2  4  6\",\n",
       "  '<python><r><matrix><pandas><rpy2>',\n",
       "  datetime.date(2014, 7, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1320.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['600',\n",
       "  '25044776',\n",
       "  'Answer',\n",
       "  'Issues creating a skew normal distribution by subclassing scipy.stats.rv_continuous',\n",
       "  \"Can not replicate the error, see:\\n\\n\\n\\n\\tIn [15]:\\n\\n\\timport scipy.stats as ss\\n\\n\\tclass skew_norm_gen(ss.rv_continuous):\\n\\n\\t\\tdef _pdf(self, x, s):\\n\\n\\t\\t\\treturn 2 * ss.norm.pdf(x) * ss.norm.cdf(x * s)\\n\\n\\tskew_norm = skew_norm_gen(name='skew_norm', shapes='s')\\n\\n\\n\\n\\tIn [17]:\\n\\n\\tskew_norm.pdf(3, 4)\\n\\n\\tOut[17]:\\n\\n\\t0.0088636968238760151\\n\\n\\n\\nYes you can pass additional `*args`:\\n\\n\\n\\n\\tIn [18]:\\n\\n\\t\\n\\n\\tclass skew_norm_gen(ss.rv_continuous):\\n\\n\\t\\tdef _pdf(self, x, s, *args):\\n\\n\\t\\t\\treturn 2 * ss.norm.pdf(x, *args) * ss.norm.cdf(x * s, *args)\\n\\n\\tskew_norm = skew_norm_gen(name='skew_norm', shapes='s')\\n\\n\\n\\n\\tIn [20]:\\n\\n\\tskew_norm.pdf(3, 4, loc=0.5, scale=3)\\n\\n\\tOut[20]:\\n\\n\\t0.18786061213807126\\n\\n\\n\\n\\tIn [21]:\\n\\n\\tskew_norm.pdf(3, s=4, loc=0.5, scale=3)\\n\\n\\tOut[21]:\\n\\n\\t0.18786061213807126\\n\\n\\tIn [22]:\\n\\n\\t\\n\\n\\tskew_norm.pdf(3, s=4, loc=0, scale=1)\\n\\n\\tOut[22]:\\n\\n\\t0.0088636968238760151\\n\\n\\tIn [28]:\\n\\n\\tplt.plot(np.linspace(-5, 5), skew_norm.pdf(np.linspace(-5,5),4), label='Skewed')\\n\\n\\tplt.plot(np.linspace(-5, 5), ss.norm.pdf(np.linspace(-5,5)), label='Normal')\\n\\n\\tplt.legend()\\t\\n\\n\\tOut[28]:\\n\\n\\t[<matplotlib.lines.Line2D at 0x1092667d0>]\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n#Edit:\\n\\n\\n\\nIn your example data, the `s` is negative, which causes resulting pdf to contain only `nan`, the default `badvalue` (I think that what is called) defined by `rv_continuous`. \\n\\n\\n\\nThe root of the problem is: there is a default `_argcheck()` method, to verify if the parameter(s) is/are valid. The default is to check if all the parameters are >0. In this case, it is not.\\n\\n\\n\\nSo the solution, is to overwrite the default `_argchek()` method, by:\\n\\n\\n\\n    class skew_norm_gen(ss.rv_continuous):\\n\\n        def _argcheck(self, skew):\\n\\n            return np.isfinite(skew) #I guess we can confine it to finite value\\n\\n        def _pdf(self, x, skew):\\n\\n            return 2 * ss.norm.pdf(x) * ss.norm.cdf(x * skew)  \\n\\n\\n\\nAnd then it should work fine.\\n\\n\\n\\n(Alos I will suggest call the additional parameter `skew`, just for readability. 's' could mean, say, standard deviation. etc.)\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/JkgRh.png\",\n",
       "  '<python><statistics><scipy>',\n",
       "  datetime.date(2014, 7, 30),\n",
       "  '2014-07-30 20:02:15',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1776.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['602',\n",
       "  '24658692',\n",
       "  'Answer',\n",
       "  'Groupby given percentiles of the values of the chosen DataFrame column',\n",
       "  \"I don't have a computer to test it right now, but I think you can do it by: `df.groupby(pd.cut(df.col0, np.percentile(df.col0, [0, 25, 75, 90, 100]), include_lowest=True)).mean()`. Will update after 150mins.\\n\\n\\n\\nSome explanations:\\n\\n\\n\\n\\tIn [42]:\\n\\n\\t#use np.percentile to get the bin edges of any percentile you want \\n\\n\\tnp.percentile(df.col0, [0, 25, 75, 90, 100])\\n\\n\\tOut[42]:\\n\\n\\t[0.0067930000000000004,\\n\\n\\t 0.907609,\\n\\n\\t 3.7436589999999996,\\n\\n\\t 13.089311200000001,\\n\\n\\t 19.319745999999999]\\n\\n\\tIn [43]:\\n\\n\\t#Need to use include_lowest=True\\n\\n\\tprint df.groupby(pd.cut(df.col0, np.percentile(df.col0, [0, 25, 75, 90, 100]), include_lowest=True)).mean()\\n\\n\\t\\t\\t\\t\\t\\t   col0     col1      col2\\n\\n\\tcol0                                          \\n\\n\\t[0.00679, 0.908]   0.457201     41.0  2.103996\\n\\n\\t(0.908, 3.744]     3.051177    923.5  5.790717\\n\\n\\t(3.744, 13.0893]        NaN      NaN       NaN\\n\\n\\t(13.0893, 19.32]  19.319746  11969.0  7.405685\\n\\n\\tIn [44]:\\n\\n\\t#Or the smallest values will be skiped\\n\\n\\tprint df.groupby(pd.cut(df.col0, np.percentile(df.col0, [0, 25, 75, 90, 100]))).mean()\\n\\n\\t\\t\\t\\t\\t\\t   col0     col1      col2\\n\\n\\tcol0                                          \\n\\n\\t(0.00679, 0.908]   0.907609     82.0  4.207991\\n\\n\\t(0.908, 3.744]     3.051177    923.5  5.790717\\n\\n\\t(3.744, 13.0893]        NaN      NaN       NaN\\n\\n\\t(13.0893, 19.32]  19.319746  11969.0  7.405685\\n\\n\\n\\n\",\n",
       "  '<python><pandas><group-by>',\n",
       "  datetime.date(2014, 7, 9),\n",
       "  '2014-07-09 17:48:07',\n",
       "  'CT Zhu (2487184)',\n",
       "  '9',\n",
       "  '',\n",
       "  '3594.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['603',\n",
       "  '23962414',\n",
       "  'Answer',\n",
       "  'matplotlib pyplot side-by-side graphics',\n",
       "  'I think `ax[0] = ppl.scatter(ypos,xpos,label=each)` should be `ax[0].scatter(ypos,xpos,label=each)` and `ax[1] = ppl.scatter(ypos,xpos,label=each)` should be `ax[1].scatter(ypos,xpos,label=each)`, change those and see if your problem get solved.\\n\\n\\n\\nI am quite sure that the issue is: you are calling `ppl.scatter(...)`, which will try to draw on the current axis, which is the 1st axes of 2 axes you generated (and it is the left one)\\n\\n\\n\\nAlso you may find that in the end, the `ax` list contains two `matplotlib.collections.PathCollection`s, bot two `axis` as you may expect.\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\nSince the solution above removes the prettiness of `prettyplot`, we shall use an alternative solution, which is to change the current working axis, by adding:\\n\\n\\n\\n    plt.sca(ax[0_or_1])\\n\\n\\n\\nBefore `ppl.scatter()`, inside each loop.',\n",
       "  '<numpy><matplotlib><prettyplotlib>',\n",
       "  datetime.date(2014, 5, 30),\n",
       "  '2014-05-30 20:29:52',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1209.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['608',\n",
       "  '25206653',\n",
       "  'Answer',\n",
       "  'Creating DataFrame from ElasticSearch Results',\n",
       "  \"There is a nice toy called `pd.DataFrame.from_dict` that you can use in situation like this:\\n\\n\\n\\n\\tIn [34]:\\n\\n\\t\\n\\n\\tData = [{u'_id': u'a1XHMhdHQB2uV7oq6dUldg',\\n\\n\\t\\t  u'_index': u'logstash-2014.08.07',\\n\\n\\t\\t  u'_score': 1.0,\\n\\n\\t\\t  u'_type': u'logs',\\n\\n\\t\\t  u'fields': {u'@timestamp': u'2014-08-07T12:36:00.086Z',\\n\\n\\t\\t   u'path': u'app2.log'}},\\n\\n\\t\\t {u'_id': u'TcBvro_1QMqF4ORC-XlAPQ',\\n\\n\\t\\t  u'_index': u'logstash-2014.08.07',\\n\\n\\t\\t  u'_score': 1.0,\\n\\n\\t\\t  u'_type': u'logs',\\n\\n\\t\\t  u'fields': {u'@timestamp': u'2014-08-07T12:36:00.200Z',\\n\\n\\t\\t   u'path': u'app1.log'}}]\\n\\n\\tIn [35]:\\n\\n\\t\\n\\n\\tdf = pd.concat(map(pd.DataFrame.from_dict, Data), axis=1)['fields'].T\\n\\n\\tIn [36]:\\n\\n\\t\\n\\n\\tprint df.reset_index(drop=True)\\n\\n\\t\\t\\t\\t\\t @timestamp      path\\n\\n\\t0  2014-08-07T12:36:00.086Z  app2.log\\n\\n\\t1  2014-08-07T12:36:00.200Z  app1.log\\n\\n\\n\\nShow it in four steps:\\n\\n\\n\\n1, Read each item in the list (which is a `dictionary`) into a `DataFrame`\\n\\n\\n\\n2, We can put all the items in the list into a big `DataFrame` by `concat` them row-wise, since we will do step#1 for each item, we can use `map` to do it.\\n\\n\\n\\n3, Then we access the columns labeled with `'fields'`\\n\\n\\n\\n4, We probably want to rotate the `DataFrame` 90 degrees (transpose) and `reset_index` if we want the index to be the default `int` sequence.\\n\\n\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/R5dKa.png\",\n",
       "  '<python><pandas><elasticsearch>',\n",
       "  datetime.date(2014, 8, 8),\n",
       "  '2014-08-08 15:23:05',\n",
       "  'CT Zhu (2487184)',\n",
       "  '10',\n",
       "  '',\n",
       "  '10573.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['615',\n",
       "  '24092962',\n",
       "  'Answer',\n",
       "  'pandas read_csv with final column containing commas',\n",
       "  'If it feasible for you to replace `{` with `\"{`, and `}` with `}\"`, it can be read correctly by: `pd.read_csv(\\'data/training.dat\\',quotechar=\\'\"\\',skipinitialspace=True)`\\n\\n\\n\\n#Edit:\\n\\nOr go for a regular expression based solution:\\n\\n\\n\\n    In [205]:\\n\\n    print pd.read_csv(\\'a.data\\',sep=\",(?![^{]*\\\\})\", header=None)\\n\\n       0  1  2  3              4\\n\\n    0  A  B  C  D              E\\n\\n    1  1  2  3  4  {K1:V1,K2:V2}\\n\\n    \\n\\n    [2 rows x 5 columns]',\n",
       "  '<python><json><csv><pandas>',\n",
       "  datetime.date(2014, 6, 7),\n",
       "  '2014-06-07 14:44:27',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1437.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['617',\n",
       "  '24690145',\n",
       "  'Answer',\n",
       "  'Time-series plotting inconsistencies in Pandas',\n",
       "  \"No, actually even the line plot is not working correctly, if you have the year show up, you will notice the problem: instead of being 2000 in the following example, the xticks are in 1989.\\n\\n\\n\\n\\tIn [49]:\\n\\n\\tdf=pd.DataFrame({'Val': np.random.random(50)})\\n\\n\\tdf.index=pd.date_range('2000-01-02', periods=50)\\n\\n\\tf     = plt.figure()\\n\\n\\tax    = f.add_subplot(1,1,1)\\n\\n\\tlines = df.plot(ax=ax)\\n\\n\\tax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%y%b\\\\n%d'))\\n\\n\\tprint ax.get_xlim()\\n\\n\\t(10958.0, 11007.0)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\tIn [50]:\\n\\n\\tmatplotlib.dates.strpdate2num('%Y-%M-%d')('2000-01-02')\\n\\n\\tOut[50]:\\n\\n\\t730121.0006944444\\n\\n\\tIn [51]:\\n\\n\\tmatplotlib.dates.num2date(730121.0006944444)\\n\\n\\tOut[51]:\\n\\n\\tdatetime.datetime(2000, 1, 2, 0, 1, tzinfo=<matplotlib.dates._UTC object at 0x051FA9F0>)\\n\\n\\n\\nTurns out datetime data is handled differently in `pandas` and `matplotlib`: in the latter, `2000-1-2` should be `730121.0006944444`, instead of `10958.0` in `pandas`\\n\\n\\n\\nTo get it right we need to avoid using `pandas`'s `plot` method:\\n\\n\\n\\n\\tIn [52]:\\n\\n\\tplt.plot_date(df.index.to_pydatetime(), df.Val, fmt='-')\\n\\n\\tax=plt.gca()\\n\\n\\tax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%y%b\\\\n%d'))\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\nSimilarly for `barplot`:\\n\\n\\n\\n\\tIn [53]:\\n\\n\\tplt.bar(df.index.to_pydatetime(), df.Val, width=0.4)\\n\\n\\tax=plt.gca()\\n\\n\\tax.xaxis.set_major_formatter(matplotlib.dates.DateFormatter('%y%b\\\\n%d'))\\n\\n\\n\\n![enter image description here][3]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/0RIxy.png\\n\\n  [2]: http://i.stack.imgur.com/u665r.png\\n\\n  [3]: http://i.stack.imgur.com/iafsi.png\",\n",
       "  '<python><matplotlib><pandas>',\n",
       "  datetime.date(2014, 7, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '1547.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['618',\n",
       "  '24811169',\n",
       "  'Answer',\n",
       "  'How do we pass two datasets in scipy.stats.anderson_ksamp?Can anyone explain with an example?',\n",
       "  'Put all of the groups into one `list` (be it two arrays or 4 arrays in this example), and pass that to `scipy.stats.anderson_ksamp`\\n\\n\\n\\n\\tIn [12]:\\n\\n\\t\\n\\n\\timport scipy.stats as ss\\n\\n\\t#data from From the example given by Scholz and Stephens (1987, p.922)\\n\\n\\tx1=[38.7,  41.5,  43.8,  44.5,  45.5,  46.0,  47.7,  58.0]\\n\\n\\tx2=[39.2,  39.3,  39.7,  41.4,  41.8,  42.9,  43.3,  45.8]\\n\\n\\tx3=[34.0,  35.0,  39.0,  40.0,  43.0,  43.0,  44.0,  45.0]\\n\\n\\tx4=[34.0,  34.8,  34.8,  35.4,  37.2,  37.8,  41.2,  42.8]\\n\\n\\tss.anderson_ksamp([x1,x2,x3,x4])\\n\\n\\tOut[12]:\\n\\n\\t(4.4797806271353506,\\n\\n\\t array([ 0.49854918,  1.3236709 ,  1.91577682,  2.49304213,  3.24593219]),\\n\\n\\t 0.0020491057074350956)\\n\\n\\n\\nIt returns 3 values, 1: Normalized k-sample Anderson-Darling test statistic., 2: The critical values for significance levels 25%, 10%, 5%, 2.5%, 1%., 3: the p-values.\\n\\n\\n\\nIn this example, p values is 0.002, we conclude the samples are drawn from different populations. ',\n",
       "  '<python><statistics><scipy><statsmodels><chi-squared>',\n",
       "  datetime.date(2014, 7, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1523.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['620',\n",
       "  '25142023',\n",
       "  'Answer',\n",
       "  'How to truncate a numpy/scipy exponential distribution in an efficient way?',\n",
       "  \"There are two ways to do this:\\n\\n\\n\\nThe first is to generate an exponentially distributed random variable and then limit the values into (1,10).\\n\\n\\n\\n\\tIn [14]:\\n\\n\\t\\n\\n\\timport matplotlib.pyplot as plt\\n\\n\\timport scipy.stats as ss\\n\\n\\tLambda = 2.5 #expected mean of exponential distribution is lambda in Scipy's parameterization\\n\\n\\tSize = 1000\\n\\n\\ttrc_ex_rv = ss.expon.rvs(scale=Lambda, size=Size)\\n\\n\\ttrc_ex_rv = trc_ex_rv[(trc_ex_rv>1)&(trc_ex_rv<10)]\\n\\n\\tIn [15]:\\n\\n\\t\\n\\n\\tplt.hist(trc_ex_rv)\\n\\n\\tplt.xlim(0, 12)\\n\\n\\tOut[15]:\\n\\n\\t(0, 12)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\t\\n\\n\\tIn [16]:\\n\\n\\t\\n\\n\\ttrc_ex_rv\\n\\n\\tOut[16]:\\n\\n\\tarray([...]) #a lot of numbers\\n\\n\\n\\nOf course, the problem is you are not going to get the exact number of random numbers (defined by `Size` here).\\n\\n\\n\\nThe other way to do it is to use [Inverse transform sampling][2], and you will get the exact number of replicates as specified:\\n\\n\\n\\n\\tIn [17]:\\n\\n\\timport numpy as np\\n\\n\\tdef trunc_exp_rv(low, high, scale, size):\\n\\n\\t\\trnd_cdf = np.random.uniform(ss.expon.cdf(x=low, scale=scale),\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tss.expon.cdf(x=high, scale=scale),\\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\tsize=size)\\n\\n\\t\\treturn ss.expon.ppf(q=rnd_cdf, scale=scale)\\n\\n\\tIn [18]:\\n\\n\\t\\n\\n\\tplt.hist(trunc_exp_rv(1, 10, Lambda, Size))\\n\\n\\tplt.xlim(0, 12)\\n\\n\\tOut[18]:\\n\\n\\t(0, 12)\\n\\n\\n\\n![enter image description here][3]\\n\\n\\n\\nIf you want the resulting bounded distribution to have an expected mean of a given value, say `2.5`, you need to solve for the scale parameter that resulting the expected mean.\\n\\n\\n\\n    import scipy.optimize as so\\n\\n    def solve_for_l(low, high, ept_mean):\\n\\n        A = np.array([low, high])\\n\\n        return 1/so.fmin(lambda L: ((np.diff(np.exp(-A*L)*(A*L+1)/L)/np.diff(np.exp(-A*L)))-ept_mean)**2,\\n\\n                         x0=0.5,\\n\\n                         full_output=False, disp=False)\\n\\n    def F(low, high, ept_mean, size):\\n\\n        return trunc_exp_rv(low, high,\\n\\n                            solve_for_l(low, high, ept_mean),\\n\\n                            size)\\n\\n    rv_data = F(1, 10, 2.5, 1e5)\\n\\n    plt.hist(rv_data, bins=50)\\n\\n    plt.xlim(0, 12)\\n\\n    print rv_data.mean()\\n\\n\\n\\nResult:\\n\\n\\n\\n    2.50386617882\\n\\n\\n\\n![enter image description here][4]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/diTca.png\\n\\n  [2]: http://en.wikipedia.org/wiki/Inverse_transform_sampling\\n\\n  [3]: http://i.stack.imgur.com/MgfMu.png\\n\\n  [4]: http://i.stack.imgur.com/Au5IC.png\",\n",
       "  '<python><statistics><scipy><distribution>',\n",
       "  datetime.date(2014, 8, 5),\n",
       "  '2014-08-05 20:54:57',\n",
       "  'CT Zhu (2487184)',\n",
       "  '9',\n",
       "  '',\n",
       "  '1817.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['623',\n",
       "  '25225009',\n",
       "  'Answer',\n",
       "  'Filtering muliple items in a multi-index Python Panda dataframe',\n",
       "  \"You can `get_level_values` in conjunction with Boolean slicing.\\n\\n\\n\\n\\tIn [50]:\\n\\n\\t\\n\\n\\tprint df[np.in1d(df.index.get_level_values(1), ['Lake', 'River', 'Upland'])]\\n\\n\\t                          Area\\n\\n\\tNSRCODE PBL_AWI               \\n\\n\\tCM      Lake      57124.819333\\n\\n\\t        River      1603.906642\\n\\n\\tLBH     Lake     258046.508310\\n\\n\\t        River     44262.807900\\n\\n\\n\\nThe same idea can be expressed in many different ways, such as `df[df.index.get_level_values('PBL_AWI').isin(['Lake', 'River', 'Upland'])]`\\n\\n\\n\\nNote that you have `'upland'` in your data instead of `'Upland'`\",\n",
       "  '<python><filter><pandas><indexing>',\n",
       "  datetime.date(2014, 8, 10),\n",
       "  '2014-08-10 02:41:59',\n",
       "  'CT Zhu (2487184)',\n",
       "  '34',\n",
       "  '',\n",
       "  '11502.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['624',\n",
       "  '24109818',\n",
       "  'Answer',\n",
       "  'numpy.genfromtxt imports tuples instead of arrays',\n",
       "  \"You get confused about how to use `skip_header` and `names`. The right way to read the data, and use the first row as variable names is:\\n\\n\\n\\n\\tIn [185]:\\n\\n\\t\\n\\n\\tnp.genfromtxt('temp.csv', delimiter=',', \\\\\\n\\n\\t                          missing_values=0,skip_header=0,dtype=float,\\\\\\n\\n\\t                          usecols=(0,2,3,4,5,6,7,8,9,10,11,17),names=True)\\n\\n\\tOut[185]:\\n\\n\\tarray([ (0.016666668, 4.3555064, 0.0, 0.002, 0.0, 118.0, 1.0, 684.3, 0.0, 0.0, 14.71, -1.0),\\n\\n\\t       (0.033333335, 4.3555064, 20.0, 0.002, 0.0, 119.0, 1.0, 684.3, 0.0, 0.0, 14.71, -1.0),\\n\\n\\t       (0.05, 4.444291, 13.0, 0.004, 0.0, 119.0, 1.0, 684.3, 0.0, 0.0, 14.71, -1.0)], \\n\\n\\t      dtype=[('Timemin', '<f8'), ('Speed', '<f8'), ('Power', '<f8'), ('Distance', '<f8'), ('Rpm', '<f8'), ('Bpm', '<f8'), ('interval', '<f8'), ('Altitude', '<f8'), ('Rate', '<f8'), ('Incline', '<f8'), ('Temp', '<f8'), ('getCombinedPedalSmoothness', '<f8')])\\n\\n\\n\\nIt is not a array of `tuple`, but a `structured array`. `skip_header=1` will result using the first row of data as names, which is probably not what you want (see how you are missing the first line of data?).\\n\\n\\n\\nYou can also get rid of the names and read the data into a ordinary `numpy` `array`. \\n\\n\\n\\n\\tIn [186]:\\n\\n\\t\\n\\n\\tnp.genfromtxt('temp.csv', delimiter=',', \\\\\\n\\n\\t                          missing_values=0,skip_header=1,dtype=float,\\\\\\n\\n\\t                          usecols=(0,2,3,4,5,6,7,8,9,10,11,17))\\n\\n\\tOut[186]:\\n\\n\\tarray([[  1.66666680e-02,   4.35550640e+00,   0.00000000e+00,\\n\\n\\t          2.00000000e-03,   0.00000000e+00,   1.18000000e+02,\\n\\n\\t          1.00000000e+00,   6.84300000e+02,   0.00000000e+00,\\n\\n\\t          0.00000000e+00,   1.47100000e+01,  -1.00000000e+00],\\n\\n\\t       [  3.33333350e-02,   4.35550640e+00,   2.00000000e+01,\\n\\n\\t          2.00000000e-03,   0.00000000e+00,   1.19000000e+02,\\n\\n\\t          1.00000000e+00,   6.84300000e+02,   0.00000000e+00,\\n\\n\\t          0.00000000e+00,   1.47100000e+01,  -1.00000000e+00],\\n\\n\\t       [  5.00000000e-02,   4.44429100e+00,   1.30000000e+01,\\n\\n\\t          4.00000000e-03,   0.00000000e+00,   1.19000000e+02,\\n\\n\\t          1.00000000e+00,   6.84300000e+02,   0.00000000e+00,\\n\\n\\t          0.00000000e+00,   1.47100000e+01,  -1.00000000e+00]])\\n\\n\",\n",
       "  '<python><arrays><csv><numpy><matrix>',\n",
       "  datetime.date(2014, 6, 8),\n",
       "  '2014-06-08 19:18:51',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1971.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['627',\n",
       "  '25289545',\n",
       "  'Answer',\n",
       "  'scipy linregress: computing only scaling/slope parameter with intercept fixed at 0',\n",
       "  'If you want to fit a model, `y~xi` without intercept, you may want to consider using more statistic oriented package such as `statsmodels`:\\n\\n\\n\\n    In [17]:\\n\\n   \\n\\n    import statsmodels.api as sm\\n\\n    import numpy as np\\n\\n    y = [0, 11, 19, 28, 41, 49, 62, 75, 81]\\n\\n    xi = np.arange(0,9)\\n\\n    model = sm.OLS(y, xi)\\n\\n    results = model.fit()\\n\\n    In [18]:\\n\\n    \\n\\n    print results.params\\n\\n    [ 10.23039216]\\n\\n\\n\\nYou can verify the result independently using `R`. Only that now you have to explicitly specify the intercept to be 0:\\n\\n\\n\\n\\n\\n    x <- c(0, 1, 2, 3, 4, 5, 6, 7, 8)\\n\\n    y <- c(0, 11, 19, 28, 41, 49, 62, 75, 81)\\n\\n    model1 <- lm(y~x+0)\\n\\n    summary(model1)\\n\\n\\n\\n    Call:\\n\\n    lm(formula = y ~ x + 0)\\n\\n\\n\\n    Residuals:\\n\\n        Min      1Q  Median      3Q     Max \\n\\n    -2.6912 -1.4608  0.0000  0.6176  3.3873 \\n\\n\\n\\n    Coefficients:\\n\\n      Estimate Std. Error t value Pr(>|t|)    \\n\\n    x   10.230      0.129   79.29 7.14e-13 ***\\n\\n    ---\\n\\n    Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1\\n\\n\\n\\n    Residual standard error: 1.843 on 8 degrees of freedom\\n\\n    Multiple R-squared:  0.9987,\\tAdjusted R-squared:  0.9986 \\n\\n    F-statistic:  6286 on 1 and 8 DF,  p-value: 7.14e-13\\n\\n\\n\\nThe calculation under the hood is simple:\\n\\n\\n\\n    In [29]:\\n\\n\\n\\n    import scipy.optimize as so\\n\\n    so.fmin(lambda b, x, y: ((b*x-y)**2).sum(), x0=0.1, args=(xi, y))\\n\\n    Optimization terminated successfully.\\n\\n             Current function value: 27.171569\\n\\n             Iterations: 27\\n\\n             Function evaluations: 54\\n\\n    Out[29]:\\n\\n    array([ 10.23039063])\\n\\n',\n",
       "  '<python><scipy><linear-regression>',\n",
       "  datetime.date(2014, 8, 13),\n",
       "  '2014-08-13 15:06:30',\n",
       "  'CT Zhu (2487184)',\n",
       "  '6',\n",
       "  '',\n",
       "  '6433.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['628',\n",
       "  '25315073',\n",
       "  'Answer',\n",
       "  'Definite integral over one variable in a function with two variables in Scipy',\n",
       "  'You probably just want the result to be a function of `y` right?:\\n\\n\\n\\n    from scipy.integrate import quad\\n\\n    import numpy as np\\n\\n    def integrand(x,y):\\n\\n        return x*np.exp(x/y)\\n\\n\\n\\n    partial_int = lambda y: quad(integrand, 1,2, args=(y,))\\n\\n    print partial_int(5)\\n\\n    #(2.050684698584342, 2.2767173686148355e-14)',\n",
       "  '<python><scipy><integrate><quad>',\n",
       "  datetime.date(2014, 8, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '4290.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['631',\n",
       "  '29779671',\n",
       "  'Answer',\n",
       "  'Aggregate all dataframe row pair combinations using pandas',\n",
       "  \"Before going too far, you should keep in mind your data gets big pretty fast.  With 5 rows, output will be ```C(5,2)``` or ```5+4+3+2+1``` and so on.\\n\\n\\n\\nThat said, I'd think about doing this in numpy for speed (you may want to add a numpy tag to your question btw).  Anyway, this isn't as vectorized as it might be, but ought to be a start at least:\\n\\n\\n\\n    df2 = df.set_index('Gene').loc[mygenes].reset_index()\\n\\n    \\n\\n    import math\\n\\n    sz = len(df2)\\n\\n    sz2 = math.factorial(sz) / ( math.factorial(sz-2) * 2 )\\n\\n    \\n\\n    Gene = df2['Gene'].tolist()\\n\\n    abc = df2.ix[:,1:].values\\n\\n    \\n\\n    import math\\n\\n    arr = np.zeros([sz2,4])\\n\\n    gene2 = []\\n\\n    k = 0\\n\\n    \\n\\n    for i in range(sz):\\n\\n        for j in range(sz):\\n\\n            if i != j and i < j:\\n\\n                gene2.append( gene[i] + gene[j] )\\n\\n                arr[k] = abc[i] + abc[j]\\n\\n                k += 1\\n\\n    \\n\\n    pd.concat( [ pd.DataFrame(gene2), pd.DataFrame(arr) ], axis=1 )\\n\\n    Out[1780]: \\n\\n              0  0  1  2  3\\n\\n    0  ABC1ABC2  1  2  0  1\\n\\n    1  ABC1ABC3  1  2  1  1\\n\\n    2  ABC1ABC4  0  1  1  2\\n\\n    3  ABC2ABC3  2  2  1  0\\n\\n    4  ABC2ABC4  1  1  1  1\\n\\n    5  ABC3ABC4  1  1  2  1\\n\\n\\n\\nDepending on size/speed issues you may need to separate the string and numerical code and vectorize the numerical piece.  This code is not likely to scale all that well if your data is big and if it is, that may determine what sort of answer you need (and also may need to think about how you store results).\\n\\n\\n\\n\",\n",
       "  '<python><pandas><aggregate><combinations><itertools>',\n",
       "  datetime.date(2015, 4, 21),\n",
       "  '2015-04-21 19:11:27',\n",
       "  'JohnE (3877338)',\n",
       "  '2',\n",
       "  '',\n",
       "  '3950.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['635',\n",
       "  '24197622',\n",
       "  'Answer',\n",
       "  'Error when trying to convert a column with string in Python Pandas to Float',\n",
       "  \"The right regular expression to use is given here, as you want to remove the `$` and `,`:\\n\\n\\n\\n\\tIn [7]:\\n\\n\\t\\n\\n\\tdf['market_cap_(in_us_$)'].replace('[\\\\$,]', '', regex=True).astype(float)\\n\\n\\tOut[7]:\\n\\n\\t0        5.41\\n\\n\\t1    18160.50\\n\\n\\t2     9038.20\\n\\n\\t3     8614.30\\n\\n\\t4      368.50\\n\\n\\t5     2603.80\\n\\n\\t6     6701.50\\n\\n\\t7     8942.40\\n\\n\\tName: market_cap_(in_us_$), dtype: float64\\n\\n\\n\\nBut since you got that `keyword argument 'regex'` error, you must be using a very old version, and should update.\",\n",
       "  '<python><regex><python-2.7><pandas>',\n",
       "  datetime.date(2014, 6, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1692.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['640',\n",
       "  '25432503',\n",
       "  'Answer',\n",
       "  'How to flush away the default suptitle of boxplot with subplots made by pandas package for python',\n",
       "  \"Those are generated by `suptitle()` calls, and the super titles are the children of `fig` object (and yes, the `suptitle()` were called 4 times, one from each subplot).\\n\\n\\n\\nTo fix it:\\n\\n\\n\\n    df = pd.DataFrame({'Emission': np.random.random(12),\\n\\n                       'Voltage': np.random.random(12),\\n\\n                       'Power': np.repeat([10,20,40,60],3)})\\n\\n    fig = plt.figure(figsize=(16,9))\\n\\n    i = 0\\n\\n    for Power in [10, 20, 40, 60]:\\n\\n        i = i+1\\n\\n        ax = fig.add_subplot(2,2,i)\\n\\n        subdf = df[df.Power==Power]\\n\\n        bp = subdf.boxplot(column='Emission', by='Voltage', ax=ax)\\n\\n    fig.texts = [] #flush the old super titles\\n\\n    plt.suptitle('Some title')\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/34vff.png\",\n",
       "  '<python><matplotlib><pandas><boxplot><subplot>',\n",
       "  datetime.date(2014, 8, 21),\n",
       "  '2014-08-21 17:42:10',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1880.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['641',\n",
       "  '24337599',\n",
       "  'Answer',\n",
       "  'What is the meaning of numpy reduceat() in python?',\n",
       "  'It is sort of like a rolling apply, see:\\n\\n\\n\\n\\tIn [59]:\\n\\n\\t\\n\\n\\tnp.add.reduceat([0,1,2,3,4,5,6,7],[0,4])\\n\\n\\tOut[59]:\\n\\n\\tarray([ 6, 22])\\n\\n\\tIn [65]:\\n\\n\\t\\n\\n\\tnp.add.reduceat([0,1,2,3,4,5,6,7],[4,1])\\n\\n\\tOut[65]:\\n\\n\\tarray([ 4, 28])\\n\\n\\tIn [66]:\\n\\n\\t\\n\\n\\tnp.add.reduceat([0,1,2,3,4,5,6,7],[1,5])\\n\\n\\tOut[66]:\\n\\n\\tarray([10, 18])\\n\\n\\tIn [64]:\\n\\n\\t\\n\\n\\tnp.add.reduceat([0,1,2,3,4,5,6,7],[5,2])\\n\\n\\tOut[64]:\\n\\n\\tarray([ 5, 27])\\n\\n\\tIn [61]:\\n\\n\\t\\n\\n\\tnp.add.reduceat([0,1,2,3,4,5,6,7],[2,6])\\n\\n\\tOut[61]:\\n\\n\\tarray([14, 13])\\n\\n\\tIn [67]:\\n\\n\\t\\n\\n\\tnp.add.reduceat([0,1,2,3,4,5,6,7],[6,3])\\n\\n\\tOut[67]:\\n\\n\\tarray([ 6, 25])\\n\\n\\tIn [62]:\\n\\n\\t\\n\\n\\tnp.add.reduceat([0,1,2,3,4,5,6,7],[3,7])\\n\\n\\tOut[62]:\\n\\n\\tarray([18,  7])\\n\\n\\n\\nIf you want just the 1st value, you can get it done in just one shot:\\n\\n\\n\\n\\tIn [63]:\\n\\n\\t\\n\\n\\tnp.add.reduceat([0,1,2,3,4,5,6,7],[0,4,1,5,2,6,3,7])\\n\\n\\tOut[63]:\\n\\n\\tarray([ 6,  4, 10,  5, 14,  6, 18,  7])',\n",
       "  '<python><numpy>',\n",
       "  datetime.date(2014, 6, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1423.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['643',\n",
       "  '24382682',\n",
       "  'Question',\n",
       "  'Reusable d3.js Components And Inheritance',\n",
       "  'I\\'m trying to create a library of reusable d3.js components along the lines of Mike Bostock\\'s [Toward Reusable Charts](http://bost.ocks.org/mike/chart/), and am having some difficulty in creating reusable components that reuse code internally.\\n\\n\\n\\nThe library, and its *i*-th component, look something like this:\\n\\n\\n\\n    d3.library = {}\\n\\n\\n\\n    d3.library.component_i = function module() {\\n\\n        var width = 800,\\n\\n            height = 800;\\n\\n\\n\\n        function exports(_selection) {\\n\\n            ...\\n\\n        }\\n\\n\\n\\n        exports.width = function(_width) {\\n\\n            if(!arguments.length) \\n\\n                return width;\\n\\n            width = _width;\\n\\n            return this;\\n\\n        }\\n\\n        exports.height = function(_height) {\\n\\n            if(!arguments.length) \\n\\n                return height;\\n\\n            height = _height;\\n\\n            return this;\\n\\n        }\\n\\n     \\n\\n        ....\\n\\n\\n\\n        return exports\\n\\n    }\\n\\n\\n\\nSome of the code, e.g. the \"members\" and \"methods\" <code>height</code> and <code>width</code>,  is essentially boilerplate code which I\\'m copy-pasting for any component *i*. I\\'d like to reuse code, instead.\\n\\n\\n\\nIn a \"classic\" OO language, e.g., Python, it would be natural to write some <code>_BaseComponent</code> base class, and subclass it. This base class would contain the common members:\\n\\n\\n\\n    class _BaseComponent(object):\\n\\n       ...  \\n\\n      \\n\\n       @property\\n\\n       def width(self):\\n\\n           ...\\n\\n\\n\\n       @width.setter\\n\\n       def width(self, width_):\\n\\n           ...\\n\\n\\n\\n\\n\\n    class ComponentI(_BaseComponent):\\n\\n       \\n\\n        ...\\n\\n          \\n\\n        # width and height inherited\\n\\n\\n\\n        ...\\n\\n\\n\\n\\n\\nSince it is a Javascript library, however, I\\'d like to retain the Javascript and d3.js \"feel\": function chaining, the select-call paradigm, and so forth. I\\'ve read up a bit on prototypal inheritance in Javascript (e.g., in [Javscript: The Good Parts](http://shop.oreilly.com/product/9780596517748.do)), but the objects there don\\'t seem to be defined along the lines of the above code (i.e., a function with local variables as \"data members\" for a single exported function, and \"methods\" which serve as accessors to the \"data members\").\\n\\n\\n\\nI can\\'t figure out how to create a library such that it simultaneously\\n\\n\\n\\n1. has a reusable hierarchy internally \\n\\n2. meshes well with the rest of d3.js, esp. externally\\n\\n\\n\\nA [related question](https://stackoverflow.com/questions/23680520/reusable-d3-chart-with-common-code-separated-out?rq=1) raises the same points, basically, but asks for help with a specific solution direction. Any solution which addresses the above 2 points is fine here, even if it involves changing all the components\\' structure. ',\n",
       "  '<javascript><d3.js>',\n",
       "  datetime.date(2014, 6, 24),\n",
       "  '2017-05-23 10:27:23',\n",
       "  'Ami Tavory (3510736), AlvaroAV (2815099), URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '4.0',\n",
       "  '1633.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['652',\n",
       "  '24318822',\n",
       "  'Answer',\n",
       "  'Legend format lost after using: ax.legend(handles, labels )',\n",
       "  \"You have issued `legend()` call twice and the second time it is called without formatting arguments, replace:\\n\\n\\n\\n    legend = ax.legend(loc='upper center', shadow=True)\\n\\n\\n\\n    handles, labels = ax.get_legend_handles_labels()\\n\\n    ax.legend(handles, labels )\\n\\n\\n\\nwith\\n\\n\\n\\n    handles, labels = ax.get_legend_handles_labels()\\n\\n    by_label = OrderedDict(zip(labels, handles))\\n\\n    ax.legend(by_label.values(), by_label.keys(), loc='upper center', shadow=True)\\n\\n\\n\\nShould do the trick.\",\n",
       "  '<python><matplotlib><label><legend><handle>',\n",
       "  datetime.date(2014, 6, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1243.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['661',\n",
       "  '25950943',\n",
       "  'Question',\n",
       "  'Why is numba faster than numpy here?',\n",
       "  \"I can't figure out why numba is beating numpy here (over 3x).  Did I make some fundamental error in how I am benchmarking here?  Seems like the perfect situation for numpy, no?  Note that as a check, I also ran a variation combining numba and numpy (not shown), which as expected was the same as running numpy without numba.\\n\\n\\n\\n(btw this is a followup question to: https://stackoverflow.com/questions/25915541/fastest-way-to-numerically-process-2d-array-dataframe-vs-series-vs-array-vs-num )\\n\\n\\n\\n    import numpy as np\\n\\n    from numba import jit\\n\\n    nobs = 10000 \\n\\n\\n\\n    def proc_numpy(x,y,z):\\n\\n\\n\\n       x = x*2 - ( y * 55 )      # these 4 lines represent use cases\\n\\n       y = x + y*2               # where the processing time is mostly\\n\\n       z = x + y + 99            # a function of, say, 50 to 200 lines\\n\\n       z = z * ( z - .88 )       # of fairly simple numerical operations\\n\\n\\n\\n       return z\\n\\n    \\n\\n    @jit\\n\\n    def proc_numba(xx,yy,zz):\\n\\n       for j in range(nobs):     # as pointed out by Llopis, this for loop \\n\\n          x, y = xx[j], yy[j]    # is not needed here.  it is here by \\n\\n                                 # accident because in the original benchmarks \\n\\n          x = x*2 - ( y * 55 )   # I was doing data creation inside the function \\n\\n          y = x + y*2            # instead of passing it in as an array\\n\\n          z = x + y + 99         # in any case, this redundant code seems to \\n\\n          z = z * ( z - .88 )    # have something to do with the code running\\n\\n                                 # faster.  without the redundant code, the \\n\\n          zz[j] = z              # numba and numpy functions are exactly the same.\\n\\n       return zz\\n\\n    \\n\\n    x = np.random.randn(nobs)\\n\\n    y = np.random.randn(nobs)\\n\\n    z = np.zeros(nobs)\\n\\n    res_numpy = proc_numpy(x,y,z)\\n\\n    \\n\\n    z = np.zeros(nobs)\\n\\n    res_numba = proc_numba(x,y,z)\\n\\n\\n\\nresults:\\n\\n\\n\\n    In [356]: np.all( res_numpy == res_numba )\\n\\n    Out[356]: True\\n\\n    \\n\\n    In [357]: %timeit proc_numpy(x,y,z)\\n\\n    10000 loops, best of 3: 105 µs per loop\\n\\n    \\n\\n    In [358]: %timeit proc_numba(x,y,z)\\n\\n    10000 loops, best of 3: 28.6 µs per loop\\n\\n\\n\\nI ran this on a 2012 macbook air (13.3), standard anaconda distribution.  I can provide more detail on my setup if it's relevant.\",\n",
       "  '<python><numpy><numba>',\n",
       "  datetime.date(2014, 9, 20),\n",
       "  '2017-05-23 11:54:34',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '16',\n",
       "  '4.0',\n",
       "  '7492.0',\n",
       "  '4.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['664',\n",
       "  '25871524',\n",
       "  'Answer',\n",
       "  'Python: fastest way to write pandas DataFrame to Excel on multiple sheets',\n",
       "  'For what it\\'s worth, this is how I format the output in xlwt.  The documentation is (or at least was) pretty spotty so I had to guess most of this!\\n\\n\\n\\n    import xlwt\\n\\n\\n\\n    style = xlwt.XFStyle()\\n\\n    style.font.name = \\'Courier\\'\\n\\n    style.font.height = 180\\n\\n    style.num_format_str = \\'#,##0\\'\\n\\n\\n\\n    # ws0 is a worksheet\\n\\n    ws0.write( row, col, value, style )\\n\\n \\n\\nAlso, I believe I duplicated your error message when attempting to format the resulting spreadsheet in excel (office 2010 version).  It\\'s weird, but some of the drop down tool bar format options work and some don\\'t.  But it looks like they all work fine if I go to \"format cells\" via a right click.',\n",
       "  '<python><excel><pandas><export><output>',\n",
       "  datetime.date(2014, 9, 16),\n",
       "  '2014-09-16 18:53:47',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '5634.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['666',\n",
       "  '26123867',\n",
       "  'Answer',\n",
       "  'Slice pandas dataframe in groups of consecutive values',\n",
       "  'My two cents just for the fun of it.\\n\\n\\n\\n    In [15]:\\n\\n\\n\\n    for grp, val in df.groupby((df.diff()-1).fillna(0).cumsum().A):\\n\\n        print val\\n\\n       A\\n\\n    a  1\\n\\n    b  2\\n\\n    c  3\\n\\n       A\\n\\n    d  6\\n\\n    e  7\\n\\n    f  8\\n\\n        A\\n\\n    g  11\\n\\n    h  12\\n\\n    i  13',\n",
       "  '<python><pandas><slice>',\n",
       "  datetime.date(2014, 9, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2119.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['668',\n",
       "  '26388851',\n",
       "  'Answer',\n",
       "  'Strip time from an object date in pandas',\n",
       "  \"You can also do it using only the standard libraries (in any format you want '%m/%d/%Y', '%m-%d-%Y' or other orders/formats):\\n\\n\\n\\n    In [118]:\\n\\n\\n\\n    import time\\n\\n    df['Created Date'] = df['Created Date'].apply(lambda x: time.strftime('%m/%d/%Y', time.strptime(x, '%m/%d/%Y %H:%M:%S')))\\n\\n    In [120]:\\n\\n\\n\\n    print df\\n\\n       InteractionID Created Date EmployeeID          Repeat Date\\n\\n    0           7927   04/01/2014       912a  04/01/2014 14:50:03\\n\\n    1           2158   04/01/2014       172r  04/04/2014 17:47:29\\n\\n    2          44279   04/01/2014       217y  04/07/2014 22:06:19\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 10, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '12037.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['671',\n",
       "  '28990872',\n",
       "  'Answer',\n",
       "  'Pandas Correlation Groupby',\n",
       "  \"You pretty much figured out all the pieces, just need to combine them:\\n\\n\\n\\n    >>> df.groupby('ID')[['Val1','Val2']].corr()\\n\\n     \\n\\n                 Val1      Val2\\n\\n    ID                         \\n\\n    A  Val1  1.000000  0.500000\\n\\n       Val2  0.500000  1.000000\\n\\n    B  Val1  1.000000  0.385727\\n\\n       Val2  0.385727  1.000000\\n\\n\\n\\nIn your case, printing out a 2x2 for each ID is excessively verbose.  I don't see an option to print a scalar correlation instead of the whole matrix, but you can do something simple like this if you only have two variables:\\n\\n\\n\\n    >>> df.groupby('ID')[['Val1','Val2']].corr().iloc[0::2,-1]\\n\\n    \\n\\n    ID       \\n\\n    A   Val1    0.500000\\n\\n    B   Val1    0.385727\\n\\n\\n\\n### For the more general case of 3+ variables ###\\n\\n\\n\\nFor 3 or more variables, it is not straightforward to create concise output but you could do something like this:\\n\\n\\n\\n    groups = list('Val1', 'Val2', 'Val3', 'Val4')\\n\\n    df2 = pd.DataFrame()\\n\\n    for i in range( len(groups)-1): \\n\\n        df2 = df2.append( df.groupby('ID')[groups].corr().stack()\\n\\n                            .loc[:,groups[i],groups[i+1]:].reset_index() )\\n\\n    \\n\\n    df2.columns = ['ID', 'v1', 'v2', 'corr']\\n\\n    df2.set_index(['ID','v1','v2']).sort_index()\\n\\n\\n\\nNote that if we didn't have the `groupby` element, it would be straightforward to use an upper or lower triangle function from numpy.  But since that element is present, it is not so easy to produce concise output in a more elegant manner as far as I can tell.\",\n",
       "  '<python><pandas><group-by><correlation>',\n",
       "  datetime.date(2015, 3, 11),\n",
       "  '2018-07-25 09:36:17',\n",
       "  'JohnE (3877338)',\n",
       "  '13',\n",
       "  '',\n",
       "  '8354.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['672',\n",
       "  '24422213',\n",
       "  'Answer',\n",
       "  'Saving a plot in python creates only an empty pdf document',\n",
       "  'Move the `plt.savefig(\"result.pdf\")` line before `plt.show()` should save the plot correctly as a pdf file.\\n\\n\\n\\nYou might be using a interactive backend. After `plt.show()`, a window containing the plot pops up. If you then close the window, the plot is gone (similar to a `plt.close()` call). Therefore, `plt.savefig()` has nothing to save if you do that after closing the plot window.',\n",
       "  '<python><pdf><matplotlib><charts><pdf-generation>',\n",
       "  datetime.date(2014, 6, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1241.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['673',\n",
       "  '24441784',\n",
       "  'Answer',\n",
       "  'How to create a list in Python with the unique values of a CSV file?',\n",
       "  \"A very concise way to do this is to use `pandas`, the benefits are: it has a faster CSV pharser; and it works in columns (so it only requires one `df.apply(set)` to get you there) :\\n\\n\\n\\n\\tIn [244]:\\n\\n\\t#Suppose the CSV is named temp.csv\\n\\n\\tdf=pd.read_csv('temp.csv',header=None)\\n\\n\\tdf.apply(set)\\n\\n\\tOut[244]:\\n\\n\\t0                        set([1994, 1995, 1996, 1998])\\n\\n\\t1            set([ Category2,  Category3,  Category1])\\n\\n\\t2    set([ Something Happened 4,  Something Happene...\\n\\n\\tdtype: object\\n\\n\\n\\nThe downside is that it returns a `pandas.Series`, and to get access each list, you need to do something like `list(df.apply(set)[0])`.\\n\\n\\n\\n#Edit\\n\\n\\n\\nIf the order has to be preserved, it can be also done very easily, for example:\\n\\n\\n\\n\\tfor i, item in df.iteritems():\\n\\n\\t    print item.unique()\\n\\n\\n\\n`item.unique()` will return `numpy.array`s, instead of `list`s.\\n\\n\",\n",
       "  '<python><list><csv><unique>',\n",
       "  datetime.date(2014, 6, 26),\n",
       "  '2014-06-28 17:22:00',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '14026.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['675',\n",
       "  '24469099',\n",
       "  'Answer',\n",
       "  'Correlation coefficients and p values for all pairs of rows of a matrix',\n",
       "  'The most consice way of doing it might be the buildin method `.corr` in `pandas`, to get r:\\n\\n\\n\\n\\tIn [79]:\\n\\n\\t\\n\\n\\timport pandas as pd\\n\\n\\tm=np.random.random((6,6))\\n\\n\\tdf=pd.DataFrame(m)\\n\\n\\tprint df.corr()\\n\\n\\t          0         1         2         3         4         5\\n\\n\\t0  1.000000 -0.282780  0.455210 -0.377936 -0.850840  0.190545\\n\\n\\t1 -0.282780  1.000000 -0.747979 -0.461637  0.270770  0.008815\\n\\n\\t2  0.455210 -0.747979  1.000000 -0.137078 -0.683991  0.557390\\n\\n\\t3 -0.377936 -0.461637 -0.137078  1.000000  0.511070 -0.801614\\n\\n\\t4 -0.850840  0.270770 -0.683991  0.511070  1.000000 -0.499247\\n\\n\\t5  0.190545  0.008815  0.557390 -0.801614 -0.499247  1.000000\\n\\n\\n\\nTo get p values using t-test:\\n\\n\\n\\n\\tIn [84]:\\n\\n\\t\\n\\n\\tn=6\\n\\n\\tr=df.corr()\\n\\n\\tt=r*np.sqrt((n-2)/(1-r*r))\\n\\n\\t \\n\\n\\timport scipy.stats as ss\\n\\n\\tss.t.cdf(t, n-2)\\n\\n\\tOut[84]:\\n\\n\\tarray([[ 1.        ,  0.2935682 ,  0.817826  ,  0.23004382,  0.01585695,\\n\\n\\t         0.64117917],\\n\\n\\t       [ 0.2935682 ,  1.        ,  0.04363408,  0.17836685,  0.69811422,\\n\\n\\t         0.50661121],\\n\\n\\t       [ 0.817826  ,  0.04363408,  1.        ,  0.39783538,  0.06700715,\\n\\n\\t         0.8747497 ],\\n\\n\\t       [ 0.23004382,  0.17836685,  0.39783538,  1.        ,  0.84993082,\\n\\n\\t         0.02756579],\\n\\n\\t       [ 0.01585695,  0.69811422,  0.06700715,  0.84993082,  1.        ,\\n\\n\\t         0.15667393],\\n\\n\\t       [ 0.64117917,  0.50661121,  0.8747497 ,  0.02756579,  0.15667393,\\n\\n\\t         1.        ]])\\n\\n\\tIn [85]:\\n\\n\\t\\n\\n\\tss.pearsonr(m[:,0], m[:,1])\\n\\n\\tOut[85]:\\n\\n\\t(-0.28277983892175751, 0.58713640696703184)\\n\\n\\tIn [86]:\\n\\n\\t#be careful about the difference of 1-tail test and 2-tail test:\\n\\n\\t0.58713640696703184/2\\n\\n\\tOut[86]:\\n\\n\\t0.2935682034835159 #the value in ss.t.cdf(t, n-2) [0,1] cell\\n\\n\\n\\nAlso you can just use the `scipy.stats.pearsonr` you mentioned in OP:\\n\\n\\n\\n\\tIn [95]:\\n\\n\\t#returns a list of tuples of (r, p, index1, index2)\\n\\n\\timport itertools\\n\\n\\t[ss.pearsonr(m[:,i],m[:,j])+(i, j) for i, j in itertools.product(range(n), range(n))]\\n\\n\\tOut[95]:\\n\\n\\t[(1.0, 0.0, 0, 0),\\n\\n\\t (-0.28277983892175751, 0.58713640696703184, 0, 1),\\n\\n\\t (0.45521036266021014, 0.36434799921123057, 0, 2),\\n\\n\\t (-0.3779357902414715, 0.46008763115463419, 0, 3),\\n\\n\\t (-0.85083961671703368, 0.031713908656676448, 0, 4),\\n\\n\\t (0.19054495489542525, 0.71764166168348287, 0, 5),\\n\\n\\t (-0.28277983892175751, 0.58713640696703184, 1, 0),\\n\\n\\t (1.0, 0.0, 1, 1),\\n\\n    #etc, etc',\n",
       "  '<python><numpy><statistics><scipy><correlation>',\n",
       "  datetime.date(2014, 6, 28),\n",
       "  '2014-06-28 17:20:19',\n",
       "  'CT Zhu (2487184)',\n",
       "  '9',\n",
       "  '',\n",
       "  '11122.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['678',\n",
       "  '24577608',\n",
       "  'Answer',\n",
       "  'Trouble to impliment scipy interpolation',\n",
       "  '`scipy.interpolate.RegularGridInterpolator` is a new function in `scipy` `0.14.x`. You are likely to be using a older version. Updating `scipy` should solve the problem.',\n",
       "  '<python><numpy><scipy><interpolation><scientific-computing>',\n",
       "  datetime.date(2014, 7, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2713.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['681',\n",
       "  '29329602',\n",
       "  'Answer',\n",
       "  'How to load data from np matrix to seaborn?',\n",
       "  'I think you loaded your data from numpy to a pandas dataframe correctly, if you just leave off ```hue```, it should work:\\n\\n\\n\\n    sns.pairplot(df, size=2.5)\\n\\n\\n\\n![seaborn_plot][1]\\n\\n\\n\\nI\\'ve had problems with ```hue``` though.  I thought this would work, and it kind of does, but there is a lot of error/warning code and the existing plot is 3x3 instead of 2x2.  In the iris example, seaborn knows not to plot species, but here it doesn\\'t?\\n\\n\\n\\n    df[\\'species\\'] = np.random.choice(2,150) \\n\\n    sns.pairplot(df, hue=\"species\", size=2.5)\\n\\n\\n\\n---------------------------------------------------------------------------\\n\\n    KeyError                                  Traceback (most recent call last)\\n\\n    <ipython-input-148-cae05573d2d1> in <module>()\\n\\n          6 \\n\\n          7 df[\\'species\\'] = np.random.choice(2,150)\\n\\n    ----> 8 sns.pairplot(df, hue=\"species\", size=2.5)\\n\\n          9 \\n\\n         10 sns.plt.show()\\n\\n    \\n\\n    /Users/John/anaconda/lib/python2.7/site-packages/seaborn/linearmodels.pyc in pairplot(data, hue, hue_order, palette, vars, x_vars, y_vars, kind, diag_kind, markers, size, aspect, dropna, plot_kws, diag_kws, grid_kws)\\n\\n       1756     # Add a legend\\n\\n       1757     if hue is not None:\\n\\n    -> 1758         grid.add_legend()\\n\\n       1759 \\n\\n       1760     return grid\\n\\n    \\n\\n    /Users/John/anaconda/lib/python2.7/site-packages/seaborn/axisgrid.pyc in add_legend(self, legend_data, title, label_order)\\n\\n         47             # Draw a full-figure legend outside the grid\\n\\n         48             figlegend = plt.figlegend(handles, label_order, \"center right\",\\n\\n    ---> 49                                       scatterpoints=1)\\n\\n         50             self._legend = figlegend\\n\\n         51             figlegend.set_title(title)\\n\\n    \\n\\n    /Users/John/anaconda/lib/python2.7/site-packages/matplotlib/pyplot.pyc in figlegend(handles, labels, loc, **kwargs)\\n\\n        662 \\n\\n        663     \"\"\"\\n\\n    --> 664     l = gcf().legend(handles, labels, loc, **kwargs)\\n\\n        665     draw_if_interactive()\\n\\n        666     return l\\n\\n    \\n\\n    /Users/John/anaconda/lib/python2.7/site-packages/matplotlib/figure.pyc in legend(self, handles, labels, *args, **kwargs)\\n\\n       1193         .. plot:: mpl_examples/pylab_examples/figlegend_demo.py\\n\\n       1194         \"\"\"\\n\\n    -> 1195         l = Legend(self, handles, labels, *args, **kwargs)\\n\\n       1196         self.legends.append(l)\\n\\n       1197         l._remove_method = lambda h: self.legends.remove(h)\\n\\n    \\n\\n    /Users/John/anaconda/lib/python2.7/site-packages/matplotlib/legend.pyc in __init__(self, parent, handles, labels, loc, numpoints, markerscale, scatterpoints, scatteryoffsets, prop, fontsize, borderpad, labelspacing, handlelength, handleheight, handletextpad, borderaxespad, columnspacing, ncol, mode, fancybox, shadow, title, framealpha, bbox_to_anchor, bbox_transform, frameon, handler_map)\\n\\n        369 \\n\\n        370         if framealpha is None:\\n\\n    --> 371             self.get_frame().set_alpha(rcParams[\"legend.framealpha\"])\\n\\n        372         else:\\n\\n        373             self.get_frame().set_alpha(framealpha)\\n\\n    \\n\\n    /Users/John/anaconda/lib/python2.7/site-packages/matplotlib/__init__.pyc in __getitem__(self, key)\\n\\n        844             except (ValueError, RuntimeError):\\n\\n        845                 # force the issue\\n\\n    --> 846                 warnings.warn(_rcparam_warn_str.format(key=repr(k),\\n\\n        847                                                        value=repr(v),\\n\\n        848                                                        func=\\'__init__\\'))\\n\\n    \\n\\n    KeyError: u\\'legend.framealpha\\'\\n\\n\\n\\n![seaborn_plot2][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/3BgC1.png\\n\\n  [2]: http://i.stack.imgur.com/TNtrW.png',\n",
       "  '<python><numpy><pandas><dataframe><seaborn>',\n",
       "  datetime.date(2015, 3, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '4989.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['684',\n",
       "  '26064442',\n",
       "  'Answer',\n",
       "  \"title in italic font in matplotlib isn't working\",\n",
       "  'Is this the intended result?\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nIt can be done in a quick and dirty way using `mathtext`, but you have to have `$` flaking each lines, such as:\\n\\n\\n\\n    newtitle = \\'\\\\n\\'.join([\\'$%s$\\'%item for item in wrap(title, width=50)]).replace(\\' \\', \\'\\\\ \\')\\n\\n    newtitle = \"%s\\\\n%s\"%(\\'text\\\\ below\\\\ the\\\\ main\\\\ title\\', newtitle)\\n\\n\\n\\nBut I think a better looking way is to have these two titles separated. You can use `suptitle` for the upper one, or maybe just use `text`, either way you can then control their properties independently. e.g.:\\n\\n\\n\\n    plt.pie(x_list, labels=label_list, explode=explode, autopct=\"%1.1f%%\", startangle=90)\\n\\n    plt.title(\\'\\\\n\\'.join(wrap(title, width=50)), size=12, style=\\'italic\\')\\n\\n    plt.suptitle(\\'some title\\', y=0.85, x=0.45) #y and x needed as you have adjusted the subplot size already.\\n\\n![enter image description here][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/oqc4E.png\\n\\n  [2]: http://i.stack.imgur.com/RyC1t.png',\n",
       "  '<python><python-2.7><python-3.x><matplotlib>',\n",
       "  datetime.date(2014, 9, 26),\n",
       "  '2014-09-26 16:56:43',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '2130.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['692',\n",
       "  '24504121',\n",
       "  'Answer',\n",
       "  'Change size of outlier labels on boxplot in R',\n",
       "  'I think you can firstly plot without the outliners, and then manually add them. In that way, you can almost do whatever you want to do to change the style, shape, color, etc. I will use the base function `boxplot` here:\\n\\n\\n\\n\\t> data <- c(10,15,2,20,30,1,50,16,18,4)\\n\\n\\t> B <- boxplot(data, outline=FALSE, ylim=c(0, 55))\\n\\n\\t> points(B$group, B$out, type = \"p\", pch=23)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/5xPVV.png',\n",
       "  '<r><boxplot><outliers>',\n",
       "  datetime.date(2014, 7, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '4320.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['693',\n",
       "  '24621201',\n",
       "  'Answer',\n",
       "  'Values not loading when using pandas read_csv',\n",
       "  \"Don't know how you data looks like exactly, but I will just take whatever in the OP:\\n\\n\\n\\n\\tIn [76]:\\n\\n\\t\\n\\n\\t%%file temp.csv\\n\\n\\t100000,138.47,51.655,97.827,27.98,0.91,124.711,2.666,3.064,41.928,197.76,1.582,1.396,0.2,32.638,1.017,0.381,51.626,2.273,-2.414,16.824,-0.277,258.733,2,67.435,2.15,0.444,46.062,1.24,-2.475,113.497,0.00265331133733,s, 100001,160.937,68.768,103.235,48.146,-999.0,-999.0,-999.0,3.473,2.078,125.157,0.879,1.414,-999.0,42.014,2.039,-3.011,36.918,0.501,0.103,44.704,-1.916,164.546,1,46.226,0.725,1.158,-999.0,-999.0,-999.0,46.226,2.23358448717,b, 100002,-999.0,162.172,125.953,35.635,-999.0,-999.0,-999.0,3.148,9.336,197.814,3.776,1.414,-999.0,32.154,-0.705,-2.093,121.409,-0.953,1.052,54.283,-2.186,260.414,1,44.251,2.053,-2.028,-999.0,-999.0,-999.0,44.251,2.34738894364,b\\n\\n\\t\\n\\n\\tIn [77]:\\n\\n\\t#make sure it is tab delimited rather than , delimited\\n\\n    #Change pd.DataFrame(data2 to pd.DataFrame(data2.values\\n\\n\\twith open('temp.csv', 'r') as f:\\n\\n\\t    data2 = pd.read_csv(f, sep=',', index_col=0, header=None)\\n\\n\\t    EventID = pd.date_range('1/1/2000', periods=1)\\n\\n\\t    df = pd.DataFrame(data2.values, index=EventID, columns=range(98))\\n\\n\\t\\n\\n\\tprint df[:3]\\n\\n\\t\\n\\n\\t                0       1       2      3     4        5      6      7   \\\\\\n\\n\\t2000-01-01  138.47  51.655  97.827  27.98  0.91  124.711  2.666  3.064   \\n\\n\\t\\n\\n\\t                8       9    ...   88      89     90     91   92   93   94  \\\\\\n\\n\\t2000-01-01  41.928  197.76   ...    1  44.251  2.053 -2.028 -999 -999 -999   \\n\\n\\t\\n\\n\\t                95        96 97  \\n\\n\\t2000-01-01  44.251  2.347389  b  \\n\\n\\t\\n\\n\\t[1 rows x 98 columns]\\n\\n\\n\\n`pd.DataFrame(data2.values` is the key here. `data2` is a `DataFrame` and has its own index. Now you want to wrap it in a new `DataFrame` with new timeseries index, `pandas` will try to match and align the original index with the new one, but there are no matches. \\n\\n\\n\\nTherefore, `pd.DataFrame(data2...` will result in a `DataFrame` full of `nan`. The solution is to pass the values, in `numpy.array`, to the constructor, by `pd.DataFrame(data2.value...`.\",\n",
       "  '<python><csv><pandas>',\n",
       "  datetime.date(2014, 7, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1300.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['694',\n",
       "  '26639037',\n",
       "  'Answer',\n",
       "  'Decrease array size by averaging adjacent values with numpy',\n",
       "  'Looks like a simple non-overlapping moving window average to me, how about:\\n\\n\\n\\n    In [3]:\\n\\n\\n\\n    import numpy as np\\n\\n    a = np.array([2,3,4,8,9,10])\\n\\n    window_sz = 3\\n\\n    a[:len(a)/window_sz*window_sz].reshape(-1,window_sz).mean(1) \\n\\n    #you want to be sure your array can be reshaped properly, so the [:len(a)/window_sz*window_sz] part\\n\\n    Out[3]:\\n\\n    array([ 3.,  9.])',\n",
       "  '<python><arrays><numpy><mean>',\n",
       "  datetime.date(2014, 10, 29),\n",
       "  '2014-10-29 20:15:01',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '4070.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['696',\n",
       "  '30144913',\n",
       "  'Answer',\n",
       "  'Pandas backfilling values based on a datetime index and a column',\n",
       "  \"I'm slightly reluctant to answer b/c it seems @chrisb may have successfully answered the original question, which later changed.  However, Chris hasn't updated the answer in a few days and this answer does take a different approach so I'm going to +1 Chris's answer and add this one.\\n\\n\\n\\nFirst, just create a new dataframe from the original with 'index'='date2'.  This will be the basis for appending to the existing dataframe (note that 'index' is a column here, not an index):\\n\\n\\n\\n    df2 = df[ df['index'] != df['date2'] ]\\n\\n    df2['index'] = df2['date2']\\n\\n    df2['value'] = np.nan\\n\\n    \\n\\n            index       date2  id  value\\n\\n    0  2006-01-26  2006-01-26   3    NaN\\n\\n    1  2006-01-26  2006-01-26   1    NaN\\n\\n    2  2006-01-26  2006-01-26   2    NaN\\n\\n    4  2006-02-26  2006-02-26   4    NaN\\n\\n\\n\\nNow, just append all of these, but drop the ones we don't need (if we already have an existing row with 'index'='date2', as for id=2 here):\\n\\n\\n\\n    df3 = df.append(df2)\\n\\n    df3 = df3.drop_duplicates(['index','date2','id'])\\n\\n    df3 = df3.reset_index(drop=True).sort(['id','index','date2'])\\n\\n    df3['value'] = df3.value.fillna(method='ffill')\\n\\n    \\n\\n            index       date2  id  value\\n\\n    1  2006-01-25  2006-01-26   1    1.0\\n\\n    6  2006-01-26  2006-01-26   1    1.0\\n\\n    2  2006-01-25  2006-01-26   2    2.0\\n\\n    3  2006-01-26  2006-01-26   2    2.1\\n\\n    0  2006-01-24  2006-01-26   3    3.0\\n\\n    5  2006-01-26  2006-01-26   3    3.0\\n\\n    4  2006-01-27  2006-02-26   4    4.0\\n\\n    7  2006-02-26  2006-02-26   4    4.0\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 9),\n",
       "  '2015-05-09 21:00:57',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '3154.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['704',\n",
       "  '25019584',\n",
       "  'Answer',\n",
       "  'Adding a colorbar to a pcolormesh with polar projection',\n",
       "  'In the way you are doing it, the `cax` axis is actually in `polar` projection.  You can verify it by: \\n\\n\\n\\n    cax = divider.append_axes(\"right\", size=\"200%\", pad=0.5)\\n\\n    #plot.colorbar(im, cax=cax)\\n\\n    cax.pcolormesh(t, r, c.T)\\n\\n\\n\\nWhile this might be a bug, I think a cleaner way to achieve it might be to use `GridSpec`:\\n\\n\\n\\n    gs = gridspec.GridSpec(1, 2,\\n\\n                           width_ratios=[10,1],\\n\\n                           )\\n\\n\\n\\n    ax1 = plt.subplot(gs[0], projection=\"polar\", aspect=1.)\\n\\n    ax2 = plt.subplot(gs[1])\\n\\n\\n\\n    t = np.linspace(0.0, 2.0 * np.pi, 360)\\n\\n    r = np.linspace(0,100,200)\\n\\n    rg, tg = np.meshgrid(r,t)\\n\\n    c = rg * np.sin(tg)\\n\\n\\n\\n    im = ax1.pcolormesh(t, r, c.T)\\n\\n    plot.colorbar(im, cax=ax2)\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/UHL6W.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2014, 7, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1976.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['707',\n",
       "  '24810954',\n",
       "  'Answer',\n",
       "  'Why does the Bartlett test from scipy.stats.bartlett gives nan as output?',\n",
       "  'Bartlett test is for testing the homogeneity of variance across groups, in your case, there are no variance, as all the groups contain equal values.\\n\\n\\n\\nA minimal example:\\n\\n\\n\\n\\tIn [7]:\\n\\n\\t\\n\\n\\timport scipy.stats as ss\\n\\n\\tIn [8]:\\n\\n\\t\\n\\n\\tdata=[[2,2,2,2,2],[1,1,1,1,1],[3,3,3,3]]\\n\\n\\tss.bartlett(*data)\\n\\n\\tOut[8]:\\n\\n\\t(nan, nan)\\n\\n\\tIn [9]:\\n\\n\\t\\n\\n\\tA=[10,7,20,14,14,12,10,23,17,20,14,13,11,17,21,11,16,14,17,17,19,21,7]\\n\\n\\tB=[1,1,1,1,1,1,1,1,2,2,2,2,2,2,2,2,2,]\\n\\n\\tss.bartlett(A, B)\\n\\n\\tOut[9]:\\n\\n\\t(47.7068477814218, 4.9495974630644599e-12)\\n\\n\\n\\nSo it is not a bug.',\n",
       "  '<python><statistics><scipy><statsmodels><anova>',\n",
       "  datetime.date(2014, 7, 17),\n",
       "  '2014-07-18 20:08:59',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1306.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['708',\n",
       "  '25039883',\n",
       "  'Question',\n",
       "  \"Iframe cutting off most of it's content inside of a <div>\",\n",
       "  'I am trying to imbed graphs from the FRED Economic Reserve into my website, and it displays the top left of the graph, yet it cuts off the rest of the content.\\n\\n\\n\\nHere is the part of the index.html file that places the iframe:\\n\\n\\n\\n    <div class=\"box\" id=\"retailbox\">\\n\\n        <iframe src=\"http:////research.stlouisfed.org/fred2/graph/graph-landing.php?g=GEt\" frameborder=\"0\" scrolling=\"no\" onload=\"\" allowtransparency=\"true\"></iframe>\\n\\n    </div>  \\n\\n\\n\\nHere are the relevant parts of the CSS:\\n\\n\\n\\n    #retailbox {\\n\\n        border: solid #a08db7;\\n\\n        position: absolute;\\n\\n        margin: 38% 53% auto;\\n\\n    }\\n\\n\\n\\nHere is a screenshot of the problem: ![enter image description here][1]\\n\\n\\n\\nI tried adding this bit of css from another topic and editing the html accordingly to no avail:\\n\\n\\n\\n    div#content iframe {\\n\\n        position: absolute;\\n\\n        top: 0;\\n\\n        bottom: 0;\\n\\n        left: 0;\\n\\n        right: 0;\\n\\n        height: 100%;\\n\\n        width: 100%;\\n\\n    }\\n\\n**Edit**:\\n\\nWhen I allow scrolling, the iframe is clearly the correct size for the div, it is just not showing most of the content.\\n\\nThanks in advance, hopefully I\\'m just making a careless mistake.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Wflxy.png',\n",
       "  '<html><css><iframe>',\n",
       "  datetime.date(2014, 7, 30),\n",
       "  '2018-02-06 22:05:10',\n",
       "  'user3483203 (3483203), Geroy290 (3624793)',\n",
       "  '2',\n",
       "  '',\n",
       "  '10488.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['710',\n",
       "  '30259712',\n",
       "  'Answer',\n",
       "  'How can I round values in Pandas DataFrame containing mixed datatypes for further data comparison?',\n",
       "  \"Here's a fairly general solution you can apply to multiple columns.  The 'To' column doesn't need to be rounded, I just included it for the generality of two columns rather than one:\\n\\n\\n\\n    df\\n\\n    \\n\\n      IDX1 IDX2  IDX3 IDX4 ValueType     From   To\\n\\n    0   A1    Q  1983   Q4         W   10.123   10\\n\\n    3   A1    Q  1983   Q4         Z      NaN  NaN\\n\\n    4   A1    Q  1984   Q1         W  110.456  110\\n\\n    \\n\\n    In [399]: df[['From','To']].astype(float).apply(np.round)\\n\\n   \\n\\n       From   To\\n\\n    0    10   10\\n\\n    3   NaN  NaN\\n\\n    4   110  110\\n\\n\\n\\nThat's the safest way in that it won't let you accidentally wipe out non-numeric values, but if you have truly mixed types in there, you can do this:\\n\\n\\n\\n    df[['From','To']].convert_objects(convert_numeric=True).apply(np.round)\\n\\n \\n\\n       From   To\\n\\n    0    10   10\\n\\n    3   NaN  NaN\\n\\n    4   110  110\\n\\n\\n\\nBut since this will convert any non-numeric values to NaN, just make sure that's what you want before you over-write anything.\\n\\n\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2015, 5, 15),\n",
       "  '2015-05-15 12:57:09',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '3130.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['712',\n",
       "  '30283894',\n",
       "  'Answer',\n",
       "  'Is there a visual profiler for Python?',\n",
       "  \"I've written a browser-based visualization tool, [profile_eye](https://pypi.python.org/pypi/ProfileEye/), which operates on the output of [gprof2dot](https://github.com/jrfonseca/gprof2dot).\\n\\n\\n\\ngprof2dot is great at grokking many profiling-tool outputs, and does a great job at graph-element placement. The final rendering is a static graphic, which is often very cluttered. \\n\\n\\n\\nUsing [d3.js](http://d3js.org/) it's possible to remove much of that clutter, through relative fading of unfocused elements, tooltips, and a [fisheye distortion](http://bost.ocks.org/mike/fisheye/). \\n\\n\\n\\nFor comparison, see [profile_eye's visualization](http://pythonhosted.org//ProfileEye/gprof.html) of the [canonical example used by gprof2dot](https://github.com/jrfonseca/gprof2dot). For Python in particular, see [a cProfile output example](http://pythonhosted.org//ProfileEye/recipe_colon.html).\",\n",
       "  '<python><user-interface><profiling><profiler>',\n",
       "  datetime.date(2015, 5, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '27983.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['714',\n",
       "  '26871003',\n",
       "  'Answer',\n",
       "  'IPython in Safari: websocket connection failure',\n",
       "  'Turns out not to be a browser issue either. It has to do with the `tornado` `4.0.2` shipped with `anaconda`, which is not compatible with `Safari` `5.1.10`. Downgrade `tornado` to `3.2.2` solved the problem, in terminal:\\n\\n\\n\\n    conda install tornado=3.2\\n\\n\\n\\n`3.2.2` appears to be the last version that will work with `Safari` `5.1.10`',\n",
       "  '<python><safari><websocket><ipython-notebook>',\n",
       "  datetime.date(2014, 11, 11),\n",
       "  '2014-11-11 21:47:02',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1367.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['716',\n",
       "  '25190070',\n",
       "  'Answer',\n",
       "  'pandas dataframe select columns in multiindex',\n",
       "  \"There is a `get_level_values` method that you can use in conjunction with boolean indexing to get the the intended result.\\n\\n\\n\\n\\tIn [13]:\\n\\n\\t\\n\\n\\tdf = pd.DataFrame(np.random.random((4,4)))\\n\\n\\tdf.columns = pd.MultiIndex.from_product([[1,2],['A','B']])\\n\\n\\tprint df\\n\\n\\t\\t\\t  1                   2          \\n\\n\\t\\t\\t  A         B         A         B\\n\\n\\t0  0.543980  0.628078  0.756941  0.698824\\n\\n\\t1  0.633005  0.089604  0.198510  0.783556\\n\\n\\t2  0.662391  0.541182  0.544060  0.059381\\n\\n\\t3  0.841242  0.634603  0.815334  0.848120\\n\\n\\tIn [14]:\\n\\n\\t\\n\\n\\tprint df.iloc[:, df.columns.get_level_values(1)=='A']\\n\\n\\t\\t\\t  1         2\\n\\n\\t\\t\\t  A         A\\n\\n\\t0  0.543980  0.756941\\n\\n\\t1  0.633005  0.198510\\n\\n\\t2  0.662391  0.544060\\n\\n\\t3  0.841242  0.815334\",\n",
       "  '<python><pandas><hierarchical><multi-index>',\n",
       "  datetime.date(2014, 8, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '44',\n",
       "  '',\n",
       "  '32544.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['718',\n",
       "  '25213993',\n",
       "  'Answer',\n",
       "  'using pd.DataFrame.apply to create multiple columns',\n",
       "  'Are you trying to do a few different calculations on your `df` and put the resulting vectors together in one larger `DataFrame`, like in this example?:\\n\\n\\n\\n\\tIn [39]:\\n\\n\\n\\n\\tprint df\\n\\n\\n\\n\\t\\t\\t  0         1\\n\\n\\t0  0.718003  0.241216\\n\\n\\t1  0.580015  0.981128\\n\\n\\t2  0.477645  0.463892\\n\\n\\t3  0.948728  0.653823\\n\\n\\t4  0.056659  0.366104\\n\\n\\t5  0.273700  0.062131\\n\\n\\t6  0.151237  0.479318\\n\\n\\t7  0.425353  0.076771\\n\\n\\t8  0.317731  0.029182\\n\\n\\t9  0.543537  0.589783\\n\\n\\n\\n\\tIn [40]:\\n\\n\\n\\n\\tprint df.apply(lambda x: pd.Series(np.hstack((x*5, x*6))), axis=1)\\n\\n\\n\\n\\t\\t\\t  0         1         2         3\\n\\n\\t0  3.590014  1.206081  4.308017  1.447297\\n\\n\\t1  2.900074  4.905639  3.480088  5.886767\\n\\n\\t2  2.388223  2.319461  2.865867  2.783353\\n\\n\\t3  4.743640  3.269114  5.692369  3.922937\\n\\n\\t4  0.283293  1.830520  0.339951  2.196624\\n\\n\\t5  1.368502  0.310656  1.642203  0.372787\\n\\n\\t6  0.756187  2.396592  0.907424  2.875910\\n\\n\\t7  2.126764  0.383853  2.552117  0.460624\\n\\n\\t8  1.588656  0.145909  1.906387  0.175091\\n\\n\\t9  2.717685  2.948917  3.261222  3.538701\\n\\n\\t',\n",
       "  '<pandas>',\n",
       "  datetime.date(2014, 8, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1763.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['721',\n",
       "  '29000926',\n",
       "  'Answer',\n",
       "  'Rolling Correlation with Groupby in Pandas',\n",
       "  \"You can actually start with the simple approach here: \\n\\n https://stackoverflow.com/questions/28988627/pandas-correlation-groupby\\n\\n\\n\\nand then add `rolling(3)` like this:\\n\\n\\n\\n    df.groupby('ID')[['Val1','Val2']].rolling(3).corr()\\n\\n\\n\\nI've changed the window from 2 to 3 because you'll only get 1 or -1 with a window size of 2.  Unfortunately, that output (not shown) is a bit verbose because it outputs a 2x2 correlation matrix when all you need is a scalar. \\n\\n But with an additional line you can make the output more concise:\\n\\n\\n\\n    df2 = df.groupby('ID')[['Val1','Val2']].rolling(3).corr()\\n\\n\\n\\n    df2.groupby(level=[0,1]).last()['Val1']\\n\\n    \\n\\n    ID   \\n\\n    A   0         NaN\\n\\n        1         NaN\\n\\n        2   -0.996539\\n\\n    B   3         NaN\\n\\n        4         NaN\\n\\n        5    0.879868\\n\\n    C   6         NaN\\n\\n        7         NaN\\n\\n        8   -0.985529\",\n",
       "  '<python><pandas><group-by><correlation>',\n",
       "  datetime.date(2015, 3, 12),\n",
       "  '2018-02-19 00:29:44',\n",
       "  'JohnE (3877338)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1643.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['722',\n",
       "  '29090762',\n",
       "  'Answer',\n",
       "  'append rows to a Pandas groupby object',\n",
       "  \"The main thing you need to do here is append your means to the main dataset.  The main trick you need before doing that is just to conform the indexes (with the ```reset_index()``` and ```set_index()``` so that after you append them they will be more or less lined up and ready to sort based on the same keys.\\n\\n\\n\\n    In [35]: df2 = df.groupby(level=0).mean()\\n\\n    \\n\\n    In [36]: df2['index2'] = 'AVG'\\n\\n    \\n\\n    In [37]: df2 = df2.reset_index().set_index(['index','index2']).append(df).sort()\\n\\n\\n\\n    In [38]: df2\\n\\n    Out[38]: \\n\\n                 metric 1     metric 2    \\n\\n                        R   P        R   P\\n\\n    index index2                          \\n\\n    bar   AVG          10  11       12  13\\n\\n          a             8   9       10  11\\n\\n          b            12  13       14  15\\n\\n    foo   AVG           2   3        4   5\\n\\n          a             0   1        2   3\\n\\n          b             4   5        6   7\\n\\n\\n\\n\\n\\nAs far as ordering the rows, the best thing is probably just to set the names so that sorting puts them in the right place (e.g. A,B,avg).  Or for a small number of rows you could just use fancy indexing:\\n\\n\\n\\n    In [39]: df2.ix[[4,5,3,1,2,0]]\\n\\n    Out[39]: \\n\\n                 metric 1     metric 2    \\n\\n                        R   P        R   P\\n\\n    index index2                          \\n\\n    foo   a             0   1        2   3\\n\\n          b             4   5        6   7\\n\\n          AVG           2   3        4   5\\n\\n    bar   a             8   9       10  11\\n\\n          b            12  13       14  15\\n\\n          AVG          10  11       12  13\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 3, 17),\n",
       "  '2015-03-17 03:49:32',\n",
       "  'JohnE (3877338)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1356.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['726',\n",
       "  '25331482',\n",
       "  'Answer',\n",
       "  'how to select inverse of indexes of a numpy array',\n",
       "  'You may want to try `in1d`\\n\\n\\n\\n    In [5]:\\n\\n\\n\\n    select = np.in1d(range(data.shape[0]), sample_indexes)\\n\\n    In [6]:\\n\\n\\n\\n    print data[select]\\n\\n    [[ 0.99121108  0.35582816]\\n\\n     [ 0.90154837  0.86254049]\\n\\n     [ 0.83149103  0.42222948]]\\n\\n    In [7]:\\n\\n\\n\\n    print data[~select]\\n\\n    [[ 0.93825827  0.26701143]\\n\\n     [ 0.27309625  0.38925281]\\n\\n     [ 0.06510739  0.58445673]\\n\\n     [ 0.61469637  0.05420098]\\n\\n     [ 0.92685408  0.62715114]\\n\\n     [ 0.22587817  0.56819403]\\n\\n     [ 0.28400409  0.21112043]]',\n",
       "  '<python><numpy><scipy>',\n",
       "  datetime.date(2014, 8, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '10296.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['728',\n",
       "  '29608872',\n",
       "  'Answer',\n",
       "  'Pandas indexing by both boolean `loc` and subsequent `iloc`',\n",
       "  \"I don't know if this is any more elegant, but it's a little different:\\n\\n\\n\\n    mask = mask & (mask.cumsum() < 3)\\n\\n    \\n\\n    df.loc[mask, 'c'] = 1\\n\\n    \\n\\n       a  b  c\\n\\n    0  0  5  0\\n\\n    1  1  5  0\\n\\n    2  2  2  1\\n\\n    3  3  2  1\\n\\n    4  4  5  0\\n\\n    5  5  5  0\\n\\n    6  6  2  0\\n\\n    7  7  2  0\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 4, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '5918.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['729',\n",
       "  '29610712',\n",
       "  'Answer',\n",
       "  'Pivot table with Pandas float and int values',\n",
       "  \"This may be a workaround or explanation more so than an answer, but FWIW.\\n\\n\\n\\nI suspect you don't really need to store anything as a decimal, just floats (in fact you refer to the decimal types as floats in your question, but they aren't the same thing, try ```df.info()```).  I would suggest starting with floats in the first place, or convert them:\\n\\n\\n\\n    value=['A','D']\\n\\n\\n\\n    df[value] = df[value].astype(float)\\n\\n\\n\\nDecimal types get stored as objects.  Unless you really need to do this, floats will be faster and easier to work with.  If you are doing this because you want to seee two decimal places, just use formats on the floats, although pandas default format choices will often be what you want anyway.  If you use ipython, check out the ```%precision``` magic which lets you specify the default formatting of floats.\\n\\n\\n\\nNow, it works fine:\\n\\n\\n\\n    df.pivot_table(columns=columns, index=rows,       values=value,\\n\\n                   margins=True,    aggfunc=np.sum)\\n\\n\\n\\n              A                  D           \\n\\n    E      2007 2008      All 2007 2008   All\\n\\n    F                                        \\n\\n    Ala  705.52  696  1401.52  525  518  1043\\n\\n    All  705.52  696  1401.52  525  518  1043\\n\\n\\n\\nNow, all of that said, I don't see any problem with what you did, and it may be a bug in pandas.  At the same time, I would stick with doing it the simpler way (floats instead of decimals) unless you really have a reason to use the decimal type, and then the problem doesn't appear.\",\n",
       "  '<python-2.7><pandas><pivot>',\n",
       "  datetime.date(2015, 4, 13),\n",
       "  '2015-04-14 12:45:50',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '2152.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['732',\n",
       "  '28997816',\n",
       "  'Answer',\n",
       "  'pandas shift time series with missing values',\n",
       "  \"    In [588]: df = pd.DataFrame({ 'date':[2000,2001,2003,2004,2005,2007],\\n\\n                                  'value':[5,10,8,72,12,13] })\\n\\n\\n\\n    In [589]: df['previous_value'] = df.value.shift()[ df.date == df.date.shift() + 1 ]\\n\\n\\n\\n    In [590]: df\\n\\n    Out[590]: \\n\\n       date  value  previous_value\\n\\n    0  2000      5             NaN\\n\\n    1  2001     10               5\\n\\n    2  2003      8             NaN\\n\\n    3  2004     72               8\\n\\n    4  2005     12              72\\n\\n    5  2007     13             NaN\\n\\n\\n\\nAlso see here for a time series approach using ```resample()```:  https://stackoverflow.com/questions/25388189/using-shift-with-unevenly-spaced-data\",\n",
       "  '<python><pandas><time-series><shift>',\n",
       "  datetime.date(2015, 3, 11),\n",
       "  '2017-05-23 12:29:34',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1100.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['734',\n",
       "  '29416421',\n",
       "  'Answer',\n",
       "  'Pandas, Computing total sum on each MultiIndex sublevel',\n",
       "  \"Quite ugly code:\\n\\n\\n\\n    In [162]:\\n\\n\\n\\n    print df\\n\\n                    values\\n\\n    first second          \\n\\n    bar   one     0.370291\\n\\n          two     0.750565\\n\\n    baz   one     0.148405\\n\\n          two     0.919973\\n\\n    foo   one     0.121964\\n\\n          two     0.394017\\n\\n    qux   one     0.883136\\n\\n          two     0.871792\\n\\n    In [163]:\\n\\n\\n\\n    print pd.concat((df.reset_index(),\\n\\n                     df.reset_index().groupby('first').aggregate('sum').reset_index())).\\\\\\n\\n                          sort(['first','second']).\\\\\\n\\n                          fillna('total').\\\\\\n\\n                          set_index(['first','second'])\\n\\n                    values\\n\\n    first second          \\n\\n    bar   one     0.370291\\n\\n          two     0.750565\\n\\n          total   1.120856\\n\\n    baz   one     0.148405\\n\\n          two     0.919973\\n\\n          total   1.068378\\n\\n    foo   one     0.121964\\n\\n          two     0.394017\\n\\n          total   0.515981\\n\\n    qux   one     0.883136\\n\\n          two     0.871792\\n\\n          total   1.754927\\n\\n\\n\\n\\n\\nBasically, since the additional rows, 'total', need to be calculated and inserted into the original dataframe, it is not going to be a one-to-one relationship between the original and resultant, neither the relationship is a many-to-one type. So, I think you have to generate the 'total' dataframe separately and `concat` it with the original dataframe.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 4, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1956.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['737',\n",
       "  '25235710',\n",
       "  'Answer',\n",
       "  'Getting standard error associated with parameter estimates from scipy.optimize.curve_fit',\n",
       "  'The variance of parameters are the diagonal elements of the variance-co variance matrix, and the standard error is the square root of it. `np.sqrt(np.diag(pcov))`\\n\\n\\n\\nRegarding getting `inf`, see and compare these two examples:\\n\\n\\n\\n\\tIn [129]:\\n\\n    import numpy as np\\n\\n\\tdef func(x, a, b, c, d):\\n\\n\\t\\treturn a * np.exp(-b * x) + c\\n\\n\\t \\n\\n\\txdata = np.linspace(0, 4, 50)\\n\\n\\ty = func(xdata, 2.5, 1.3, 0.5, 1)\\n\\n\\tydata = y + 0.2 * np.random.normal(size=len(xdata))\\n\\n\\tpopt, pcov = so.curve_fit(func, xdata, ydata)\\n\\n\\tprint np.sqrt(np.diag(pcov))\\n\\n\\t[ inf  inf  inf  inf]\\n\\n\\n\\nAnd:\\n\\n\\n\\n\\tIn [130]:\\n\\n\\n\\n\\tdef func(x, a, b, c):\\n\\n\\t\\treturn a * np.exp(-b * x) + c\\n\\n\\t \\n\\n\\txdata = np.linspace(0, 4, 50)\\n\\n\\ty = func(xdata, 2.5, 1.3, 0.5)\\n\\n\\tydata = y + 0.2 * np.random.normal(size=len(xdata))\\n\\n\\tpopt, pcov = so.curve_fit(func, xdata, ydata)\\n\\n\\tprint np.sqrt(np.diag(pcov))\\n\\n\\t[ 0.11097646  0.11849107  0.05230711]\\n\\n\\n\\nIn this extreme example, `d` has no effect on the function `func`, hence it will be associated with variance of `+inf`, or in another word, it can be just about any value. Removing `d` from `func` will get what will make sense.\\n\\n\\n\\nIn reality, if parameters are of very different scale, say:\\n\\n\\n\\n\\tdef func(x, a, b, c, d):\\n\\n\\t\\t#return a * np.exp(-b * x) + c\\n\\n        return a * np.exp(-b * x) + c + d*1e-10\\n\\n\\n\\nYou will also get `inf` due to float point overflow/underflow.\\n\\n\\n\\nIn your case, I think you never used `a` and `b`. So it is just like the first example here. ',\n",
       "  '<python><scipy><mathematical-optimization><curve-fitting>',\n",
       "  datetime.date(2014, 8, 11),\n",
       "  '2018-05-06 07:38:17',\n",
       "  'CT Zhu (2487184), pradeep (6529212)',\n",
       "  '7',\n",
       "  '',\n",
       "  '5199.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['739',\n",
       "  '25270271',\n",
       "  'Answer',\n",
       "  'Solving a bounded non-linear minimization with scipy in python',\n",
       "  \"It should be:\\n\\n\\n\\n    print minimize(my_func, mean_period, bounds=((2,200),))\\n\\n\\n\\n      status: 0\\n\\n     success: True\\n\\n        nfev: 57\\n\\n         fun: array([-0.08191999])\\n\\n           x: array([ 12.34003932])\\n\\n     message: 'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL'\\n\\n         jac: array([  2.17187379e-06])\\n\\n         nit: 4\\n\\n\\n\\nFor each parameter you have to provide a bound, therefore here we need to pass a `tuple`, which contains only one `tuple` `(2,200)`, to `minimize()`. \",\n",
       "  '<python><optimization><scipy><nonlinear-optimization><minimization>',\n",
       "  datetime.date(2014, 8, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '3957.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['741',\n",
       "  '29886466',\n",
       "  'Question',\n",
       "  'Distribution-type graphs (histogram/kde) with weighted data',\n",
       "  \"In a nutshell, what is my best option for a distribution-type graphs (histogram or kde) when my data is weighted?\\n\\n\\n\\n    df = pd.DataFrame({ 'x':[1,2,3,4], 'wt':[7,5,3,1] })\\n\\n\\n\\n    df.x.plot(kind='hist',weights=df.wt.values)\\n\\n\\n\\nThat works fine but seaborn won't accept a weights kwarg, i.e.\\n\\n\\n\\n    sns.distplot( df.x, bins=4,              # doesn't work like this\\n\\n                  weights=df.wt.values )     # or with kde=False added\\n\\n\\n\\nIt would also be nice if kde would accept weights but neither pandas nor seaborn seems to allow it.  \\n\\n\\n\\nI realize btw that the data could be expanded to fake the weighting and that's easy here but not of much use with my real data with weights in the hundreds or thousand, so I'm not looking for a workaround like that.\\n\\n\\n\\nAnyway, that's all.  I'm just trying to find out what (if anything) I can do with weighted data besides the basic pandas histogram.  I haven't fooled around with bokeh yet, but bokeh suggestions are also welcome.\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><pandas><matplotlib><bokeh><seaborn>',\n",
       "  datetime.date(2015, 4, 27),\n",
       "  '2015-04-27 03:33:58',\n",
       "  'JohnE (3877338)',\n",
       "  '5',\n",
       "  '2.0',\n",
       "  '1725.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['744',\n",
       "  '29439876',\n",
       "  'Answer',\n",
       "  'Pandas: Elsement-wise average and standard deviation across multiple dataframes',\n",
       "  \"One simple solution here is to simply concatenate the existing dataframes into a single dataframe while adding an ID variable to track the original source:\\n\\n\\n\\n    dfa = pd.DataFrame( np.random.randn(2,2), columns=['a','b'] ).assign(id='a')\\n\\n    dfb = pd.DataFrame( np.random.randn(2,2), columns=['a','b'] ).assign(id='b')\\n\\n\\n\\n    df = pd.concat([df1,df2])\\n\\n    \\n\\n              a         b id\\n\\n    0 -0.542652  1.609213  a\\n\\n    1 -0.192136  0.458564  a\\n\\n    0 -0.231949 -0.000573  b\\n\\n    1  0.245715 -0.083786  b\\n\\n\\n\\nSo now you have two 2x2 dataframes combined into a single 4x2 dataframe.  The 'id' columns identifies the source dataframe so you haven't lost any generality, and can select on 'id' to do the same thing you would to any single dataframe.  E.g. `df[ df['id'] == 'a' ]`.\\n\\n    \\n\\nBut now you can also use `groupby` to do any pandas method such as `mean()` or `std()` on an element by element basis:\\n\\n\\n\\n    df.groupby('id').mean()\\n\\n    \\n\\n                  a         b\\n\\n    index                    \\n\\n    0      0.198164 -0.811475\\n\\n    1      0.639529  0.812810\\n\\n    \\n\\n\\n\\n\",\n",
       "  '<python><python-3.x><pandas>',\n",
       "  datetime.date(2015, 4, 3),\n",
       "  '2018-03-02 01:28:38',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1085.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['747',\n",
       "  '29434620',\n",
       "  'Answer',\n",
       "  'pandas/matplotlib datetime tick labels',\n",
       "  \"You should be able to set the major ticks to the format you want, using `set_major_formatter`:\\n\\n\\n\\n    In [14]:\\n\\n\\n\\n    import matplotlib as mpl\\n\\n    import matplotlib.dates\\n\\n    df = pd.DataFrame({'column1': [1,2,3,4],\\n\\n                       'column2': [2,3,4,5]},\\n\\n                       index =pd.to_datetime([1e8,2e8,3e8,4e8]))\\n\\n    def plot(df):\\n\\n        ax = df.plot(y='column1', figsize=(20, 8))\\n\\n        df.plot(y='column2', ax=ax)\\n\\n\\n\\n        ax.get_yaxis().get_major_formatter().set_useOffset(False)\\n\\n        ax.get_xaxis().set_major_formatter(matplotlib.dates.DateFormatter('%H:%M:%S.%f'))\\n\\n\\n\\n        #mpl.pyplot.show()\\n\\n        return ax\\n\\n\\n\\n    print df\\n\\n                                column1  column2\\n\\n    1970-01-01 00:00:00.100000        1        2\\n\\n    1970-01-01 00:00:00.200000        2        3\\n\\n    1970-01-01 00:00:00.300000        3        4\\n\\n    1970-01-01 00:00:00.400000        4        5\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nIf the problem do go away, then I think somewhere in the code the formatter format is specified incorrectly, namely `%%f` instead of `%f`, which returns a literal `'%'` character.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/K1Ppj.png\",\n",
       "  '<python><pandas><matplotlib>',\n",
       "  datetime.date(2015, 4, 3),\n",
       "  '2015-04-06 15:12:18',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1995.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['749',\n",
       "  '29830090',\n",
       "  'Answer',\n",
       "  'Pandas Categorical data type not behaving as expected',\n",
       "  \"I don't think you can specify an order, [`pd.factorize`](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.factorize.html) appears to give that option, but it is not implemented, see [here](https://github.com/pydata/pandas/issues/6926).\\n\\n\\n\\nBased on what you described, you are looking for coding the `code` variable into an *ordinal* variable, not a *categorical* variable, which are [slightly different](http://www.ats.ucla.edu/stat/mult_pkg/whatstat/nominal_ordinal_interval.htm).\\n\\n\\n\\nIf you can assume the difference between `'one'` and `'two'` is equal to that between `'two'` and `'three'`. I guess you can just code them into `int`s `(0, 1, 2, 3 ...)`.\\n\\n\\n\\nIf you use [`patsy`](https://github.com/pydata/patsy), then there is a [nice example for ordinal variables](http://statsmodels.sourceforge.net/devel/contrasts.html#backward-difference-coding)\",\n",
       "  '<python><pandas><categorical-data><ordinal>',\n",
       "  datetime.date(2015, 4, 23),\n",
       "  '2016-12-13 13:59:06',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '2487.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['751',\n",
       "  '30109512',\n",
       "  'Answer',\n",
       "  'How to assert if a std::mutex is locked?',\n",
       "  \"Strictly speaking, the question was about checking the lockedness of ``std::mutex`` directly. However, if encapsulating it in a new class is allowed, it's very easy to do so:\\n\\n\\n\\n    class mutex :\\n\\n        public std::mutex\\n\\n    {\\n\\n    public:\\n\\n    #ifndef NDEBUG\\n\\n        void lock()\\n\\n        {\\n\\n            std::mutex::lock();\\n\\n            m_holder = std::this_thread::get_id(); \\n\\n        }\\n\\n    #endif // #ifndef NDEBUG\\n\\n\\n\\n    #ifndef NDEBUG\\n\\n        void unlock()\\n\\n        {\\n\\n            m_holder = std::thread::id();\\n\\n            std::mutex::unlock();\\n\\n        }\\n\\n    #endif // #ifndef NDEBUG\\n\\n\\n\\n    #ifndef NDEBUG\\n\\n        /**\\n\\n        * @return true iff the mutex is locked by the caller of this method. */\\n\\n        bool locked_by_caller() const\\n\\n        {\\n\\n            return m_holder == std::this_thread::get_id();\\n\\n        }\\n\\n    #endif // #ifndef NDEBUG\\n\\n\\n\\n    private:\\n\\n    #ifndef NDEBUG\\n\\n        std::thread::id m_holder;\\n\\n    #endif // #ifndef NDEBUG\\n\\n    };\\n\\n\\n\\nNote the following:\\n\\n\\n\\n1. In release mode, this has zero overhead over ``std::mutex`` except possibly for construction/destruction (which is a non-issue for mutex objects).\\n\\n2. The ``m_holder`` member is only accessed between taking the mutex and releasing it. Thus the mutex itself serves as the mutex of ``m_holder``. With very weak assumptions on the type ``std::thread::id``, ``locked_by_caller`` will work correctly.\\n\\n3. Other STL components, e.g., ``std::lock_guard`` are templates, so they work well with this new class.\",\n",
       "  '<c++><linux><gcc><c++11>',\n",
       "  datetime.date(2015, 5, 7),\n",
       "  '2017-01-13 06:33:35',\n",
       "  'Ami Tavory (3510736), Carlo Wood (1487069)',\n",
       "  '13',\n",
       "  '',\n",
       "  '13276.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['755',\n",
       "  '25915541',\n",
       "  'Question',\n",
       "  'Fastest way to numerically process 2d-array: dataframe vs series vs array vs numba',\n",
       "  \"**Edit to add**:  I don't think the numba benchmarks are fair, notes below\\n\\n\\n\\nI'm trying to benchmark different approaches to numerically processing data for the following use case:\\n\\n\\n\\n 1. Fairly big dataset (100,000+ records)\\n\\n 2. 100+ lines of fairly simple code (z = x + y)\\n\\n 3. Don't need to sort or index\\n\\n\\n\\nIn other words, the full generality of series and dataframes is not needed, although they are included here b/c they are still convenient ways to encapsulate the data and there is often pre- or post-processing that does require the generality of pandas over numpy arrays.\\n\\n\\n\\n**Question**:  Based on this use case, are the following benchmarks appropriate and if not, how can I improve them?\\n\\n\\n\\n    # importing pandas, numpy, Series, DataFrame in standard way\\n\\n    from numba import jit\\n\\n    nobs = 10000\\n\\n    nlines = 100\\n\\n    \\n\\n    def proc_df():\\n\\n       df = DataFrame({ 'x': np.random.randn(nobs),\\n\\n                        'y': np.random.randn(nobs) })\\n\\n       for i in range(nlines):\\n\\n          df['z'] = df.x + df.y\\n\\n       return df.z\\n\\n    \\n\\n    def proc_ser():\\n\\n       x = Series(np.random.randn(nobs))\\n\\n       y = Series(np.random.randn(nobs))\\n\\n       for i in range(nlines):\\n\\n          z = x + y\\n\\n       return z\\n\\n    \\n\\n    def proc_arr():\\n\\n       x = np.random.randn(nobs)\\n\\n       y = np.random.randn(nobs)\\n\\n       for i in range(nlines):\\n\\n          z = x + y\\n\\n       return z\\n\\n    \\n\\n    @jit\\n\\n    def proc_numba():\\n\\n       xx = np.random.randn(nobs)\\n\\n       yy = np.random.randn(nobs)\\n\\n       zz = np.zeros(nobs)\\n\\n       for j in range(nobs):\\n\\n          x, y = xx[j], yy[j]\\n\\n          for i in range(nlines):\\n\\n             z = x + y\\n\\n          zz[j] = z\\n\\n       return zz\\n\\n\\n\\nResults (Win 7, 3 year old Xeon workstation (quad-core).  Standard and recent anaconda distribution or very close.)\\n\\n\\n\\n    In [1251]: %timeit proc_df()\\n\\n    10 loops, best of 3: 46.6 ms per loop\\n\\n    \\n\\n    In [1252]: %timeit proc_ser()\\n\\n    100 loops, best of 3: 15.8 ms per loop\\n\\n    \\n\\n    In [1253]: %timeit proc_arr()\\n\\n    100 loops, best of 3: 2.02 ms per loop\\n\\n    \\n\\n    In [1254]: %timeit proc_numba()\\n\\n    1000 loops, best of 3: 1.04 ms per loop   # may not be valid result (see note below)\\n\\n\\n\\n**Edit to add** (response to jeff) alternate results from passing df/series/array into functions rather than creating them inside of functions (i.e. move the code lines containing 'randn' from inside function to outside function):\\n\\n\\n\\n    10 loops, best of 3: 45.1 ms per loop\\n\\n    100 loops, best of 3: 15.1 ms per loop\\n\\n    1000 loops, best of 3: 1.07 ms per loop\\n\\n    100000 loops, best of 3: 17.9 µs per loop   # may not be valid result (see note below)\\n\\n\\n\\n**Note on numba results**:  I think the numba compiler must be optimizing on the for loop and reducing the for loop to a single iteration.  I don't know that but it's the only explanation I can come up as it couldn't be 50x faster than numpy, right?  Followup question here:  https://stackoverflow.com/questions/25950943/why-is-numba-faster-than-numpy-here\",\n",
       "  '<python><numpy><pandas><numba>',\n",
       "  datetime.date(2014, 9, 18),\n",
       "  '2017-05-23 12:18:02',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '1',\n",
       "  '3.0',\n",
       "  '1816.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['756',\n",
       "  '30367448',\n",
       "  'Answer',\n",
       "  'Are C++11 containers supported by Cython?',\n",
       "  'Current cython versions allow them. \\n\\n\\n\\nMake sure your ``setup.py`` contains something like:\\n\\n\\n\\n    ext_module = Extension(\\n\\n        \"foo\",\\n\\n        [\"foo.pyx\"],\\n\\n        language=\"c++\",\\n\\n        extra_compile_args=[\"-std=c++11\"],\\n\\n        extra_link_args=[\"-std=c++11\"]\\n\\n    )\\n\\n\\n\\nYou can then use\\n\\n\\n\\n    from libcpp.unordered_map cimport unordered_map\\n\\n\\n\\nlike for any other STL class.',\n",
       "  '<python><c++><c++11><cython><c++-standard-library>',\n",
       "  datetime.date(2015, 5, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '17',\n",
       "  '',\n",
       "  '3020.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['758',\n",
       "  '30392309',\n",
       "  'Answer',\n",
       "  'Minimizing a multivariable function with scipy. Derivative not known',\n",
       "  'The [Nelder-Mead Simplex Method](http://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method) (suggested by [Cristián Antuña](https://stackoverflow.com/users/4454801/cristi%c3%a1n-antu%c3%b1a) in the comments above) is well known to be a good choice for optimizing (posibly ill-behaved) functions with no knowledge of derivatives (see [Numerical Recipies In C, Chapter 10](https://www.google.co.il/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&cad=rja&uact=8&ved=0CCUQFjAB&url=https%3A%2F%2Fwww.uam.es%2Fpersonal_pdi%2Fciencias%2Fppou%2FCNC%2FTEMA6%2Ff10.pdf&ei=_OheVZiBCYGrUN2kgdgB&usg=AFQjCNGvo3F22nKexeFK95ZRsgR44C9RmA&bvm=bv.93990622,d.d24)).\\n\\n\\n\\nThere are two somewhat specific aspects to your question. The first is the constraints on the inputs, and the second is a scaling problem. The following suggests solutions to these points, but you might need to manually iterate between them a few times until things work.\\n\\n\\n\\n\\n\\n**Input Constraints**\\n\\n\\n\\nAssuming your input constraints form a [convex region](http://encyclopedia2.thefreedictionary.com/Convex+region) (as your examples above indicate, but I\\'d like to generalize it a bit), then you can write a function\\n\\n\\n\\n    is_in_bounds(p):\\n\\n        # Return if p is in the bounds\\n\\n\\n\\nUsing this function, assume that the algorithm wants to move from point ``from_`` to point ``to``, where ``from_`` is known to be in the region. Then the following function will efficiently find the furthermost point on the line between the two points on which it can proceed:\\n\\n\\n\\n    from numpy.linalg import norm\\n\\n\\n\\n    def progress_within_bounds(from_, to, eps):\\n\\n        \"\"\"\\n\\n        from_ -- source (in region)\\n\\n        to -- target point\\n\\n        eps -- Eucliedan precision along the line\\n\\n        \"\"\"\\n\\n\\n\\n        if norm(from_, to) < eps:\\n\\n            return from_\\n\\n        mid = (from_ + to) / 2\\n\\n        if is_in_bounds(mid):\\n\\n            return progress_within_bounds(mid, to, eps)\\n\\n        return progress_within_bounds(from_, mid, eps)\\n\\n\\n\\n(Note that this function can be optimized for some regions, but it\\'s hardly worth the bother, as it doesn\\'t even call your original object function, which is the expensive one.)\\n\\n\\n\\nOne of the nice aspects of Nelder-Mead is that the function does a series of steps which are so intuitive. Some of these points can obviously throw you out of the region, but it\\'s easy to modify this. Here is an [implementation of Nelder Mead](https://github.com/fchollet/nelder-mead) with modifications made marked between pairs of lines of the form ``##################################################################``:\\n\\n\\n\\n    import copy\\n\\n\\n\\n    \\'\\'\\'\\n\\n        Pure Python/Numpy implementation of the Nelder-Mead algorithm.\\n\\n        Reference: https://en.wikipedia.org/wiki/Nelder%E2%80%93Mead_method\\n\\n    \\'\\'\\'\\n\\n\\n\\n\\n\\n    def nelder_mead(f, x_start, \\n\\n            step=0.1, no_improve_thr=10e-6, no_improv_break=10, max_iter=0,\\n\\n            alpha = 1., gamma = 2., rho = -0.5, sigma = 0.5):\\n\\n        \\'\\'\\'\\n\\n            @param f (function): function to optimize, must return a scalar score \\n\\n                and operate over a numpy array of the same dimensions as x_start\\n\\n            @param x_start (numpy array): initial position\\n\\n            @param step (float): look-around radius in initial step\\n\\n            @no_improv_thr,  no_improv_break (float, int): break after no_improv_break iterations with \\n\\n                an improvement lower than no_improv_thr\\n\\n            @max_iter (int): always break after this number of iterations.\\n\\n                Set it to 0 to loop indefinitely.\\n\\n            @alpha, gamma, rho, sigma (floats): parameters of the algorithm \\n\\n                (see Wikipedia page for reference)\\n\\n        \\'\\'\\'\\n\\n\\n\\n        # init\\n\\n        dim = len(x_start)\\n\\n        prev_best = f(x_start)\\n\\n        no_improv = 0\\n\\n        res = [[x_start, prev_best]]\\n\\n\\n\\n        for i in range(dim):\\n\\n            x = copy.copy(x_start)\\n\\n            x[i] = x[i] + step\\n\\n            score = f(x)\\n\\n            res.append([x, score])\\n\\n\\n\\n        # simplex iter\\n\\n        iters = 0\\n\\n        while 1:\\n\\n            # order\\n\\n            res.sort(key = lambda x: x[1])\\n\\n            best = res[0][1]\\n\\n\\n\\n            # break after max_iter\\n\\n            if max_iter and iters >= max_iter:\\n\\n                return res[0]\\n\\n            iters += 1\\n\\n\\n\\n            # break after no_improv_break iterations with no improvement\\n\\n            print \\'...best so far:\\', best\\n\\n\\n\\n            if best < prev_best - no_improve_thr:\\n\\n                no_improv = 0\\n\\n                prev_best = best\\n\\n            else:\\n\\n                no_improv += 1\\n\\n        \\n\\n            if no_improv >= no_improv_break:\\n\\n                return res[0]\\n\\n\\n\\n            # centroid\\n\\n            x0 = [0.] * dim\\n\\n            for tup in res[:-1]:\\n\\n                for i, c in enumerate(tup[0]):\\n\\n                    x0[i] += c / (len(res)-1)\\n\\n\\n\\n            # reflection\\n\\n            xr = x0 + alpha*(x0 - res[-1][0])\\n\\n            ##################################################################\\n\\n            ##################################################################\\n\\n            xr = progress_within_bounds(x0, x0 + alpha*(x0 - res[-1][0]), prog_eps)\\n\\n            ##################################################################\\n\\n            ##################################################################\\n\\n            rscore = f(xr)\\n\\n            if res[0][1] <= rscore < res[-2][1]:\\n\\n                del res[-1]\\n\\n                res.append([xr, rscore])\\n\\n                continue\\n\\n\\n\\n            # expansion\\n\\n            if rscore < res[0][1]:\\n\\n                xe = x0 + gamma*(x0 - res[-1][0])\\n\\n                ##################################################################\\n\\n                ##################################################################\\n\\n                xe = progress_within_bounds(x0, x0 + gamma*(x0 - res[-1][0]), prog_eps)\\n\\n                ##################################################################\\n\\n                ################################################################## \\n\\n                escore = f(xe)\\n\\n                if escore < rscore:\\n\\n                    del res[-1]\\n\\n                    res.append([xe, escore])\\n\\n                    continue\\n\\n                else:\\n\\n                    del res[-1]\\n\\n                    res.append([xr, rscore])\\n\\n                    continue\\n\\n\\n\\n            # contraction\\n\\n            xc = x0 + rho*(x0 - res[-1][0])\\n\\n            ##################################################################\\n\\n            ##################################################################\\n\\n            xc = progress_within_bounds(x0, x0 + rho*(x0 - res[-1][0]), prog_eps)\\n\\n            ##################################################################\\n\\n            ################################################################## \\n\\n            cscore = f(xc)\\n\\n            if cscore < res[-1][1]:\\n\\n                del res[-1]\\n\\n                res.append([xc, cscore])\\n\\n                continue\\n\\n\\n\\n            # reduction\\n\\n            x1 = res[0][0]\\n\\n            nres = []\\n\\n            for tup in res:\\n\\n                redx = x1 + sigma*(tup[0] - x1)\\n\\n                score = f(redx)\\n\\n                nres.append([redx, score])\\n\\n            res = nres\\n\\n\\n\\n**Note** This implementation is [GPL](https://github.com/fchollet/nelder-mead/blob/master/LICENSE), which is either fine for you or not. It\\'s extremely easy to modify NM from any pseudocode, though, and you might want to throw in [simulated annealing](http://en.wikipedia.org/wiki/Simulated_annealing) in any case.\\n\\n\\n\\n**Scaling**\\n\\n\\n\\nThis is a trickier problem, but [jasaarim](https://stackoverflow.com/users/4867417/jasaarim) has made an interesting point regarding that. Once the modified NM algorithm has found a point, you might want to run [``matplotlib.contour``](http://matplotlib.org/examples/pylab_examples/contour_demo.html) while fixing a few dimensions, in order to see how the function behaves. At this point, you might want to rescale one or more of the dimensions, and rerun the modified NM.\\n\\n\\n\\n\\t\\n\\n–  ',\n",
       "  '<python><scipy><mathematical-optimization><minimization>',\n",
       "  datetime.date(2015, 5, 22),\n",
       "  '2017-05-23 11:55:15',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '1',\n",
       "  '',\n",
       "  '5344.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['770',\n",
       "  '25334088',\n",
       "  'Answer',\n",
       "  'Remove all-zero rows in a 2D matrix',\n",
       "  'Boolean indexing will do it:\\n\\n\\n\\n    In [2]:\\n\\n\\n\\n    a\\n\\n    Out[2]:\\n\\n    array([[4, 1, 1, 2, 0, 4],\\n\\n           [3, 4, 3, 1, 4, 4],\\n\\n           [1, 4, 3, 1, 0, 0],\\n\\n           [0, 4, 4, 0, 4, 3],\\n\\n           [0, 0, 0, 0, 0, 0]])\\n\\n    In [3]:\\n\\n\\n\\n    a[~(a==0).all(1)]\\n\\n    Out[3]:\\n\\n    array([[4, 1, 1, 2, 0, 4],\\n\\n           [3, 4, 3, 1, 4, 4],\\n\\n           [1, 4, 3, 1, 0, 0],\\n\\n           [0, 4, 4, 0, 4, 3]])\\n\\n',\n",
       "  '<python><numpy><scipy>',\n",
       "  datetime.date(2014, 8, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '4606.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['777',\n",
       "  '30426108',\n",
       "  'Answer',\n",
       "  'How to insert ascending numbers in a Red Black Tree',\n",
       "  'It depends on what you mean by \" insert numbers 1-10 in a Red-black tree\":\\n\\n\\n\\n- If you mean a legal tree obtained from these values, you can build a red-black tree from an ordered vector in linear time.\\n\\n\\n\\n- If you mean a visualization of 10 insert operations, you can try [here]( https://www.cs.usfca.edu/~galles/visualization/RedBlack.html).\\n\\n\\n\\n![Drawn with the help of the link mentioned here][1]\\n\\n\\n\\n- For a general description of the insert operation, and detailed examples for various cases, see [\"Introduction To Algorithms\"](http://mitpress.mit.edu/books/introduction-algorithms).\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/FnNCE.png',\n",
       "  '<red-black-tree>',\n",
       "  datetime.date(2015, 5, 24),\n",
       "  '2015-05-24 17:37:32',\n",
       "  'Am_I_Helpful (3482140)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1047.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['789',\n",
       "  '29978353',\n",
       "  'Answer',\n",
       "  'Combining pandas data frames with overlapping columns / rows',\n",
       "  \"Here's a quick guess at both the convenience and efficiency angles, based on non-overlapping datapoints and assuming very regular data (everything 3x3 in this case).\\n\\n\\n\\n    df1=pd.DataFrame( np.random.randn(3,3), index=list('ABC'), columns=list('123') )\\n\\n    df2=pd.DataFrame( np.random.randn(3,3), index=list('DEF'), columns=list('123') )\\n\\n    df3=pd.DataFrame( np.random.randn(3,3), index=list('ABC'), columns=list('456') )\\n\\n    df4=pd.DataFrame( np.random.randn(3,3), index=list('DEF'), columns=list('456') )\\n\\n    \\n\\nThe `combine_first` way has the advantage that you can just dump everything in a list without worrying about the order:\\n\\n\\n\\n    %%timeit\\n\\n    comb_df = pd.DataFrame()\\n\\n    for df in [df1,df2,df3,df4]:  \\n\\n        comb_df = comb_df.combine_first( df )\\n\\n    \\n\\n    100 loops, best of 3: 8.92 ms per loop\\n\\n    \\n\\nThe `concat` way requires you to group things in a specific order, but is more than twice as fast:\\n\\n\\n\\n    %%timeit\\n\\n    df5 = pd.concat( [df1,df2], axis=0 )\\n\\n    df6 = pd.concat( [df3,df4], axis=0 )\\n\\n    df7 = pd.concat( [df5,df6], axis=1 )\\n\\n    \\n\\n    100 loops, best of 3: 3.84 ms per loop\\n\\n\\n\\nQuick check that both ways work the same:\\n\\n\\n\\n    all( comb_df == df7 )\\n\\n    True\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 4, 30),\n",
       "  '2015-04-30 22:39:06',\n",
       "  'JohnE (3877338)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1388.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['792',\n",
       "  '30214901',\n",
       "  'Answer',\n",
       "  'What is the difference between pandas.qcut and pandas.cut?',\n",
       "  \"To begin, note that quantiles is just the most general term for things like percentiles, quartiles, and medians.  You specified five bins in your example, so you are asking `qcut` for quintiles.\\n\\n\\n\\nSo, when you ask for quintiles with `qcut`, the bins will be chosen so that you have the same number of records in each bin.  You have 30 records, so should have 6 in each bin (your output should look like this, although the breakpoints will differ due to the random draw):\\n\\n\\n\\n    pd.qcut(factors, 5).value_counts()\\n\\n    \\n\\n    [-2.578, -0.829]    6\\n\\n    (-0.829, -0.36]     6\\n\\n    (-0.36, 0.366]      6\\n\\n    (0.366, 0.868]      6\\n\\n    (0.868, 2.617]      6\\n\\n\\n\\nConversely, for `cut` you will see something more uneven:\\n\\n\\n\\n    pd.cut(factors, 5).value_counts()\\n\\n     \\n\\n    (-2.583, -1.539]    5\\n\\n    (-1.539, -0.5]      5\\n\\n    (-0.5, 0.539]       9\\n\\n    (0.539, 1.578]      9\\n\\n    (1.578, 2.617]      2\\n\\n\\n\\nThat's because `cut` will choose the bins to be evenly spaced according to the values themselves and not the *frequency* of those values.  Hence, because you drew from a random normal, you'll see higher frequencies in the inner bins and fewer in the outer.  This is essentially going to be a tabular form of a histogram (which you would expect to be fairly bell shaped with 30 records).\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 13),\n",
       "  '2015-05-13 16:58:14',\n",
       "  'JohnE (3877338)',\n",
       "  '124',\n",
       "  '',\n",
       "  '28029.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['796',\n",
       "  '26415986',\n",
       "  'Answer',\n",
       "  'Read multiple *.txt files into Pandas Dataframe with filename as column header',\n",
       "  \"You can read them into multiple dataframes and concat them together afterwards. Suppose you have two of those files, containing the data shown.\\n\\n\\n\\n    In [6]:\\n\\n    filelist = ['val1.txt', 'val2.txt']\\n\\n    print pd.concat([pd.read_csv(item, names=[item[:-4]]) for item in filelist], axis=1)\\n\\n        val1  val2\\n\\n    0     16    16\\n\\n    1     54    54\\n\\n    2   -314  -314\\n\\n    3      1     1\\n\\n    4     15    15\\n\\n    5      4     4\\n\\n    6    153   153\\n\\n    7     86    86\\n\\n    8      4     4\\n\\n    9     64    64\\n\\n    10   373   373\\n\\n    11     3     3\\n\\n    12   434   434\\n\\n    13    31    31\\n\\n    14    93    93\\n\\n    15    53    53\\n\\n    16   873   873\\n\\n    17    43    43\\n\\n    18    11    11\\n\\n    19   533   533\\n\\n    20    46    46\",\n",
       "  '<python-2.7><csv><text><pandas><dataframe>',\n",
       "  datetime.date(2014, 10, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '7563.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['798',\n",
       "  '30492281',\n",
       "  'Answer',\n",
       "  'pandas: how do I select first row in each GROUP BY group?',\n",
       "  \"Here's an alternative approach using `groupby().rank()`:\\n\\n\\n\\n    df[ df.groupby('A')['B'].rank() == 1 ]\\n\\n\\n\\n         A  B\\n\\n    1  foo  1\\n\\n    6  bar  1\\n\\n\\n\\nThis gives you the same answer as @EdChum's for the OP's sample dataframe, but could give a different answer if you have any ties during the sort, for example, with data like this:\\n\\n\\n\\n    df = pd.DataFrame({'A': ['foo', 'foo', 'bar', 'bar'], \\n\\n                       'B': ['2', '1', '1', '1'] })\\n\\n\\n\\nIn this case you have some options using the optional `method` argument, depending on how you wish to handle sorting ties:\\n\\n\\n\\n    df[ df.groupby('A')['B'].rank(method='average') == 1 ]   # the default\\n\\n    df[ df.groupby('A')['B'].rank(method='min')     == 1 ]\\n\\n    df[ df.groupby('A')['B'].rank(method='first')   == 1 ]   # doesn't work, not sure why\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 27),\n",
       "  '2018-04-24 14:53:45',\n",
       "  'JohnE (3877338)',\n",
       "  '5',\n",
       "  '',\n",
       "  '9039.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['802',\n",
       "  '30261629',\n",
       "  'Answer',\n",
       "  'Slow Stochastic Implementation in Python Pandas',\n",
       "  \"You can do this with the [``rolling_*``](http://pandas.pydata.org/pandas-docs/stable/computation.html#moving-rolling-statistics-moments) family of functions.\\n\\n\\n\\nE.g., ``100[(C - L14)/(H14 - L14)]`` can be found by:\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    l, h = pd.rolling_min(c, 4), pd.rolling_max(c, 4)\\n\\n    k = 100 * (c - l) / (h - l) \\n\\n\\n\\nand the rolling mean can be found by:\\n\\n\\n\\n    pd.rolling_mean(k, 3)\\n\\n\\n\\n----------------------------\\n\\n\\n\\nMoreover, if you're into this stuff, you can check out [pandas & econometrics](https://www.google.co.il/search?client=ubuntu&channel=fs&q=python+econometrics&ie=utf-8&oe=utf-8&gfe_rd=cr&ei=bv9VVZ7tGMSH8QfLroCAAg#channel=fs&q=pandas+econometrics).\",\n",
       "  '<numpy><pandas><matplotlib>',\n",
       "  datetime.date(2015, 5, 15),\n",
       "  '2015-05-15 14:19:33',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '5',\n",
       "  '',\n",
       "  '3296.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['803',\n",
       "  '30264839',\n",
       "  'Answer',\n",
       "  'Shared state in multiprocessing Processes',\n",
       "  \"[multiprocessing](https://docs.python.org/3/library/multiprocessing.html) runs stuff in separate **processes**. It is almost inconceivable that things are **not** copied as they're sent, as sharing stuff between processes requires shared memory or communication. \\n\\n\\n\\nIn fact, if you peruse the module, you can see the amount of effort it takes to actually share anything between the processes after the diverge, either through [explicit communication](https://docs.python.org/3/library/multiprocessing.html#exchanging-objects-between-processes), or through [explicitly-shared objects](https://docs.python.org/3/library/multiprocessing.html#sharing-state-between-processes) (which are of a very limited subset of the language, and have to be managed by a ``Manager``).\",\n",
       "  '<python><multiprocessing>',\n",
       "  datetime.date(2015, 5, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1194.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['805',\n",
       "  '30112105',\n",
       "  'Answer',\n",
       "  'How do I append 3 lists efficiently in Prolog?',\n",
       "  \"Hope I understood the question (and I don't think the following is more efficient than the other solutions here), but did you mean something like this?\\n\\n\\n\\n    append([],[],L,L).\\n\\n    append([],[H|T],L,[H|R]) :- append([],T,L,R).\\n\\n    append([H|T],L0,L1,[H|R]) :- append(T,L0,L1,R).\",\n",
       "  '<prolog>',\n",
       "  datetime.date(2015, 5, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1133.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['809',\n",
       "  '30174021',\n",
       "  'Answer',\n",
       "  'groupby of a groupby to select values in pandas',\n",
       "  \"You can't, for the reason you wrote above in your comment about the ``AssertionError``. Pandas expects to do the (second) ``groupby``  according to some sequence which has exactly the same length as the ``DataFrame`` getting grouped. If you're unwilling to first create a ``DataFrame`` describing the ``EA`` values, you're basically stuck with creating it again on the fly.\\n\\n\\n\\nNot only is that less legible, it is unnecessarily expensive. Speaking of which, I'd rewrite your code like this:\\n\\n\\n\\n    eas = df[df.marker == 'EA']\\n\\n    eas.value.groupby(eas.date).mean().plot();\\n\\n\\n\\nDoing a ``groupby`` and retaining a single group is a very expensive way of just filtering according to the key.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1280.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['812',\n",
       "  '25793732',\n",
       "  'Answer',\n",
       "  'What is correct syntax to swap column values for selected rows in a pandas data frame using just one line?',\n",
       "  \"The key thing to note here is that pandas attempts to automatically align rows and columns using the index and column names.  Hence, you need to somehow tell pandas to ignore the column names here.  One way is as @DSM does, by converting to a numpy array.  Another way is to rename the columns:\\n\\n\\n\\n    >>> df.loc[idx] = df.loc[idx].rename(columns={'R':'L','L':'R'})\\n\\n\\n\\n          L      R  VALUE\\n\\n    0  left  right     -1\\n\\n    1  left  right      1\\n\\n    2  left  right     -1\\n\\n    3  left  right      1\\n\\n    4  left  right     -1\\n\\n    5  left  right      1\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 9, 11),\n",
       "  '2017-12-31 02:53:19',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '7113.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['813',\n",
       "  '25917201',\n",
       "  'Answer',\n",
       "  'Fill data gaps with average of data from adjacent days',\n",
       "  'This should be a faster and more concise way to do it.  Main thing is to use the shift() function instead of the loop.  Simple version would be this:\\n\\n\\n\\n    df[ df.isnull() ] = np.nanmean( [ df.shift(-48), df.shift(48) ] )\\n\\n\\n\\nIt turned out to be really hard to generalize this, but this seems to work:\\n\\n\\n\\n    df[ df.isnull() ] = np.nanmean( [ df.shift(x).values for x in \\n\\n                                         range(-48*window,48*(window+1),48) ], axis=0 )\\n\\n\\n\\nI\\'m not sure, but suspect there might be a bug with nanmean and it\\'s also the same reason you got missing values yourself.  It seems to me that nanmean cannot handle nans if you feed it a dataframe.  But if I convert to an array (with .values) and use axis=0 then it seems to work. \\n\\n\\n\\nCheck results for window=1:\\n\\n    \\n\\n    print df.ix[\"2014-01-04 12:30\":\"2014-01-04 14:00\", \"B\"]\\n\\n    print df.ix[\"2014-01-03 12:30\":\"2014-01-03 14:00\", \"B\"]\\n\\n    print df.ix[\"2014-01-05 12:30\":\"2014-01-05 14:00\", \"B\"]    \\n\\n\\n\\n    2014-01-04 12:30:00    0.940193     # was nan, now filled\\n\\n    2014-01-04 13:00:00    0.078160\\n\\n    2014-01-04 13:30:00   -0.662918\\n\\n    2014-01-04 14:00:00   -0.967121\\n\\n    \\n\\n    2014-01-03 12:30:00    0.947915     # day before\\n\\n    2014-01-03 13:00:00    0.167218\\n\\n    2014-01-03 13:30:00   -0.391444\\n\\n    2014-01-03 14:00:00   -1.157040\\n\\n    \\n\\n    2014-01-05 12:30:00    0.932471     # day after\\n\\n    2014-01-05 13:00:00   -0.010899\\n\\n    2014-01-05 13:30:00   -0.934391\\n\\n    2014-01-05 14:00:00   -0.777203\\n\\n\\n\\nRegarding problem #2, it will depend on your data but if you precede the above with\\n\\n\\n\\n`df = df.resample(\\'30min\\')`\\n\\n\\n\\nthat will give you a row of nans for all the missing rows and then you can fill them in the same as all the other nans.  That\\'s probably the simplest and fastest way if it works.\\n\\n\\n\\nAlternatively, you could do something with groupby.  My groupby-fu is weak but to give you the flavor of it, something like:\\n\\n\\n\\n`df.groupby( df.index.hour ).fillna(method=\\'pad\\')`\\n\\n\\n\\nwould correctly deal the issue of missing rows, but not the other things.',\n",
       "  '<python><pandas><time-series>',\n",
       "  datetime.date(2014, 9, 18),\n",
       "  '2014-09-18 21:42:46',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1113.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['816',\n",
       "  '26871453',\n",
       "  'Answer',\n",
       "  \"datetime index KeyError: 'the label [2000-01-03 00:00:00] is not in the [index]'\",\n",
       "  \"The [documentation][1] provides a number of ways to index a `datetime` index `DataFrame`, a few examples:\\n\\n\\n\\n    In [64]:\\n\\n\\n\\n    index\\n\\n    Out[64]:\\n\\n    Timestamp('2000-01-03 00:00:00')\\n\\n    In [65]:\\n\\n\\n\\n    print df.ix[index.to_datetime()]\\n\\n                call/put  expiration  strike  ask  bid\\n\\n    2000-01-03  0.830035    -0.42598     0.5    0    0\\n\\n    2000-01-03  0.830035    -0.42598     0.6    1    1\\n\\n    2000-01-03  0.830035    -0.42598     0.4    1    1\\n\\n    In [66]:\\n\\n\\n\\n    print df.ix[index]\\n\\n                call/put  expiration  strike  ask  bid\\n\\n    2000-01-03  0.830035    -0.42598     0.5    0    0\\n\\n    2000-01-03  0.830035    -0.42598     0.6    1    1\\n\\n    2000-01-03  0.830035    -0.42598     0.4    1    1\\n\\n    In [67]:\\n\\n\\n\\n    print df[index.strftime('%m/%d/%y')]\\n\\n                call/put  expiration  strike  ask  bid\\n\\n    2000-01-03  0.830035    -0.42598     0.5    0    0\\n\\n    2000-01-03  0.830035    -0.42598     0.6    1    1\\n\\n    2000-01-03  0.830035    -0.42598     0.4    1    1\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#datetime-indexing\",\n",
       "  '<python><datetime><pandas>',\n",
       "  datetime.date(2014, 11, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '4595.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['823',\n",
       "  '30278001',\n",
       "  'Answer',\n",
       "  'How to speed up pandas with cython (or numpy)',\n",
       "  \"If you're just trying to do it faster and not specifically using cython, I'd just do it in plain numpy (about 50x faster).\\n\\n\\n\\n    def numpy_foo(arr):\\n\\n        vals = {i: (arr[i, :] + arr[i:, :]).sum(axis=1).tolist()\\n\\n                for i in range(arr.shape[0])}   \\n\\n        return vals\\n\\n    \\n\\n    %timeit foo(df)\\n\\n    100 loops, best of 3: 7.2 ms per loop\\n\\n    \\n\\n    %timeit numpy_foo(df.values)\\n\\n    10000 loops, best of 3: 144 µs per loop\\n\\n    \\n\\n    foo(df) == numpy_foo(df.values)\\n\\n    Out[586]: True\\n\\n\\n\\nGenerally speaking, pandas gives you a lot of conveniences relative to numpy, but there are overhead costs.  So in situations where pandas isn't really adding anything, you can generally speed things up by doing it in numpy.  For another example, see this [question][1] I asked which showed a roughly comparable speed difference (about 23x).\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/questions/25915541/fastest-way-to-numerically-process-2d-array-dataframe-vs-series-vs-array-vs-num\",\n",
       "  '<python><numpy><pandas><cython>',\n",
       "  datetime.date(2015, 5, 16),\n",
       "  '2017-05-23 11:54:46',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '17',\n",
       "  '',\n",
       "  '7200.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['825',\n",
       "  '30539755',\n",
       "  'Answer',\n",
       "  'Python: Read several json files from a folder',\n",
       "  \"Iterating a (flat) directory is easy with the [``glob``](https://docs.python.org/2/library/glob.html) module\\n\\n\\n\\n    from glob import glob\\n\\n\\n\\n    for f_name in glob('foo/*.json'):\\n\\n        ...\\n\\n\\n\\nAs for reading JSON directly into ``pandas``, see [here](http://hayd.github.io/2013/pandas-json/).\",\n",
       "  '<python><json><pandas>',\n",
       "  datetime.date(2015, 5, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '25085.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['831',\n",
       "  '30336266',\n",
       "  'Answer',\n",
       "  'How to plot a multivariate function in Python?',\n",
       "  \"You could use [``mplot3d``](http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html#scatter-plots). For a scatter plot, you can use something like\\n\\n\\n\\n    fig = plt.figure()\\n\\n    ax = fig.add_subplot(111, projection='3d')\\n\\n    ax.scatter(xs, ys, zs)\\n\\n\",\n",
       "  '<python><numpy><matplotlib><linear-regression>',\n",
       "  datetime.date(2015, 5, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '9359.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['834',\n",
       "  '26039160',\n",
       "  'Answer',\n",
       "  'How do I extend a pandas DataFrame by repeating the last row?',\n",
       "  \"Here's an alternate (fancy indexing) way to do it:\\n\\n\\n\\n    df.append( df.iloc[[-1]*3] )\\n\\n\\n\\n    Out[757]: \\n\\n                A  B  C  D\\n\\n    2014-01-01  1  0  0  0\\n\\n    2014-01-02  0  1  0  0\\n\\n    2014-01-03  0  0  1  0\\n\\n    2014-01-04  0  0  0  1\\n\\n    2014-01-04  0  0  0  1\\n\\n    2014-01-04  0  0  0  1\\n\\n    2014-01-04  0  0  0  1\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2014, 9, 25),\n",
       "  '2014-09-25 13:36:02',\n",
       "  'JohnE (3877338)',\n",
       "  '6',\n",
       "  '',\n",
       "  '1345.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['835',\n",
       "  '26064683',\n",
       "  'Answer',\n",
       "  'matplotlib fill between discrete points',\n",
       "  'Please just do: \\n\\n\\n\\n    plt.fill_betweenx(X, Y, 35) #or max(X) or whatever cut-off you may want.\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/ojGUh.png',\n",
       "  '<python><matplotlib><fill>',\n",
       "  datetime.date(2014, 9, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2005.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['838',\n",
       "  '26124537',\n",
       "  'Answer',\n",
       "  \"Pandas plot subplots of a 'group by' result\",\n",
       "  \"Not so tricky in `matplotlib`, see:\\n\\n\\n\\n    In [54]:\\n\\n\\n\\n    print df\\n\\n      cat1  cat2       val\\n\\n    0    A     1  0.011887\\n\\n    1    A     2  0.880121\\n\\n    2    A     3  0.034244\\n\\n    3    A     4  0.530230\\n\\n    4    B     1  0.510812\\n\\n    5    B     2  0.405322\\n\\n    6    B     3  0.406259\\n\\n    7    B     4  0.406405\\n\\n    In [55]:\\n\\n\\n\\n    col_list = ['r', 'g']\\n\\n    ax = plt.subplot(111)\\n\\n    for (idx, (grp, val)) in enumerate(df.groupby('cat1')):\\n\\n        ax.bar(val.cat2+0.25*idx-0.25, \\n\\n               val.val, width=0.25,  \\n\\n               color=col_list[idx], \\n\\n               label=grp)\\n\\n    plt.legend()\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Jofme.png\",\n",
       "  '<python><numpy><matplotlib><pandas>',\n",
       "  datetime.date(2014, 9, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1473.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['853',\n",
       "  '30370897',\n",
       "  'Answer',\n",
       "  'Efficiently applying a function to a grouped pandas DataFrame in parallel',\n",
       "  \"From the comments above, it seems that this is planned for ``pandas`` some time (there's also an interesting-looking [``rosetta`` project](https://pypi.python.org/pypi/rosetta/0.2.4) which I just noticed).\\n\\n\\n\\nHowever, until every parallel functionality is incorporated into ``pandas``, I noticed that it's very easy to write efficient & non-memory-copying parallel augmentations to ``pandas`` directly using [``cython``](http://cython.org/) + [OpenMP](http://www.google.co.il/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0CB0QFjAA&url=http%3A%2F%2Fwww.openmp.org%2F&ei=HKpdVfyVJcj8ULXHgcAF&usg=AFQjCNGlD5aZM8ZP3Qx7WXT74Y7C54jLNQ&bvm=bv.93756505,d.d24) and C++.\\n\\n\\n\\nHere's a short example of writing a parallel groupby-sum, whose use is something like this:\\n\\n\\n\\n    import pandas as pd\\n\\n    import para_group_demo\\n\\n\\n\\n    df = pd.DataFrame({'a': [1, 2, 1, 2, 1, 1, 0], 'b': range(7)})\\n\\n    print para_group_demo.sum(df.a, df.b)\\n\\n\\n\\nand output is:\\n\\n\\n\\n         sum\\n\\n    key     \\n\\n    0      6\\n\\n    1      11\\n\\n    2      4\\n\\n\\n\\n---------------------------\\n\\n\\n\\n**Note** Doubtlessly, this simple example's functionality will eventually be part of ``pandas``. Some things, however, will be more natural to parallelize in C++ for some time, and it's important to be aware of how easy it is to combine this into ``pandas``.\\n\\n\\n\\n---------------------------\\n\\n\\n\\nTo do this, I wrote a simple single-source-file extension whose code follows.\\n\\n\\n\\nIt starts with some imports and type definitions\\n\\n\\n\\n    from libc.stdint cimport int64_t, uint64_t\\n\\n    from libcpp.vector cimport vector\\n\\n    from libcpp.unordered_map cimport unordered_map\\n\\n\\n\\n    cimport cython\\n\\n    from cython.operator cimport dereference as deref, preincrement as inc\\n\\n    from cython.parallel import prange\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    ctypedef unordered_map[int64_t, uint64_t] counts_t\\n\\n    ctypedef unordered_map[int64_t, uint64_t].iterator counts_it_t\\n\\n    ctypedef vector[counts_t] counts_vec_t\\n\\n\\n\\nThe C++ ``unordered_map`` type is for summing by a single thread, and the ``vector`` is for summing by all threads.\\n\\n\\n\\nNow to the function ``sum``. It starts off with [typed memory views](http://docs.cython.org/src/userguide/memoryviews.html) for fast access:\\n\\n\\n\\n    def sum(crit, vals):\\n\\n        cdef int64_t[:] crit_view = crit.values\\n\\n        cdef int64_t[:] vals_view = vals.values\\n\\n\\n\\nThe function continues by dividing the semi-equally to the threads (here hardcoded to 4), and having each thread sum the entries in its range:\\n\\n\\n\\n        cdef uint64_t num_threads = 4\\n\\n        cdef uint64_t l = len(crit)\\n\\n        cdef uint64_t s = l / num_threads + 1\\n\\n        cdef uint64_t i, j, e\\n\\n        cdef counts_vec_t counts\\n\\n        counts = counts_vec_t(num_threads)\\n\\n        counts.resize(num_threads)\\n\\n        with cython.boundscheck(False):\\n\\n            for i in prange(num_threads, nogil=True): \\n\\n                j = i * s\\n\\n                e = j + s\\n\\n                if e > l:\\n\\n                    e = l\\n\\n                while j < e:\\n\\n                    counts[i][crit_view[j]] += vals_view[j]\\n\\n                    inc(j)\\n\\n\\n\\nWhen the threads have completed, the function merges all the results (from the different ranges) into a single ``unordered_map``:\\n\\n\\n\\n        cdef counts_t total\\n\\n        cdef counts_it_t it, e_it\\n\\n        for i in range(num_threads):\\n\\n            it = counts[i].begin()\\n\\n            e_it = counts[i].end()\\n\\n            while it != e_it:\\n\\n                total[deref(it).first] += deref(it).second\\n\\n                inc(it)        \\n\\n\\n\\nAll that's left is to create a ``DataFrame`` and return the results:\\n\\n\\n\\n        key, sum_ = [], []\\n\\n        it = total.begin()\\n\\n        e_it = total.end()\\n\\n        while it != e_it:\\n\\n            key.append(deref(it).first)\\n\\n            sum_.append(deref(it).second)\\n\\n            inc(it)\\n\\n\\n\\n        df = pd.DataFrame({'key': key, 'sum': sum_})\\n\\n        df.set_index('key', inplace=True)\\n\\n        return df\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><pandas><multiprocessing><shared-memory>',\n",
       "  datetime.date(2015, 5, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '12',\n",
       "  '',\n",
       "  '6426.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['854',\n",
       "  '30282780',\n",
       "  'Answer',\n",
       "  'Proper way to do row correlations in a pandas dataframe',\n",
       "  \"I don't know if these are any better than what you did, but here's a way with numpy:\\n\\n\\n\\n    np.corrcoef(df_example.iloc[1:3, :-1])\\n\\n    \\n\\n    array([[ 1.        , -0.37194563],\\n\\n           [-0.37194563,  1.        ]])\\n\\n    \\n\\nAnd here's a way with pandas:\\n\\n\\n\\n    df_example.iloc[1:3, :-1].T.corr()\\n\\n     \\n\\n              1         2\\n\\n    1  1.000000 -0.371946\\n\\n    2 -0.371946  1.000000\\n\\n\\n\\nIf you want to compare non-contiguous rows, adjust `iloc` like this:\\n\\n\\n\\n    df_example.iloc[[1, 4], :-1].T.corr()\\n\\n\\n\\n\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2015, 5, 17),\n",
       "  '2015-05-17 03:28:07',\n",
       "  'JohnE (3877338), Alexander (2411802)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2589.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['862',\n",
       "  '30396773',\n",
       "  'Answer',\n",
       "  'Python - iterating beginning with the middle of the list and then checking either side',\n",
       "  'To begin with, [here](https://stackoverflow.com/questions/243865/how-do-i-merge-two-python-iterators) is a very useful general purpose utility to interleave two sequences:\\n\\n\\n\\n    def imerge(a, b):\\n\\n        for i, j in itertools.izip_longest(a,b):\\n\\n            yield i\\n\\n            if j is not None:\\n\\n                yield j\\n\\n\\n\\nwith that, you just need to ``imerge``\\n\\n\\n\\n    a[len(a) / 2: ]\\n\\n\\n\\nwith\\n\\n  \\n\\n    reversed(a[: len(a) / 2])',\n",
       "  '<python><iteration>',\n",
       "  datetime.date(2015, 5, 22),\n",
       "  '2017-05-23 12:00:56',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '6',\n",
       "  '',\n",
       "  '1192.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['863',\n",
       "  '30407572',\n",
       "  'Answer',\n",
       "  'checking if key exist in map then updating value',\n",
       "  'The STL was designed to do this efficiently, and it pays to see how.\\n\\n\\n\\nBut first, note that in your code, the lines\\n\\n\\n\\n    two.find(string(pairs[i]) );\\n\\n\\n\\n    two[string(pairs[i])]=1.0;\\n\\n\\n\\nperform **two** lookups, which is a bit of a waste.\\n\\n\\n\\nIf you look at the [signature for ``map::insert``](http://www.cplusplus.com/reference/map/map/insert/), you can see that the return value is ``std::pair<iterator, bool>``. The second is a boolean indicating whether the element was actually inserted. The first is an iterator to either the previous element (if it existed, in which case it was not overwritten), or to the new element.\\n\\n\\n\\nSo, the way to do it efficiently is to write\\n\\n\\n\\n    auto ins = two.insert(make_pair(pairs[i], 0));\\n\\n    ins.first->second += 1;',\n",
       "  '<c++><dictionary>',\n",
       "  datetime.date(2015, 5, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1343.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['865',\n",
       "  '30304253',\n",
       "  'Answer',\n",
       "  'Which vector and map, uses less memory (large set of data and unknown size)',\n",
       "  'For the specific sizes you note, you might want to consider something that does not go to either of the extremes: not a continuous array of memory, and not a single-node based tree.\\n\\n\\n\\nSome options are \\n\\n\\n\\n- a (memory residing) [B tree](http://en.wikipedia.org/wiki/B-tree)\\n\\n\\n\\n- a digital tree',\n",
       "  '<c++><stdvector><stdmap><large-data>',\n",
       "  datetime.date(2015, 5, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1263.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['868',\n",
       "  '30336133',\n",
       "  'Answer',\n",
       "  'Excel like formulas with pandas',\n",
       "  \"I'm not sure how you want to deal with right-most columns (you just have a '?') but you can adjust fairly easily starting from the following code or just pad out data with placeholder numbers or NaNs:\\n\\n\\n\\n    df2 = df.copy()    \\n\\n    for i in range(1,len(df.columns)):\\n\\n        df2.iloc[:,i] = ((df.iloc[:,i].notnull()) & \\n\\n                         (df.iloc[:,i+1:i+4].apply(lambda x: all(x.isnull()),axis=1)))\\n\\n\\n\\nStarting data `df`:\\n\\n\\n\\n       User_id  2014-01  2014-02  2014-03  2014-04  2014-05\\n\\n    0        1        7      NaN      NaN      NaN      NaN\\n\\n    1        2      NaN        5      NaN      NaN        9\\n\\n    2        3        2        4      NaN      NaN      NaN\\n\\n\\n\\nResults `df2`:\\n\\n\\n\\n       User_id 2014-01 2014-02 2014-03 2014-04 2014-05\\n\\n    0        1    True   False   False   False   False\\n\\n    1        2   False   False   False   False   False\\n\\n    2        3   False    True   False   False   False\\n\\n\\n\\nFor the aforementioned padding, you could add three placeholder columns and then tweak the remaining code slightly:\\n\\n\\n\\n    df[['pad1','pad2','pad3']] = np.nan\\n\\n\\n\\n    df2 = df.copy().iloc[:,:-3]    \\n\\n    for i in range(1,len(df2.columns)):\\n\\n        df2.iloc[:,i] = ((df.iloc[:,i].notnull()) & \\n\\n                         (df.iloc[:,i+1:i+4].apply(lambda x: all(x.isnull()),axis=1)))\\n\\n\\n\\nAnd now you have one 'True' in the last column:\\n\\n\\n\\n       User_id 2014-01 2014-02 2014-03 2014-04 2014-05\\n\\n    0        1    True   False   False   False   False\\n\\n    1        2   False   False   False   False    True\\n\\n    2        3   False    True   False   False   False\\n\\n\",\n",
       "  '<python><excel><pandas>',\n",
       "  datetime.date(2015, 5, 19),\n",
       "  '2015-05-19 21:19:01',\n",
       "  'JohnE (3877338)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1008.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['870',\n",
       "  '29197066',\n",
       "  'Answer',\n",
       "  'Add multiple columns to a Pandas dataframe quickly',\n",
       "  \"I'm a little confused at exactly what your dataframe should look like, but it's easy to speed this up a lot with a general technique.  Basically for pandas/numpy speed you want to avoid ```for``` and any ```concat/merge/join/append```, if possible.\\n\\n\\n\\nYour best bet here is most likely to use ```numpy``` to create an array that will be the input to a dataframe and then name the columns however you like.  Both of these operations should be trivial as far as computation time.\\n\\n\\n\\nHere's the numpy part, it looks like you already know how to construct column names.\\n\\n\\n\\n    %timeit pd.DataFrame(  np.ones([10,100]).cumsum(axis=0) \\n\\n                         + np.ones([10,100]).cumsum(axis=1) )\\n\\n    10000 loops, best of 3: 158 µs per loop\\n\\n\\n\\nI think you are trying to make something like this?  (If not, just check out numpy if you aren't familiar with it, it has all sorts of array operations that should make it very easy to do whatever you are trying to do here).\\n\\n\\n\\n    In [63]: df.ix[:5,:10]\\n\\n    Out[63]: \\n\\n       0   1   2   3   4   5   6   7   8   9   10\\n\\n    0   2   3   4   5   6   7   8   9  10  11  12\\n\\n    1   3   4   5   6   7   8   9  10  11  12  13\\n\\n    2   4   5   6   7   8   9  10  11  12  13  14\\n\\n    3   5   6   7   8   9  10  11  12  13  14  15\\n\\n    4   6   7   8   9  10  11  12  13  14  15  16\\n\\n    5   7   8   9  10  11  12  13  14  15  16  17\\n\\n\\n\\n\",\n",
       "  '<python><performance><numpy><pandas><dataframe>',\n",
       "  datetime.date(2015, 3, 22),\n",
       "  '2015-03-22 16:51:42',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '4032.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['871',\n",
       "  '29242900',\n",
       "  'Answer',\n",
       "  'Select multiple columns by labels (pandas)',\n",
       "  \"### Name- or Label-Based (using regular expression syntax) ###\\n\\n\\n\\n    df.filter(regex='[A-CEG-I]')   # does NOT depend on the column order\\n\\n\\n\\n\\n\\n### Location-Based (depends on column order) ###\\n\\n\\n\\n    cols = list(df.loc[:,'A':'C']) + ['E'] + list(df.loc[:,'G':'I'])\\n\\n\\n\\nNote that unlike the label-based method, this only works if your columns are alphabetically sorted.  This is not necessarily a problem, however.  For example, if your columns go `['A','C','B']`, then you could replace `'A':'C'` above with `'A':'B'`.\\n\\n    \\n\\n### The Long Way ###\\n\\n\\n\\nAnd for completeness, you always have the option shown by @Magdalena of simply listing each column individually, although it could be much more verbose as the number of columns increases:\\n\\n\\n\\n    df[['A','B','C','E','G','H','I']]   # does NOT depend on the column order\\n\\n\\n\\n\\n\\n### Results for any of the above methods ###\\n\\n\\n\\n              A         B         C         E         G         H         I\\n\\n    0 -0.814688 -1.060864 -0.008088  2.697203 -0.763874  1.793213 -0.019520\\n\\n    1  0.549824  0.269340  0.405570 -0.406695 -0.536304 -1.231051  0.058018\\n\\n    2  0.879230 -0.666814  1.305835  0.167621 -1.100355  0.391133  0.317467\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 3, 24),\n",
       "  '2018-06-12 09:28:23',\n",
       "  'JohnE (3877338)',\n",
       "  '31',\n",
       "  '',\n",
       "  '57064.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['876',\n",
       "  '30470860',\n",
       "  'Answer',\n",
       "  'Find duplicate element in a list of lists',\n",
       "  \"If you're willing to forgo the elegance of list comprehension, you could do the following:\\n\\n\\n\\n    seen, dups = set(), set()\\n\\n    for l in ll:\\n\\n        dups = dups.union(seen.intersection(set(l)))\\n\\n        seen = seen.union(set(l))\\n\\n\\n\\nYour answer should be in ``dups``.\\n\\n    \\n\\n**Edit**\\n\\n\\n\\nAs [Steven Rumbalski](https://stackoverflow.com/users/1322401/steven-rumbalski) pointed out below, the ``set`` inside the ``set``-member operations' arguments, is redundant (and needlessly expensive).\",\n",
       "  '<python><duplicates><nested-lists>',\n",
       "  datetime.date(2015, 5, 26),\n",
       "  '2017-05-23 11:45:58',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2986.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['879',\n",
       "  '30462002',\n",
       "  'Answer',\n",
       "  'Write a user defined fillna function in pandas dataframe to fill np.nan different values with conditions',\n",
       "  \"I think your code is pretty solid, the main issue is you are iterating through it more times than you need to.  `shift()` only goes back one line at a time, but if you change that to `fillna(method='ffill')` then you essentially get an unlimitied number of shifts but only have to do this one time instead of with multiple iterations (how many iterations will depend on your data).\\n\\n\\n\\n    conditions = [\\n\\n        (np.isnan(x['position'])) & (x['position'].fillna(method='ffill')*x['change']>0),\\n\\n        (np.isnan(x['position'])) & (x['position'].fillna(method='ffill')*x['change']<=0)]\\n\\n\\n\\nBut I believe you can go one step further and eliminate the `while` by adding another `fillna` at the end:\\n\\n\\n\\n    conditions = [\\n\\n        (np.isnan(x['position'])) & (x['position'].fillna(method='ffill')*x['change']>0),\\n\\n        (np.isnan(x['position'])) & (x['position'].fillna(method='ffill')*x['change']<=0)]\\n\\n\\n\\n    choices=[x['position'].shift(1),0]\\n\\n    x['position'] = np.select(conditions,choices,default=x['position'])\\n\\n\\n\\n    x['position'] = x['position'].fillna(method='ffill')\\n\\n\\n\\nOn your sample data, the first change is about 2x faster than your code, and the second is about 4x.  I get the same answers as you, but of course you'll want to verify this on the real data to be sure.\",\n",
       "  '<python><pandas><dataframe><user-defined-functions><nan>',\n",
       "  datetime.date(2015, 5, 26),\n",
       "  '2015-05-26 15:06:25',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1233.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['882',\n",
       "  '30579738',\n",
       "  'Answer',\n",
       "  'Wish for Better c++ Stack Implementation using Vectors',\n",
       "  '    T pop()\\n\\n    {\\n\\n        const T popped = top();\\n\\n        // original code of pop goes here.\\n\\n        return popped\\n\\n    }\\n\\n\\n\\nAs to your question \\n\\n\\n\\n> shouldn\\'t a better implementation return the value \"popped\"\\n\\n\\n\\n In general, returning an object can cause copying, which can throw an exception, at it might require additional resources. This is dangerous - you could get a system that is resource strapped, and so you want to free things by popping stuff (for example). The problem is - you can\\'t, it\\'ll just require more resources.',\n",
       "  '<c++><vector><stack>',\n",
       "  datetime.date(2015, 6, 1),\n",
       "  '2015-06-01 17:47:20',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1321.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['886',\n",
       "  '30357342',\n",
       "  'Answer',\n",
       "  'Pandas - FillNa with another column',\n",
       "  \"You could do\\n\\n\\n\\n    df.Cat1 = np.where(df.Cat1.isnull(), df.Cat2, df.Cat1)\\n\\n\\n\\nThe overall construct on the RHS uses [the ternary pattern from the ``pandas`` cookbook](https://stackoverflow.com/questions/19913659/pandas-conditional-creation-of-a-series-dataframe-column) (which it pays to read in any case). It's a vector version of ``a? b: c``. \",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 20),\n",
       "  '2017-05-23 10:31:20',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '11',\n",
       "  '',\n",
       "  '29959.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['889',\n",
       "  '29305456',\n",
       "  'Answer',\n",
       "  'Pandas Efficient VWAP Calculation',\n",
       "  \"Getting into one pass vs one line starts to get a little semantical.  How about this for a distinction:  you can do it with 1 line of pandas, 1 line of numpy, or several lines of numba.\\n\\n\\n\\n    from numba import jit\\n\\n    \\n\\n    df=pd.DataFrame( np.random.randn(10000,3), columns=['v','h','l'] )\\n\\n    \\n\\n    df['vwap_pandas'] = (df.v*(df.h+df.l)/2).cumsum() / df.v.cumsum()\\n\\n    \\n\\n    @jit\\n\\n    def vwap():\\n\\n        tmp1 = np.zeros_like(v)\\n\\n        tmp2 = np.zeros_like(v)\\n\\n        for i in range(0,len(v)):\\n\\n            tmp1[i] = tmp1[i-1] + v[i] * ( h[i] + l[i] ) / 2.\\n\\n            tmp2[i] = tmp2[i-1] + v[i]\\n\\n        return tmp1 / tmp2\\n\\n    \\n\\n    v = df.v.values\\n\\n    h = df.h.values\\n\\n    l = df.l.values\\n\\n    \\n\\n    df['vwap_numpy'] = np.cumsum(v*(h+l)/2) / np.cumsum(v)\\n\\n    \\n\\n    df['vwap_numba'] = vwap()\\n\\n\\n\\nTimings:\\n\\n\\n\\n    %timeit (df.v*(df.h+df.l)/2).cumsum() / df.v.cumsum()  # pandas\\n\\n    1000 loops, best of 3: 829 µs per loop\\n\\n    \\n\\n    %timeit np.cumsum(v*(h+l)/2) / np.cumsum(v)            # numpy\\n\\n    10000 loops, best of 3: 165 µs per loop\\n\\n    \\n\\n    %timeit vwap()                                         # numba\\n\\n    10000 loops, best of 3: 87.4 µs per loop\\n\\n\\n\\n  \\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><numpy><pandas><apply><cumulative-sum>',\n",
       "  datetime.date(2015, 3, 27),\n",
       "  '2017-02-11 21:06:43',\n",
       "  'JohnE (3877338)',\n",
       "  '8',\n",
       "  '',\n",
       "  '4986.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['897',\n",
       "  '30507018',\n",
       "  'Answer',\n",
       "  'Increasing performance of nearest neighbors of rows in Pandas',\n",
       "  \"Your biggest (and easiest) performance gain is likely to be from merely doing this in numpy rather than pandas.  I'm seeing over a 200x improvement just from a quick conversion of the code to numpy:\\n\\n\\n\\n    arr = df.values\\n\\n    def fsi_numpy(item_id):\\n\\n        tmp_arr = arr - arr[item_id]\\n\\n        tmp_ser = np.sum( np.square( tmp_arr ), axis=1 )\\n\\n        return tmp_ser\\n\\n    \\n\\n    df['dist'] = fsi_numpy(5)\\n\\n    df = df.sort_values('dist').head(5)\\n\\n    \\n\\n                 X         Y         Z      dist\\n\\n    5     0.272985  0.131939  0.449750  0.000000\\n\\n    5130  0.272429  0.138705  0.425510  0.000634\\n\\n    4609  0.264882  0.103006  0.476723  0.001630\\n\\n    1794  0.245371  0.175648  0.451705  0.002677\\n\\n    6937  0.221363  0.137457  0.463451  0.002883\\n\\n\\n\\nCheck that it gives the same result as your function (since we have different random draws):\\n\\n\\n\\n    df.loc[ pd.DataFrame( find_similiar_items(5)).index].head(5)\\n\\n    \\n\\n                 X         Y         Z\\n\\n    5     0.272985  0.131939  0.449750\\n\\n    5130  0.272429  0.138705  0.425510\\n\\n    4609  0.264882  0.103006  0.476723\\n\\n    1794  0.245371  0.175648  0.451705\\n\\n    6937  0.221363  0.137457  0.463451\\n\\n\\n\\nTimings:\\n\\n\\n\\n    %timeit df.loc[ pd.DataFrame( find_similiar_items(5)).index].head(5)\\n\\n    1 loops, best of 3: 638 ms per loop\\n\\n    \\n\\n    In [105]: %%timeit\\n\\n         ...: df['dist'] = fsi_numpy(5)\\n\\n         ...: df = df.sort_values('dist').head(5)\\n\\n         ...: \\n\\n    100 loops, best of 3: 2.69 ms per loop\\n\\n\\n\\n\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2015, 5, 28),\n",
       "  '2018-11-20 14:29:52',\n",
       "  'JohnE (3877338), User33029 (8897340)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1182.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['904',\n",
       "  '30673754',\n",
       "  'Answer',\n",
       "  'Pandas dataframe first x columns',\n",
       "  \"I wasn't sure if you meant rows or columns.\\n\\n\\n\\nIf it's rows, then\\n\\n\\n\\n    df.head(50)\\n\\n\\n\\nwill do the trick.\\n\\n\\n\\nIf it's columns, then\\n\\n\\n\\n    df.iloc[:, : 50]\\n\\n\\n\\nwill work.\\n\\n\\n\\nOf course, you can combine them.\\n\\n\\n\\n------------------------\\n\\n\\n\\nYou can see this stuff at [Indexing and Selecting Data](http://pandas.pydata.org/pandas-docs/stable/indexing.html).\",\n",
       "  '<pandas><dataframe>',\n",
       "  datetime.date(2015, 6, 5),\n",
       "  '2018-09-29 16:00:28',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2655.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['905',\n",
       "  '30395364',\n",
       "  'Answer',\n",
       "  'Why are unsigned integers error prone?',\n",
       "  'One possible aspect is that unsigned integers can lead to somewhat hard-to-spot problems in loops, because the underflow leads to large numbers. I cannot count (even with an unsigned integer!) how many times I made a variant of this bug\\n\\n\\n\\n    for(size_t i = foo.size(); i >= 0; --i)\\n\\n        ...\\n\\nNote that, by definition, ``i >= 0`` is always true. (What causes this in the first place is that if ``i`` is signed, the compiler will warn about a possible overflow with the ``size_t`` of ``size()``).\\n\\n\\n\\nThere are other reasons mentioned [Danger – unsigned types used here!](http://critical.eschertech.com/2010/04/07/danger-unsigned-types-used-here/), the strongest of which, in my opinion, is the implicit type conversion between signed and unsigned.',\n",
       "  '<c++><unsigned-integer>',\n",
       "  datetime.date(2015, 5, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '45',\n",
       "  '',\n",
       "  '5424.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['908',\n",
       "  '30431930',\n",
       "  'Answer',\n",
       "  'Linear Regression from Time Series Pandas',\n",
       "  'I\\'m not sure why `pd.ols` is so picky there (it does appear to me that you followed the example correctly).  I suspect this is due to changes in how pandas handles or stores datetime indexes but am too lazy to explore this further.  Anyway, since your datetime variable differs only in the hour, you could just extract the hour with a `dt` accessor:\\n\\n\\n\\n    pd.ols(x=pd.to_datetime(z[\"index\"]).dt.hour, y=z[0])\\n\\n\\n\\nHowever, that gives you an r-squared of 1, since your model is overspecified with the inclusion of an intercept (and y being a linear function of x).  You could change the `range` to `np.random.randn` and then you\\'d get something that looks like normal regression results.\\n\\n\\n\\n    In [6]: z = pd.Series(np.random.randn(4), index = rng).reset_index()                                                               \\n\\n            pd.ols(x=pd.to_datetime(z[\"index\"]).dt.hour, y=z[0])\\n\\n    Out[6]: \\n\\n    \\n\\n    -------------------------Summary of Regression Analysis-------------------------\\n\\n    \\n\\n    Formula: Y ~ <x> + <intercept>\\n\\n    \\n\\n    Number of Observations:         4\\n\\n    Number of Degrees of Freedom:   2\\n\\n    \\n\\n    R-squared:         0.7743\\n\\n    Adj R-squared:     0.6615\\n\\n    \\n\\n    Rmse:              0.5156\\n\\n    \\n\\n    F-stat (1, 2):     6.8626, p-value:     0.1200\\n\\n    \\n\\n    Degrees of Freedom: model 1, resid 2\\n\\n    \\n\\n    -----------------------Summary of Estimated Coefficients------------------------\\n\\n          Variable       Coef    Std Err     t-stat    p-value    CI 2.5%   CI 97.5%\\n\\n    --------------------------------------------------------------------------------\\n\\n                 x    -0.6040     0.2306      -2.62     0.1200    -1.0560    -0.1521\\n\\n         intercept     0.2915     0.4314       0.68     0.5689    -0.5540     1.1370\\n\\n    ---------------------------------End of Summary---------------------------------\\n\\n\\n\\nAlternatively, you could convert the index to an integer, although I found this didn\\'t work very well (I\\'m assuming because the integers represent nanoseconds since the epoch or something like that, and hence are very large and cause precision issues), but converting to integer and dividing by a trillion or so did work and gave essentially the same results as using `dt.hour` (i.e. same r-squared):\\n\\n\\n\\n    pd.ols(x=pd.to_datetime(z[\"index\"]).astype(int)/1e12, y=z[0])\\n\\n\\n\\n**Source of the error message**\\n\\n\\n\\nFWIW, it looks like that error message is coming from something like this:\\n\\n\\n\\n    pd.to_datetime(z[\"index\"]).astype(float)\\n\\n\\n\\nAlthough a fairly obvious workaround is this:\\n\\n\\n\\n    pd.to_datetime(z[\"index\"]).astype(int).astype(float)\\n\\n',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 25),\n",
       "  '2015-05-25 06:06:59',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '5481.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['909',\n",
       "  '30514622',\n",
       "  'Answer',\n",
       "  'Divide one column in array by another numpy',\n",
       "  'If you want to do it in place, you could do\\n\\n\\n\\n    a[:, 0] = a[:, 0] / a[:, 1]\\n\\n    a[:, 1] = 0\\n\\n\\n\\nIf not\\n\\n\\n\\n    b = np.zeros(6).reshape(2, 3)\\n\\n    b[:, 0] = (a[:, 0] / a[:, 1])',\n",
       "  '<python><numpy>',\n",
       "  datetime.date(2015, 5, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1091.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['912',\n",
       "  '30538371',\n",
       "  'Answer',\n",
       "  'Interpolate (or extrapolate) only small gaps in pandas dataframe',\n",
       "  \"So here is a mask that ought to solve the problem.  Just `interpolate` and then apply the mask to reset appropriate values to NaN.  Honestly, this was a bit more work than I realized it would be because I had to loop through each column but then groupby didn't quite work without me providing some dummy columns like 'ones'.\\n\\n\\n\\nAnyway, I can explain if anything is unclear but really only a couple of the lines are somewhat hard to understand.  See [here][1] for a little bit more of an explanation of the trick on the `df['new']` line or just print out individual lines to better see what is going on.\\n\\n\\n\\n    mask = data.copy()\\n\\n    for i in list('abcdefgh'):\\n\\n        df = pd.DataFrame( data[i] )\\n\\n        df['new'] = ((df.notnull() != df.shift().notnull()).cumsum())\\n\\n        df['ones'] = 1\\n\\n        mask[i] = (df.groupby('new')['ones'].transform('count') < 5) | data[i].notnull()\\n\\n    \\n\\n    In [7]: data\\n\\n    Out[7]: \\n\\n                             a      b      c   d       e       f       g       h\\n\\n    2014-02-21 14:50:00  123.5  433.5  123.5 NaN     NaN     NaN  2330.3  2330.3\\n\\n    2014-02-21 14:51:00    NaN  523.2  132.3 NaN     NaN     NaN     NaN     NaN\\n\\n    2014-02-21 14:52:00  136.3  536.3  136.3 NaN     NaN     NaN     NaN     NaN\\n\\n    2014-02-21 14:53:00  164.3  464.3  164.3 NaN     NaN     NaN     NaN     NaN\\n\\n    2014-02-21 14:54:00  213.0  413.0    NaN NaN     NaN  2763.0     NaN     NaN\\n\\n    2014-02-21 14:55:00  164.3  164.3    NaN NaN     NaN  2142.3     NaN     NaN\\n\\n    2014-02-21 14:56:00  213.0  213.0    NaN NaN     NaN  2127.3     NaN     NaN\\n\\n    2014-02-21 14:57:00  221.1  221.1    NaN NaN  2330.3  2330.3     NaN  2777.7\\n\\n    \\n\\n    In [8]: mask\\n\\n    Out[8]: \\n\\n                            a     b     c      d      e     f      g      h\\n\\n    2014-02-21 14:50:00  True  True  True  False  False  True   True   True\\n\\n    2014-02-21 14:51:00  True  True  True  False  False  True  False  False\\n\\n    2014-02-21 14:52:00  True  True  True  False  False  True  False  False\\n\\n    2014-02-21 14:53:00  True  True  True  False  False  True  False  False\\n\\n    2014-02-21 14:54:00  True  True  True  False  False  True  False  False\\n\\n    2014-02-21 14:55:00  True  True  True  False  False  True  False  False\\n\\n    2014-02-21 14:56:00  True  True  True  False  False  True  False  False\\n\\n    2014-02-21 14:57:00  True  True  True  False   True  True  False   True\\n\\n\\n\\nIt's easy from there if you don't do anything fancier with respect to extrapolation:\\n\\n\\n\\n    In [9]: data.interpolate().bfill()[mask]\\n\\n    Out[9]: \\n\\n                             a      b      c   d       e       f       g       h\\n\\n    2014-02-21 14:50:00  123.5  433.5  123.5 NaN     NaN  2763.0  2330.3  2330.3\\n\\n    2014-02-21 14:51:00  129.9  523.2  132.3 NaN     NaN  2763.0     NaN     NaN\\n\\n    2014-02-21 14:52:00  136.3  536.3  136.3 NaN     NaN  2763.0     NaN     NaN\\n\\n    2014-02-21 14:53:00  164.3  464.3  164.3 NaN     NaN  2763.0     NaN     NaN\\n\\n    2014-02-21 14:54:00  213.0  413.0  164.3 NaN     NaN  2763.0     NaN     NaN\\n\\n    2014-02-21 14:55:00  164.3  164.3  164.3 NaN     NaN  2142.3     NaN     NaN\\n\\n    2014-02-21 14:56:00  213.0  213.0  164.3 NaN     NaN  2127.3     NaN     NaN\\n\\n    2014-02-21 14:57:00  221.1  221.1  164.3 NaN  2330.3  2330.3     NaN  2777.7\\n\\n\\n\\n**Edit to add:**  Here's a faster (about 2x on this sample data) and slightly simpler way, by moving some stuff outside of the loop:\\n\\n\\n\\n    mask = data.copy()\\n\\n    grp = ((mask.notnull() != mask.shift().notnull()).cumsum())\\n\\n    grp['ones'] = 1\\n\\n    for i in list('abcdefgh'):\\n\\n        mask[i] = (grp.groupby(i)['ones'].transform('count') < 5) | data[i].notnull()\\n\\n\\n\\n  [1]: https://stackoverflow.com/questions/29499049/testing-subsequent-values-in-a-dataframe/29500132#29500132\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><pandas><interpolation><extrapolation>',\n",
       "  datetime.date(2015, 5, 29),\n",
       "  '2017-05-23 12:01:59',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '6',\n",
       "  '',\n",
       "  '1675.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['914',\n",
       "  '29500132',\n",
       "  'Answer',\n",
       "  'Testing subsequent values in a DataFrame',\n",
       "  'FWIW, here\\'s a fairly pandastic answer that requires no functions or applies.  Borrows from [here][1] (among other answers I\\'m sure) and thanks to @DSM for mentioning the ascending=False option:\\n\\n\\n\\n    df = pd.DataFrame({\"a\": [2, -1, -3, -1, 1, 1, -1, 1, -1, -2]})\\n\\n    \\n\\n    df[\\'pos\\'] = df.a > 0\\n\\n    df[\\'grp\\'] = ( df[\\'pos\\'] != df[\\'pos\\'].shift()).cumsum()\\n\\n    dfg = df.groupby(\\'grp\\')\\n\\n    df[\\'c\\'] = np.where( df[\\'a\\'] < 0, dfg.cumcount(ascending=False)+1, 0 )\\n\\n\\n\\n       a  b    pos  grp  c\\n\\n    0  2  0   True    1  0\\n\\n    1 -1  3  False    2  3\\n\\n    2 -3  2  False    2  2\\n\\n    3 -1  1  False    2  1\\n\\n    4  1  0   True    3  0\\n\\n    5  1  0   True    3  0\\n\\n    6 -1  1  False    4  1\\n\\n    7  1  0   True    5  0\\n\\n    8 -1  1  False    6  2\\n\\n    9 -2  1  False    6  1\\n\\n\\n\\nI think a nice thing about this method is that once you set up the \\'grp\\' variable you can do lots of things very easily with standard groupby methods.\\n\\n\\n\\n  [1]: https://stackoverflow.com/questions/29142487/calculating-the-number-of-specific-consecutive-equal-values-in-a-vectorized-way/29143354#29143354',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 4, 7),\n",
       "  '2017-05-23 11:59:42',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1483.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['917',\n",
       "  '30523731',\n",
       "  'Answer',\n",
       "  'Pandas: Printing the Names and Values in a Series',\n",
       "  'The [``index``](http://pandas.pydata.org/pandas-docs/dev/dsintro.html) member of a series is the \"names\", so:\\n\\n\\n\\n    for name, val in w.index:\\n\\n        print name\\n\\n\\n\\nwill iterate over the names. For both, you can use\\n\\n\\n\\n    import itertools\\n\\n\\n\\n    for name, val in itertools.izip(w.index, w):\\n\\n        print name, val',\n",
       "  '<python><database><python-3.x><pandas>',\n",
       "  datetime.date(2015, 5, 29),\n",
       "  '2015-05-29 07:38:09',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '10111.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['925',\n",
       "  '29449324',\n",
       "  'Answer',\n",
       "  'Fastest way to compare row and previous row in pandas dataframe with millions of rows',\n",
       "  \"I was thinking along the same lines as Andy, just with ```groupby``` added, and I think this is complementary to Andy's answer.  Adding groupby is just going to have the effect of putting a NaN in the first row whenever you do a ```diff``` or ```shift```.  (Note that this is not an attempt at an exact answer, just to sketch out some basic techniques.)\\n\\n\\n\\n    df['time_diff'] = df.groupby('User')['Time'].diff()\\n\\n    \\n\\n    df['Col1_0'] = df['Col1'].apply( lambda x: x[0] )\\n\\n    \\n\\n    df['Col1_0_prev'] = df.groupby('User')['Col1_0'].shift()\\n\\n    \\n\\n       User  Time                 Col1  time_diff Col1_0 Col1_0_prev\\n\\n    0     1     6     [cat, dog, goat]        NaN    cat         NaN\\n\\n    1     1     6         [cat, sheep]          0    cat         cat\\n\\n    2     1    12        [sheep, goat]          6  sheep         cat\\n\\n    3     2     3          [cat, lion]        NaN    cat         NaN\\n\\n    4     2     5  [fish, goat, lemur]          2   fish         cat\\n\\n    5     3     9           [cat, dog]        NaN    cat         NaN\\n\\n    6     4     4          [dog, goat]        NaN    dog         NaN\\n\\n    7     4    11                [cat]          7    cat         dog\\n\\n\\n\\nAs a followup to Andy's point about storing objects, note that what I did here was to extract the first element of the list column (and add a shifted version also).  Doing it like this you only have to do an expensive extraction once and after that can stick to standard pandas methods.\",\n",
       "  '<python><performance><pandas><bigdata><cython>',\n",
       "  datetime.date(2015, 4, 4),\n",
       "  '2015-04-04 17:49:26',\n",
       "  'JohnE (3877338)',\n",
       "  '11',\n",
       "  '',\n",
       "  '28204.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['949',\n",
       "  '29927921',\n",
       "  'Answer',\n",
       "  'Creating a partial SAS PROC SUMMARY replacement in Python/Pandas',\n",
       "  'Well, here\\'s a quickie that does get at two issues (but still requires a different function for weighted mean).  Mostly it uses the trick [here][1] (credit to @DSM) to get around your empty group by doing ```groupby(lamda x: True)```.  It would be great if there was a kwarg for \\'weights\\' on stuff like means but there is not, to my knowledge.  Apparently there is a package for weighted quantiles mentioned [here][2] based on numpy but I don\\'t know anything about it.  Great project btw!\\n\\n\\n\\n(note that names are mostly the same as yours, I just added a \\'2\\' to wmean_grouped and my_summary, otherwise you can use the same calling interface)\\n\\n\\n\\n    def wmean_grouped2 (group, var_name_in, var_name_weight):\\n\\n        d = group[var_name_in]\\n\\n        w = group[var_name_weight]\\n\\n        return (d * w).sum() / w.sum()\\n\\n    \\n\\n    FUNCS = { \"mean\"  : np.mean ,\\n\\n              \"sum\"   : np.sum ,\\n\\n              \"count\" : np.count_nonzero }\\n\\n    \\n\\n    def my_summary2 (\\n\\n            data ,\\n\\n            var_names_in ,\\n\\n            var_names_out ,\\n\\n            var_functions ,\\n\\n            var_name_weight = None ,\\n\\n            var_names_group = None ):\\n\\n    \\n\\n        result = pd.DataFrame()\\n\\n    \\n\\n        if var_names_group is None:\\n\\n            grouped = data.groupby (lambda x: True)\\n\\n        else:\\n\\n            grouped = data.groupby (var_names_group)\\n\\n    \\n\\n        for var_name_in, var_name_out, var_function in \\\\\\n\\n                zip(var_names_in,var_names_out,var_functions):\\n\\n            if var_function == \"wmean\":\\n\\n                func = lambda x : wmean_grouped2 (x, var_name_in, var_name_weight)\\n\\n                result[var_name_out] = pd.Series(grouped.apply(func))\\n\\n            else:\\n\\n                func = FUNCS[var_function]\\n\\n                result[var_name_out] = grouped[var_name_in].apply(func)\\n\\n        \\n\\n        return result\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/questions/11492215/how-to-do-a-groupby-on-an-empty-set-of-columns-in-pandas\\n\\n  [2]: https://stackoverflow.com/questions/26102867/python-weighted-median-algorithm-with-pandas',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 4, 28),\n",
       "  '2017-05-23 12:22:26',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1037.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['950',\n",
       "  '29983687',\n",
       "  'Answer',\n",
       "  'Complexity - determining the order of growth',\n",
       "  \"There's a very simple rule for the type of questions you posted.\\n\\n\\n\\nSuppose you're trying to find the order of growth of ``f(n)``, and you find some simple function ``g(n)`` such that\\n\\n\\n\\n    lim {n -> inf} f(n) / g(n) = k\\n\\n\\n\\nwhere ``k`` is a positive finite constant. Then\\n\\n\\n\\n    f(n) = Theta(g(n))\\n\\n\\n\\n(It's easy to see this from the calculus definitions.)\\n\\n\\n\\nNow let's see how this applies to your examples:\\n\\n\\n\\n    lim {n -> inf} (a/n^3 + bn^2) / n^2 = b\\n\\n\\n\\nso it's ``Theta(n^2)``.\\n\\n\\n\\n    lim {n -> inf} (a n^3 - bn^2) / n^3 = a\\n\\n\\n\\nso it's ``Theta(n^2)``.\\n\\n\\n\\n(of course, assuming a and b are positive.)\\n\\n\",\n",
       "  '<algorithm><big-o><complexity-theory><division>',\n",
       "  datetime.date(2015, 5, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2281.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['954',\n",
       "  '30715507',\n",
       "  'Answer',\n",
       "  'unary operator in std::transform',\n",
       "  'The line\\n\\n\\n\\n    std::transform(cb, --ce, ++b, [] (CI::value_type n) { return ++n; });\\n\\n\\n\\nsays:\\n\\n\\n\\n1. decrement ``ce``, increment ``b``\\n\\n\\n\\n2. For each value starting at ``cb`` and ending at ``ce`` (noninclusive!), place an incremented-by-1 value starting at ``b``.\\n\\n\\n\\n',\n",
       "  '<c++><algorithm><stl>',\n",
       "  datetime.date(2015, 6, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1058.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['963',\n",
       "  '30486721',\n",
       "  'Answer',\n",
       "  'IndexError: index 1 is out of bounds for axis 1 with size 1',\n",
       "  \"I bet this will be easier than you expected.  First, let's make `ds2` a dictionary rather than a dataframe.\\n\\n\\n\\n     ds2 = dict([\\n\\n           [ 4,  1],\\n\\n           [ 5,  3],\\n\\n           [ 6,  1],\\n\\n           [ 7,  2],\\n\\n           [ 4,  1],\\n\\n           [ 8,  2],\\n\\n           [ 9,  3],\\n\\n           [12,  1],\\n\\n           [13,  2],\\n\\n           [22,  3]])\\n\\n\\n\\nNow, we'll just use `ds2` to directly map all the elements in `ds1`:    \\n\\n\\n\\n    ds3 = ds1.copy()\\n\\n    for i in range(4):\\n\\n        ds3[i] = ds3[i].map( ds2 )\\n\\n    \\n\\n       0   1  2   3\\n\\n    0  1   2  1   3\\n\\n    1  2   1  3   2\\n\\n    2  2 NaN  1   3\\n\\n    3  3   2  1 NaN\\n\\n\\n\\nIf you want `0`'s instead of NaN, just do `ds3.fillna(0)`.\\n\\n\\n\\nFor some reason, I couldn't get this to work:\\n\\n\\n\\n    ds3.applymap( ds2 )\\n\\n\\n\\nBut this works and avoids the looping over columns, though the syntax is not quite as simple as it is for a series:\\n\\n\\n\\n    ds1.applymap( lambda x: ds2.get(x,0) )\\n\\n\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2015, 5, 27),\n",
       "  '2015-05-27 17:32:08',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '4663.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['968',\n",
       "  '30625236',\n",
       "  'Answer',\n",
       "  'Pandas isin() function for continuous intervals',\n",
       "  \"You could also do the same thing with `cut()`.  No real advantage if there are just two categories:\\n\\n\\n\\n    >>> df['numdum'] = pd.cut( df['number'], [-99,10,99], labels=[1,0] )\\n\\n\\n\\n       number numdum\\n\\n    0       8      1\\n\\n    1       9      1\\n\\n    2      10      1\\n\\n    3      11      0\\n\\n    4      12      0\\n\\n    5      13      0\\n\\n    6      14      0\\n\\n\\n\\nBut it's nice if you have multiple categories:\\n\\n\\n\\n    >>> df['numdum'] = pd.cut( df['number'], [-99,8,10,99], labels=[1,2,3] )\\n\\n\\n\\n       number numdum\\n\\n    0       8      1\\n\\n    1       9      2\\n\\n    2      10      2\\n\\n    3      11      3\\n\\n    4      12      3\\n\\n    5      13      3\\n\\n    6      14      3\\n\\n\\n\\nLabels can be `True` and `False` if that is preferred, or you can not specify the label at all, in which case the labels will contain info on the cutoff points.\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 6, 3),\n",
       "  '2015-06-03 16:12:02',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1263.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['970',\n",
       "  '29569196',\n",
       "  'Answer',\n",
       "  'python pandas conditional count across columns',\n",
       "  \"I'm just doing this with a flat dataframe but it's the same for panel.  You can do one of two ways.  The first way is what you did, just change the ```count()``` to ```sum()```:\\n\\n\\n\\n    ( df > 0 ).sum(axis=1)\\n\\n\\n\\nThe underlying structure is boolean and True and False both get counted, whereas if you sum them it is interpreted more like you were expecting (0/1).\\n\\n\\n\\nBut a more standard way to do it would be like this:\\n\\n\\n\\n    df[ df > 0 ].count(axis=1)\\n\\n\\n\\nWhile the former method was based on a dataframe of booleans, the latter looks like this:\\n\\n\\n\\n    df[ df > 0 ]\\n\\n  \\n\\n        a   b   c   d   e   f   g   h   i   j\\n\\n    0   1 NaN NaN NaN NaN NaN NaN NaN   1 NaN\\n\\n    1 NaN   1 NaN NaN NaN   1 NaN NaN NaN NaN\\n\\n    2   1 NaN NaN NaN NaN NaN NaN NaN NaN NaN\\n\\n    3 NaN NaN NaN NaN NaN NaN NaN   1 NaN NaN\\n\\n    4 NaN NaN NaN   1 NaN NaN NaN NaN NaN NaN\\n\\n\\n\\nIn this case it doesn't really matter which method you use, but in general the latter is going to be better, because you can do more with it.  For example, with the former method (which has binary outcomes by design), all you can really do is count, but in the latter method you can count, sum, multiply, etc.\\n\\n\\n\\nThe potential usefulness of this may be more obvious for the case of ```df != 0```, where there are more than two possible values:\\n\\n\\n\\n    df[ df != 0 ]\\n\\n\\n\\n        a   b   c   d   e   f   g   h   i   j\\n\\n    0   1 NaN NaN  -1 NaN NaN  -1 NaN   1 NaN\\n\\n    1 NaN   1 NaN NaN NaN   1 NaN NaN NaN  -1\\n\\n    2   1 NaN NaN NaN NaN  -1 NaN NaN NaN NaN\\n\\n    3 NaN  -1 NaN NaN NaN NaN NaN   1 NaN NaN\\n\\n    4 NaN NaN NaN   1 NaN NaN  -1 NaN NaN  -1\\n\\n\\n\\n\",\n",
       "  '<python><pandas><conditional><dataframe><vectorization>',\n",
       "  datetime.date(2015, 4, 10),\n",
       "  '2015-04-10 19:39:59',\n",
       "  'JohnE (3877338)',\n",
       "  '4',\n",
       "  '',\n",
       "  '4244.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['971',\n",
       "  '30125571',\n",
       "  'Answer',\n",
       "  'multithreading check membership in Queue and stop the threads',\n",
       "  \"A couple of things that I think can be improved:\\n\\n\\n\\n1. Due to the [GIL](http://en.wikipedia.org/wiki/Global_Interpreter_Lock), you might want to use the [``multiprocessing``](https://docs.python.org/2/library/multiprocessing.html) (rather than ``threading``) module. In general, CPython threading will not cause CPU intensive work to speed up. (Depending on what exactly is the context of your question, it's also possible that ``multiprocessing`` won't, but ``threading`` almost certainly won't.)\\n\\n2. A function like your ``is_inqueue`` would likely lead to high contention.\\n\\n\\n\\nThe locked time seems linear in the number of items that need to be traversed:\\n\\n\\n\\n    def is_in_queue(x, q):\\n\\n        with q.mutex:\\n\\n            return x in q.queue\\n\\n\\n\\nSo, instead, you could possibly do the following.\\n\\n\\n\\nUse ``multiprocessing`` with a shared ``dict``:\\n\\n\\n\\n     from multiprocessing import Process, Manager\\n\\n\\n\\n     manager = Manager()\\n\\n     d = manager.dict()\\n\\n\\n\\n     # Fn definitions and such\\n\\n\\n\\n     p1 = Process(target=p1, args=(d,))\\n\\n     p2 = Process(target=p2, args=(d,))\\n\\n\\n\\nwithin each function, check for the item like this:\\n\\n\\n\\n    def p1(d):\\n\\n    \\n\\n        # Stuff\\n\\n    \\n\\n        if 'foo' in d:\\n\\n            return \",\n",
       "  '<python><multithreading><python-2.7>',\n",
       "  datetime.date(2015, 5, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1264.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['975',\n",
       "  '30876502',\n",
       "  'Answer',\n",
       "  'Is there a concise way to show all rows in pandas for just the current command?',\n",
       "  'You could write a function that explicitly calls [`display`](http://ipython.org/ipython-doc/2/api/generated/IPython.core.display.html)\\n\\n\\n\\nE.g., consider this function:\\n\\n\\n\\n    from IPython.display import display\\n\\n\\n\\n    def show_more(df, lines):\\n\\n        foo = 1\\n\\n        display(df)\\n\\n        foo = 2\\n\\n\\n\\nWhen I call the function (just tried it):\\n\\n\\n\\n    >> show_more(df, 1000)\\n\\n    ... # <- Shows here the DF\\n\\n\\n\\nthen it displays the dataframe, even though the line `foo = 2` is executed *after*. It follows that you can set the options instead of the line `foo = 1` and unset it in the line `foo = 2`. In fact, you can just use the context manager from your question, probably. ',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 6, 16),\n",
       "  '2015-06-16 19:48:00',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '0',\n",
       "  '',\n",
       "  '7870.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['978',\n",
       "  '30873832',\n",
       "  'Answer',\n",
       "  'Convert hhmmss to time using python pandas',\n",
       "  \"Apparently there's some inherent problem with the data, as it just doesn't correspond to hours, minutes, seconds:\\n\\n\\n\\n     import pandas as pd\\n\\n\\n\\n     df = pd.DataFrame({\\n\\n        'a': ['123045', '134500', '102367']})\\n\\n     pd.to_datetime(df.a.astype(str), format='%h%M%S')\\n\\n\\n\\ngives an error\\n\\n\\n\\n    ---------------------------------------------------------------------------\\n\\n    ValueError                                Traceback (most recent call last)\\n\\n    <ipython-input-5-b4af62ff61ff> in <module>()\\n\\n    ----> 1 pd.to_datetime(df.a.astype(str), format='%H%M%S')\\n\\n    \\n\\n    /home/amit/.local/lib/python2.7/site-packages/pandas/tseries/tools.pyc in to_datetime(arg, errors, dayfirst, utc, box, format, exact, coerce, unit, infer_datetime_format)\\n\\n    335         return arg\\n\\n    336     elif isinstance(arg, Series):\\n\\n    --> 337         values = _convert_listlike(arg.values, False, format)\\n\\n    338         return Series(values, index=arg.index, name=arg.name)\\n\\n    339     elif com.is_list_like(arg):\\n\\n\\n\\n    /home/amit/.local/lib/python2.7/site-packages/pandas/tseries/tools.pyc in _convert_listlike(arg, box, format)\\n\\n    328                 return DatetimeIndex._simple_new(values, None, tz=tz)\\n\\n    329             except (ValueError, TypeError):\\n\\n    --> 330                 raise e\\n\\n    331 \\n\\n    332     if arg is None:\\n\\n\\n\\n    ValueError: unconverted data remains: 7\\n\\n\\n\\n\\n\\nwhich, unfortunately, makes a lot of sense, no? How is `102367` to be interpreted in this format?\",\n",
       "  '<datetime><pandas>',\n",
       "  datetime.date(2015, 6, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1267.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['980',\n",
       "  '30625711',\n",
       "  'Answer',\n",
       "  'Iterate over a list of images and assign them as variables in python',\n",
       "  'Change:\\n\\n\\n\\n    for image in AtoC:\\n\\n        AtoCIm = []\\n\\n        AtoCIm.append(Image.open(image))\\n\\n\\n\\nto \\n\\n\\n\\n    AtoCIm = []\\n\\n    for image in AtoC:\\n\\n        AtoCIm.append(Image.open(image))\\n\\n\\n\\nwill do it.\\n\\n\\n\\nYou are creating a new list every iteration, rather than creating a list once and appending new items to it. Therefore, your list always has only 1 item. Attempting to get the 2nd item, `AtoCIm[1]`, will raise exception.',\n",
       "  '<python><image><iteration><python-imaging-library><pillow>',\n",
       "  datetime.date(2015, 6, 3),\n",
       "  '2015-06-04 14:37:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '6',\n",
       "  '',\n",
       "  '3543.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['984',\n",
       "  '30922618',\n",
       "  'Answer',\n",
       "  'How to append data to pandas multi-index dataframe',\n",
       "  \"Am using simplified versions of your DataFrames.\\n\\n\\n\\nSuppose you start with:\\n\\n\\n\\n    import pandas as pd\\n\\n    import numpy as np\\n\\n\\n\\n    arrays = [np.array(['bar', 'bar', 'baz', 'baz', 'foo', 'foo', 'qux', 'qux']),\\n\\n        np.array(['one', 'two', 'one', 'two', 'one', 'two', 'one', 'two'])]\\n\\n\\n\\n    s = pd.DataFrame(index=arrays)\\n\\n\\n\\nso that\\n\\n\\n\\n    >> s\\n\\n    bar one\\n\\n        two\\n\\n    baz one\\n\\n        two\\n\\n    foo one\\n\\n        two\\n\\n    qux one\\n\\n        two\\n\\n\\n\\n(this is your parent)\\n\\n\\n\\nand also\\n\\n\\n\\n    c = pd.DataFrame(index=['one', 'two'], data=[23, 33])\\n\\n\\n\\nso that \\n\\n    \\n\\n    >> c\\n\\n     \\t0\\n\\n \\tone \\t23\\n\\n \\ttwo \\t33\\n\\n\\n\\n(this is your first DataFrame)\\n\\n\\n\\nSo, a `merge` + `groupby` give\\n\\n\\n\\n    >> pd.merge(s.reset_index(), c, left_on='level_1', right_index=True).groupby(['level_0', 'level_1']).sum()\\n\\n     \\t\\t0\\n\\n    level_0 \\tlevel_1 \\t\\n\\n    bar one \\t23\\n\\n        two \\t33\\n\\n    baz one \\t23\\n\\n        two \\t33\\n\\n    foo one \\t23\\n\\n        two \\t33\\n\\n    qux one \\t23\\n\\n        two \\t33\",\n",
       "  '<pandas><dataframe>',\n",
       "  datetime.date(2015, 6, 18),\n",
       "  '2015-06-18 21:44:42',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2372.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['986',\n",
       "  '30942851',\n",
       "  'Answer',\n",
       "  'Plotting multiple time series after a groupby in pandas',\n",
       "  '(Am a bit amused, as this question caught me doing the exact same thing.)\\n\\n\\n\\nYou could do something like\\n\\n\\n\\n    valgdata\\\\\\n\\n        .groupby([valgdata.dato_uden_tid.name, valgdata.news_site.name])\\\\\\n\\n        .mean()\\\\\\n\\n        .unstack()\\n\\n\\n\\nwhich would \\n\\n\\n\\n- reverse the groupby\\n\\n\\n\\n- unstack the new sites to be columns\\n\\n\\n\\nTo plot, just do the previous snippet immediately followed by `.plot()`:\\n\\n\\n\\n    valgdata\\\\\\n\\n        .groupby([valgdata.dato_uden_tid.name, valgdata.news_site.name])\\\\\\n\\n        .mean()\\\\\\n\\n        .unstack()\\\\\\n\\n        .plot()',\n",
       "  '<python><pandas><group-by><time-series>',\n",
       "  datetime.date(2015, 6, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1245.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['988',\n",
       "  '30194958',\n",
       "  'Answer',\n",
       "  'what is the inverse function of Sinc',\n",
       "  'In general, even if restricted to small intervals where *sinc* is [bijective](http://en.wikipedia.org/wiki/Bijection) (which I don\\'t think is the case for your requirements), [it has no simple inverse](https://www.physicsforums.com/threads/inverse-of-sinc.282941/).\\n\\n\\n\\nPerhaps you could do one of the following:\\n\\n\\n\\n1. You could calculate the inverse \"online\" using the minimization of of *abs(sinc(x) - y)* (see, e.g., [Numerical Recipes in C](https://www.google.co.il/search?q=numerical+methods+c+minimizing&ie=utf-8&oe=utf-8&gws_rd=cr&ei=rhdSVdWEAsizsQGM74H4BQ). Note that you\\'re in luck as it\\'s a smooth function, and so you can use the derivatives.\\n\\n\\n\\n2. Create \"offline\" a lookup table for various values in the required range, and given an \"online\" query, interpolate between two pre-calculated results. ',\n",
       "  '<math><numerical-methods>',\n",
       "  datetime.date(2015, 5, 12),\n",
       "  '2015-05-12 15:22:10',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '6',\n",
       "  '',\n",
       "  '3218.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['990',\n",
       "  '30206191',\n",
       "  'Answer',\n",
       "  'distance between two points in prolog',\n",
       "  \"You are defining ``distance`` in terms of ``distance``. Once there's a query for ``distance``, the computer will call ``distance``, which will again call ``distance``, and so forth. This is known as an [infinite recursion](http://en.wiktionary.org/wiki/infinite_recursion).\\n\\n\\n\\nSee also [this SO question](https://stackoverflow.com/questions/20979780/error-out-of-local-stack-in-my-prolog-code).\\n\\n\\n\\nYou should change your code so that the right hand side of \\n\\n\\n\\n    distance(N1,N2,D):-distance(point(N1,X1,Y2),point(N2,X2,Y2),Z)\\n\\n\\n\\ndoes not always refer to the left hand side.\",\n",
       "  '<prolog>',\n",
       "  datetime.date(2015, 5, 13),\n",
       "  '2017-05-23 11:58:11',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1734.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['995',\n",
       "  '30523907',\n",
       "  'Answer',\n",
       "  'Python dictionary as html table in ipython notebook',\n",
       "  \"You're probably looking for something like [ipy_table](https://pypi.python.org/pypi/ipy_table).\\n\\n\\n\\nA different way would be to use [pandas](http://pandas.pydata.org/) for a dataframe, but that might be an overkill.\",\n",
       "  '<python><ipython-notebook>',\n",
       "  datetime.date(2015, 5, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '12',\n",
       "  '',\n",
       "  '12530.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['996',\n",
       "  '30530641',\n",
       "  'Answer',\n",
       "  'List with many dictionaries VS dictionary with few lists?',\n",
       "  'Some answers regarding the pandas aspect:\\n\\n\\n\\n 1. Both dataframes are indeed the same and are column oriented, which is good, because pandas works best when data in each column is homogeneous (i.e. numbers can be stored as ints and floats).  A key reason for using pandas in the first place is that you can do vectorized numerical operations that are orders of magnitude faster than pure python -- but this relies on columnar organization when data is of heterogeneous type.\\n\\n 2. You could do `pd_users.T` to transpose, if you wanted to, and would then see (via `info()` or `dtypes`) that everything is then stored as a general purpose object because the column contains both strings and numbers.\\n\\n 3. Once converted, you can do `pd_users.set_index(\\'id\\')` so that your dataframe is essentially a dictionary with `id` as the keys.  Or vice versa with `name`.\\n\\n 4. It\\'s pretty common (and generally pretty fast) to change indexes, then change them back, transpose, subset, etc. when working with pandas so it\\'s usually not necessary to think too much about the structure at the beginning.  Just change it as you need to on the fly.\\n\\n 5. This may be getting off on a tangent, but the a simpler pandas analog of what you have above may be a `Series` rather than `DataFrame`.  A series is essentially a column of a dataframe though it really is just a one-dimensional data array with an index (\"keys\").\\n\\n\\n\\nQuick demo (using `df` as the dataframe name, the common convention):\\n\\n\\n\\n    >>> df.set_index(\\'name\\')\\n\\n     \\n\\n             id\\n\\n    name       \\n\\n    Ashley    0\\n\\n    Ben       1\\n\\n    Conrad    2\\n\\n    Doug      3\\n\\n    Evin      4\\n\\n    Florian   5\\n\\n    Gerald    6\\n\\n    \\n\\n    >>> df.set_index(\\'name\\').T\\n\\n     \\n\\n    name  Ashley  Ben  Conrad  Doug  Evin  Florian  Gerald\\n\\n    id         0    1       2     3     4        5       6\\n\\n    \\n\\n    >>> df.set_index(\\'name\\').loc[\\'Doug\\']\\n\\n     \\n\\n    id    3\\n\\n    Name: Doug, dtype: int64\\n\\n\\n\\n',\n",
       "  '<python><pandas><dataset>',\n",
       "  datetime.date(2015, 5, 29),\n",
       "  '2015-09-24 15:52:46',\n",
       "  'JohnE (3877338)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1582.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['997',\n",
       "  '29657530',\n",
       "  'Answer',\n",
       "  \"Avoiding Excel's Scientific Notation Rounding when Parsing with Pandas\",\n",
       "  'Pandas might very well be pulling the full value but not showing it in its default output:\\n\\n\\n\\n    df = pd.DataFrame({ \\'x\\':[135061808695.] })\\n\\n\\n\\n    df.x\\n\\n    0    1.350618e+11  \\n\\n    Name: x, dtype: float64\\n\\n\\n\\nStandard python format:\\n\\n\\n\\n    print \"%15.0f\" % df.x\\n\\n    135061808695\\n\\n\\n\\nOr in pandas, convert to an integer type to get integer formatting:\\n\\n\\n\\n    df.x.astype(np.int64)\\n\\n \\n\\n    0    135061808695\\n\\n    Name: x, dtype: int64\\n\\n',\n",
       "  '<python><parsing><pandas><import-from-excel>',\n",
       "  datetime.date(2015, 4, 15),\n",
       "  '2015-04-15 18:19:21',\n",
       "  'JohnE (3877338)',\n",
       "  '6',\n",
       "  '',\n",
       "  '1903.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1002',\n",
       "  '30685058',\n",
       "  'Answer',\n",
       "  'efficient way to get index in sorted vector in c++',\n",
       "  'The exact same proof of the lower bound on sorting applies here. Sans additional information (key distribution, etc.), it is *n log(n)* at a lower bound, and you might as well sort. Formally, anything lower would allow you to compress permutations below the [Kolmogorov complexity](http://en.wikipedia.org/wiki/Kolmogorov_complexity).\\n\\n\\n\\n----------------------\\n\\n\\n\\nThat being said, there is the question of how to sort the indices. See [here](https://stackoverflow.com/questions/1577475/c-sorting-and-keeping-track-of-indexes).',\n",
       "  '<c++><c++11>',\n",
       "  datetime.date(2015, 6, 6),\n",
       "  '2017-05-23 10:26:52',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '6',\n",
       "  '',\n",
       "  '1228.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1005',\n",
       "  '30920309',\n",
       "  'Answer',\n",
       "  'Using Cross-Validation on a Scikit-Learn Classifer',\n",
       "  \"For what you're describing, you just need to use [`train_test_split`](http://scikit-learn.org/stable/modules/cross_validation.html) with a following split on its results.\\n\\n\\n\\nAdapting the tutorial there, start with something like this:\\n\\n\\n\\n    import numpy as np\\n\\n    from sklearn import cross_validation\\n\\n    from sklearn import datasets\\n\\n    from sklearn import svm\\n\\n\\n\\n    iris = datasets.load_iris()\\n\\n    iris.data.shape, iris.target.shape\\n\\n    ((150, 4), (150,))\\n\\n\\n\\nThen, just like there, make the initial train/test partition:\\n\\n\\n\\n    X_train, X_test, y_train, y_test = cross_validation.train_test_split(iris.data, iris.target, test_size=0.1, random_state=0)\\n\\n\\n\\nNow you just need to split the 0.9 of the train data into two more parts:\\n\\n\\n\\n    X_train_cv_train, X_test_cv_train, y_train_cv_train, y_test_cv_train = \\\\\\n\\n    cross_validation.train_test_split(X_train, y_train, test_size=0.2/0.9)\\n\\n\\n\\nIf you want 10 random train/test cv sets, repeat the last line 10 times (this will give you sets with overlap).\\n\\n\\n\\nAlternatively, you could replace the last line with 10-fold validation (see [the relevant classes](http://scikit-learn.org/stable/modules/cross_validation.html#k-fold)). \\n\\n\\n\\nThe main point is to build the CV sets from the train part of the initial train/test partition.\",\n",
       "  '<python><scikit-learn><cross-validation>',\n",
       "  datetime.date(2015, 6, 18),\n",
       "  '2015-06-18 16:37:41',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '4273.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1006',\n",
       "  '30960811',\n",
       "  'Answer',\n",
       "  'Pandas: Difference between pivot and pivot_table. Why is only pivot_table working?',\n",
       "  \"I'm not sure I understand, but I'll give it a try.  I usually use stack/unstack instead of pivot, is this closer to what you want?\\n\\n\\n\\n    df.set_index(['struct_id','resNum','score_type_name']).unstack()\\n\\n\\n\\n                      score_value                                              \\n\\n    score_type_name        fa_dun fa_dun_dev fa_dun_rot fa_dun_semi     omega   \\n\\n    struct_id  resNum                                                           \\n\\n    4294967297 1         2.185618   0.000027        NaN    2.185591  0.064840   \\n\\n               2         1.378923   0.028560   1.350362         NaN  0.222345   \\n\\n               3         0.020352   0.025507  -0.005156         NaN  0.005106   \\n\\n               4         4.218029   0.003712        NaN    4.214317  0.212160   \\n\\n               5         3.663050   0.004953        NaN         NaN  0.061867   \\n\\n    \\n\\n                                                     \\n\\n    score_type_name     p_aa_pp      rama       ref  \\n\\n    struct_id  resNum                                \\n\\n    4294967297 1            NaN       NaN -1.191180  \\n\\n               2      -0.442467 -0.795161  0.249477  \\n\\n               3      -0.096847  0.267443  0.979644  \\n\\n               4      -0.462765 -1.403292 -1.960940  \\n\\n               5            NaN -0.600053       NaN  \\n\\n\\n\\n\\n\\nI'm not sure why your pivot isn't working (kinda seems to me like it should, but I could be wrong), but it does seem to work (or at least not give an error) if I leave off 'struct_id'.  Of course, that's not really a useful solution for the full dataset where you have more than one different values for 'struct_id'.\\n\\n\\n\\n    df.pivot(columns='score_type_name',values='score_value',index='resNum')\\n\\n    \\n\\n    score_type_name    fa_dun  fa_dun_dev  fa_dun_rot  fa_dun_semi     omega  \\n\\n    resNum                                                                     \\n\\n    1                2.185618    0.000027         NaN     2.185591  0.064840   \\n\\n    2                1.378923    0.028560    1.350362          NaN  0.222345   \\n\\n    3                0.020352    0.025507   -0.005156          NaN  0.005106   \\n\\n    4                4.218029    0.003712         NaN     4.214317  0.212160   \\n\\n    5                3.663050    0.004953         NaN          NaN  0.061867   \\n\\n    \\n\\n    score_type_name   p_aa_pp      rama       ref  \\n\\n    resNum                                         \\n\\n    1                     NaN       NaN -1.191180  \\n\\n    2               -0.442467 -0.795161  0.249477  \\n\\n    3               -0.096847  0.267443  0.979644  \\n\\n    4               -0.462765 -1.403292 -1.960940  \\n\\n    5                     NaN -0.600053       NaN  \\n\\n\\n\\n**Edit to add:**  `reset_index()` will convert from a multi-index (hierarchical) to a flatter style.  There is still some hierarchy in the column names, sometimes the easiest way to get rid of those is just to do `df.columns=['var1','var2',...]` although there are more sophisticated ways if you do some searching.\\n\\n\\n\\n`df.set_index(['struct_id','resNum','score_type_name']).unstack().reset_index()`\\n\\n   \\n\\n                      struct_id resNum score_value                            \\n\\n    score_type_name                         fa_dun fa_dun_dev fa_dun_rot   \\n\\n    0                4294967297      1    2.185618   0.000027        NaN   \\n\\n    1                4294967297      2    1.378923   0.028560   1.350362   \\n\\n    2                4294967297      3    0.020352   0.025507  -0.005156   \\n\\n    3                4294967297      4    4.218029   0.003712        NaN   \\n\\n    4                4294967297      5    3.663050   0.004953        NaN   \\n\\n                                                                    \\n\\n\\n\\n\",\n",
       "  '<python><pandas><pivot>',\n",
       "  datetime.date(2015, 6, 21),\n",
       "  '2015-06-21 03:20:51',\n",
       "  'JohnE (3877338)',\n",
       "  '6',\n",
       "  '',\n",
       "  '10815.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1012',\n",
       "  '30968978',\n",
       "  'Answer',\n",
       "  'Pandas map to DataFrame from dictionary',\n",
       "  \"I doubt this is the most elegant way, but it should do the trick:\\n\\n\\n\\n    df['fbm'] = df['name']\\n\\n    for i in companies:\\n\\n        df.loc[ df.name.str.contains(i), 'fbm' ] = companies[i]\\n\\n\\n\\n                      name                 fbm\\n\\n    0   BULL AXP UN X3 VON    American Express\\n\\n    1   BEAR AXP UN X3 VON    American Express\\n\\n    2  BULL GOOG UN X5 VON              Google\\n\\n    3  BEAR GOOG UN X5 VON              Google\\n\\n    4   BEAR ABC123 X2 CBZ  BEAR ABC123 X2 CBZ\\n\\n\\n\\nOne thing to keep in mind here is that because this is not a dictionary lookup, you could have more than one match.  For example, 'ABC' and 'UN' are both valid tickers and 'BEAR' is or was.  With this method, the last match will be kept and any prior matches discarded.\",\n",
       "  '<python><dictionary><pandas>',\n",
       "  datetime.date(2015, 6, 21),\n",
       "  '2015-06-22 00:47:56',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1245.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1015',\n",
       "  '31000695',\n",
       "  'Answer',\n",
       "  'Check if a list is a rotation of another list that works with duplicates',\n",
       "  \"The following meta-algorithm will solve it.\\n\\n\\n\\n- Build a concatenation of `a`, e.g., `a =  [3,1,2,3,4]` => `aa =  [3,1,2,3,4,3,1,2,3,4]`.\\n\\n\\n\\n- Run any string adaptation of a string-matching algorithm, e.g., [Boyer Moore](https://en.wikipedia.org/wiki/Boyer%E2%80%93Moore_string_search_algorithm) to find `b` in `aa`.\\n\\n\\n\\n--------------\\n\\n\\n\\nOne particularly easy implementation, which I would first try, is to use [Rabin Karp](https://en.wikipedia.org/wiki/Rabin%E2%80%93Karp_algorithm) as the underlying algorithm. In this, you would \\n\\n\\n\\n- calculate the [Rabin Fingerprint](https://en.wikipedia.org/wiki/Rabin_fingerprint) for `b`\\n\\n\\n\\n- calculate the Rabin fingerprint for `aa[: len(b)]`, `aa[1: len(b) + 1]`, ..., and compare the lists only when the fingerprints match\\n\\n\\n\\nNote that\\n\\n\\n\\n- The Rabin fingerprint for a sliding window can be calculated iteratively very efficiently (read about it in the Rabin-Karp link)\\n\\n\\n\\n- If your list is of integers, you actually have a slightly easier time than for strings, as you don't need to think what is the numerical hash value of a letter\\n\\n\\n\\n\\n\\n- \",\n",
       "  '<python><arrays><algorithm><time-complexity>',\n",
       "  datetime.date(2015, 6, 23),\n",
       "  '2015-06-23 11:20:46',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '28',\n",
       "  '',\n",
       "  '4607.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1016',\n",
       "  '30805167',\n",
       "  'Answer',\n",
       "  'How to unzip an iterator?',\n",
       "  'Suppose you have some iterable of pairs:\\n\\n\\n\\n    a = zip(range(10), range(10))\\n\\n\\n\\nIf I\\'m correctly interpreting what you are asking for, you could generate independent iterators for the firsts and seconds using [``itertools.tee``](https://docs.python.org/2/library/itertools.html):\\n\\n\\n\\n     xs, ys = itertools.tee(a)\\n\\n     xs, ys = (x[0] for x in xs), (y[1] for y in ys)\\n\\n\\n\\n**Note** this will keep in memory the \"difference\" between how much you iterate one of them vs. the other.',\n",
       "  '<python><iterator><generator><itertools>',\n",
       "  datetime.date(2015, 6, 12),\n",
       "  '2018-01-26 22:36:20',\n",
       "  'Benjamin R (2469559)',\n",
       "  '6',\n",
       "  '',\n",
       "  '2323.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1018',\n",
       "  '30825623',\n",
       "  'Answer',\n",
       "  \"calling function with dataframe data gives error (cannot convert the series to <class 'float'>)\",\n",
       "  \"@JonD's answer is good, but here's an alternate answer that will be faster if you dataframe has more than a few rows:\\n\\n\\n\\n    from scipy.stats import norm\\n\\n\\n\\n    def BS2(df):\\n\\n        d1 = (np.log(df.S/df.X)+(.001+df.v*df.v/2)*df['T'])/(df.v*np.sqrt(df['T']))\\n\\n        d2 = d1-df.v*np.sqrt(df['T'])\\n\\n        return (df.S*norm.cdf(d1)-df.X*np.exp(-.001*df['T'])*norm.cdf(d2))\\n\\n\\n\\nChanges:\\n\\n\\n\\n 1. Main point is to vectorize the function.  Syntax-wise the main change is to explicitly use numpy versions of `sqrt`, `log`, and `exp`.  Otherwise you don't have to change much because numpy/pandas support basic math operations in an elementwise manner.\\n\\n 2. Replaced user-written CND with `norm.cdf` from scipy.  Much faster b/c built in functions are almost always as fast as possible.\\n\\n 3. This is minor, but I went with shortcut notation on `df.X` and others, but `df['T']` needs to be written out since `df.T` would be interpreted as `df.transpose()`.  I guess this is a good example of why you should avoid the shortcut notation but I'm lazy...\\n\\n\\n\\nBtw, if you want even more speed, the next thing to try would be to do it in numpy rather than pandas.  You could also check if others have already written Black-Scholes functions/libraries (probably, though I don't know anything about it).\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2015, 6, 14),\n",
       "  '2015-06-14 03:18:11',\n",
       "  'JohnE (3877338)',\n",
       "  '0',\n",
       "  '',\n",
       "  '5755.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1021',\n",
       "  '31007243',\n",
       "  'Answer',\n",
       "  'Transforming outliers in Pandas DataFrame using .apply, .applymap, .groupby',\n",
       "  \"Here's a replacement for the outliers part.  It's about 5x faster for your sample data on my computer.\\n\\n\\n\\n    >>> pd.DataFrame( np.where( np.abs(df) > df.mean(), 2, df ), columns=df.columns )\\n\\n\\n\\n        a   b\\n\\n    0 NaN   2\\n\\n    1   2   3\\n\\n    2   3  -4\\n\\n    3   4   5\\n\\n    4   5   6\\n\\n    5   0   2\\n\\n    6  -7   7\\n\\n    7   9   9\\n\\n    8  10 NaN\\n\\n\\n\\nYou could also do it with apply, but it will be slower than the `np.where` approach (but approximately the same speed as what you are currently doing), though much simpler.  That's probably a good example of why you should always avoid `apply` if possible, when you care about speed.\\n\\n\\n\\n    >>> df[ df.apply( lambda x: abs(x) > x.mean() ) ] = 2\\n\\n\\n\\nYou could also do this, which is faster than `apply` but slower than `np.where`:\\n\\n\\n\\n    >>> mask = np.abs(df) > df.mean()\\n\\n    >>> df[mask] = 2\\n\\n\\n\\nOf course, these things don't always scale linearly, so test them on your real data and see how that compares.\\n\\n\\n\\n\",\n",
       "  '<python><numpy><pandas><outliers>',\n",
       "  datetime.date(2015, 6, 23),\n",
       "  '2015-06-23 16:44:36',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1786.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1025',\n",
       "  '31125522',\n",
       "  'Answer',\n",
       "  'Show y_ticklabels in a seaborn pairplot with shared axes',\n",
       "  'Just turn the `yticklabels` back to visible and you are good to go, in the desired subplot:\\n\\n\\n\\n    import seaborn as sns\\n\\n    iris = sns.load_dataset(\"iris\")\\n\\n    x_vars = [\\'sepal_length\\', \\'sepal_width\\', \\'petal_length\\']\\n\\n    y_vars = [\\'petal_width\\']\\n\\n    pp = sns.pairplot(data=iris, x_vars=x_vars, y_vars=y_vars)\\n\\n    _ = plt.setp(pp.axes[0,1].get_yticklabels(), visible=True) #changing the 2nd plot\\n\\n\\n\\n\\n\\n`_ = ...` here is to suppress unwanted print out in interactive environments.\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/RV5a5.png',\n",
       "  '<python><matplotlib><seaborn>',\n",
       "  datetime.date(2015, 6, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1049.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1026',\n",
       "  '31142319',\n",
       "  'Answer',\n",
       "  'Matplotlib density plot in polar coordinates?',\n",
       "  'In this example, `a` is you theta1...thetan, `b` is your r1...rn, `c` is your f(a, b):\\n\\n\\n\\n    #fake data:\\n\\n    a = np.linspace(0,2*np.pi,50)\\n\\n    b = np.linspace(0,1,50)\\n\\n    A, B = np.meshgrid(a, b)\\n\\n    c = np.random.random(A.shape)\\n\\n\\n\\n    #actual plotting\\n\\n    import matplotlib.cm as cm\\n\\n    ax = plt.subplot(111, polar=True)\\n\\n    ax.set_yticklabels([])\\n\\n    ctf = ax.contourf(a, b, c, cmap=cm.jet)\\n\\n    plt.colorbar(ctf)\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nEssentially a filled contour plot in polar axis.\\n\\nYou can specify alternative colormap with the `cmap=...`. `cm.jet` goes from blue to red, with red being the largest value.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/4Bzok.png',\n",
       "  '<python><arrays><matplotlib><plot><density-plot>',\n",
       "  datetime.date(2015, 6, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2311.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1027',\n",
       "  '30546143',\n",
       "  'Answer',\n",
       "  'Load CSV file in DC.JS with D3.csv',\n",
       "  \"There are a couple of mistakes in your code (well, one and a half).\\n\\n\\n\\nTo begin with, the signature for csv calls for a use like\\n\\n\\n\\n    d3.csv(file_name, function(data, error) {...\\n\\n\\n\\nnote that ``data`` and ``error`` are reversed in your code.\\n\\n\\n\\nSecondly, if nothing is displayed, you should first put in your callback function stuff like:\\n\\n\\n\\n    console.log(data);\\n\\n    console.log(error);\\n\\n\\n\\nCheck online for the proper way to see these log outputs in your specific browser. Otherwise, debugging Javascript is impossible.\\n\\n\\n\\n---------------------------------\\n\\n\\n\\nRegarding your final question - d3 does not have an API for synchronous loading of CSV (or any other format, for that matter). Of course, you're not limited to d3 for loading CSV, but there's a reason it's not included - modern Javascript is increasingly moving away from that stuff, in order to make page responsiveness better.\",\n",
       "  '<javascript><csv><d3.js><dc.js>',\n",
       "  datetime.date(2015, 5, 30),\n",
       "  '2015-05-30 13:05:51',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1413.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1031',\n",
       "  '30280377',\n",
       "  'Answer',\n",
       "  'PROLOG: Determining if elements in list are equal if order does not matter',\n",
       "  \"The other answers are all elegant (way above my own Prolog level), but it struck me that the question stated\\n\\n\\n\\n> efficient for the regular uses.\\n\\n\\n\\nThe accepted answer is *O(max(|A| log(|A|), |B|log(|B|))*, **irrespective of whether the lists are equal (up to permutation) or not**. \\n\\n\\n\\nAt the very least, it would pay to check the lengths before bothering to sort, which would decrease the runtime to something linear in the lengths of the lists in the case where they are not of equal length. \\n\\n\\n\\nExpanding this, it is not difficult to modify the solution so that its runtime is effectively linear in the general case where the lists are not equal (up to permutation), using random [digests](http://www.webopedia.com/TERM/M/message_digest.html).\\n\\n\\n\\nSuppose we define \\n\\n\\n\\n    digest(L, D) :- digest(L, 1, D).\\n\\n    digest([], D, D) :- !.\\n\\n    digest([H|T], Acc, D) :-\\n\\n        term_hash(H, TH),\\n\\n        NewAcc is mod(Acc * TH, 1610612741),\\n\\n        digest(T, NewAcc, D).\\n\\n\\n\\nThis is the Prolog version of the mathematical function *Prod_i h(a_i) | p*, where *h* is the hash, and *p* is a prime. It effectively maps each list to a random (in the hashing sense) value in the range *0, ...., p - 1* (in the above, *p* is the large prime 1610612741).\\n\\n\\n\\nWe can now check if two lists have the same digest:\\n\\n\\n\\n    same_digests(A, B) :-\\n\\n        digest(A, DA),\\n\\n        digest(B, DB),\\n\\n        DA =:= DB.\\n\\n\\n\\nIf two lists have different digests, they cannot be equal. If two lists have the same digest, then there is a tiny chance that they are unequal, but this still needs to be checked. For this case I shamelessly stole Paulo Moura's excellent answer.\\n\\n\\n\\nThe final code is this:\\n\\n\\n\\n    equal_elements(A, B) :-\\n\\n        same_digests(A, B),\\n\\n        sort(A, SortedA),\\n\\n        sort(B, SortedB),\\n\\n        SortedA == SortedB.\\n\\n\\n\\n    same_digests(A, B) :-\\n\\n        digest(A, DA),\\n\\n        digest(B, DB),\\n\\n        DA =:= DB.\\n\\n\\n\\n    digest(L, D) :- digest(L, 1, D).\\n\\n    digest([], D, D) :- !.\\n\\n    digest([H|T], Acc, D) :-\\n\\n        term_hash(H, TH),\\n\\n        NewAcc is mod(Acc * TH, 1610612741),\\n\\n        digest(T, NewAcc, D).\\n\\n\",\n",
       "  '<prolog>',\n",
       "  datetime.date(2015, 5, 16),\n",
       "  '2015-05-17 15:55:21',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2580.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1038',\n",
       "  '30834774',\n",
       "  'Answer',\n",
       "  'Count the number of nodes in an AVL tree in a given range',\n",
       "  \"This is a famous problem - [dynamic order statistcs by tree augmentation](https://www.google.co.il/url?sa=t&rct=j&q=&esrc=s&source=web&cd=4&ved=0CDAQFjADahUKEwjZ_Ifgj5DGAhXDWhQKHWTHAEM&url=https%3A%2F%2Fwww.cs.rochester.edu%2F~gildea%2Fcsc282%2Fslides%2FC14-augmenting.pdf&ei=WvF9VdnnGcO1UeSOg5gE&usg=AFQjCNGCtABJKoRinXTYjDMB_sfGVlndzg&bvm=bv.95515949,d.d24). \\n\\n\\n\\nYou basically need to augment your nodes so that when you look at a child pointer, you know how many children are in the child's subtree at time O(1). It's easy to see that this can be done without affecting the complexity. \\n\\n\\n\\nOnce you have that, you can answer any query (between this and that, inclusive/exclusive - all possibilities) by performing two traversals from node to roots. The exact traversals depend on the details (check the functions ``lower_bound`` and ``upper_bound`` in C++ for example).\",\n",
       "  '<c++><algorithm><tree>',\n",
       "  datetime.date(2015, 6, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1362.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1040',\n",
       "  '30885579',\n",
       "  'Answer',\n",
       "  'Fail to filter pandas dataframe by categorical column',\n",
       "  'The [documentation](http://pandas-docs.github.io/pandas-docs-travis/categorical.html#working-with-categories) states:\\n\\n\\n\\n> **Note** New categorical data are NOT automatically ordered. You must explicity pass ordered=True to indicate an ordered Categorical.\\n\\n\\n\\nWhen you first create a category you want to be ordered, just specify this:\\n\\n\\n\\n    In [1]: import pandas as pd\\n\\n\\n\\n    In [3]: s = pd.Series([\"a\",\"b\",\"c\",\"a\"]).astype(\\'category\\', ordered=True)\\n\\n\\n\\n    In [5]: s\\n\\n    Out[5]: \\n\\n    0    a\\n\\n    1    b\\n\\n    2    c\\n\\n    3    a\\n\\n    dtype: category\\n\\n    Categories (3, object): [a < b < c]\\n\\n\\n\\n    In [4]: s > \\'a\\'\\n\\n    Out[4]: \\n\\n    0    False\\n\\n    1     True\\n\\n    2     True\\n\\n    3    False\\n\\n    dtype: bool',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 6, 17),\n",
       "  '2015-06-17 08:10:49',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1414.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1045',\n",
       "  '30288625',\n",
       "  'Answer',\n",
       "  'How do I solve this p\\u200cr\\u200co\\u200cb\\u200cl\\u200ce\\u200cm using Dynamic Programming Top Down approach?',\n",
       "  'There are several problems:\\n\\n\\n\\n**Wrong Search**\\n\\n\\n\\nIn your lines\\n\\n\\n\\n    for(int i=0;i<3;i++)\\n\\n    {\\n\\n        if(n>=a[i])\\n\\n        {\\n\\n            val=1+dp(n-a[i]);\\n\\n        }\\n\\n    }\\n\\n    if(val>m)\\n\\n        m=val;\\n\\n\\n\\nYou should be checking for the maximum of the different ``val``s obtained for the different choices of ``i``.\\n\\n\\n\\n**Wrong Termination**\\n\\n\\n\\nIf the length is not 0 and no ribbon can be cut, you should return something like minus infinity. You currently return ``m`` which is initially -1 (more on this later). This is wrong, and for long ribbons will essentially ensure that you just choose the minimum of a, b, and c.\\n\\n\\n\\n**Use of Globals**\\n\\n\\n\\nSome globals, e.g., ``m`` are initialized once but are modified by the recursion. It\\'s not \"just\" bad programming habits - it\\'s not doing what you want.\\n\\n\\n\\n**No Reuse**\\n\\n\\n\\nBy calling the recursion unconditionally, and not reusing previous calls, your running time is needlessly high.\\n\\n',\n",
       "  '<c++><algorithm><dynamic-programming>',\n",
       "  datetime.date(2015, 5, 17),\n",
       "  '2015-05-17 15:36:58',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1481.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1048',\n",
       "  '30698609',\n",
       "  'Answer',\n",
       "  \"optimal way to find sum(S) of all contiguous sub-array's max difference\",\n",
       "  'Suppose you have a sequence of length *n*, and you wish to calculate the minimum (or maximum) of a sliding window of some fixed size *m < n*. Then (surprisingly), [this can be done in *O(n)* time](http://people.cs.uct.ac.za/~ksmith/2011/sliding-window-minimum.html).\\n\\n\\n\\nSo now for window sizes *m = 1, ..., n*, you need to run the sliding window from left to right; for each slide of the window, you just need to add the max - min of the elements inside the window. By the above, the running time is *Theta(n^2)*. This improves your naive algorithm which is *Theta(n^3)*.',\n",
       "  '<arrays><algorithm><optimization><subset>',\n",
       "  datetime.date(2015, 6, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1135.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1049',\n",
       "  '30741763',\n",
       "  'Answer',\n",
       "  'Pandas get previous dataframe row by date',\n",
       "  \"In pandas, you basically want to avoid loops, as they kill performance.\\n\\n\\n\\nHer's a DataFrame similar to yours (I was lazy about the dates, so they're ints; it's the same idea).\\n\\n\\n\\n    df = pd.DataFrame({\\n\\n        'id': ['abc', 'abc', 'def', 'def', 'def'],\\n\\n        'date': [505, 501, 418, 312, 212]})\\n\\n\\n\\nAnd here's a function that, for each group, appends the previous date:\\n\\n\\n\\n    def prev_dates(g):\\n\\n        g.sort(columns=['date'])\\n\\n        g['prev'] = g.date.shift(-1)\\n\\n        return g\\n\\n\\n\\nSo all that's needed is to connect things:\\n\\n\\n\\n     >> df.groupby(df.id).apply(prev_dates)\\n\\n      \\tdate \\tid \\tprev\\n\\n     0 \\t505 \\tabc \\t501\\n\\n     1 \\t501 \\tabc \\tNaN\\n\\n     2 \\t418 \\tdef \\t312\\n\\n     3 \\t312 \\tdef \\t212\\n\\n     4 \\t212 \\tdef \\tNaN\\n\\n\\n\\n**Edit**\\n\\n\\n\\nAs noted by @julius below, ``sort(columns=`` has since been deprecated, and should be replaced by ``sort_values(by=''.\\n\\n\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2015, 6, 9),\n",
       "  '2016-01-13 21:40:12',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '12',\n",
       "  '',\n",
       "  '2890.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1051',\n",
       "  '31196158',\n",
       "  'Answer',\n",
       "  'What is the correct way of passing parameters to stats.friedmanchisquare based on a DataFrame?',\n",
       "  'You could pass it using the [\"star operator\"](https://stackoverflow.com/questions/2921847/what-does-the-star-operator-mean-in-python), similarly to this:\\n\\n\\n\\n    a = np.array([[1, 2, 3], [2, 3, 4] ,[4, 5, 6]])\\n\\n    friedmanchisquare(*(a[i, :] for i in range(a.shape[0])))',\n",
       "  '<python><numpy><pandas><scipy>',\n",
       "  datetime.date(2015, 7, 2),\n",
       "  '2017-05-23 10:32:48',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1137.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1061',\n",
       "  '30427245',\n",
       "  'Answer',\n",
       "  'How to concat two lists in Prolog',\n",
       "  'I find that it helps me to read Prolog from Right To Left (which I guess is natural from both our native tongues anyway, I guess).\\n\\n\\n\\nSo the first part says:\\n\\n\\n\\n    concatenation([ ], Lista, Lista).\\n\\n\\n\\nor: regardless of anything (the right-hand-side is empty), you can always concatenate an empty list to some list to obtain the same list. \\n\\n\\n\\nNow the second part says, starting from the right,\\n\\n\\n\\n    concatenation(Lista1, Lista2, Lista3)\\n\\n\\n\\nmeaning you can concatenate ``Lista1`` to ``Lista2``, obtaining ``Lista3``.\\n\\n\\n\\nand then the entire second part says\\n\\n\\n\\n    concatenation([ Elem | Lista1], Lista2, [Elem | Lista3]):-\\n\\n        concatenation(Lista1, Lista2, Lista3).\\n\\n\\n\\nmeaning: if you can concatenate ``Lista1`` to ``Lista2``, obtaining ``Lista3``, then you can concatenate ``Lista1`` prepended by ``Elem`` to ``Lista2``, obtaining ``Lista3`` prepended by ``Elem``.',\n",
       "  '<prolog>',\n",
       "  datetime.date(2015, 5, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1877.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1065',\n",
       "  '30769127',\n",
       "  'Answer',\n",
       "  'Finding longest common subsequence in O(NlogN) time',\n",
       "  \"The [dynamic programming approach](http://en.wikipedia.org/wiki/Longest_common_subsequence_problem), which is *O(n<sup>2</sup>)* is completely general. For *certain* other cases, there are lower-complexity algorithms:\\n\\n\\n\\n- For a fixed alphabet size (which doesn't grow with *n*), there's the [Method of Four Russians](http://en.wikipedia.org/wiki/Method_of_Four_Russians) which brings the time down to *O(n<sup>2</sup>/log n)* (see [here](http://en.wikipedia.org/wiki/Longest_common_subsequence_problem#Further_optimized_algorithms)).\\n\\n\\n\\n- See [here](https://www.ics.uci.edu/~eppstein/pubs/p-sparsedp.html) another further optimized case.\",\n",
       "  '<algorithm><dynamic-programming><lcs>',\n",
       "  datetime.date(2015, 6, 10),\n",
       "  '2017-08-03 16:31:08',\n",
       "  'vallismortis (2074605)',\n",
       "  '3',\n",
       "  '',\n",
       "  '4442.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1068',\n",
       "  '31413896',\n",
       "  'Answer',\n",
       "  'Pandas: Filter dataframe for values that are too frequent or too rare',\n",
       "  \"I would go with one of the following:\\n\\n\\n\\n**Option A**\\n\\n\\n\\n    m = 0.03 * len(df)\\n\\n    df[np.all(\\n\\n        df.apply(\\n\\n            lambda c: c.isin(c.value_counts()[c.value_counts() > m].index).as_matrix()), \\n\\n        axis=1)]\\n\\n\\n\\n----------------------\\n\\n\\n\\nExplanation:\\n\\n\\n\\n- `m = 0.03 * len(df)` is the threshold (it's nice to take the constant out of the complicated expression)\\n\\n\\n\\n- `df[np.all(..., axis=1)]` retains the rows where some condition was obtained across all columns.\\n\\n\\n\\n- `df.apply(...).as_matrix` applies a function to all columns, and makes a matrix of the results.\\n\\n\\n\\n- `c.isin(...)` checks, for each column item, whether it is in some set.\\n\\n\\n\\n- `c.value_counts()[c.value_counts() > m].index` is the set of all values in a column whose count is above `m`.\\n\\n\\n\\n**Option B**\\n\\n\\n\\n    m = 0.03 * len(df)\\n\\n    for c in df.columns:\\n\\n        df = df[df[c].isin(df[c].value_counts()[df[c].value_counts() > m].index)]\\n\\n\\n\\nThe explanation is similar to the one above.\\n\\n\\n\\n-------------------------------\\n\\n\\n\\nTradeoffs:\\n\\n\\n\\n- Personally, I find B more readable.\\n\\n\\n\\n- B creates a new DataFrame for each filtering of a column; for large DataFrames, it's probably more expensive.\",\n",
       "  '<python><pandas><filtering><selection>',\n",
       "  datetime.date(2015, 7, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '2966.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1069',\n",
       "  '30990017',\n",
       "  'Answer',\n",
       "  'Reindex a dataframe with duplicate index values',\n",
       "  \"It's pretty easy to replicate your error with this sample data:\\n\\n\\n\\n    In [92]: data = pd.DataFrame( [33,55,88,22], columns=['x'], index=[0,0,1,2] )\\n\\n\\n\\n    In [93]: data.index.is_unique\\n\\n    Out[93]: False\\n\\n\\n\\n    In [94:] data.reindex(np.arange(len(data)))  # same error message\\n\\n\\n\\nThe problem is because `reindex` requires unique index values.  In this case, you don't want to preserve the old index values, you merely want new index values that are unique.  The easiest way to do that is:\\n\\n\\n\\n    In [95]: data.reset_index(drop=True)\\n\\n    Out[72]: \\n\\n        x\\n\\n    0  33\\n\\n    1  55\\n\\n    2  88\\n\\n    3  22\\n\\n\\n\\nNote that you can leave off `drop=True` if you want to retain the old index values.\\n\\n\",\n",
       "  '<python><pandas><reindex>',\n",
       "  datetime.date(2015, 6, 22),\n",
       "  '2017-10-04 16:23:05',\n",
       "  'JohnE (3877338)',\n",
       "  '6',\n",
       "  '',\n",
       "  '11349.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1074',\n",
       "  '31571972',\n",
       "  'Answer',\n",
       "  'pandas - histogram from two columns?',\n",
       "  \"You can always drop to the lower-level [`matplotlib.hist`](http://matplotlib.org/examples/pylab_examples/histogram_demo_extended.html):\\n\\n\\n\\n    from matplotlib.pyplot import hist\\n\\n    df = pd.DataFrame({\\n\\n        '_id': np.random.randn(100),\\n\\n        'total': 100 * np.random.rand()\\n\\n    })\\n\\n    hist(df._id, weights=df.total)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/jouSi.png\",\n",
       "  '<python><pandas><plot><histogram>',\n",
       "  datetime.date(2015, 7, 22),\n",
       "  '2015-07-22 19:19:53',\n",
       "  'Challensois (5123306)',\n",
       "  '5',\n",
       "  '',\n",
       "  '10001.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1076',\n",
       "  '29831447',\n",
       "  'Answer',\n",
       "  'Pandas Categorical data type not behaving as expected',\n",
       "  \"Here's a short example with an ordered categorical variable and (to me) a surprising result from using ```rank()``` (as a sort of distance measure):\\n\\n\\n\\n    df = pd.DataFrame({ 'code':['one','two','three','one'], 'num':[1,2,3,1] }) \\n\\n    df.code = df.code.astype('category', categories=['one','two','three'], ordered=True)\\n\\n     \\n\\n        code  num\\n\\n    0    one    1\\n\\n    1    two    2\\n\\n    2  three    3\\n\\n    3    one    1\\n\\n    \\n\\n    df.sort('code')\\n\\n    \\n\\n        code  num\\n\\n    0    one    1\\n\\n    3    one    1\\n\\n    1    two    2\\n\\n    2  three    3\\n\\n    \\n\\nSo ```sort()``` works as expected, in the order specified.  But ```rank()``` doesn't do what I would have guessed, it ranks lexicographically and ignores the ordering of the categorical variable.  \\n\\n\\n\\n     df.sort('code').rank()\\n\\n\\n\\n       code  num\\n\\n    0   1.5  1.5\\n\\n    3   1.5  1.5\\n\\n    1   4.0  3.0\\n\\n    2   3.0  4.0\\n\\n\\n\\n\\n\\nAll of which is perhaps a longer way of asking:  Maybe you just want an integer type?  I mean, you could make up some kind of distance function here post-sorting, but ultimately that's going to be a lot more work than what you could do with a standard int or float (and possibly problematic if you look at how ```rank()``` handles an ordered categorical.\\n\\n\\n\\n**edit to add**:  Part of the above may not work for pandas 15.2 but I believe you can still do this to specify order:\\n\\n\\n\\n    df['code'].cat.categories = ['one','two','three']\\n\\n\\n\\nWhat will happen in 15.2 by default (as I understand it) is that ordered will be True by default (but False in version 16.0), but order will be lexicographical rather than as specified in the constructor.  I'm not sure though, and am working in 16.0 so you'll have to just observe how your version behaves.  Remember that Categorical is still fairly new...\",\n",
       "  '<python><pandas><categorical-data><ordinal>',\n",
       "  datetime.date(2015, 4, 23),\n",
       "  '2015-04-24 11:54:33',\n",
       "  'JohnE (3877338)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2487.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1086',\n",
       "  '31096574',\n",
       "  'Answer',\n",
       "  'Pandas dataframe: calculate sum of values for each hour?',\n",
       "  \"You can use a [``groupby``+``sum`` combination](http://pandas.pydata.org/pandas-docs/stable/groupby.html):\\n\\n\\n\\n    df.values.groupby(df.Hour).sum()\\n\\n\\n\\n------------------\\n\\n\\n\\nIn general, you might want to avoid loops with your `pandas` code - you're losing out on the speed.\",\n",
       "  '<sorting><pandas><sum><line>',\n",
       "  datetime.date(2015, 6, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1583.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1087',\n",
       "  '31117902',\n",
       "  'Answer',\n",
       "  'Exporting ints with missing values to csv in Pandas',\n",
       "  'I\\'m expanding the sample data here to hopefully make sure this is handling the situations you are dealing with:\\n\\n\\n\\n    df = pd.DataFrame([[1.1,2,9.9,44,1.0],\\n\\n                       [3.3,np.nan,4.4,22,3.0],\\n\\n                       [5.5,8,np.nan,66,4.0]],\\n\\n                      columns=list(\\'abcde\\'),\\n\\n                      index=[\"i_1\",\"i_2\",\"i_3\"])\\n\\n\\n\\n           a   b    c   d  e\\n\\n    i_1  1.1   2  9.9  44  1\\n\\n    i_2  3.3 NaN  4.4  22  3\\n\\n    i_3  5.5   8  NaN  66  4\\n\\n\\n\\n    df.dtypes\\n\\n\\n\\n    a    float64\\n\\n    b    float64\\n\\n    c    float64\\n\\n    d      int64\\n\\n    e    float64\\n\\n\\n\\nI think if you want a general solution, it\\'s going to have to be explicitly coded due to pandas not allowing NaNs in int columns.  What I do below here is check for integers *values* (since we can\\'t really check the type as they will have been recast to float if they contain NaNs), and if it\\'s an integer value then convert to a string format and also convert `\\'NAN\\'` to `\\'\\'` (empty).  Of course, this is not how you want to store the integers except as a final step before outputting. \\n\\n\\n\\n    for col in df.columns:\\n\\n        if any( df[col].isnull() ):\\n\\n            tmp = df[col][ df[col].notnull() ]\\n\\n            if all( tmp.astype(int).astype(float) == tmp.astype(float) ):\\n\\n                df[col] = df[col].map(\\'{:.0F}\\'.format).replace(\\'NAN\\',\\'\\')\\n\\n\\n\\n    df.to_csv(\\'x.csv\\')\\n\\n    \\n\\nHere\\'s the output file and also what it looks like if you read it back into pandas although the purpose of this is presumably to read it into other numerical packages.\\n\\n\\n\\n    %more x.csv\\n\\n\\n\\n    ,a,b,c,d,e\\n\\n    i_1,1.1,2,9.9,44,1.0\\n\\n    i_2,3.3,,4.4,22,3.0\\n\\n    i_3,5.5,8,,66,4.0\\n\\n\\n\\n    pd.read_csv(\\'x.csv\\')\\n\\n    \\n\\n      Unnamed: 0    a   b    c   d  e\\n\\n    0        i_1  1.1   2  9.9  44  1\\n\\n    1        i_2  3.3 NaN  4.4  22  3\\n\\n    2        i_3  5.5   8  NaN  66  4\\n\\n    \\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "  '<csv><pandas><int><nan><missing-data>',\n",
       "  datetime.date(2015, 6, 29),\n",
       "  '2015-06-29 18:10:27',\n",
       "  'JohnE (3877338)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2056.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1088',\n",
       "  '31126932',\n",
       "  'Answer',\n",
       "  'The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()',\n",
       "  \"You're focusing on the conjunction of the clauses, but it's the clauses themselves. You probably want here something like:\\n\\n\\n\\n    if numpy.all(0 <= x) and numpy.all(x < d):\\n\\n        ...\\n\\n\\n\\nSee the docs for [`numpy.all`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.all.html).\",\n",
       "  '<python><arrays><numpy><error-code>',\n",
       "  datetime.date(2015, 6, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '23696.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1091',\n",
       "  '30774000',\n",
       "  'Answer',\n",
       "  'Union of multiple sets in python',\n",
       "  \"Using the [unpacking operator ``*``](https://docs.python.org/2/tutorial/controlflow.html#unpacking-argument-lists):\\n\\n\\n\\n    >> list(set.union(*map(set, a)))\\n\\n    [1, '44', '30', '42', '43', '40', '41', '34']\\n\\n\\n\\n(Thanks Raymond Hettinger for the comment!)\\n\\n\\n\\n(Note that \\n\\n\\n\\n    set.union(*tup)\\n\\n\\n\\nwill unpack to \\n\\n\\n\\n    set.union(tup[0], tup[1], ... tup[n - 1])\\n\\n\\n\\n)\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><list><python-3.x><set-union>',\n",
       "  datetime.date(2015, 6, 11),\n",
       "  '2015-06-11 07:50:21',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '13',\n",
       "  '',\n",
       "  '9751.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1094',\n",
       "  '30471284',\n",
       "  'Answer',\n",
       "  'Get top biggest values from each column of the pandas.DataFrame',\n",
       "  'The other solutions (at the time of writing this), sort the DataFrame with super-linear complexity *per column*, but it can actually be done with linear time per column.\\n\\n\\n\\nfirst, [``numpy.partition``](http://docs.scipy.org/doc/numpy-dev/reference/generated/numpy.partition.html) partitions the *k* smallest elements at the *k* first positions (unsorted otherwise). To get the *k* largest elements, we can use\\n\\n\\n\\n    import numpy as np\\n\\n\\n\\n    -np.partition(-v, k)[: k]\\n\\n\\n\\nCombining this with dictionary comprehension, we can use:\\n\\n\\n\\n    >>> pd.DataFrame({c: -np.partition(-data[c], 3)[: 3] for c in data.columns})\\n\\n        first \\tsecond \\tthird\\n\\n    0 \\t89 \\t76 \\t98\\n\\n    1 \\t56 \\t45 \\t87\\n\\n    2 \\t40 \\t45 \\t67\\n\\n\\n\\n\\n\\n',\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2015, 5, 27),\n",
       "  '2017-05-22 20:49:20',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '6721.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1096',\n",
       "  '30510907',\n",
       "  'Answer',\n",
       "  'What is the difference between &vector[0] and vector.begin()?',\n",
       "  'Formally, one produces an iterator, and the other a pointer, but I think the major difference is that ``vec[0]`` will do bad stuff if the vector is empty, while ``vec.begin()`` will not. ',\n",
       "  '<c++><vector>',\n",
       "  datetime.date(2015, 5, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2163.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1097',\n",
       "  '30520277',\n",
       "  'Answer',\n",
       "  'pandas DataFrame set value on boolean mask',\n",
       "  \"I'm not 100% sure but I suspect the error message relates to the fact that there is not identical treatment of missing data across different dtypes.  Only float has NaN, but integers can be automatically converted to floats so it's not a problem there.  But it appears mixing number dtypes and object dtypes does not work so easily...\\n\\n\\n\\nRegardless of that, you could get around it pretty easily with `np.where`:\\n\\n\\n\\n    df[:] = np.where( mask, 30, df ) \\n\\n\\n\\n        A   B\\n\\n    0  30  30\\n\\n    1   2   b\\n\\n    2  30   f\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 29),\n",
       "  '2015-05-29 03:10:44',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '10437.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1099',\n",
       "  '30058091',\n",
       "  'Answer',\n",
       "  'Interpolate and fill pandas dataframe with datetime index',\n",
       "  \"Just as an add on to @JohnGalt's answer, you could also use `resample` which is slightly more convenient than `reindex` here:\\n\\n\\n\\n    df.resample('D').interpolate('cubic')\\n\\n    \\n\\n                      value\\n\\n    date                   \\n\\n    2010-05-31   669.000000\\n\\n    2010-06-01   830.400272\\n\\n    2010-06-02   983.988431\\n\\n    2010-06-03  1129.919466\\n\\n    2010-06-04  1268.348368\\n\\n    2010-06-05  1399.430127\\n\\n    2010-06-06  1523.319734\\n\\n    \\n\\n    ...\\n\\n    \\n\\n    2010-06-25  2716.850752\\n\\n    2010-06-26  2729.445324\\n\\n    2010-06-27  2738.102544\\n\\n    2010-06-28  2742.977403\\n\\n    2010-06-29  2744.224892\\n\\n    2010-06-30  2742.000000\\n\\n    2010-07-01  2736.454249\\n\\n    2010-07-02  2727.725284\\n\\n    2010-07-03  2715.947277\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 5),\n",
       "  '2015-05-05 16:33:25',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '6933.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1100',\n",
       "  '30080062',\n",
       "  'Answer',\n",
       "  'Find most repeated phrase on huge text',\n",
       "  \"I'd suggest combining ideas from two fields, here: [Streaming Algorithms](http://en.wikipedia.org/wiki/Streaming_algorithm), and the [Apriori Algorithm From Market-Basket Analysis](http://en.wikipedia.org/wiki/Apriori_algorithm).\\n\\n\\n\\n1. Let's start with the problem of finding the *k* most frequent single words without loading the entire corpus into memory. A very simple algorithm, **Sampling** (see [Finding Frequent Items in Data Streams](http://www.mathcs.emory.edu/~cheung/papers/StreamDB/Frequency-count/FrequentStream.pdf)]), can do so very easily. Moreover, it is very amenable to parallel implementation (described below). There is a plethora of work on top-k queries, including some on distributed versions (see, e.g., [Efficient Top-K Query Calculation in Distributed Networks](http://crypto.stanford.edu/~cao/topk.pdf)).\\n\\n\\n\\n2. Now to the problem of *k* most frequent phrases (of possibly multiple phrases). Clearly, the most frequent phrases of length *l + 1* must contain the most frequent phrases of length *l* as a prefix, as appending a word to a phrase cannot increase its popularity. Hence, once you have the *k* most frequent single words, you can scan the corpus for only them (which is faster) to build the most frequent phrases of length 2. Using this, you can build the most frequent phrases of length 3, and so on. The stopping condition is when a phrase of length *l + 1* does not evict any phrase of length *l*.\\n\\n\\n\\n---------\\n\\n\\n\\n**A Short Description of The Sampling Algorithm**\\n\\n\\n\\nThis is a very simple algorithm which will, with high probability, find the top *k* items out of those having frequency at least *f*. It operates in two stages: the first finds candidate elements, and the second counts them.\\n\\n\\n\\nIn the first stage, randomly select *~ log(n) / f* words from the corpus (note that this is much less than *n*). With high probability, all your desired words appear in the set of these words.\\n\\n\\n\\nIn the second stage, maintain a dictionary of the counts of these candidate elements; scan the corpus, and count the occurrences.\\n\\n\\n\\nOutput the top *k* of the items resulting from the second stage.\\n\\n\\n\\nNote that the second stage is very amenable to parallel implementation. If you partition the text into different segments, and count the occurrences in each segment, you can easily combine the dictionaries at the end.\\n\\n\",\n",
       "  '<search><text><full-text-search><bigdata>',\n",
       "  datetime.date(2015, 5, 6),\n",
       "  '2015-05-06 18:30:46',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1770.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1104',\n",
       "  '30581930',\n",
       "  'Answer',\n",
       "  'Prolog returning true/false instead of variable',\n",
       "  \"It's possible there are other problems, but \\n\\n\\n\\n1. \\n\\n\\n\\n        reverse([], ReversedList).\\n\\n\\n\\n    is almost surely not what you want here. The reverse of an empty list is an empty list, translates to\\n\\n\\n\\n        reverse([], []).\\n\\n\\n\\n2. Additionally,\\n\\n\\n\\n        reverse([A,B], ReversedList)\\n\\n\\n\\n     is also probably not what you want. It is *not* a list with head A and tail B, but rather a 2-element list.\\n\\n    \",\n",
       "  '<prolog>',\n",
       "  datetime.date(2015, 6, 1),\n",
       "  '2015-06-01 19:49:15',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2069.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1105',\n",
       "  '31211400',\n",
       "  'Answer',\n",
       "  'No matching function for call to Class Constructor',\n",
       "  \"This is because you are not initializing the member in the initialization list, but rather assigning to it in the body. Consequently, it is first constructing it using the default ctor, and hence your compiler's complaint. \\n\\n\\n\\nConsider changing things to:\\n\\n\\n\\n    // Circle.cpp\\n\\n    Circle::Circle(const Point& center, double radius) :\\n\\n        center_pt(center),\\n\\n        radius_size(radius)\\n\\n    {\\n\\n        \\n\\n    }\",\n",
       "  '<c++><class><constructor>',\n",
       "  datetime.date(2015, 7, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '14539.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1108',\n",
       "  '31358325',\n",
       "  'Answer',\n",
       "  'How to plot kernel density plot of dates in Pandas?',\n",
       "  \"I imagine there is some better and automatic way to do this, but if not then this ought to be a decent workaround.  First, let's set up some sample data:\\n\\n\\n\\n    np.random.seed(479)\\n\\n    start_date = '2011-1-1'\\n\\n    df = pd.DataFrame({ 'date':np.random.choice( \\n\\n                        pd.date_range(start_date, periods=365*5, freq='D'), 50) })\\n\\n\\n\\n    df['rel'] = df['date'] - pd.to_datetime(start_date)\\n\\n    df.rel = df.rel.astype('timedelta64[D]')\\n\\n\\n\\n            date   rel\\n\\n    0 2014-06-06  1252\\n\\n    1 2011-10-26   298\\n\\n    2 2013-08-24   966\\n\\n    3 2014-09-25  1363\\n\\n    4 2011-12-23   356\\n\\n\\n\\nAs you can see, 'rel' is just the number of days since the starting day.  It's essentially an integer, so all you really need to do is normalize it with respect to the starting date.\\n\\n\\n\\n    df['year_as_float'] = pd.to_datetime(start_date).year + df.rel / 365.\\n\\n\\n\\n            date   rel  year_as_float\\n\\n    0 2014-06-06  1252    2014.430137\\n\\n    1 2011-10-26   298    2011.816438\\n\\n    2 2013-08-24   966    2013.646575\\n\\n    3 2014-09-25  1363    2014.734247\\n\\n    4 2011-12-23   356    2011.975342\\n\\n\\n\\nYou'd need to adjust that slightly for a date not starting on Jan 1.  That's also ignoring any leap years which really isn't a practical issue if you're just producing a KDE plot over 5 years, but it could matter depending on what else you might want to do.\\n\\n\\n\\nHere's the plot\\n\\n\\n\\n    df['year_as_float']d.plot(kind='kde')\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/rMidV.png\",\n",
       "  '<python><pandas><matplotlib><time-series><kernel-density>',\n",
       "  datetime.date(2015, 7, 11),\n",
       "  '2015-07-11 14:56:44',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '4248.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1113',\n",
       "  '31999950',\n",
       "  'Answer',\n",
       "  'How to change color bar to align with main plot in Matplotlib?',\n",
       "  \"`imshow` enforces a 1:1 aspect (by default, but you can change it with `aspect` parameter), which makes things a little trickier.  To always get consistent result, I might suggest manually specify the size of axes:\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib\\n\\n    import matplotlib.pyplot as plt\\n\\n    %matplotlib inline\\n\\n\\n\\n    def plot_matrix(mat, figsize, title='example', cmap=plt.cm.Blues):\\n\\n        f = plt.figure(figsize=figsize)\\n\\n        ax = plt.axes([0, 0.05, 0.9, 0.9 ]) #left, bottom, width, height\\n\\n        #note that we are forcing width:height=1:1 here, \\n\\n        #as 0.9*8 : 0.9*8 = 1:1, the figure size is (8,8)\\n\\n        #if the figure size changes, the width:height ratio here also need to be changed\\n\\n        im = ax.imshow(mat, interpolation='nearest', cmap=cmap)\\n\\n        ax.grid(False)\\n\\n        ax.set_title(title)\\n\\n        cax = plt.axes([0.95, 0.05, 0.05,0.9 ])\\n\\n        plt.colorbar(mappable=im, cax=cax)\\n\\n        return ax, cax\\n\\n\\n\\n    data = np.random.random((20, 20))\\n\\n    ax, cax = plot_matrix(data, (8,8)) \\n\\n\\n\\nNow you have the axis where the colorbar is plotted in, `cax`. You can do a lot of thing with that, say, rotate the labels, using `plt.setp(cax.get_yticklabels(), rotation=45)`\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/IbLqy.png\",\n",
       "  '<python><numpy><pandas><matplotlib><plot>',\n",
       "  datetime.date(2015, 8, 13),\n",
       "  '2015-08-14 15:21:17',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1488.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1115',\n",
       "  '32168947',\n",
       "  'Answer',\n",
       "  'How to create a pandas DatetimeIndex with year as frequency?',\n",
       "  \"With all those hacks, there is a clear way:\\n\\n\\n\\n    pd.date_range(start=datetime.datetime.now(),periods=5,freq='A')\\n\\n\\n\\n`A` :  Annually.\\n\\n\\n\\n`365D`? Really? What about [leap years](https://en.wikipedia.org/wiki/Leap_year)?\",\n",
       "  '<python><python-3.x><pandas><date-range>',\n",
       "  datetime.date(2015, 8, 23),\n",
       "  '2015-08-23 16:46:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '16426.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1121',\n",
       "  '31362478',\n",
       "  'Answer',\n",
       "  'Why am I getting a negative information gain?',\n",
       "  \"To begin with, I'm assuming your *S* variable is *EnjoySport*. (I think you could phrase the text more clearly, BTW.)\\n\\n\\n\\nSo the entropy of *S* is 0.8113, but that's the last part with which I agree. \\n\\n\\n\\nThe entropy of *S* given *Normal* is 0, as it is deterministic.\\n\\n\\n\\nThe entropy of *S* given *High* is 0.91829583405448945, but you need to multiply that by 0.75, because that is the probability of *Normal*. So that gives you 0.68872187554086706.\\n\\n\\n\\nThe difference is non-negative, as expected.\\n\\n\\n\\n--------------------\\n\\n\\n\\nNote that the Information gain is the [expected difference in Entropy](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees#Formal_definition), and the expectation needs to take into account the probability of the conditioned event.\",\n",
       "  '<machine-learning><decision-tree><entropy>',\n",
       "  datetime.date(2015, 7, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2793.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1122',\n",
       "  '31362798',\n",
       "  'Answer',\n",
       "  'What is the identity of \"ndim, shape, size, ..etc\" of ndarray in numpy',\n",
       "  \"Regarding your first question, Python has syntactic sugar for [*properties*](https://docs.python.org/2/library/functions.html#property), including fine-grained control of getting, setting, deleting them, as well as restricting any of the above. \\n\\n\\n\\nSo, for example, if you have \\n\\n\\n\\n    class Foo(object):\\n\\n        @property\\n\\n        def shmip(self):\\n\\n            return 3\\n\\n\\n\\nthen you can write `Foo().shmip` to obtain `3`, but, if that is the class definition, you've disabled setting `Foo().shmip = 4`. \\n\\n\\n\\nIn other words, those are read-only properties.\",\n",
       "  '<python><numpy><multidimensional-array>',\n",
       "  datetime.date(2015, 7, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '2363.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1128',\n",
       "  '30916191',\n",
       "  'Answer',\n",
       "  'Import SAS data file into python data frame',\n",
       "  \"When you have the option to download a SAS dataset you will often also have the option to download a Stata dataset (this is indeed the case for PSID btw).  In that case, the easiest way will likely be to import with `read_stata` (this might change in the future, but I believe is a very accurate statement as of today).\\n\\n\\n\\nLess convenient, but almost always an option, is to download a text file (usually referred to as text, ascii, or csv).  Those tend to come in two flavors:  delimited (with comma or tab), or space separated (columnar or tabulated).  If the file is comma or tab delimited, use `read_csv` and set the delimiter as appropriate.  If it's space delimited or tabular, you might have good luck with `read_csv`, or you might be better off with `read_fwf` or `read_table`.  Depends a bit on the variable types and formatting.\\n\\n\\n\\nFrom what I have read, `sas7bdat` mentioned by @hd1 seems to work well but is not part of pandas yet.  For that reason, I tend to default to `read_stata` or `read_csv` but hopefully `sas7bdat` also works well and perhaps will be brought into pandas in the future.  Also, I'm wondering about the speed of `sas7bdat`.  `read_csv` has been pretty fast for a long time and `read_stata` is very fast in the latest versions (since 15.0, I believe).  I'm not sure about the speed of `sas7bdat`?\",\n",
       "  '<python><pandas><sas>',\n",
       "  datetime.date(2015, 6, 18),\n",
       "  '2015-06-18 13:25:38',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '9997.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1129',\n",
       "  '32699052',\n",
       "  'Answer',\n",
       "  'groupby shifting in pandas',\n",
       "  \"    df_prev = df.copy()\\n\\n\\n\\n    df[     'prev_year'] = df[     'year'] - 1    \\n\\n    df_prev['prev_year'] = df_prev['year']\\n\\n    df_prev[     'year'] = df_prev['year'] + 1\\n\\n    \\n\\n    df2 = df.merge( df_prev, how='outer', on=['year','prev_year','artist','genre'], \\n\\n                    suffixes=['','_prev'] )\\n\\n    \\n\\n    df2.sort(['artist','genre','year']).fillna(0)\\n\\n\\n\\n        year artist     genre  genre_sales  prev_year  genre_sales_prev\\n\\n    0   1999      A  Pop/Rock           10       1998                 0\\n\\n    3   2000      A  Pop/Rock           11       1999                10\\n\\n    9   2001      A  Pop/Rock            0       2000                11\\n\\n    1   1999      B   Hip/Hop           15       1998                 0\\n\\n    4   2000      B   Hip/Hop           14       1999                15\\n\\n    6   2001      B   Hip/Hop           18       2000                14\\n\\n    11  2002      B   Hip/Hop            0       2001                18\\n\\n    2   1999      C   Country            8       1998                 0\\n\\n    8   2000      C   Country            0       1999                 8\\n\\n    7   2001      C   Country           10       2000                 0\\n\\n    12  2002      C   Country            0       2001                10\\n\\n    5   2000      D      Jazz            1       1999                 0\\n\\n    10  2001      D      Jazz            0       2000                 1\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 9, 21),\n",
       "  '2015-09-21 18:10:30',\n",
       "  'JohnE (3877338)',\n",
       "  '-1',\n",
       "  '',\n",
       "  '1582.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1130',\n",
       "  '33075573',\n",
       "  'Answer',\n",
       "  'creating complement of DNA sequence and reversing it C++',\n",
       "  'Standard modern C++ makes this low-level, pointer-oriented programming, unnecessary (in fact, you\\'re effectively writing C).\\n\\n\\n\\nOnce you have a function, say ``complement``, which transforms a nucleotide to its complement, you just need to apply some standard library function like [``transform``](http://en.cppreference.com/w/cpp/algorithm/transform). \\n\\n\\n\\nHere is a rewrite of your program in C++11:\\n\\n \\n\\n\\n\\n    #include <string>\\n\\n    #include <iostream>                                                                                                                                                                                          \\n\\n    #include <algorithm>\\n\\n    #include <cassert>\\n\\n\\n\\n\\n\\n    using namespace std;\\n\\n\\n\\n\\n\\n    char complement(char n)\\n\\n    {   \\n\\n        switch(n)\\n\\n        {   \\n\\n        case \\'A\\':\\n\\n            return \\'T\\';\\n\\n        case \\'T\\':\\n\\n            return \\'A\\';\\n\\n        case \\'G\\':\\n\\n            return \\'C\\';\\n\\n        case \\'C\\':\\n\\n            return \\'G\\';\\n\\n        }   \\n\\n        assert(false);\\n\\n        return \\' \\';\\n\\n    }   \\n\\n\\n\\n\\n\\n    int main() \\n\\n    {   \\n\\n        string nucs = \"ACAATTGGA\";\\n\\n        transform(\\n\\n            begin(nucs),\\n\\n            end(nucs),\\n\\n            begin(nucs),\\n\\n            complement);\\n\\n        cout << nucs << endl;\\n\\n    }   \\n\\n   ',\n",
       "  '<c++><arrays><pointers><dna-sequence>',\n",
       "  datetime.date(2015, 10, 12),\n",
       "  '2015-10-12 08:38:29',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2768.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1139',\n",
       "  '31480335',\n",
       "  'Answer',\n",
       "  'Excluding data from a pandas dataframe based on percentiles',\n",
       "  \"Here's a smaller dataframe based on just your first 5 rows.\\n\\n\\n\\n    df\\n\\n    \\n\\n             v1          v2          v3\\n\\n    0  0.012200    0.012000    0.000002\\n\\n    1  0.012244    0.014200    0.000002\\n\\n    2  0.010400    0.010100    0.000001\\n\\n    3  0.013658    0.018562    5.011580\\n\\n    4  0.011322    0.022145    0.078593\\n\\n    \\n\\nAnd here's mask that selects only values between the 25th and 75th percentiles.  Note that the syntax for this is somewhat precise so be careful with the parentheses and such.\\n\\n\\n\\n     ( df > df.quantile(.25) ) & ( df < df.quantile(.75) )\\n\\n    \\n\\n          v1     v2     v3\\n\\n    0   True   True   True\\n\\n    1   True   True   True\\n\\n    2  False  False  False\\n\\n    3   True  False  False\\n\\n    4  False  False   True\\n\\n\\n\\nThis is column-based, btw.  I just glanced quickly at your code and couldn't easily tell if the percentile measures were intended per-column of for the combination of the 3 columns.  For the whole dataframe you can do:\\n\\n\\n\\n    ( df > df.stack().quantile(.25) ) & ( df < df.stack().quantile(.75) )\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/groupby.html#transformation\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 7, 17),\n",
       "  '2017-01-30 20:45:16',\n",
       "  'JohnE (3877338)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2166.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1143',\n",
       "  '33244153',\n",
       "  'Answer',\n",
       "  'Reading a specific number of lines of a .csv in python',\n",
       "  \"You wrote\\n\\n\\n\\n> I am an R programmer and so to me, the way I would get around this is to read in the file excluding the first row and last row but I am unsure of how to do this in python\\n\\n\\n\\nThis can be done with [`readlines`](https://docs.python.org/2/tutorial/inputoutput.html) and [list slicing](https://docs.python.org/2.3/whatsnew/section-slices.html) like so:\\n\\n\\n\\n    open('foo.csv').readlines()[1: -1]\\n\\n\\n\\nFurthermore, note that [`csv.reader`](https://docs.python.org/2/library/csv.html) takes both a file object and a list:\\n\\n\\n\\n> csvfile can be any object which supports the iterator protocol and returns a string each time its next() method is called — file objects and list objects are both suitable.\\n\\n\\n\\nSo you can just use:\\n\\n\\n\\n    for l in csv.reader(open('foo.csv').readlines()[1: -1]):\\n\\n        ...\\n\\n\",\n",
       "  '<python><csv>',\n",
       "  datetime.date(2015, 10, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1251.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1144',\n",
       "  '31287881',\n",
       "  'Answer',\n",
       "  'Logarithmic returns in pandas dataframe',\n",
       "  \"The results might *seem* similar, but that is just because of the [Taylor expansion for the logarithm](https://en.wikipedia.org/wiki/Taylor_series). Since *log(1 + x) ~ x*, the results can be similar. \\n\\n\\n\\nHowever, \\n\\n\\n\\n> I am using the following code to get logarithmic returns, but it gives the exact same values as the pct.change() function.\\n\\n\\n\\nis not quite correct.\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    df = pd.DataFrame({'p': range(10)})\\n\\n\\n\\n    df['pct_change'] = df.pct_change()\\n\\n    df['log_stuff'] = \\\\\\n\\n        np.log(df['p'].astype('float64')/df['p'].astype('float64').shift(1))\\n\\n    df[['pct_change', 'log_stuff']].plot();\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/qobGR.png\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 7, 8),\n",
       "  '2015-07-08 09:00:30',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '6',\n",
       "  '',\n",
       "  '76549.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1146',\n",
       "  '31324037',\n",
       "  'Answer',\n",
       "  'get_dummies python memory error',\n",
       "  \"**Update:** Starting with version 0.19.0, get_dummies returns an 8bit integer rather than 64bit float, which will fix this problem in many cases and make the `as_type` solution below unnecessary.   See:  [get_dummies -- pandas 0.19.0][1]\\n\\n\\n\\nBut in other cases, the `sparse` option descibed below may still be helpful. \\n\\n\\n\\n**Original Answer:**  Here are a couple of possibilities to try.  Both will reduce the memory footprint of the dataframe substantially but you could still run into memory issues later.  It's hard to predict, you'll just have to try.\\n\\n\\n\\n(note that I am simplifying the output of `info()` below)\\n\\n\\n\\n    df = pd.DataFrame({ 'itemID': np.random.randint(1,4,100) })\\n\\n\\n\\n    pd.concat([df, pd.get_dummies(df['itemID'],prefix = 'itemID_')], axis=1).info()\\n\\n\\n\\n    itemID       100 non-null int32\\n\\n    itemID__1    100 non-null float64\\n\\n    itemID__2    100 non-null float64\\n\\n    itemID__3    100 non-null float64\\n\\n\\n\\n    memory usage: 3.5 KB\\n\\n\\n\\nHere's our baseline.  Each dummy column takes up 800 bytes because the sample data has 100 rows and `get_dummies` appears to default to float64 (8 bytes).  This seems like an unnecessarily inefficient way to store dummies as you could use as little as a bit to do it, but there may be some reason for that which I'm not aware of.\\n\\n\\n\\nSo, first attempt, just change to a one byte integer (this doesn't seem to be an option for `get_dummies` so it has to be done as a conversion with `astype(np.int8)`.\\n\\n\\n\\n    pd.concat([df, pd.get_dummies(df['itemID'],prefix = 'itemID_').astype(np.int8)], \\n\\n                                  axis=1).info()\\n\\n\\n\\n    itemID       100 non-null int32\\n\\n    itemID__1    100 non-null int8\\n\\n    itemID__2    100 non-null int8\\n\\n    itemID__3    100 non-null int8\\n\\n\\n\\n    memory usage: 1.5 KB\\n\\n\\n\\nEach dummy column now takes up 1/8 the memory as before.\\n\\n\\n\\nAlternatively, you can use the `sparse` option of `get_dummies`.\\n\\n\\n\\n    pd.concat([df, pd.get_dummies(df['itemID'],prefix = 'itemID_',sparse=True)], \\n\\n                                  axis=1).info()\\n\\n    \\n\\n    itemID       100 non-null int32\\n\\n    itemID__1    100 non-null float64\\n\\n    itemID__2    100 non-null float64\\n\\n    itemID__3    100 non-null float64\\n\\n\\n\\n    memory usage: 2.0 KB\\n\\n\\n\\nFairly comparable savings.  The `info()` output somewhat hides the way savings are occurring, but you can look at the value of memory usage to see to total savings.\\n\\n\\n\\nWhich of these will work better in practice will depend on your data, so you'll just need to give them each a try (or you could even combine them).\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/version/0.19.1/whatsnew.html#get-dummies-now-returns-integer-dtypes\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 7, 9),\n",
       "  '2018-04-10 20:14:22',\n",
       "  'JohnE (3877338)',\n",
       "  '23',\n",
       "  '',\n",
       "  '6733.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1150',\n",
       "  '34097048',\n",
       "  'Question',\n",
       "  'Selecting only the first few characters in a string C++',\n",
       "  'I want to select the first 8 characters of a string using C++.  Right now I create a temporary string which is 8 characters long, and fill it with the first 8 characters of another string.\\n\\n\\n\\nHowever, if the other string is not 8 characters long, I am left with unwanted whitespace.\\n\\n\\n\\n    string message = \"        \";\\n\\n\\n\\n\\tconst char * word = holder.c_str();\\n\\n\\n\\n\\tfor(int i = 0; i<message.length(); i++)\\n\\n\\t\\tmessage[i] = word[i];\\n\\n\\n\\nIf `word` is `\"123456789abc\"`, this code works correctly and `message` contains `\"12345678\"`.\\n\\n\\n\\nHowever, if `word` is shorter, something like `\"1234\"`, message ends up being `\"1234    \"`\\n\\n\\n\\nHow can I select either the first eight characters of a string, or the entire string if it is shorter than 8 characters?',\n",
       "  '<c++><string><c-strings>',\n",
       "  datetime.date(2015, 12, 4),\n",
       "  '2018-06-22 17:33:19',\n",
       "  'user3483203 (3483203)',\n",
       "  '6',\n",
       "  '',\n",
       "  '17529.0',\n",
       "  '5.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1154',\n",
       "  '30155619',\n",
       "  'Question',\n",
       "  'Expected build-failure tests in CMake',\n",
       "  \"Sometimes it's good to check that certain things fail to build, e.g.:\\n\\n\\n\\n    // Next line should fail to compile: can't convert const iterator to iterator.\\n\\n    my_new_container_type::iterator it = my_new_container_type::const_iterator();\\n\\n\\n\\nIs it possible to incorporate these types of things into CMake/CTest? I'm looking for something like this in `CMakeLists.txt`:\\n\\n\\n\\n    add_build_failure_executable(\\n\\n        test_iterator_conversion_build_failure\\n\\n        iterator_conversion_build_failure.cpp)\\n\\n    add_build_failure_test(\\n\\n        test_iterator_conversion_build_failure\\n\\n        test_iterator_conversion_build_failure)\\n\\n\\n\\n(Of course, these specific CMake directives don't exist, to the best of my knowledge.)\",\n",
       "  '<c++><cmake><automated-tests><ctest>',\n",
       "  datetime.date(2015, 5, 10),\n",
       "  '2015-05-12 12:58:08',\n",
       "  'Fraser (2556117)',\n",
       "  '23',\n",
       "  '9.0',\n",
       "  '2330.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1157',\n",
       "  '31498460',\n",
       "  'Answer',\n",
       "  'how to get pandas get_dummies to emit N-1 variables to avoid collinearity?',\n",
       "  \"There are a number of ways of doing so. \\n\\n\\n\\nPossibly the simplest is replacing one of the values by `None` before calling `get_dummies`. Say you have:\\n\\n\\n\\n    import pandas as pd\\n\\n    import numpy as np\\n\\n    s = pd.Series(list('babca'))\\n\\n    >> s\\n\\n    0    b\\n\\n    1    a\\n\\n    2    b\\n\\n    3    c\\n\\n    4    a\\n\\n\\n\\nThen use:\\n\\n\\n\\n    >> pd.get_dummies(np.where(s == s.unique()[0], None, s))\\n\\n     \\ta \\tc\\n\\n    0 \\t0 \\t0\\n\\n    1 \\t1 \\t0\\n\\n    2 \\t0 \\t0\\n\\n    3 \\t0 \\t1\\n\\n    4 \\t1 \\t0\\n\\n\\n\\nto drop `b`.\\n\\n\\n\\n(Of course, you need to consider if your category column doesn't already contain `None`.)\\n\\n\\n\\n-------------------------------------\\n\\n\\n\\nAnother way is to use the `prefix` argument to `get_dummies`:\\n\\n\\n\\n> `pandas.get_dummies(data, prefix=None, prefix_sep='_', dummy_na=False, columns=None, sparse=False)`\\n\\n  \\n\\n> **prefix**: string, list of strings, or dict of strings, default None - String to append DataFrame column names Pass a list with length equal to the number of columns when calling get_dummies on a DataFrame. Alternativly, prefix can be a dictionary mapping column names to prefixes.\\n\\n\\n\\nThis will append some prefix to all of the resulting columns, and you can then erase one of the columns with this prefix (just make it unique). \\n\\n\",\n",
       "  '<python><pandas><machine-learning><dummy-variable>',\n",
       "  datetime.date(2015, 7, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '4671.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1158',\n",
       "  '31521773',\n",
       "  'Answer',\n",
       "  'Convert currency to float (and parentheses indicate negative amounts)',\n",
       "  \"Just add `)` to the existing command, and then convert `(` to `-` to make numbers in parentheses negative.  Then convert to float.\\n\\n\\n\\n    (df['Currency'].replace( '[\\\\$,)]','', regex=True )\\n\\n                   .replace( '[(]','-',   regex=True ).astype(float))\\n\\n    \\n\\n       Currency\\n\\n    0         1\\n\\n    1      2000\\n\\n    2     -3000\",\n",
       "  '<python><pandas><currency>',\n",
       "  datetime.date(2015, 7, 20),\n",
       "  '2015-07-20 16:56:45',\n",
       "  'JohnE (3877338)',\n",
       "  '28',\n",
       "  '',\n",
       "  '11196.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1159',\n",
       "  '31526689',\n",
       "  'Answer',\n",
       "  'performance of pandas custom business day offset',\n",
       "  \"I think the way you are implementing it via lambda is slowing it down.  Consider this method (taken more or less straight from the [documentaion][1] )\\n\\n\\n\\n    from pandas.tseries.offsets import CustomBusinessDay\\n\\n    bday_us = CustomBusinessDay(calendar=USFederalHolidayCalendar())\\n\\n    mydate + bday_us\\n\\n\\n\\n    Out[13]: Timestamp('2014-12-26 00:00:00')\\n\\n\\n\\nThe first part is slow, but you only need to do it once.  The second part is very fast though.\\n\\n\\n\\n    %timeit bday_us = CustomBusinessDay(calendar=USFederalHolidayCalendar())\\n\\n    10 loops, best of 3: 66.5 ms per loop\\n\\n    \\n\\n    %timeit mydate + bday_us\\n\\n    10000 loops, best of 3: 44 µs per loop\\n\\n    \\n\\nTo get apples to apples, here are the other timings on my machine:\\n\\n\\n\\n    %timeit with_holiday = mydate + bday_offset(1)\\n\\n    10 loops, best of 3: 23.1 ms per loop\\n\\n    \\n\\n    %timeit without_holiday = mydate + pd.datetools.offsets.BDay(1)\\n\\n    10000 loops, best of 3: 36.6 µs per loop\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#custom-business-days-experimental\",\n",
       "  '<python><pandas><time-series>',\n",
       "  datetime.date(2015, 7, 20),\n",
       "  '2015-07-20 21:51:11',\n",
       "  'JohnE (3877338)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1091.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1165',\n",
       "  '31815812',\n",
       "  'Answer',\n",
       "  'Plotting a NACA 4-series airfoil',\n",
       "  \"First, `t` should be `0.12` not `12`. Second, to make a smoother plot, increase the sample points.\\n\\n\\n\\nIt is also a good idea to use vectorize method in `numpy`:\\n\\n\\n\\n    %matplotlib inline\\n\\n    import math\\n\\n    import matplotlib.pyplot as pyplot\\n\\n    import numpy as np\\n\\n\\n\\n    #https://en.wikipedia.org/wiki/NACA_airfoil#Equation_for_a_cambered_4-digit_NACA_airfoil\\n\\n    def camber_line( x, m, p, c ):\\n\\n        return np.where((x>=0)&(x<=(c*p)),\\n\\n                        m * (x / np.power(p,2)) * (2.0 * p - (x / c)),\\n\\n                        m * ((c - x) / np.power(1-p,2)) * (1.0 + (x / c) - 2.0 * p ))\\n\\n\\n\\n    def dyc_over_dx( x, m, p, c ):\\n\\n        return np.where((x>=0)&(x<=(c*p)),\\n\\n                        ((2.0 * m) / np.power(p,2)) * (p - x / c),\\n\\n                        ((2.0 * m ) / np.power(1-p,2)) * (p - x / c ))\\n\\n\\n\\n    def thickness( x, t, c ):\\n\\n        term1 =  0.2969 * (np.sqrt(x/c))\\n\\n        term2 = -0.1260 * (x/c)\\n\\n        term3 = -0.3516 * np.power(x/c,2)\\n\\n        term4 =  0.2843 * np.power(x/c,3)\\n\\n        term5 = -0.1015 * np.power(x/c,4)\\n\\n        return 5 * t * c * (term1 + term2 + term3 + term4 + term5)\\n\\n\\n\\n    def naca4(x, m, p, t, c=1):\\n\\n        dyc_dx = dyc_over_dx(x, m, p, c)\\n\\n        th = np.arctan(dyc_dx)\\n\\n        yt = thickness(x, t, c)\\n\\n        yc = camber_line(x, m, p, c)  \\n\\n        return ((x - yt*np.sin(th), yc + yt*np.cos(th)), \\n\\n                (x + yt*np.sin(th), yc - yt*np.cos(th)))\\n\\n\\n\\n\\n\\n    #naca2412 \\n\\n    m = 0.02\\n\\n    p = 0.4\\n\\n    t = 0.12\\n\\n    c = 1.0\\n\\n\\n\\n    x = np.linspace(0,1,200)\\n\\n    for item in naca4(x, m, p, t, c):\\n\\n        plt.plot(item[0], item[1], 'b')\\n\\n\\n\\n    plt.plot(x, camber_line(x, m, p, c), 'r')\\n\\n    plt.axis('equal')\\n\\n    plt.xlim((-0.05, 1.05))\\n\\n    # figure.set_size_inches(16,16,forward=True)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/2bFNW.png\",\n",
       "  '<python><numpy><matplotlib><plot>',\n",
       "  datetime.date(2015, 8, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1125.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1167',\n",
       "  '31350524',\n",
       "  'Answer',\n",
       "  'Decile Pandas DataFrame on column',\n",
       "  'IIUC, you can use the fact that [`qcut`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.qcut.html) returns something that you can use for `groupby`, so it would simply be something like:\\n\\n\\n\\n     df.groupby(pd.qcut(beta.res,10)).mean()',\n",
       "  '<python><sorting><python-3.x><statistics><dataframe>',\n",
       "  datetime.date(2015, 7, 10),\n",
       "  '2015-07-10 22:46:52',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1241.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1178',\n",
       "  '31049371',\n",
       "  'Answer',\n",
       "  'Pandas to_csv index=False not working',\n",
       "  \"The [`to_csv` method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html) has a ``header`` parameter, indicating if to output the header. In this case, you probably do *not* want this for writes that are not the first write.\\n\\n\\n\\nSo, you could do something like this:\\n\\n\\n\\n    for i, chunk in enumerate(pd.read_fwf(...)):\\n\\n        first = i == 0\\n\\n        chunk.to_csv(outfile, header=first, mode='a')\",\n",
       "  '<python><csv><pandas>',\n",
       "  datetime.date(2015, 6, 25),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2203.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1179',\n",
       "  '31076322',\n",
       "  'Answer',\n",
       "  'Pandas Pivot_Table : Percentage of row calculation for non-numeric values',\n",
       "  'The possible duplicate noted by @maxymoo is pretty close to a solution, but I\\'ll go ahead and write it up as an answer since there are a couple of differences that are not completely straightforward.\\n\\n\\n\\n    table = pd.pivot_table(df, values=[\"Document\"], index=[\"Name\"],columns=[\"Time\"], \\n\\n                           aggfunc=len, margins=True, dropna=True,fill_value=0)\\n\\n\\n\\n           Document                      \\n\\n    Time 1 - 2 HOUR 1 HOUR 2 - 3 HOUR All\\n\\n    Name                                 \\n\\n    A             1      1          1   3\\n\\n    B             1      1          0   2\\n\\n    C             0      1          1   2\\n\\n    All           2      3          2   7\\n\\n\\n\\nThe main tweak there is to add `fill_value=0` because what you really want there is a count value of zero, not a NaN.\\n\\n\\n\\nThen you can basically use the solution @maxymoo linked to, but you need to use `iloc` or similar b/c the table columns are a little complicated now (being a multi-indexed result of the pivot table).\\n\\n\\n\\n    table2 = table.div( table.iloc[:,-1], axis=0 )\\n\\n\\n\\n           Document                         \\n\\n    Time 1 - 2 HOUR    1 HOUR 2 - 3 HOUR All\\n\\n    Name                                    \\n\\n    A      0.333333  0.333333   0.333333   1\\n\\n    B      0.500000  0.500000   0.000000   1\\n\\n    C      0.000000  0.500000   0.500000   1\\n\\n    All    0.285714  0.428571   0.285714   1\\n\\n\\n\\nYou\\'ve still got some minor formatting work to do there (flip first and second columns and convert to %), but those are the numbers you are looking for.  \\n\\n\\n\\nBtw, it\\'s not necessary here, but you might want to think about converting \\'Time\\' to an ordered categorical variable, which would be one way to solve the column ordering problem (I think), but may or may not be worth the bother depending on what else you are doing with the data.',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 6, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '6432.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1182',\n",
       "  '34926896',\n",
       "  'Answer',\n",
       "  'std::future.get() Multiple Calls (from Different Threads)',\n",
       "  'You should consider using [`shared_future`](http://en.cppreference.com/w/cpp/thread/shared_future) for this.\\n\\n\\n\\n> The class template std::shared_future provides a mechanism to access the result of asynchronous operations, similar to std::future, except that multiple threads are allowed to wait for the same shared state.... Access to the same shared state from multiple threads is safe if each thread does it through its own copy of a shared_future object.',\n",
       "  '<c++><multithreading><c++11><future>',\n",
       "  datetime.date(2016, 1, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2262.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1189',\n",
       "  '30249951',\n",
       "  'Answer',\n",
       "  'Fast, efficient way to remove rows from large Pandas DataFrame',\n",
       "  \"If I'm understanding correctly, the idea is you have lots of users in one dataframe.  So I've expanded it to have 2 users.  If that's right, then something like this ought to be pretty fast:\\n\\n\\n\\n    df['keep'] = np.where( df['event'] == 'start', 1, np.nan )\\n\\n    df['keep'] = np.where( df['event'].shift() == 'signed up', 0, df['keep'] )\\n\\n    df['keep'] = df['keep'].ffill()\\n\\n    \\n\\n                   event   user  keep\\n\\n    0              start  user1     1\\n\\n    1       visited blog  user1     1\\n\\n    2          page view  user1     1\\n\\n    3          signed up  user1     1\\n\\n    4   viewed blog post  user1     0\\n\\n    5          page view  user1     0\\n\\n    6                end  user1     0\\n\\n    7              start  user2     1\\n\\n    8       visited blog  user2     1\\n\\n    9          signed up  user2     1\\n\\n    10  viewed blog post  user2     0\\n\\n    11               end  user2     0\\n\\n    \\n\\n    df[df['keep']==1]\\n\\n   \\n\\n              event   user  keep\\n\\n    0         start  user1     1\\n\\n    1  visited blog  user1     1\\n\\n    2     page view  user1     1\\n\\n    3     signed up  user1     1\\n\\n    7         start  user2     1\\n\\n    8  visited blog  user2     1\\n\\n    9     signed up  user2     1\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1798.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1194',\n",
       "  '31885145',\n",
       "  'Answer',\n",
       "  'How to create Matplotlib figure with image and profile plots that fit together?',\n",
       "  'you can use `sharex` and `sharey` to do this, replace your `ax=` line with this:\\n\\n\\n\\n    ax = [plt.subplot(gs[0]),]\\n\\n    ax.append(plt.subplot(gs[1], sharey=ax[0]))\\n\\n    ax.append(plt.subplot(gs[2], sharex=ax[0]))\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Pw7kV.png',\n",
       "  '<python><matplotlib><plot><2d>',\n",
       "  datetime.date(2015, 8, 7),\n",
       "  '2015-08-07 19:28:19',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1634.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1196',\n",
       "  '31886899',\n",
       "  'Answer',\n",
       "  'Numpy 1 Degree of Freedom',\n",
       "  \"Degrees of freedom is an important concept which you may want to [look it up](https://en.wikipedia.org/wiki/Degrees_of_freedom_(statistics)), but the computational difference is actually straight forward, consider these:\\n\\n\\n\\n    In [20]:\\n\\n\\n\\n    x = np.array([6,5,4,6,6,7,2])\\n\\n    In [21]:\\n\\n\\n\\n    np.std(x)\\n\\n    Out[21]:\\n\\n    1.5518257844571737\\n\\n\\n\\n    #default is ddof=0, what this actually does:\\n\\n    In [22]:\\n\\n\\n\\n    np.sqrt((((x-x.mean())**2)/len(x)).sum())\\n\\n    Out[22]:\\n\\n    1.5518257844571737\\n\\n    In [23]:\\n\\n\\n\\n    np.std(x, ddof=1)\\n\\n    Out[23]:\\n\\n    1.6761634196950517\\n\\n\\n\\n    #what ddof=1 does:\\n\\n    In [24]:\\n\\n\\n\\n    np.sqrt((((x-x.mean())**2)/(len(x)-1)).sum())\\n\\n    Out[24]:\\n\\n    1.6761634196950517\\n\\n\\n\\nIn most languages (`R`, `SAS` etc), the default is to return std of ddof=1. `numpy`'s default is ddof=0, which something worth noting. \",\n",
       "  '<python><numpy><statistics>',\n",
       "  datetime.date(2015, 8, 7),\n",
       "  '2015-08-07 21:33:52',\n",
       "  'CT Zhu (2487184)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1177.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1199',\n",
       "  '30283887',\n",
       "  'Answer',\n",
       "  'Profiling visualization tools?',\n",
       "  \"I've written a browser-based visualization tool, [profile_eye](https://pypi.python.org/pypi/ProfileEye/), which operates on the output of [gprof2dot](https://github.com/jrfonseca/gprof2dot).\\n\\n\\n\\ngprof2dot is great at grokking many profiling-tool outputs, and does a great job at graph-element placement. The final rendering is a static graphic, which is often very cluttered. \\n\\n\\n\\nUsing [d3.js](http://d3js.org/) it's possible to remove much of that clutter, through relative fading of unfocused elements, tooltips, and a [fisheye distortion](http://bost.ocks.org/mike/fisheye/). \\n\\n\\n\\nFor comparison, see [profile_eye's visualization](http://pythonhosted.org//ProfileEye/gprof.html) of the [canonical example used by gprof2dot](https://github.com/jrfonseca/gprof2dot). \",\n",
       "  '<performance>',\n",
       "  datetime.date(2015, 5, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '10689.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1201',\n",
       "  '30314504',\n",
       "  'Answer',\n",
       "  'Weighting results in pandas crosstab',\n",
       "  \"This is really wasteful of memory and only works if weights can be interpreted as frequencies (i.e. weights are integers), but it's fairly simple to do:\\n\\n\\n\\n    df2 = df.iloc[ np.repeat( df.index.values, df.weight ) ]\\n\\n\\n\\nThat's just using advanced/fancy indexing to expand the rows in proportion to the weights:\\n\\n\\n\\n         A  B  weight\\n\\n    0  foo  1       2\\n\\n    0  foo  1       2\\n\\n    1  bar  1       3\\n\\n    1  bar  1       3\\n\\n    1  bar  1       3\\n\\n\\n\\nThen you can run the crosstab normally:\\n\\n\\n\\n    pd.crosstab(df2.A, df2.B)\\n\\n    \\n\\n    B     0  1\\n\\n    A         \\n\\n    bar  11  3\\n\\n    foo   4  2\\n\\n\\n\\nI suspect it's necessary to write a custom version of crosstab in order to handle weights properly and efficiently as there are very few (if any?) functions in pandas that do weights for you automatically.  It wouldn't be all that hard though and maybe someone else will do it as an answer.\\n\\n\\n\\nPossibly scipy or statsmodels has an automatic way to do this?\\n\\n\",\n",
       "  '<python><pandas><scipy><crosstab><statsmodels>',\n",
       "  datetime.date(2015, 5, 18),\n",
       "  '2015-05-18 23:46:09',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1005.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1205',\n",
       "  '30792137',\n",
       "  'Answer',\n",
       "  'Is there an easy way to group columns in a Pandas DataFrame?',\n",
       "  \"You basically just need to manipulate the column names, in your case.\\n\\n\\n\\nStarting with your original DataFrame (and a tiny index manipulation):\\n\\n\\n\\n    from StringIO import StringIO\\n\\n    import numpy as np\\n\\n    a = pd.read_csv(StringIO('T,Ax,Ay,Az,Bx,By,Bz,Cx,Cy,Cz,Dx,Dy,Dz\\\\n\\\\\\n\\n        0,1,2,1,3,2,1,4,2,1,5,2,1\\\\n\\\\\\n\\n        1,8,2,3,3,2,9,9,1,3,4,9,1\\\\n\\\\\\n\\n        2,4,5,7,7,7,1,8,3,6,9,2,3'))\\n\\n    a.set_index('T', inplace=True)\\n\\n\\n\\nSo that:\\n\\n\\n\\n    >> a\\n\\n    Ax \\tAy \\tAz \\tBx \\tBy \\tBz \\tCx \\tCy \\tCz \\tDx \\tDy \\tDz\\n\\n    T \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n    0 \\t1 \\t2 \\t1 \\t3 \\t2 \\t1 \\t4 \\t2 \\t1 \\t5 \\t2 \\t1\\n\\n    1 \\t8 \\t2 \\t3 \\t3 \\t2 \\t9 \\t9 \\t1 \\t3 \\t4 \\t9 \\t1\\n\\n    2 \\t4 \\t5 \\t7 \\t7 \\t7 \\t1 \\t8 \\t3 \\t6 \\t9 \\t2 \\t3\\n\\n\\n\\nThen simply create a list of tuples for your columns, and use [``MultiIndex.from_tuples``](http://pandas.pydata.org/pandas-docs/stable/advanced.html):\\n\\n\\n\\n    a.columns = pd.MultiIndex.from_tuples([(c[0], c[1]) for c in a.columns])\\n\\n\\n\\n    >> a\\n\\n     \\tA \\t        B \\t        C \\t        D\\n\\n    \\tx \\ty \\tz \\tx \\ty \\tz \\tx \\ty \\tz \\tx \\ty \\tz\\n\\n    T \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\n\\n    0 \\t1 \\t2 \\t1 \\t3 \\t2 \\t1 \\t4 \\t2 \\t1 \\t5 \\t2 \\t1\\n\\n    1 \\t8 \\t2 \\t3 \\t3 \\t2 \\t9 \\t9 \\t1 \\t3 \\t4 \\t9 \\t1\\n\\n    2 \\t4 \\t5 \\t7 \\t7 \\t7 \\t1 \\t8 \\t3 \\t6 \\t9 \\t2 \\t3\",\n",
       "  '<pandas><dataframe><indices><columnname>',\n",
       "  datetime.date(2015, 6, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1465.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1210',\n",
       "  '35019420',\n",
       "  'Answer',\n",
       "  'Hiding code in jupiter in R',\n",
       "  'Add some `CSS` code in the first cell, change that cell to \\'Raw NBConvert\\' and the format specified in the `CSS` will be applied to the HTML generated:\\n\\n\\n\\nTo hide input blocks:\\n\\n\\n\\n    <style type=\"text/css\">\\n\\n    .input_hidden{\\n\\n        display: none\\n\\n    }\\n\\n    </style>\\n\\n\\n\\nOther style definition can also go there. \\n\\n\\n\\nThen run `ipython nbconvert the_name_of_the_stuff.ipynb --to slides` to generate the HTML (without the input blocks).',\n",
       "  '<r><jupyter><jupyter-notebook><jupyter-irkernel>',\n",
       "  datetime.date(2016, 1, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2048.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1215',\n",
       "  '31499100',\n",
       "  'Answer',\n",
       "  'How do I plot a pie chart using Pandas with this data',\n",
       "  \"Say you start with:\\n\\n\\n\\n    import pandas as pd\\n\\n    from matplotlib.pyplot import pie, axis, show\\n\\n\\n\\n    df = pd.DataFrame({\\n\\n        'Sex': ['female', 'male', 'female'],\\n\\n        'Smoke': [1, 1, 1]})\\n\\n\\n\\nYou can always do something like this:\\n\\n\\n\\n    sums = df.Smoke.groupby(df.Sex).sum()\\n\\n    axis('equal');\\n\\n    pie(sums, labels=sums.index);\\n\\n    show()\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/cgVq4.png\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 7, 19),\n",
       "  '2015-07-19 08:08:00',\n",
       "  'Paul (103081)',\n",
       "  '9',\n",
       "  '',\n",
       "  '14861.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1216',\n",
       "  '31529717',\n",
       "  'Answer',\n",
       "  'Converting a string of numbers to hex and back to dec pandas python',\n",
       "  'Pandas is designed to work primarily with integers and floats, with no particular facilities for hexadecimal that I know of, but you can use `apply` to access standard python conversion functions like `hex` and `int`:\\n\\n\\n\\n    df=pd.DataFrame({ \\'a\\':[52894036999, 78893201999, 45790373999] })\\n\\n    df[\\'b\\'] = df[\\'a\\'].apply( hex )\\n\\n    df[\\'c\\'] = df[\\'b\\'].apply( int, base=0 )\\n\\n    \\n\\nResults:\\n\\n\\n\\n                 a             b            c\\n\\n    0  52894036999   0xc50baf407  52894036999\\n\\n    1  78893201999  0x125e66ba4f  78893201999\\n\\n    2  45790373999   0xaa951a86f  45790373999\\n\\n\\n\\nNote that this answer is for Python 3.  For Python 2 you may need to strip off the trailing \"L\" in column \"b\" with `str[:-1]`.\\n\\n',\n",
       "  '<python-2.7><pandas><hex><decimal><string-conversion>',\n",
       "  datetime.date(2015, 7, 21),\n",
       "  '2017-10-15 14:36:59',\n",
       "  'JohnE (3877338)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2639.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1217',\n",
       "  '32143354',\n",
       "  'Answer',\n",
       "  'quick pandas groupby calculations with cumprod',\n",
       "  \"If you want a fast but not very pretty workaround, you could do something like the following.  Here's some sample data and your default approach.\\n\\n\\n\\n    df=pd.DataFrame({ 'x':np.repeat(range(200),4), 'y':np.random.randn(800) })\\n\\n    df = df.sort('x')\\n\\n    df['cp_group'] = df.groupby('x').cumprod()\\n\\n\\n\\nAnd here's the workaround.  It's looks rather long (it is) but each individual step is simple and fast.  (The timings are at the bottom.)  The key is simply to avoid using `groupby` at all in this case by replacing with `shift` and such -- but because of that you also need to make sure your data is sorted by the groupby column.\\n\\n\\n\\n    df['cp_nogroup'] = df.y.cumprod()\\n\\n    df['last'] = np.where( df.x == df.x.shift(-1), 0, df.y.cumprod() )\\n\\n    df['last'] = np.where( df['last'] == 0., np.nan, df['last'] )\\n\\n    df['last'] = df['last'].shift().ffill().fillna(1)\\n\\n    df['cp_fast'] = df['cp_nogroup'] / df['last']\\n\\n    df['dif'] = df.cp_group - df.cp_fast\\n\\n\\n\\nHere's what it looks like.  'cp_group' is your default and 'cp_fast' is the above workaround.  If you look at the 'dif' column you'll see that several of these are off by very small amounts.  This is just a precision issue and not anything to worry about.\\n\\n\\n\\n        x         y  cp_group  cp_nogroup      last   cp_fast           dif\\n\\n    0   0  1.364826  1.364826    1.364826  1.000000  1.364826  0.000000e+00\\n\\n    1   0  0.410126  0.559751    0.559751  1.000000  0.559751  0.000000e+00\\n\\n    2   0  0.894037  0.500438    0.500438  1.000000  0.500438  0.000000e+00\\n\\n    3   0  0.092296  0.046189    0.046189  1.000000  0.046189  0.000000e+00\\n\\n    4   1  1.262172  1.262172    0.058298  0.046189  1.262172  0.000000e+00\\n\\n    5   1  0.832328  1.050541    0.048523  0.046189  1.050541  2.220446e-16\\n\\n    6   1 -0.337245 -0.354289   -0.016364  0.046189 -0.354289 -5.551115e-17\\n\\n    7   1  0.758163 -0.268609   -0.012407  0.046189 -0.268609 -5.551115e-17\\n\\n    8   2 -1.025820 -1.025820    0.012727 -0.012407 -1.025820  0.000000e+00\\n\\n    9   2  1.175903 -1.206265    0.014966 -0.012407 -1.206265  0.000000e+00\\n\\n\\n\\n**Timings**\\n\\n\\n\\nDefault method:\\n\\n\\n\\n    In [86]: %timeit df.groupby('x').cumprod()\\n\\n    10 loops, best of 3: 100 ms per loop\\n\\n\\n\\nStandard `cumprod` but without the `groupby`.  This should be a good approximation of the maximum possible speed you could achieve.\\n\\n\\n\\n    In [87]: %timeit df.cumprod()\\n\\n    1000 loops, best of 3: 536 µs per loop\\n\\n\\n\\nAnd here's the workaround:\\n\\n\\n\\n    In [88]: %%timeit\\n\\n    ...: df['cp_nogroup'] = df.y.cumprod()\\n\\n    ...: df['last'] = np.where( df.x == df.x.shift(-1), 0, df.y.cumprod() )\\n\\n    ...: df['last'] = np.where( df['last'] == 0., np.nan, df['last'] )\\n\\n    ...: df['last'] = df['last'].shift().ffill().fillna(1)\\n\\n    ...: df['cp_fast'] = df['cp_nogroup'] / df['last']\\n\\n    ...: df['dif'] = df.cp_group - df.cp_fast\\n\\n   \\n\\n    100 loops, best of 3: 2.3 ms per loop\\n\\n\\n\\nSo the workaround is about 40x faster for this sample dataframe but the speedup will depend on the dataframe (in particular on the number of groups).\",\n",
       "  '<pandas>',\n",
       "  datetime.date(2015, 8, 21),\n",
       "  '2015-08-21 15:28:51',\n",
       "  'JohnE (3877338)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1134.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1218',\n",
       "  '32148268',\n",
       "  'Answer',\n",
       "  'quick pandas groupby calculations with cumprod',\n",
       "  \"Numba appears to work pretty well here.  In fact, these results seem almost too good to be true with the numba function below being about 4,000x faster than the original method and **5x faster than plain `cumprod` without a `groupby`**.  Hopefully these are correct, let me know if there is an error.\\n\\n\\n\\n    np.random.seed(1234)\\n\\n    df=pd.DataFrame({ 'x':np.repeat(range(200),4), 'y':np.random.randn(800) })\\n\\n    df = df.sort('x')\\n\\n    df['cp_groupby'] = df.groupby('x').cumprod()\\n\\n    \\n\\n    from numba import jit\\n\\n\\n\\n    @jit\\n\\n    def group_cumprod(x,y):\\n\\n        z = np.ones(len(x))\\n\\n        for i in range(len(x)):\\n\\n            if x[i] == x[i-1]:\\n\\n                z[i] = y[i] * z[i-1]\\n\\n            else:\\n\\n                z[i] = y[i]\\n\\n        return z\\n\\n    \\n\\n    df['cp_numba'] = group_cumprod(df.x.values,df.y.values)\\n\\n    \\n\\n    df['dif'] = df.cp_groupby - df.cp_numba\\n\\n\\n\\nTest that both ways give the same answer:\\n\\n    \\n\\n    all(df.cp_groupby==df.cp_numba)\\n\\n    Out[1447]: True\\n\\n    \\n\\nTimings:\\n\\n\\n\\n    %timeit df.groupby('x').cumprod()\\n\\n    10 loops, best of 3: 102 ms per loop\\n\\n    \\n\\n    %timeit df['y'].cumprod()\\n\\n    10000 loops, best of 3: 133 µs per loop\\n\\n\\n\\n    %timeit group_cumprod(df.x.values,df.y.values)\\n\\n    10000 loops, best of 3: 24.4 µs per loop\\n\\n    \\n\\n\",\n",
       "  '<pandas>',\n",
       "  datetime.date(2015, 8, 21),\n",
       "  '2015-08-21 23:32:28',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1134.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1221',\n",
       "  '35070548',\n",
       "  'Answer',\n",
       "  'How to correct spelling in a Pandas DataFrame',\n",
       "  \"You could do something like:\\n\\n\\n\\n    df.two.apply(lambda txt: ''.join(textblob.TextBlob(txt).correct()))\\n\\n\\n\\nUsing [`pandas.Series.apply`](http://pandas.pydata.org/pandas-docs/version/0.17.1/generated/pandas.Series.apply.html).\",\n",
       "  '<python><pandas><nlp><textblob>',\n",
       "  datetime.date(2016, 1, 28),\n",
       "  '2016-01-28 19:56:01',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1226.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1227',\n",
       "  '30357598',\n",
       "  'Answer',\n",
       "  'PyArray_SimpleNewFromData example',\n",
       "  \"The function [is](http://students.mimuw.edu.pl/~pbechler/numpy_doc/user/c-info.how-to-extend.html):\\n\\n\\n\\n     PyObject *\\n\\n        PyArray_SimpleNewFromData(\\n\\n            int nd, \\n\\n            npy_intp* dims, \\n\\n            int typenum, \\n\\n             void* data)\\n\\n\\n\\n- The last argument (``data``) is a buffer to the data. Let's dispense with that.\\n\\n\\n\\n- The second argument (``dims``) is a buffer, whose each entry is a dimension; so for a 1d array, it could be a length-1 buffer (or even an integer, since each integer is a length-1 buffer)\\n\\n\\n\\n- Since the second argument is a buffer, the first argument (``nd``) tells its length\\n\\n\\n\\n- The third argument (``typenum``) indicates the type.\\n\\n\\n\\n-------------\\n\\n\\n\\nFor example, say you have 4 64-bit ints at ``x``:\\n\\n\\n\\nTo create an array, use \\n\\n    \\n\\n    int dims[1];\\n\\n    dims[0] = 4;\\n\\n    PyArray_SimpleNewFromData(1, dims, NPY_INT64, x)\\n\\n\\n\\nTo create a 2X2 matrix, use\\n\\n\\n\\n    int dims[2];\\n\\n    dims[0] = dims[1] = 2;\\n\\n    PyArray_SimpleNewFromData(2, dims, NPY_INT64, x)\",\n",
       "  '<python><c++><numpy>',\n",
       "  datetime.date(2015, 5, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '5911.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1232',\n",
       "  '31167119',\n",
       "  'Answer',\n",
       "  'pandas: merge on column of collections.Counter (or even just dict) objects?',\n",
       "  \"One way of getting around this is to create a column (for each DataFrame) of a version of your original data structure converted to a hashable type.\\n\\n\\n\\nE.g.,\\n\\n\\n\\n    a['IDHash'] = a.ID.apply(lambda r: tuple(sorted(r.iteritems())))\\n\\n    b['IDHash'] = b.ID.apply(lambda r: tuple(sorted(r.iteritems())))\\n\\n\\n\\nand then\\n\\n\\n\\n    pd.merge(a, b, on='IDHash')\\n\\n\\n\\nAfter that, just erase the columns.\",\n",
       "  '<python><pandas><merge>',\n",
       "  datetime.date(2015, 7, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1896.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1234',\n",
       "  '35105613',\n",
       "  'Answer',\n",
       "  'Feature selection algorithms in Scikit-learn',\n",
       "  \"In [`sklearn.feature_selection`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection): \\n\\n[`SelectKBest`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest) and [`SelectPercentile`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile) assess subset performance, and [`RFE`](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE) does recursive feature elimination.\\n\\n\\n\\nAs for Best First Search, see [Smart Feature Selection with scikit-learn and BigML’s API](http://blog.bigml.com/2014/02/26/smart-feature-selection-with-scikit-learn-and-bigmls-api/).\\n\\n\\n\\nI am very skeptical you'll be able to exactly reproduce some experimental results based on the names of the algorithms used, FWIW.\",\n",
       "  '<python><machine-learning><scikit-learn>',\n",
       "  datetime.date(2016, 1, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '4429.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1239',\n",
       "  '32315542',\n",
       "  'Answer',\n",
       "  'Transpose multiple variables in rows to columns depending on a groupby using pandas',\n",
       "  \"Note that:\\n\\n\\n\\n    In [58]:\\n\\n\\n\\n    print grouped.la.apply(lambda x: pd.Series(data=x.values)).unstack()\\n\\n                        0      1   2\\n\\n    acct seq1 seq2                  \\n\\n    8888 3    20    38.43  37.53 NaN\\n\\n    9999 1    10    20.01  19.05  30\\n\\n         2    11    26.77  24.96 NaN\\n\\n\\n\\nand:\\n\\n\\n\\n    In [59]:\\n\\n\\n\\n    print grouped.ln.apply(lambda x: pd.Series(data=x.values)).unstack()\\n\\n                      0  1   2\\n\\n    acct seq1 seq2            \\n\\n    8888 3    20    218  1 NaN\\n\\n    9999 1    10    100  1   1\\n\\n         2    11    100  1 NaN\\n\\n\\n\\nTherefore:\\n\\n\\n\\n    In [60]:\\n\\n\\n\\n    df2 = pd.concat((grouped.la.apply(lambda x: pd.Series(data=x.values)).unstack(),\\n\\n                     grouped.ln.apply(lambda x: pd.Series(data=x.values)).unstack()),\\n\\n                    keys= ['la', 'ln'], axis=1)\\n\\n    print df2\\n\\n                       la              ln       \\n\\n                        0      1   2    0  1   2\\n\\n    acct seq1 seq2                              \\n\\n    8888 3    20    38.43  37.53 NaN  218  1 NaN\\n\\n    9999 1    10    20.01  19.05  30  100  1   1\\n\\n         2    11    26.77  24.96 NaN  100  1 NaN\\n\\n\\n\\nThe only problem is that the column index are `MultiIndex`. If we don't want it, we can transform them to `la0....` by:\\n\\n\\n\\n    df2.columns = map(lambda x: x[0]+str(x[1]), df2.columns.tolist())\\n\\n\\n\\nI don't know what do you think. But I prefer the `SAS` `PROC TRANSPOSE` syntax for better readability. `Pandas` syntax is concise but less readable in this particular case. \",\n",
       "  '<python><python-2.7><pandas><sas>',\n",
       "  datetime.date(2015, 8, 31),\n",
       "  '2015-08-31 16:19:58',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1111.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1244',\n",
       "  '30829806',\n",
       "  'Answer',\n",
       "  'Multiple pandas.dataframe to one csv file',\n",
       "  \"A very straightforward way would be to [`concat`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html) pairs horizontally,  concat the results vertically, and write it all out using [`to_csv`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html):\\n\\n\\n\\n     import pandas as pd\\n\\n\\n\\n     pd.concat([\\n\\n        pd.concat([df1, df2], axis=1),\\n\\n        pd.concat([df3, df4], axis=1)]).to_csv('foo.csv')\\n\\n\\n\\n--------------------------------------\\n\\n\\n\\nA possibly more memory-conserving way would be to write it piecemeal:\\n\\n\\n\\n    with open('foo.csv', 'w') as f:\\n\\n         pd.concat([df1, df2], axis=1).to_csv(f)\\n\\n    with open('foo.csv', 'a') as f:\\n\\n         pd.concat([df3, df4], axis=1).to_csv(f, header=False)\\n\\n\\n\\nOmitting `headers=False` would repeat the headers.\\n\\n\\n\\n\",\n",
       "  '<python><csv><pandas>',\n",
       "  datetime.date(2015, 6, 14),\n",
       "  '2018-11-21 20:50:53',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '6',\n",
       "  '',\n",
       "  '7152.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1254',\n",
       "  '35131258',\n",
       "  'Answer',\n",
       "  'Filtering multiple columns Pandas',\n",
       "  \"You can use the [`*args`](https://stackoverflow.com/questions/3394835/args-and-kwargs) keyword to pass a list of pairs:\\n\\n\\n\\n    def filter_df(df, *args):\\n\\n        for k, v in args:\\n\\n            df = df[df[k] == v]\\n\\n        return df\\n\\n\\n\\nIt can be used like this:\\n\\n\\n\\n    df = pd.DataFrame({'a': [1, 2, 1, 1], 'b': [1, 3, 3, 3]})\\n\\n        \\n\\n    >>> filter_df(df, ('a', 1), ('b', 2))\\n\\n        a \\tb\\n\\n    2 \\t1 \\t3\\n\\n    3 \\t1 \\t3\\n\\n\\n\\n**Note**\\n\\n\\n\\nIn theory, you could use `**kwargs`, which would have a more pleasing usage:\\n\\n\\n\\n    filter_df(df, a=1, b=2)\\n\\n\\n\\nbut then you could only use it for columns whose names are valid Python identifiers.\\n\\n\\n\\n**Edit**\\n\\n\\n\\nSee comment below by @Goyo for a better implementation point.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 2, 1),\n",
       "  '2017-05-23 12:34:09',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1034.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1256',\n",
       "  '33242080',\n",
       "  'Answer',\n",
       "  'Getting Colorbar instance of scatter plot in pandas/matplotlib',\n",
       "  \"`pandas` does not return the axis for the colorbar, therefore we have to locate it:\\n\\n\\n\\n1st, let's get the `figure` instance: i.e., use `plt.gcf()`\\n\\n\\n\\n    In [61]:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import pandas as pd\\n\\n    import numpy as np\\n\\n    import itertools as it\\n\\n\\n\\n    # [ (0,0), (0,1), ..., (9,9) ]\\n\\n    xy_positions = list( it.product( range(10), range(10) ) )\\n\\n\\n\\n    df = pd.DataFrame( xy_positions, columns=['x','y'] )\\n\\n\\n\\n    # draw 100 floats\\n\\n    df['score'] = np.random.random( 100 )\\n\\n\\n\\n    ax = df.plot( kind='scatter',\\n\\n                  x='x',\\n\\n                  y='y',\\n\\n                  c='score',\\n\\n                  s=500)\\n\\n    ax.set_xlim( [-0.5,9.5] )\\n\\n    ax.set_ylim( [-0.5,9.5] )\\n\\n\\n\\n    f = plt.gcf()\\n\\n\\n\\n2, how many axes does this figure have?\\n\\n\\n\\n    In [62]:\\n\\n\\n\\n    f.get_axes()\\n\\n    Out[62]:\\n\\n    [<matplotlib.axes._subplots.AxesSubplot at 0x120a4d450>,\\n\\n     <matplotlib.axes._subplots.AxesSubplot at 0x120ad0050>]\\n\\n\\n\\n3, The first axes (that is, the first one created), contains the plot\\n\\n\\n\\n    In [63]:\\n\\n\\n\\n    ax\\n\\n    Out[63]:\\n\\n    <matplotlib.axes._subplots.AxesSubplot at 0x120a4d450>\\n\\n\\n\\n4, Therefore, the second axis is the colorbar axes\\n\\n\\n\\n    In [64]:\\n\\n\\n\\n    cax = f.get_axes()[1]\\n\\n    #and we can modify it, i.e.:\\n\\n    cax.set_ylabel('test')\\n\\n\",\n",
       "  '<pandas><matplotlib><plot>',\n",
       "  datetime.date(2015, 10, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '9',\n",
       "  '',\n",
       "  '3005.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1259',\n",
       "  '33537383',\n",
       "  'Answer',\n",
       "  'How can we change data type of a dataframe row in pandas?',\n",
       "  \"No, it's not possible. Somewhat simplistically, you can think of a `DataFrame` as something like a column `dict` of [`numpy.array`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.array.html)s, and those are homogeneously typed.\\n\\n\\n\\nYou write \\n\\n\\n\\n> That's the way I am collecting the results of API calls. It's more robust to fix the columns and append rows, rather than keep increasing columns and fix rows.\\n\\n\\n\\nGiven this usage pattern and types, you might consider if DataFrames are right for you at all. From my experience, DataFrames have horrible performance for dynamic row-by-row appending. You might consider using regular Python `dict`s and `lists`s for the aggregation phase, then somehow process the data and stick it into a `DataFrame`.\",\n",
       "  '<python>',\n",
       "  datetime.date(2015, 11, 5),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2852.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1268',\n",
       "  '30931812',\n",
       "  'Answer',\n",
       "  'Thread joining using a destructor',\n",
       "  'You could do that. However, are you really interested in the *mechanism* of launching a new thread, or are you perhaps interested in the *effect* of performing something asynchronously? If it\\'s the latter, you can move to the higher level [`async`](http://en.cppreference.com/w/cpp/thread/async) mechanism:\\n\\n\\n\\n    #include <iostream>\\n\\n    #include <future>\\n\\n\\n\\n\\n\\n    void bar()\\n\\n    {\\n\\n        std::cout << \"I\\'m bar\" << std::endl;\\n\\n    }\\n\\n\\n\\n    class foo\\n\\n    {\\n\\n    public:\\n\\n        foo() :\\n\\n            m_f(std::async(\\n\\n                std::launch::async,\\n\\n                bar))\\n\\n        {\\n\\n\\n\\n        }\\n\\n\\n\\n        ~foo()\\n\\n        {\\n\\n            m_f.get();\\n\\n        }\\n\\n\\n\\n    private:\\n\\n        std::future<void> m_f;\\n\\n    };\\n\\n\\n\\n    int main ()\\n\\n    {\\n\\n        foo f;\\n\\n    }\\n\\n\\n\\n- You\\'re asking in the constructor to launch `bar` asynchronously. You don\\'t care about managing the thread yourself - let the library handle that. \\n\\n\\n\\n- Place the resulting `future` in a member.\\n\\n\\n\\n- In the dtor, `get` the future.',\n",
       "  '<c++><multithreading><destructor>',\n",
       "  datetime.date(2015, 6, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1742.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1271',\n",
       "  '31861477',\n",
       "  'Answer',\n",
       "  'Rotate tick labels for seaborn barplot',\n",
       "  'You need a different method call, namely `.set_rotation` for each `ticklable`s.\\n\\nSince you already have the ticklabels, just change their rotations:\\n\\n\\n\\n    for item in by_school.get_xticklabels():\\n\\n        item.set_rotation(45)\\n\\n\\n\\n`barplot` returns a `matplotlib.axes` object (as of `seaborn` 0.6.0), therefore you have to rotate the labels this way. In other cases, when the method returns a `FacetGrid` object, refer to https://stackoverflow.com/questions/26540035/rotate-label-text-in-seaborn-factorplot?lq=1',\n",
       "  '<python><matplotlib><seaborn>',\n",
       "  datetime.date(2015, 8, 6),\n",
       "  '2017-05-23 12:26:27',\n",
       "  'CT Zhu (2487184), URL Rewriter Bot (n/a)',\n",
       "  '26',\n",
       "  '',\n",
       "  '23527.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1275',\n",
       "  '33901297',\n",
       "  'Answer',\n",
       "  'Select last observation per group',\n",
       "  \"Using [`pandas.shift`](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.shift.html), you can do something like:\\n\\n\\n\\n    df['group_indicator'] = df.group_id != df.group_id.shift(-1)\\n\\n\\n\\n(or \\n\\n\\n\\n    df['group_indicator'] = (df.group_id != df.group_id.shift(-1)).astype(int)\\n\\n\\n\\nif it's actually important for you to have it as an integer.)\\n\\n\\n\\n-----------------\\n\\n\\n\\n\\n\\n**Note:** \\n\\n\\n\\n1. for large datasets, this should be much faster than list comprehension (not to mention loops).\\n\\n\\n\\n2. As Alexander notes, this assumes the DataFrame is sorted as it is in the example.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 11, 24),\n",
       "  '2015-11-24 18:40:26',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1103.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1279',\n",
       "  '30424537',\n",
       "  'Answer',\n",
       "  'How to make good reproducible pandas examples',\n",
       "  \"##How to create sample datasets \\n\\n\\n\\nThis is to mainly to expand on @AndyHayden's answer by providing examples of how you can create sample dataframes.  Pandas and (especially) numpy give you a variety of tools for this such that you can generally create a reasonable facsimile of any real dataset with just a few lines of code.\\n\\n\\n\\nAfter importing numpy and pandas, be sure to provide a random seed if you want folks to be able to exactly reproduce your data and results.\\n\\n\\n\\n    import numpy as np\\n\\n    import pandas as pd\\n\\n\\n\\n    np.random.seed(123)\\n\\n\\n\\n###A kitchen sink example\\n\\n\\n\\nHere's an example showing a variety of things you can do.  All kinds of useful sample dataframes could be created from a subset of this:\\n\\n\\n\\n    df = pd.DataFrame({ \\n\\n    \\n\\n        # some ways to create random data\\n\\n        'a':np.random.randn(6),\\n\\n        'b':np.random.choice( [5,7,np.nan], 6),\\n\\n        'c':np.random.choice( ['panda','python','shark'], 6),\\n\\n    \\n\\n        # some ways to create systematic groups for indexing or groupby\\n\\n        # this is similar to r's expand.grid(), see note 2 below\\n\\n        'd':np.repeat( range(3), 2 ),\\n\\n        'e':np.tile(   range(2), 3 ),\\n\\n    \\n\\n        # a date range and set of random dates\\n\\n        'f':pd.date_range('1/1/2011', periods=6, freq='D'),\\n\\n        'g':np.random.choice( pd.date_range('1/1/2011', periods=365, \\n\\n                              freq='D'), 6, replace=False) \\n\\n        })\\n\\n\\n\\nThis produces:\\n\\n\\n\\n              a   b       c  d  e          f          g\\n\\n    0 -1.085631 NaN   panda  0  0 2011-01-01 2011-08-12\\n\\n    1  0.997345   7   shark  0  1 2011-01-02 2011-11-10\\n\\n    2  0.282978   5   panda  1  0 2011-01-03 2011-10-30\\n\\n    3 -1.506295   7  python  1  1 2011-01-04 2011-09-07\\n\\n    4 -0.578600 NaN   shark  2  0 2011-01-05 2011-02-27\\n\\n    5  1.651437   7  python  2  1 2011-01-06 2011-02-03\\n\\n\\n\\nSome notes:\\n\\n\\n\\n 1. `np.repeat` and `np.tile` (columns `d` and `e`) are very useful for creating groups and indices in a very regular way.  For 2 columns, this can be used to easily duplicate r's `expand.grid()` but is also more flexible in ability to provide a subset of all permutations.  However, for 3 or more columns the syntax quickly becomes unwieldy.\\n\\n 2. For a more direct replacement for r's `expand.grid()` see the `itertools` solution in the [pandas cookbook][1] or the `np.meshgrid` solution shown [here][2].  Those will allow any number of dimensions.\\n\\n 3. You can do quite a bit with `np.random.choice`.  For example, in column `g`, we have a random selection of 6 dates from 2011.  Additionally, by setting `replace=False` we can assure these dates are unique -- very handy if we want to use this as an index with unique values.\\n\\n\\n\\n###Fake stock market data\\n\\n\\n\\nIn addition to taking subsets of the above code, you can further combine the techniques to do just about anything.  For example, here's a short example that combines `np.tile` and `date_range` to create sample ticker data for 4 stocks covering the same dates:\\n\\n\\n\\n    stocks = pd.DataFrame({ \\n\\n        'ticker':np.repeat( ['aapl','goog','yhoo','msft'], 25 ),\\n\\n        'date':np.tile( pd.date_range('1/1/2011', periods=25, freq='D'), 4 ),\\n\\n        'price':(np.random.randn(100).cumsum() + 10) })\\n\\n\\n\\nNow we have a sample dataset with 100 lines (25 dates per ticker), but we have only used 4 lines to do it, making it easy for everyone else to reproduce without copying and pasting 100 lines of code.  You can then display subsets of the data if it helps to explain your question:\\n\\n\\n\\n    >>> stocks.head(5)\\n\\n     \\n\\n            date      price ticker\\n\\n    0 2011-01-01   9.497412   aapl\\n\\n    1 2011-01-02  10.261908   aapl\\n\\n    2 2011-01-03   9.438538   aapl\\n\\n    3 2011-01-04   9.515958   aapl\\n\\n    4 2011-01-05   7.554070   aapl\\n\\n    \\n\\n    >>> stocks.groupby('ticker').head(2)\\n\\n     \\n\\n             date      price ticker\\n\\n    0  2011-01-01   9.497412   aapl\\n\\n    1  2011-01-02  10.261908   aapl\\n\\n    25 2011-01-01   8.277772   goog\\n\\n    26 2011-01-02   7.714916   goog\\n\\n    50 2011-01-01   5.613023   yhoo\\n\\n    51 2011-01-02   6.397686   yhoo\\n\\n    75 2011-01-01  11.736584   msft\\n\\n    76 2011-01-02  11.944519   msft\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/version/0.16.1/cookbook.html#creating-example-data\\n\\n  [2]: https://stackoverflow.com/questions/12130883/r-expand-grid-function-in-python\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 24),\n",
       "  '2017-05-23 11:47:22',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '54',\n",
       "  '',\n",
       "  '10451.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1286',\n",
       "  '32014178',\n",
       "  'Answer',\n",
       "  'matplotlib circle, animation, how to remove old circle in animation',\n",
       "  'I think you can just use `set_radius` to change the circle size every time:\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    import matplotlib.animation as animation\\n\\n\\n\\n\\n\\n    testData = np.random.rand(100,2)-[0.5, 0.5]\\n\\n\\n\\n    fig, ax = plt.subplots()\\n\\n    circle1=plt.Circle(testData[20,:],0,color=\\'r\\',fill=False, clip_on = False)\\n\\n    ax.add_artist(circle1)\\n\\n    # i is the radius\\n\\n    def animate(i):\\n\\n        #init()\\n\\n        #you don\\'t want to redraw the same dots over and over again, do you?\\n\\n        circle1.set_radius(i)\\n\\n\\n\\n\\n\\n    def init():\\n\\n        sctPlot, = ax.plot(testData[:,0], testData[:,1], \".\")\\n\\n        return sctPlot,\\n\\n\\n\\n    ani = animation.FuncAnimation(fig, animate, np.arange(0.4, 2, 0.1), init_func=init,\\n\\n        interval=25, blit=False)\\n\\n    plt.show()\\n\\n\\n\\nSo you are not removing and redrawing patches every round, which I think may be more efficient. ',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2015, 8, 14),\n",
       "  '2015-08-14 16:10:22',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2235.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1300',\n",
       "  '31037177',\n",
       "  'Answer',\n",
       "  'Python - Iterate through CSV rows and create XML string',\n",
       "  \"Python is [Batteries Included](https://docs.python.org/2/tutorial/stdlib.html#batteries-included).\\n\\n\\n\\nIn this case, you can use the [`csv` module](https://docs.python.org/2/library/csv.html) and the [`xml` module](http://pymotw.com/2/xml/etree/ElementTree/create.html), with code that looks like this:\\n\\n\\n\\n    # CSV module\\n\\n    import csv\\n\\n    # Stuff from the XML module\\n\\n    from xml.etree.ElementTree import Element, SubElement, tostring\\n\\n\\n\\n    # Topmost XML element\\n\\n    top = Element('top')\\n\\n    # Open a file\\n\\n    with open('stuff.csv') as csvfile:\\n\\n        # And use a dictionary-reader\\n\\n        for d in csv.DictReader(csvfile)\\n\\n            # For each mapping in the dictionary\\n\\n            for (k, v) in d.iteritems():\\n\\n                # Create an XML node\\n\\n                child = SubElement(top, k)\\n\\n                child.text = v\\n\\n    print tostring(top)\",\n",
       "  '<python><xml><csv>',\n",
       "  datetime.date(2015, 6, 24),\n",
       "  '2015-06-24 21:24:39',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1203.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1302',\n",
       "  '32122016',\n",
       "  'Answer',\n",
       "  'label size in panda plot (scatter_matrix)',\n",
       "  \"The return of `scatter_matrix()` is a number of axis, therefore, there is no easy way to set the font size in one pass (except override it using `plt.rcParam`, such as `plt.rcParams['axes.labelsize'] = 20` for changing the label size), and it has to be set one by one, such as: `plt.setp(axes[0,0].yaxis.get_majorticklabels(), 'size', 15)`\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nTo have all the tick labels changed, assuming `Axes=scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal='kde')`:\\n\\n\\n\\n    import pandas as pd\\n\\n    import matplotlib.pyplot as plt\\n\\n    from numpy.random import randn\\n\\n    from pandas.tools.plotting import scatter_matrix\\n\\n\\n\\n    df = pd.DataFrame(randn(1000, 4), columns=['a', 'b', 'c', 'd'])\\n\\n\\n\\n    Axes = scatter_matrix(df, alpha=0.2, figsize=(6, 6), diagonal='kde')\\n\\n\\n\\n    #y ticklabels\\n\\n    [plt.setp(item.yaxis.get_majorticklabels(), 'size', 15) for item in Axes.ravel()]\\n\\n    #x ticklabels\\n\\n    [plt.setp(item.xaxis.get_majorticklabels(), 'size', 5) for item in Axes.ravel()]\\n\\n    #y labels\\n\\n    [plt.setp(item.yaxis.get_label(), 'size', 20) for item in Axes.ravel()]\\n\\n    #x labels\\n\\n    [plt.setp(item.xaxis.get_label(), 'size', 3) for item in Axes.ravel()]\\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/qMayp.png\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 8, 20),\n",
       "  '2015-08-20 15:22:55',\n",
       "  'CT Zhu (2487184)',\n",
       "  '9',\n",
       "  '',\n",
       "  '4232.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1303',\n",
       "  '32127545',\n",
       "  'Answer',\n",
       "  'Python 2.7 / Pandas: writing new string from each row in dataframe',\n",
       "  'There is a [`to_dict()`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_dict.html) method you may want to consider in this case:\\n\\n\\n\\n    In [178]:\\n\\n    df.columns = [\\'table\\',\\'name\\',\\'type\\',\\'value\\']\\n\\n    [[\"<%s=\\'%s\\'>\"%(k,v) for k,v in D.items()] for D in df.to_dict(\\'records\\')]\\n\\n\\n\\n    Out[178]:\\n\\n    [[\"<table=\\'CRASH\\'>\", \"<type=\\'integer\\'>\", \"<name=\\'CRASH_ID\\'>\", \"<value=\\'1.0\\'>\"],\\n\\n     [\"<table=\\'CRASH\\'>\", \"<type=\\'range\\'>\", \"<name=\\'SER_NO\\'>\", \"<value=\\'0.0\\'>\"],\\n\\n     [\"<table=\\'CRASH\\'>\", \"<type=\\'code\\'>\", \"<name=\\'SER_NO\\'>\", \"<value=\\'99999.0\\'>\"],\\n\\n     [\"<table=\\'CRASH\\'>\", \"<type=\\'string\\'>\", \"<name=\\'CRASH_DT\\'>\", \"<value=\\'nan\\'>\"]]',\n",
       "  '<python><python-2.7><pandas><dataframe>',\n",
       "  datetime.date(2015, 8, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1162.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1304',\n",
       "  '32148239',\n",
       "  'Answer',\n",
       "  'How to groupby time series by 10 minutes using pandas?',\n",
       "  \"There is a `pandas.TimeGrouper` for this sort of thing, what you described would be some thing like:\\n\\n\\n\\n\\n\\n    agg_10m = df.groupby(pd.TimeGrouper(freq='10Min')).aggregate(numpy.sum) #or other function\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 8, 21),\n",
       "  '2015-08-22 01:16:48',\n",
       "  'CT Zhu (2487184)',\n",
       "  '11',\n",
       "  '',\n",
       "  '6525.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1305',\n",
       "  '31077694',\n",
       "  'Answer',\n",
       "  'Bubble plot or Heatmap in matplotlib',\n",
       "  \"I was hoping this might work by just changing 'Name' and 'Place' to categoricals, but no luck there (with either plot or seaborn).  It will basically work if you convert them to integers but then you lose the labels that you'd have with strings or categoricals.  FWIW:\\n\\n\\n\\n    df2 = df.copy()\\n\\n    for c in ['Place','Name']:\\n\\n        df2[c] = df2[c].astype('category').cat.codes\\n\\n\\n\\n    fig = plt.figure()\\n\\n    ax = fig.add_subplot(1,1,1)\\n\\n    ax.scatter(df2['Place'],df2['Name'], s=df2['00:00:00'])\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\nOr maybe a heatmap would work better?  It seems to accept categoricals, so you get the labeling for free.\\n\\n\\n\\n    df3 = df.copy()\\n\\n    for c in ['Place','Name']:\\n\\n        df3[c] = df3[c].astype('category')\\n\\n\\n\\n    sns.heatmap( df3.pivot_table( index='Place', columns='Name', values='00:00:00' ) )\\n\\n\\n\\n![enter image description here][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/QoHxA.png\\n\\n  [2]: http://i.stack.imgur.com/BC0jQ.png\",\n",
       "  '<python><file><pandas><matplotlib><seaborn>',\n",
       "  datetime.date(2015, 6, 26),\n",
       "  '2015-06-26 16:15:46',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '8095.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1310',\n",
       "  '35410087',\n",
       "  'Answer',\n",
       "  \"Why doesn't std::unique_ptr implicitly convert to T* and const T*?\",\n",
       "  \"I don't think your question is specific to `unique_ptr`, but rather asking about smart pointers in general.\\n\\n\\n\\n[Herb Sutter wrote about this a long time ago](http://herbsutter.com/2012/06/21/reader-qa-why-dont-modern-smart-pointers-implicitly-convert-to/). Apparently, it would allow you to write logically-erroneous code such as:\\n\\n\\n\\n    unique_ptr<something> p;\\n\\n    ...\\n\\n    delete p; // p is a smart pointer - probably not what you want.\\n\\n\\n\\nand other code like that.\\n\\n\",\n",
       "  '<c++><c++11>',\n",
       "  datetime.date(2016, 2, 15),\n",
       "  '2016-02-15 14:28:23',\n",
       "  'Cody Gray (366904), Ami Tavory (3510736)',\n",
       "  '14',\n",
       "  '',\n",
       "  '1090.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1311',\n",
       "  '35433946',\n",
       "  'Answer',\n",
       "  'How to remove duplicates from a dataframe?',\n",
       "  \"Consider this:\\n\\n\\n\\n    df = pd.DataFrame({'a': [1, 2, 3, 3, 3], 'b': [1, 2, None, 1, None]})\\n\\n    \\n\\nThen\\n\\n\\n\\n    >>> df.sort_values(by=['a', 'b']).groupby(df.a).first()[['b']].reset_index()\\n\\n        a\\tb\\n\\n    0\\t1\\t1\\n\\n    1\\t2\\t2\\n\\n    2\\t3\\t1\\n\\n\\n\\nSorts the items by first `a`, then `b` (thus pushing the `None` values in each group last), then selects the first item per group.\\n\\n\\n\\nI believe you can modify this to the specifics of your problem.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 2, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '5787.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1313',\n",
       "  '30487369',\n",
       "  'Question',\n",
       "  'Map vs applymap when passing a dictionary',\n",
       "  'I thought I understood map vs applymap pretty well, but am having a problem (see [here][1] for additional background, if interested).\\n\\n\\n\\nA simple example:\\n\\n\\n\\n    df  = pd.DataFrame( [[1,2],[1,1]] ) \\n\\n    dct = { 1:\\'python\\', 2:\\'gator\\' }\\n\\n    \\n\\n    df[0].map( lambda x: x+90 )\\n\\n    df.applymap( lambda x: x+90 )\\n\\n\\n\\nThat works as expected -- both operate on an elementwise basis, map on a series, applymap on a dataframe (explained very well [here][2] btw).\\n\\n    \\n\\nIf I use a dictionary rather than a lambda, map still works fine:\\n\\n\\n\\n    df[0].map( dct )\\n\\n\\n\\n    0    python\\n\\n    1    python\\n\\n\\n\\nbut not applymap:\\n\\n\\n\\n    df.applymap( dct )\\n\\n    ---------------------------------------------------------------------------\\n\\n    TypeError                                 Traceback (most recent call last)\\n\\n    <ipython-input-100-7872ff604851> in <module>()\\n\\n    ----> 1 df.applymap( dct )\\n\\n    \\n\\n    C:\\\\Users\\\\johne\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\frame.pyc in applymap(self, func)\\n\\n       3856                 x = lib.map_infer(_values_from_object(x), f)\\n\\n       3857             return lib.map_infer(_values_from_object(x), func)\\n\\n    -> 3858         return self.apply(infer)\\n\\n       3859 \\n\\n       3860     #----------------------------------------------------------------------\\n\\n    \\n\\n    C:\\\\Users\\\\johne\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\frame.pyc in apply(self, func, axis, broadcast, raw, reduce, args, **kwds)\\n\\n       3687                     if reduce is None:\\n\\n       3688                         reduce = True\\n\\n    -> 3689                     return self._apply_standard(f, axis, reduce=reduce)\\n\\n       3690             else:\\n\\n       3691                 return self._apply_broadcast(f, axis)\\n\\n    \\n\\n    C:\\\\Users\\\\johne\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\frame.pyc in _apply_standard(self, func, axis, ignore_failures, reduce)\\n\\n       3777             try:\\n\\n       3778                 for i, v in enumerate(series_gen):\\n\\n    -> 3779                     results[i] = func(v)\\n\\n       3780                     keys.append(v.name)\\n\\n       3781             except Exception as e:\\n\\n    \\n\\n    C:\\\\Users\\\\johne\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\frame.pyc in infer(x)\\n\\n       3855                 f = com.i8_boxer(x)\\n\\n       3856                 x = lib.map_infer(_values_from_object(x), f)\\n\\n    -> 3857             return lib.map_infer(_values_from_object(x), func)\\n\\n       3858         return self.apply(infer)\\n\\n       3859 \\n\\n    \\n\\n    C:\\\\Users\\\\johne\\\\AppData\\\\Local\\\\Continuum\\\\Anaconda\\\\lib\\\\site-packages\\\\pandas\\\\lib.pyd in pandas.lib.map_infer (pandas\\\\lib.c:56990)()\\n\\n    \\n\\n    TypeError: (\"\\'dict\\' object is not callable\", u\\'occurred at index 0\\')\\n\\n\\n\\nSo, my question is why don\\'t map and applymap work in an analogous manner here?  Is it a bug with applymap, or am I doing something wrong?\\n\\n\\n\\n\\n\\n**Edit to add**:  I have discovered that I can work around this fairly easily with this:\\n\\n\\n\\n    df.applymap( lambda x: dct[x] )\\n\\n\\n\\n            0       1\\n\\n    0  python   gator\\n\\n    1  python  python\\n\\n\\n\\nOr better yet via this [answer][3] which requires no lambda.\\n\\n\\n\\n    df.applymap( dct.get )\\n\\n\\n\\nSo that is pretty much exactly equivalent, right?  Must be something with how applymap parses the syntax and I guess the explicit form of a function/method works better than a dictionary.  Anyway, I guess now there is no practical problem remaining here but am still interested in what is going on here if anyone wants to answer.\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/questions/30431492/indexerror-index-1-is-out-of-bounds-for-axis-1-with-size-1/30486721#30486721\\n\\n  [2]: https://stackoverflow.com/questions/19798153/difference-between-map-applymap-and-apply-methods-in-pandas\\n\\n  [3]: https://stackoverflow.com/questions/24216425/adding-a-new-pandas-column-with-mapped-value-from-a-dictionary?rq=1',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 27),\n",
       "  '2017-05-23 12:17:14',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '7',\n",
       "  '3.0',\n",
       "  '1756.0',\n",
       "  '1.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1314',\n",
       "  '31373317',\n",
       "  'Answer',\n",
       "  'Python Pandas plot. Script does not see a numeric value in text file',\n",
       "  \"It's very likely that if you would type\\n\\n\\n\\n    df.Two.dtype\\n\\n\\n\\nIt would output that its type is a string (It would say it's `Object` or something like that).\\n\\n\\n\\nThis is easily remedied. If you need to do something numeric with it, use [`astype`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.astype.html):\\n\\n\\n\\n    df.Two.astype(float)\\n\\n\\n\\nIn fact, you can just reassign it back to itself:\\n\\n\\n\\n    df.Two = df.Two.astype(float)\",\n",
       "  '<python>',\n",
       "  datetime.date(2015, 7, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '2853.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1315',\n",
       "  '31412861',\n",
       "  'Answer',\n",
       "  'Solving overdetermined system in numpy when the value of one variable is already known',\n",
       "  'Say you need to solve\\n\\n\\n\\n    |1 0 0 1 0|     |n1|     |B1|\\n\\n    |1 0 0 0 1|     |n2|     |B2|\\n\\n    |0 1 0 1 0|  X  |n3|  =  |B3|\\n\\n    |0 1 0 0 1|     |t1|     |B4|\\n\\n    |0 0 1 1 0|     |t2|     |B5|\\n\\n    |0 0 1 0 1|              |B6|  \\n\\n\\n\\nand you know t1. Then you need to solve\\n\\n\\n\\n    |1 0 0 0|     |n1|     |B1| - 1 t1\\n\\n    |1 0 0 1|     |n2|     |B2| - 0 t1\\n\\n    |0 1 0 0|  X  |n3|  =  |B3| - 1 t1\\n\\n    |0 1 0 1|     |t2|     |B4| - 0 t1\\n\\n    |0 0 1 0|              |B5| - 1 t1\\n\\n    |0 0 1 1|              |B6| - 0 t1 \\n\\n\\n\\nso that basically you:\\n\\n\\n\\n- remove the 4th column from the matrix\\n\\n \\n\\n- subtract the right-hand-side by this 4th column multipled by t1\\n\\n\\n\\n- remove t1 as a variable\\n\\n\\n\\nOnce you have the appropriate matrices, just call [`numpy.linalg.solve`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.solve.html) (or something similar). I suggest that you don\\'t concern yourself with whether you\\'re \"doing least squares\", or whether it\\'s unique or not. Let `linalg.solve` find the optimal solution (in the L2 sense); if the solution is unique, then it is unique in the L2 sense as well.',\n",
       "  '<python><numpy><system><equations>',\n",
       "  datetime.date(2015, 7, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2276.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1316',\n",
       "  '31435414',\n",
       "  'Answer',\n",
       "  'sort pandas value_counts() primarily by descending counts and secondarily by ascending values',\n",
       "  \"The output of value_counts is a series itself (just like the input), so you have available all of the standard sorting options as with any series.  For example:\\n\\n\\n\\n    df = pd.DataFrame({ 'fruit':['apples']*5  + ['peaches']*5 + ['bananas']*3 +\\n\\n                                ['carrots']*3 + ['apricots'] })\\n\\n\\n\\n    df.fruit.value_counts().reset_index().sort([0,'index'],ascending=[False,True])\\n\\n \\n\\n          index  0\\n\\n    0    apples  5\\n\\n    1   peaches  5\\n\\n    2   bananas  3\\n\\n    3   carrots  3\\n\\n    4  apricots  1\\n\\n\\n\\nI'm actually getting the same results by default so here's a test with `ascending=[False,False]` to demonstrate that this is actually working as suggested.\\n\\n\\n\\n    df.fruit.value_counts().reset_index().sort([0,'index'],ascending=[False,False])\\n\\n \\n\\n          index  0\\n\\n    1   peaches  5\\n\\n    0    apples  5\\n\\n    3   carrots  3\\n\\n    2   bananas  3\\n\\n    4  apricots  1\\n\\n\\n\\nI'm actually a bit confused about exactly what desired output here in terms of ascending vs descending, but regardless, there are 4 possible combos here and you can get it however you like by altering the `ascending` keyword argument.\",\n",
       "  '<python-3.x><pandas>',\n",
       "  datetime.date(2015, 7, 15),\n",
       "  '2015-07-15 16:20:47',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '6646.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1317',\n",
       "  '35044845',\n",
       "  'Answer',\n",
       "  'How to plot multiple Seaborn Jointplot in Subplot',\n",
       "  'It can not be easily done without hacking.  `jointplot` calls `JointGrid` method, which in turn creates a new `figure` object every time it is called. \\n\\n\\n\\nTherefore, the hack is to make two jointplots (`JG1` `JG2`), then make a new figure, then migrate the axes objects from `JG1` `JG2` to the new figure created.\\n\\n\\n\\nFinally, we adjust the sizes and the positions of subplots in the new figure we just created. \\n\\n\\n\\n    JG1 = sns.jointplot(\"C1\", \"C2\", data=df, kind=\\'reg\\')\\n\\n    JG2 = sns.jointplot(\"C1\", \"C2\", data=df, kind=\\'kde\\')\\n\\n    \\n\\n    #subplots migration\\n\\n    f = plt.figure()\\n\\n    for J in [JG1, JG2]:\\n\\n        for A in J.fig.axes:\\n\\n            f._axstack.add(f._make_key(A), A)\\n\\n\\n\\n    #subplots size adjustment\\n\\n    f.axes[0].set_position([0.05, 0.05, 0.4,  0.4])\\n\\n    f.axes[1].set_position([0.05, 0.45, 0.4,  0.05])\\n\\n    f.axes[2].set_position([0.45, 0.05, 0.05, 0.4])\\n\\n    f.axes[3].set_position([0.55, 0.05, 0.4,  0.4])\\n\\n    f.axes[4].set_position([0.55, 0.45, 0.4,  0.05])\\n\\n    f.axes[5].set_position([0.95, 0.05, 0.05, 0.4])\\n\\n\\n\\nIt is a hack because we are now using `_axstack` and `_add_key` private methods, which might and might not stay the same as they are now in `matplotlib` future versions.\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Bm6gQ.png',\n",
       "  '<python><python-3.x><pandas><matplotlib><seaborn>',\n",
       "  datetime.date(2016, 1, 27),\n",
       "  '2016-09-22 23:46:56',\n",
       "  'CT Zhu (2487184)',\n",
       "  '22',\n",
       "  '',\n",
       "  '13735.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1320',\n",
       "  '35490484',\n",
       "  'Answer',\n",
       "  \"'colon' and 'auto' in for loop c++? need some help understanding the syntax\",\n",
       "  'The \"new\" `for` loop simply iterates over all the elements of `deviceList`. In each iteration of the body of the loop, `ioDev` is a const reference to each of the elments of `deviceList`, successively.\\n\\n\\n\\nAs to the type of `ioDev`: it is of type `Device *const &`, as you can see in the following:\\n\\n\\n\\n    #include <vector>                                                                                                                                                                                            \\n\\n    #include <type_traits>\\n\\n\\n\\n\\n\\n    using namespace std;\\n\\n\\n\\n\\n\\n    int main()\\n\\n    {\\n\\n        vector<int *> v;\\n\\n\\n\\n        for(const auto &r: v)\\n\\n        {\\n\\n            static_assert(is_same<decltype(r), int *const &>::value, \"wrong type\");\\n\\n        }\\n\\n    }\\n\\n',\n",
       "  '<c++><c++11>',\n",
       "  datetime.date(2016, 2, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '12376.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1324',\n",
       "  '35079666',\n",
       "  'Answer',\n",
       "  'How to create a DOT file in Python?',\n",
       "  \"You might consider [pygraphviz](https://pygraphviz.github.io/examples.html).\\n\\n\\n\\n    >>> import pygraphviz as pgv\\n\\n    >>> G=pgv.AGraph()\\n\\n    >>> G.add_node('a')\\n\\n    >>> G.add_edge('b','c')\\n\\n    >>> G\\n\\n    strict graph {\\n\\n            a;\\n\\n            b -- c;\\n\\n    }\\n\\n\\n\\nI disagree with @MatteoItalia's comment (perhaps it's a matter of taste). You should become familiar with available packages for your task. You start off with simple graphs, and don't see a reason to use a (very simple) package. At some point, your graphs' complexity might grow, but you'll keep on rolling your own solution to something readily available.\",\n",
       "  '<python><graphviz><dot>',\n",
       "  datetime.date(2016, 1, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '2238.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1328',\n",
       "  '31089478',\n",
       "  'Answer',\n",
       "  'Force Python to release objects to free up memory',\n",
       "  \"You have some measure of control over this stuff using the [`gc`](https://docs.python.org/2/library/gc.html) module. Specifically, you might try incorporating\\n\\n\\n\\n    gc.collect() \\n\\n\\n\\nin your loop's body.\",\n",
       "  '<python><memory>',\n",
       "  datetime.date(2015, 6, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '6495.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1330',\n",
       "  '31093331',\n",
       "  'Answer',\n",
       "  'Python routine to extract linear independent rows from a rank deficient matrix',\n",
       "  'The [Gram Schmidt process](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) finds a basis (equivalently largest independent subset) using linear combinations, and the [QR Decomposition](https://en.wikipedia.org/wiki/QR_decomposition) effectively mimics this.\\n\\n\\n\\nTherefore, one way to do what you want is to apply [`numpy.linalg.qr`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.linalg.qr.html) to the transpose, and check the non-zero components of the *R* matrix. The corresponding columns (in the transpose matrix, i.e., the rows in your original matrix) are independent.\\n\\n\\n\\n---------------------------------\\n\\n\\n\\n**Edit** After some searching, I believe [this Berkeley lecture](https://inst.eecs.berkeley.edu/~ee127a/book/login/l_mats_qr.html) explains it, but here are examples\\n\\n\\n\\n    import numpy as np\\n\\n\\n\\n    # 2nd column is redundant\\n\\n    a = np.array([[1, 0, 0], [0, 0, 0], [1, 0, 1]])\\n\\n    >> np.linalg.qr(a)[1] # 2nd row empty\\n\\n    array([[ 1.41421356,  0.        ,  0.70710678],\\n\\n       [ 0.        ,  0.        ,  0.        ],\\n\\n       [ 0.        ,  0.        ,  0.70710678]])\\n\\n\\n\\n    # 3rd column is redundant\\n\\n    a = np.array([[1, 0, 0], [1, 0, 1], [0, 0, 0], ])\\n\\n    >> np.linalg.qr(a)[1] # 3rd row empty\\n\\n    array([[ 1.41421356,  0.        ,  0.70710678],\\n\\n       [ 0.        ,  0.        , -0.70710678],\\n\\n       [ 0.        ,  0.        ,  0.        ]])\\n\\n\\n\\n    # No column redundant\\n\\n    a = np.array([[1, 0, 0], [1, 0, 1], [2, 3, 4], ])\\n\\n    >> np.linalg.qr(a)[1] # No row empty\\n\\n    array([[ 2.44948974,  2.44948974,  3.67423461],\\n\\n       [ 0.        ,  1.73205081,  1.73205081],\\n\\n       [ 0.        ,  0.        ,  0.70710678]])\\n\\n',\n",
       "  '<python><matrix><linear>',\n",
       "  datetime.date(2015, 6, 27),\n",
       "  '2015-06-27 21:09:58',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '5',\n",
       "  '',\n",
       "  '3031.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1331',\n",
       "  '32254386',\n",
       "  'Answer',\n",
       "  'Optimization with Python (scipy.optimize)',\n",
       "  \"Sometimes, numerical optimizer doesn't work for whatever reason. We can parametrize the problem slightly different and it will just work. (and might work faster)\\n\\n\\n\\nFor example, for bounds of `(0,1)`, we can have a transform function such that values in `(-inf, +inf)`, after being transformed, will end up in `(0,1)`\\n\\n\\n\\nWe can do a similar trick with the equality constraints. For example, we can reduce the dimension from 3 to 2, since the last element in `x` has to be `1-sum(x)`.\\n\\n\\n\\nIf it still won't work, we can switch to a optimizer that dose not require information from derivative, such as `Nelder Mead`.\\n\\n\\n\\nAnd also there is [Lagrange multiplier](https://en.wikipedia.org/wiki/Lagrange_multiplier).\\n\\n\\n\\n    In [111]:\\n\\n\\n\\n    def trans_x(x):\\n\\n        x1 = x**2/(1+x**2)\\n\\n        z  = np.hstack((x1, 1-sum(x1)))\\n\\n        return z\\n\\n\\n\\n    def F(x, y, gamma = 0.2):\\n\\n        z = trans_x(x)\\n\\n        return -(((z/y)**gamma).sum())**(1./gamma)\\n\\n    In [112]:\\n\\n\\n\\n    opt = minimize(F, np.array([0., 1.]), args=(np.array(y),),\\n\\n                   method='Nelder-Mead')\\n\\n    opt\\n\\n    Out[112]:\\n\\n      status: 0\\n\\n        nfev: 96\\n\\n     success: True\\n\\n         fun: -265.27701747828007\\n\\n           x: array([ 0.6463264,  0.7094782])\\n\\n     message: 'Optimization terminated successfully.'\\n\\n         nit: 52\\n\\n\\n\\nThe result is:\\n\\n\\n\\n    In [113]:\\n\\n\\n\\n    trans_x(opt.x)\\n\\n    Out[113]:\\n\\n    array([ 0.29465097,  0.33482303,  0.37052601])\\n\\n\\n\\nAnd we can visualize it, with:\\n\\n\\n\\n    In [114]:\\n\\n\\n\\n    x1 = np.linspace(0,1)\\n\\n    y1 = np.linspace(0,1)\\n\\n    X,Y = np.meshgrid(x1,y1)\\n\\n    Z = np.array([F(item, y) for item \\n\\n                  in np.vstack((X.ravel(), Y.ravel())).T]).reshape((len(x1), -1), order='F')\\n\\n    Z = np.fliplr(Z)\\n\\n    Z = np.flipud(Z)\\n\\n    plt.contourf(X, Y, Z, 50)\\n\\n    plt.colorbar()\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/z87FS.png\",\n",
       "  '<python><optimization><scipy><mathematical-optimization>',\n",
       "  datetime.date(2015, 8, 27),\n",
       "  '2015-08-27 16:20:35',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1236.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1332',\n",
       "  '32263409',\n",
       "  'Answer',\n",
       "  \"Cython: Buffer type mismatch, expected 'int' but got 'long'\",\n",
       "  \"You are using Cython's `int` type, which is just `C` `int`. I think on Mac (or most architectures) it is int 32-bit. See [wiki](https://en.wikipedia.org/wiki/Integer_%28computer_science%29) or [intel](https://software.intel.com/en-us/articles/size-of-long-integer-type-on-different-architecture-and-os) or https://stackoverflow.com/questions/2331751/does-the-size-of-an-int-depend-on-the-compiler-and-or-processor\\n\\n\\n\\nOn the other hand, `long` means int64. `dtype='int'` or `dtype=np.int` are all equivalent to `np.int64`.\\n\\n\\n\\nI think you might just explicitly define it as one of the `numpy` type:\\n\\n\\n\\n    cimport numpy as np\\n\\n    import numpy as np\\n\\n    cdef myfunction(np.ndarray[np.int64_t, ndim=1] y):\\n\\n         #do something\\n\\n         pass\\n\\n\\n\\nThat way it reads more clear and there will not be any confusion later.\\n\\n\\n\\n*EDIT*\\n\\n\\n\\nThe newer [memoryviews](http://docs.cython.org/src/userguide/memoryviews.html) syntax would be like this:\\n\\n\\n\\n    cdef myfunction(double[:] y):\\n\\n        #do something with y\\n\\n        pass\",\n",
       "  '<python><numpy><cython><memoryview>',\n",
       "  datetime.date(2015, 8, 28),\n",
       "  '2017-05-23 11:54:59',\n",
       "  'CT Zhu (2487184), URL Rewriter Bot (n/a)',\n",
       "  '12',\n",
       "  '',\n",
       "  '5824.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1333',\n",
       "  '35530378',\n",
       "  'Answer',\n",
       "  'How to replace values in a range in a pandas dataframe with another value in the same dataframe based on a condition',\n",
       "  \"Not sure how to do it in Pandas per se, but it's not that difficult if you drop down to numpy.\\n\\n\\n\\n-----------------\\n\\n\\n\\nIf you're lucky enough so that your entire DataFrame is numerical, you can do so as follows:\\n\\n\\n\\n    import numpy as np\\n\\n\\n\\n    m = df.as_matrix()\\n\\n    >>> pd.DataFrame(\\n\\n        np.where(np.logical_or(np.isnan(m), m > 0), np.tile(m[:, [4]], 5), m), \\n\\n        columns=df.columns)\\n\\n        A \\tB \\tC \\tD \\tcolumn_with_value_I_want\\n\\n    0 \\t22 \\t22 \\t22 \\t22 \\t22\\n\\n    1 \\t0 \\t0 \\t0 \\t0 \\t15\\n\\n    2 \\t0 \\t0 \\t0 \\t0 \\t90\\n\\n    3 \\t10 \\t10 \\t10 \\t10 \\t10\\n\\n    4 \\t0 \\t0 \\t0 \\t0 \\tNaN\\n\\n    5 \\t0 \\t557 \\t557 \\t0 \\t557\\n\\n\\n\\n-------------\\n\\n\\n\\n* `as_matrix` converts a DataFrame to a numpy `array`.\\n\\n* `np.where` is `numpy`'s ternary conditional.\\n\\n* `np.logical_or` is `numpy`'s or.\\n\\n* `np.isnan` is a check if a value is not `nan`.\\n\\n* `np.tile` tiles (in this case) a 2d single column to a matrix.\\n\\n\\n\\n-----------------\\n\\n\\n\\nUnfortunately, the above will fail if some of your columns (even those not involved in this operation) are inherently non-numerical. In this case, you can do the following:\\n\\n\\n\\n    for col in ['A', 'B', 'C', 'D']:\\n\\n        df[col] = np.where(df[col] > 0, df[col], df.column_with_value_I_want)\\n\\n\\n\\nwhich will work as long as the 5 relevant columns are numerical.\\n\\n\\n\\nThis uses a loop (which is frowned upon in numerical Python), but at least it does so over columns, and not rows. Assuming your data is longer than wider, it should be OK.\\n\\n\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2016, 2, 20),\n",
       "  '2016-02-21 00:12:32',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1484.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1338',\n",
       "  '35172262',\n",
       "  'Answer',\n",
       "  'std::thread --- no matching constructor',\n",
       "  \"Not exactly sure without all the details, but it is striking that you're passing `thread_launcher` as an argument. This is not a function, but rather a *function template*.\\n\\n\\n\\nConsider the following:\\n\\n\\n\\n    template<typename T>\\n\\n    void foo(T)\\n\\n    {\\n\\n\\n\\n    }\\n\\n\\n\\n\\n\\n    int main()\\n\\n    {\\n\\n        thread t(foo<int>, 3);\\n\\n        return 0;\\n\\n    }\\n\\n\\n\\nBy me, this builds, but when I change it to \\n\\n\\n\\n    thread t(foo, 3);\\n\\n\\n\\nit fails to build.\\n\\n\\n\\nBy you, then, you might want to change things to\\n\\n\\n\\n    thread(\\n\\n         thread_launcher<decltype(begin(copy))>, \\n\\n         begin(copy),\\n\\n         end(copy), \\n\\n         sort_type)\\n\\n\\n\\n\",\n",
       "  '<c++><multithreading><c++11>',\n",
       "  datetime.date(2016, 2, 3),\n",
       "  '2016-02-03 08:50:37',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1307.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1340',\n",
       "  '35205413',\n",
       "  'Answer',\n",
       "  'Numpy array slicing using commas',\n",
       "  \"It might pay to explore the `shape` and individual entries as we go along.\\n\\n\\n\\nLet's start with\\n\\n\\n\\n    >>> a = np.array([1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16])\\n\\n    >>> a.shape\\n\\n    (16, )\\n\\n\\n\\nThis is a one-dimensional array of length 16.\\n\\n\\n\\nNow let's try\\n\\n\\n\\n    >>> a = a.reshape(2,2,2,2)\\n\\n    >>> a.shape\\n\\n    (2, 2, 2, 2)\\n\\n\\n\\nIt's a multi-dimensional array with 4 dimensions.\\n\\n\\n\\nLet's see the 0, 1 element:\\n\\n\\n\\n    >>> a[0, 1]\\n\\n    array([[5, 6],\\n\\n       [7, 8]])\\n\\n\\n\\nSince there are two dimensions left, it's a matrix of two dimensions.\\n\\n\\n\\n--------------\\n\\n\\n\\nNow `a[:, 1]` says: take `a[i, 1` for all possible values of `i`:\\n\\n\\n\\n    >>> a[:, 1]\\n\\n    array([[[ 5,  6],\\n\\n        [ 7,  8]],\\n\\n\\n\\n       [[13, 14],\\n\\n        [15, 16]]])\\n\\n\\n\\n\\n\\nIt gives you an array where the first item is `a[0, 1]`, and the second item is `a[1, 1]`.\\n\\n\",\n",
       "  '<python><arrays><numpy>',\n",
       "  datetime.date(2016, 2, 4),\n",
       "  '2016-02-04 16:09:35',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2670.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1346',\n",
       "  '30513726',\n",
       "  'Answer',\n",
       "  'Get mapping of categorical variables in pandas',\n",
       "  \"You can create a dictionary mapping by enumerating (similar to creating a dictionary from a list by creating dictionary keys from the list indices):\\n\\n\\n\\n    >>> dict( enumerate(df['x'].cat.categories) )\\n\\n\\n\\n    {0: 'bad', 1: 'good', 2: 'great'}\\n\\n\\n\\nTo verify that this works, just print out the underlying integer codes like this:\\n\\n\\n\\n    >>> df['x'].cat.codes\\n\\n\\n\\n    0    1\\n\\n    1    0\\n\\n    2    1\\n\\n    3    2\\n\\n    dtype: int8\\n\\n\\n\\nOr do a roundtrip from category to integer to string to category:\\n\\n\\n\\n    >>> df['x'].cat.codes.map( dict( enumerate(df['x'].cat.categories) ) ).astype('category')\\n\\n\\n\\nFor general info on pandas categorical variables see the [official documentation][1]\\n\\n\\n\\n\\n\\n  [1]: https://pandas.pydata.org/pandas-docs/stable/categorical.html\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 5, 28),\n",
       "  '2017-08-25 18:14:57',\n",
       "  'JohnE (3877338)',\n",
       "  '33',\n",
       "  '',\n",
       "  '17126.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1361',\n",
       "  '35608701',\n",
       "  'Answer',\n",
       "  'Using numpy to build an array of all combinations of two arrays',\n",
       "  \"***In newer version of `numpy` (>1.8.x), `np.meshgrid` provides a much faster implementation:***\\n\\n\\n\\n@pv's solution\\n\\n\\n\\n    In [113]:\\n\\n\\n\\n    %timeit cartesian(([1, 2, 3], [4, 5], [6, 7]))\\n\\n    10000 loops, best of 3: 135 µs per loop\\n\\n    In [114]:\\n\\n\\n\\n    cartesian(([1, 2, 3], [4, 5], [6, 7]))\\n\\n\\n\\n    Out[114]:\\n\\n    array([[1, 4, 6],\\n\\n           [1, 4, 7],\\n\\n           [1, 5, 6],\\n\\n           [1, 5, 7],\\n\\n           [2, 4, 6],\\n\\n           [2, 4, 7],\\n\\n           [2, 5, 6],\\n\\n           [2, 5, 7],\\n\\n           [3, 4, 6],\\n\\n           [3, 4, 7],\\n\\n           [3, 5, 6],\\n\\n           [3, 5, 7]])\\n\\n\\n\\n`numpy.meshgrid` use to be 2D only, now it is capable of ND. In this case, 3D:\\n\\n\\n\\n    In [115]:\\n\\n\\n\\n    %timeit np.array(np.meshgrid([1, 2, 3], [4, 5], [6, 7])).T.reshape(-1,3)\\n\\n    10000 loops, best of 3: 74.1 µs per loop\\n\\n    In [116]:\\n\\n\\n\\n    np.array(np.meshgrid([1, 2, 3], [4, 5], [6, 7])).T.reshape(-1,3)\\n\\n\\n\\n    Out[116]:\\n\\n    array([[1, 4, 6],\\n\\n           [1, 5, 6],\\n\\n           [2, 4, 6],\\n\\n           [2, 5, 6],\\n\\n           [3, 4, 6],\\n\\n           [3, 5, 6],\\n\\n           [1, 4, 7],\\n\\n           [1, 5, 7],\\n\\n           [2, 4, 7],\\n\\n           [2, 5, 7],\\n\\n           [3, 4, 7],\\n\\n           [3, 5, 7]])\\n\\n\\n\\nNote that the order of the final resultant is slightly different.\",\n",
       "  '<python><arrays><multidimensional-array><numpy>',\n",
       "  datetime.date(2016, 2, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '69',\n",
       "  '',\n",
       "  '78906.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1364',\n",
       "  '35687517',\n",
       "  'Answer',\n",
       "  'Expressing pandas subset using pipe',\n",
       "  \"As long as you can categorize a step as something that returns a DataFrame, and takes a DataFrame (with possibly more arguments), then you can use `pipe`. Whether there's an advantage to doing so, is another question.\\n\\n\\n\\nHere, e.g., you can use\\n\\n\\n\\n    df\\\\\\n\\n        .pipe(lambda df_, x, y: df_[(df_.a >= x) & (df_.b <= y)], 2, 8)\\\\\\n\\n        .pipe(lambda df_: df_.groupby(df_.x))\\\\\\n\\n        .mean()\\n\\n\\n\\nNotice how the first stage is a lambda that takes 3 arguments, with the 2 and 8 passed as parameters. That's not the only way to do so - it is equivalent to \\n\\n\\n\\n        .pipe(lambda df_: df_[(df_.a >= 2) & (df_.b <= 8)])\\\\\\n\\n\\n\\n\\n\\nAlso note that you can use\\n\\n\\n\\n    df\\\\\\n\\n        .pipe(lambda df_, x, y: df[(df.a >= x) & (df.b <= y)], 2, 8)\\\\\\n\\n        .groupby('x')\\\\\\n\\n        .mean()\\n\\n\\n\\nHere the lambda takes `df_`, but operates on `df`, and the second `pipe` has been replaced with a `groupby`.\\n\\n\\n\\n* The first change works here, but is gragile. It *happens* to work since this is the *first* pipe stage. If it would be a later stage, it might take a DataFrame with one dimension, and attempt to filter it on a mask with another dimension, for example.\\n\\n\\n\\n* The second change is fine. In face, I think it is more readable. Basically, anything that takes a DataFrame and returns one, can be either be called directly or through `pipe`.\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><pandas><pipe>',\n",
       "  datetime.date(2016, 2, 28),\n",
       "  '2016-02-28 20:06:05',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1796.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1370',\n",
       "  '31184496',\n",
       "  'Answer',\n",
       "  'pandas group by and summing over integer and timedelta',\n",
       "  \"By default, when you `groupby`-`sum` a DataFrame, pandas doesn't assume that you want to do so for all the columns that are not of the classic numeric types. If you'd have a column of strings, it wouldn't try to apply the sum to them too.\\n\\n\\n\\nHowever, since, as you pointed out, you can force the sum for this column, how about\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    grouped = df.groupby('key')\\n\\n    pd.concat([grouped.sum(), grouped.val2.sum()], axis=1) \\n\\n\\n\\nNote that you're not repeating the expensive `groupby` op itself.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 7, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2370.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1376',\n",
       "  '35605951',\n",
       "  'Answer',\n",
       "  'Python: count number of elements in list for if condition',\n",
       "  'In the specific instances you presented\\n\\n\\n\\n    [i for i in x if 60 < i < 70]\\n\\n\\n\\nactually generates a brand-new list, then takes its `len`. Conversely, \\n\\n\\n\\n    (1 for i in x if 60 < i < 70)\\n\\n\\n\\nis a [generator expression](https://www.python.org/dev/peps/pep-0289/) over which you take a `sum`. \\n\\n\\n\\nFor large enough relevant items, the second version will be more efficient (esp. in terms of memory).\\n\\n\\n\\n\\n\\n----------------------\\n\\n\\n\\n**Timings**\\n\\n\\n\\n    x = [65] * 9999999\\n\\n\\n\\n    %%time\\n\\n\\n\\n    len([i for i in x if 60 < i < 70])\\n\\n\\n\\n    CPU times: user 724 ms, sys: 44 ms, total: 768 ms\\n\\n    Wall time: 768 ms\\n\\n    Out[7]:\\n\\n    9999999\\n\\n\\n\\n    %%time\\n\\n\\n\\n    sum(1 for i in x if 60 < i < 70)\\n\\n    CPU times: user 592 ms, sys: 0 ns, total: 592 ms\\n\\n    Wall time: 593 ms',\n",
       "  '<python>',\n",
       "  datetime.date(2016, 2, 24),\n",
       "  '2016-02-24 15:24:24',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2768.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1378',\n",
       "  '32335083',\n",
       "  'Answer',\n",
       "  'linear regression for timeseries python (numpy or pandas)',\n",
       "  \"You could convert the datetime to days in the following way.\\n\\n\\n\\n    data['days_since'] = (data.date - pd.to_datetime('2003-02-25') ).astype('timedelta64[D]')\\n\\n\\n\\n            date  days_since\\n\\n    0 2003-02-25           0\\n\\n    1 2003-03-18          21\\n\\n    2 2003-03-31          34\\n\\n\\n\\nNow you should be able to regress as you did above.\\n\\n\\n\\n    slope, intercept, r_value, p_value, std_err = stats.linregress(data.days_since, \\n\\n                                                                   data.TotP)\\n\\n    slope, intercept\\n\\n    (0.1466591166477916, 13.977916194790488)\\n\\n\\n\\nYou might also want to consider other regression options such as the [statsmodels][1] package, especially if you'll be doing this sort of thing very often.  (Note that x and y are reversed compared to linregress)\\n\\n\\n\\n    import statsmodels.formula.api as smf\\n\\n\\n\\n    smf.ols( 'TotP ~ days_since', data=data ).fit().params\\n\\n\\n\\n    Intercept     13.977916\\n\\n    days_since     0.146659\\n\\n\\n\\nThat's just a fraction of the statsmodels output btw (use `summary()` instead of `params` to get the extra output.\\n\\n\\n\\n\\n\\n  [1]: http://www.statsmodels.org/stable/index.html\",\n",
       "  '<python><numpy><pandas><statsmodels>',\n",
       "  datetime.date(2015, 9, 1),\n",
       "  '2017-01-24 16:03:58',\n",
       "  'JohnE (3877338)',\n",
       "  '9',\n",
       "  '',\n",
       "  '6278.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1380',\n",
       "  '32948830',\n",
       "  'Answer',\n",
       "  'Compute percentile for pandas dataframe row based on previous years data',\n",
       "  \"Set up with a small sample dataframe:\\n\\n\\n\\n    np.random.seed(1234)\\n\\n    df = pd.DataFrame({ 'jd':  np.tile([1,2],3),\\n\\n                        'yr':  np.repeat([2008,2009,2010],2),\\n\\n                        'val': np.random.randn(6) })\\n\\n\\n\\nThen it's just one line:\\n\\n\\n\\n    df['pctile'] = df.groupby('jd')['val'].rank(pct=True)\\n\\n    \\n\\nHere's the output, sorted with `sort_values(['jd','val'])`\\n\\n\\n\\n       jd       val    yr    pctile\\n\\n    4   1 -0.720589  2010  0.333333\\n\\n    0   1  0.471435  2008  0.666667\\n\\n    2   1  1.432707  2009  1.000000\\n\\n    1   2 -1.190976  2008  0.333333\\n\\n    3   2 -0.312652  2009  0.666667\\n\\n    5   2  0.887163  2010  1.000000\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 10, 5),\n",
       "  '2017-07-03 17:18:30',\n",
       "  'JohnE (3877338)',\n",
       "  '7',\n",
       "  '',\n",
       "  '4670.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1382',\n",
       "  '35754349',\n",
       "  'Answer',\n",
       "  'Summing Two DataFrames by Index',\n",
       "  \"You can concatenate by row, fill missing values by 0, and sum by row:\\n\\n\\n\\n    >>> pd.concat([df1, df2], axis=1).fillna(0).sum(axis=1)\\n\\n    1    1\\n\\n    2    2\\n\\n    3    2\\n\\n    4    2\\n\\n    5    2\\n\\n    6    1\\n\\n    dtype: float64\\n\\n\\n\\nIf you want it as a DataFrame, simply do\\n\\n\\n\\n    pd.DataFrame({\\n\\n        'A': pd.concat([df1, df2], axis=1).fillna(0).sum(axis=1)})\\n\\n\\n\\n(Also, note that if you need to do this just for specific *Series* `A`, Just use \\n\\n\\n\\n    pd.concat([df1.A, df2.A], axis=1).fillna(0).sum(axis=1)\\n\\n\\n\\n)\",\n",
       "  '<python><dataframe>',\n",
       "  datetime.date(2016, 3, 2),\n",
       "  '2016-03-02 18:44:58',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1141.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1392',\n",
       "  '35648726',\n",
       "  'Answer',\n",
       "  \"Does ipython notebook 'run all cells' execute simultaneously or in sequence?\",\n",
       "  'It is very easy to see that it runs the cells sequentially. Simply change one cell to have something like\\n\\n\\n\\n    i = 39882\\n\\n\\n\\nand in the following cell place\\n\\n\\n\\n    print i\\n\\n\\n\\nIf the cells would run concurrently, the latter cell would not print the value modified in the previous cell.',\n",
       "  '<python><ipython><jupyter-notebook>',\n",
       "  datetime.date(2016, 2, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '2049.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1404',\n",
       "  '35962226',\n",
       "  'Answer',\n",
       "  'Given a singly linked list, group all odd nodes together followed by the even nodes',\n",
       "  \"Say you start with a singly-linked list. It will typically have a head and tail pointers (left and right slanted here), and each link points to the next one. \\n\\n\\n\\nIn the following diagram, the even nodes are blue, and the odd ones are grey.\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nOne way to solve this problem would be to hold 6 pointers:\\n\\n\\n\\n* A pointer to the link you're currently processing in the original list, and a pointer to the tail of the original list.\\n\\n* A pointer to the head and tail of the even nodes you've already processed.\\n\\n* A pointer to the head and tail of the odd nodes you've already processed.\\n\\n\\n\\n(This is still *O(1)*.)\\n\\n\\n\\nIn the diagram above, the original list (what's left of it) is to the right. The processed even-nodes list is in the top left, and the processed odd-nodes list is in the bottom left.\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\nNow, while you still have unprocessed nodes in the original list, move the current node to either the even or odd list, as necessary. At the end, just make the tail link of one point to the head node of the other. Set the head and tail pointers of the original list be this resulting list, and you're done.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/icNBs.jpg\\n\\n  [2]: http://i.stack.imgur.com/KBsuP.jpg\",\n",
       "  '<java><c++><algorithm><linked-list>',\n",
       "  datetime.date(2016, 3, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1073.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1411',\n",
       "  '32997939',\n",
       "  'Answer',\n",
       "  'The difference between using fstream constructor and open function',\n",
       "  \"There are no differences in terms of the state of the objects following your two snippets.\\n\\n\\n\\nWhy are there two versions?\\n\\n\\n\\n1. The ctor exists in order to create `fstream` objects that are immediately  associated with a stream.\\n\\n\\n\\n2. The `open` exists because these types of objects cannot be copied. Hence you cannot assign an `fstream` object to a different stream by writing:\\n\\n        \\n\\n        fstream foo('bblskd');\\n\\n        // ...\\n\\n        foo = fstream('skdjf');\\n\\n\\n\\n(Note that this interface was devised before move semantics).\\n\\n\\n\\n--------------------------\\n\\n\\n\\nYou can find the default open mode [here](http://www.cplusplus.com/reference/fstream/fstream/fstream/).\",\n",
       "  '<c++><io><fstream>',\n",
       "  datetime.date(2015, 10, 7),\n",
       "  '2015-10-07 16:49:12',\n",
       "  'Lightness Races in Orbit (560648)',\n",
       "  '8',\n",
       "  '',\n",
       "  '2704.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1414',\n",
       "  '35711221',\n",
       "  'Answer',\n",
       "  'Extract dictionary value from column in data frame',\n",
       "  'If you `apply` a `Series`, you get a quite nice `DataFrame`:\\n\\n\\n\\n    >>> df.dic.apply(pn.Series)\\n\\n        Feature1\\tFeature2\\tFeature3\\n\\n    0\\taa1\\tbb1\\tcc2\\n\\n    1\\taa2\\tbb2\\tNaN\\n\\n    2\\taa1\\tcc1\\tNaN\\n\\n\\n\\nFrom this point, you can just use regular pandas operations.',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 2, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '10973.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1415',\n",
       "  '35740512',\n",
       "  'Answer',\n",
       "  'flask restful: how to give multiple arguments to get methhod of any Api using flask restful?',\n",
       "  \"From modifying my own code, something like this should work.\\n\\n\\n\\nStart by defining a class like this:\\n\\n\\n\\n    class DBAccessor(Resource):                                                                                                                      \\n\\n        def put(self, ques_no, i):      \\n\\n            ...                             \\n\\n            return 'ok'                             \\n\\n\\n\\nNow connect it like this:\\n\\n\\n\\n    app = Flask('wasp')                                                          \\n\\n    api = Api(app)                                                                  \\n\\n                                                                                    \\n\\n    api.add_resource(                                                               \\n\\n        DBAccessor,                                                \\n\\n        '/option/<ques_no>/<i>')            \",\n",
       "  '<python><flask-restful>',\n",
       "  datetime.date(2016, 3, 2),\n",
       "  '2016-03-02 08:20:06',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1329.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1419',\n",
       "  '35986913',\n",
       "  'Answer',\n",
       "  'Linear Regression with positive coefficients in Python',\n",
       "  \"IIUC, this is a problem which can be solved by the [`scipy.optimize.nnls`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.nnls.html), which can do non-negative least squares.\\n\\n\\n\\n> Solve argmin_x || Ax - b ||_2 for x>=0.\\n\\n\\n\\nIn your case, *b* is the *y*, *A* is the *X*, and *x* is the *&beta;* (coefficients), but, otherwise, it's the same, no?\",\n",
       "  '<python><machine-learning><scikit-learn><linear-regression>',\n",
       "  datetime.date(2016, 3, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1762.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1420',\n",
       "  '35989527',\n",
       "  'Answer',\n",
       "  'Scipy: distance correlation is higher than 1',\n",
       "  \"I don't see why this is a problem according to the documentation.\\n\\n\\n\\nFrom the [documentation](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.correlation.html):\\n\\n\\n\\n> The correlation distance between u and v, is defined as *1 - \\\\frac{(u - \\\\bar{u}) \\\\cdot (v - \\\\bar{v})}\\n\\n        {{||(u - \\\\bar{u})||}_2 {||(v - \\\\bar{v})||}_2}*\\n\\n\\n\\nBy the [Cauchy-Schwarz Inequality](https://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality), the expression following the minus sign has an *absolute value that is at most 1*. There is nothing stipulating that it won't be negative, though - in fact, this will happen if the (mean normalized) vectors are anticorrelated.\\n\\n\\n\\nAFAICT, you should be surprised if you'd get a value larger than 2 or smaller than 0. Using the comment by @Cleb and the fact that the range is [0, 2], I'm guessing that some other packages simply define the distance as *half* this expression.\",\n",
       "  '<python><python-2.7><scipy><correlation>',\n",
       "  datetime.date(2016, 3, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1749.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1421',\n",
       "  '36043040',\n",
       "  'Answer',\n",
       "  'How to shift a column in Pandas DataFrame without losing value',\n",
       "  \"Use `loc` to add a new blank row to the DataFrame, then perform the shift.\\n\\n\\n\\n    df.loc[max(df.index)+1, :] = None\\n\\n    df.x2 = df.x2.shift(1)\\n\\n\\n\\nThe code above assumes that your index is integer based, which is the pandas default.  If you're using a non-integer based index, replace `max(df.index)+1` with whatever you want the new last index to be.\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 3, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '2030.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1425',\n",
       "  '30676044',\n",
       "  'Answer',\n",
       "  'How to solve this convex optimization?',\n",
       "  'This is a problem in [Quadratic Programming](http://en.wikipedia.org/wiki/Quadratic_programming). \\n\\n\\n\\n- Python - [CVXOPT](http://cvxopt.org/)\\n\\n\\n\\n- Matlab - [quadprog](http://www.mathworks.com/help/optim/ug/quadprog.html)',\n",
       "  '<scipy><convex-optimization><convex><quadratic-programming>',\n",
       "  datetime.date(2015, 6, 5),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1065.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1431',\n",
       "  '31283068',\n",
       "  'Answer',\n",
       "  'Get total number of hours from a Pandas Timedelta?',\n",
       "  \"Just find out how many `timedelta`s of 1 hour fit into it:\\n\\n\\n\\n    import numpy as np\\n\\n\\n\\n    >> td / np.timedelta64(1, 'h')\\n\\n    26.0\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 7, 8),\n",
       "  '2015-07-08 03:40:22',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '28',\n",
       "  '',\n",
       "  '13291.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1434',\n",
       "  '31303616',\n",
       "  'Answer',\n",
       "  'Python pandas dataframe group by based on a condition',\n",
       "  \"The grouped result is a regular DataFrame, so just filter the results as usual:\\n\\n\\n\\n     import pandas as pd\\n\\n\\n\\n     df = pd.DataFrame({'a': ['a', 'b', 'a', 'a', 'b', 'c', 'd']})\\n\\n     after = df.groupby('a').size()\\n\\n     >> after\\n\\n     a\\n\\n     a    3\\n\\n     b    2\\n\\n     c    1\\n\\n     d    1\\n\\n     dtype: int64\\n\\n\\n\\n     >> after[after > 2]\\n\\n     a\\n\\n     a    3\\n\\n     dtype: int64\",\n",
       "  '<python><pandas><group-by><condition><dataframe>',\n",
       "  datetime.date(2015, 7, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '17',\n",
       "  '',\n",
       "  '19963.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1435',\n",
       "  '36047836',\n",
       "  'Answer',\n",
       "  'how to speed-up a very slow pandas apply function?',\n",
       "  \"Nothing really fancy here, just tweaked in a couple of places.  There is really no need to put in a function, so I didn't.  On this tiny sample data, it's about twice as fast as the original.\\n\\n\\n\\n    reshaped.sort_values(by=['trader', 'stock','day'], inplace=True)\\n\\n    reshaped['cumq']=reshaped.groupby(['trader', 'stock']).delta.cumsum()\\n\\n    reshaped.loc[ reshaped.cumq == 0, '_spell' ] = 1\\n\\n    reshaped['_spell'] = reshaped.groupby(['trader','stock'])['_spell'].cumsum()\\n\\n    reshaped['_spell'] = reshaped.groupby(['trader','stock'])['_spell'].bfill().fillna(0)\\n\\n\\n\\nResult:\\n\\n\\n\\n       day  delta  out stock trader  cumq  _spell\\n\\n    0    0     10    1     a      a    10     1.0\\n\\n    1    1    -10    1     a      a     0     1.0\\n\\n    2    2     15    2     a      a    15     2.0\\n\\n    3    4    -10    2     a      a     5     2.0\\n\\n    4    5     -5    2     a      a     0     2.0\\n\\n    5   10      5    0     a      a     5     0.0\\n\\n    6    1      0    1     b      a     0     1.0\",\n",
       "  '<python><performance><pandas>',\n",
       "  datetime.date(2016, 3, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '3082.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1442',\n",
       "  '32256720',\n",
       "  'Answer',\n",
       "  'How to control the cell size of a pyplot pcolor heatmap?',\n",
       "  'Just 2 more lines:\\n\\n\\n\\n    axis.set_aspect(\\'equal\\') # X scale matches Y scale\\n\\n    plt.colorbar(mappable=heatmap) # Tells plt where it should find the color info.\\n\\n\\n\\nCan\\'t answer your final question very well. Part of it is due to we have two branches of doing things in `matplotlib`: the axis way (`axis.do_something`...) and the `MATLAB` clone way `plt.some_plot_method`.  Unfortunately we can\\'t change that, and it is a good feature for people to migrate into `matplotlib`.  As far as the \"Clean way\" is concerned, I prefer to use whatever produces the shorter code.  I guess that is inline with `Python` motto: *Simple is better than complex* and *Readability counts*.\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/huG8S.png',\n",
       "  '<python><matplotlib><heatmap>',\n",
       "  datetime.date(2015, 8, 27),\n",
       "  '2015-08-27 18:23:17',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2246.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1445',\n",
       "  '36272972',\n",
       "  'Answer',\n",
       "  'Convert all numeric columns of dataframe to absolute value',\n",
       "  \"Borrowing from an answer to [this question](https://stackoverflow.com/questions/25039626/find-numeric-columns-in-pandas-python), how about selecting the columns that are numeric? \\n\\n\\n\\nSay you start with\\n\\n\\n\\n    df = pd.DataFrame({'a': ['-1', '2'], 'b': [-1, 2]})\\n\\n    >>> df        \\n\\n        a \\tb\\n\\n    0 \\t-1 \\t-1\\n\\n    1 \\t2 \\t2\\n\\n\\n\\nThen just do\\n\\n\\n\\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\\n\\n    for c in [c for c in df.columns if df[c].dtype in numerics]:\\n\\n        df[c] = df[c].abs()\\n\\n    >>> df\\n\\n        a \\tb\\n\\n    0 \\t-1 \\t1\\n\\n    1 \\t2 \\t2\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 3, 28),\n",
       "  '2017-05-23 11:59:32',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2551.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1448',\n",
       "  '35807989',\n",
       "  'Answer',\n",
       "  'select by common values in multiple pandas dataframes',\n",
       "  \"You can first find the common values:\\n\\n\\n\\n    common = \\\\\\n\\n        set.intersection(set(df1.Col1), set(df2.Col1), set(df3.Col1))\\n\\n\\n\\nThen concatenate the rows whose values are within the set of common values:\\n\\n\\n\\n    pd.concat([\\n\\n        df1[df1.Col1.isin(common)],\\n\\n        df2[df2.Col1.isin(common)],\\n\\n        df3[df3.Col1.isin(common)]]).sort_values(by='Col1')\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 3, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1020.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1449',\n",
       "  '35812074',\n",
       "  'Question',\n",
       "  'Shortest Syntax To Use numpy 1d-array As sklearn X',\n",
       "  \"I often have two `numpy` 1d arrays, `x` and `y`, and would like to perform some quick sklearn fitting + prediction using them.\\n\\n\\n\\n     import numpy as np\\n\\n     from sklearn import linear_model\\n\\n\\n\\n     # This is an example for the 1d aspect - it's obtained from something else.\\n\\n     x = np.array([1, 3, 2, ...]) \\n\\n     y = np.array([12, 32, 4, ...])\\n\\n\\n\\nNow I'd like to do something like\\n\\n\\n\\n     linear_model.LinearRegression().fit(x, y)...\\n\\n\\n\\nThe problem is that it [expects an `X` which is a 2d column array](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline.fit). For this reason, I usually feed it \\n\\n\\n\\n     x.reshape((len(x), 1))\\n\\n\\n\\nwhich I find cumbersome and hard to read. \\n\\n\\n\\nIs there some shorter way to transform a 1d array to a 2d column array (or, alternatively, get `sklearn` to accept 1d arrays)?\",\n",
       "  '<python><numpy><scikit-learn>',\n",
       "  datetime.date(2016, 3, 5),\n",
       "  '2016-03-05 10:07:51',\n",
       "  '',\n",
       "  '4',\n",
       "  '1.0',\n",
       "  '1674.0',\n",
       "  '1.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1453',\n",
       "  '34477995',\n",
       "  'Answer',\n",
       "  'Intuition for perceptron weight update rule',\n",
       "  \"The perceptron's output is the hard limit of the dot product between the instance and the weight. Let's see how this changes after the update. Since\\n\\n\\n\\n*w(t + 1) = w(t) + y(t)x(t)*,\\n\\n\\n\\nthen\\n\\n\\n\\n*x(t) &sdot; w(t + 1) = x(t) &sdot; w(t) + x(t) &sdot; (y(t) x(t)) = x(t) &sdot; w(t) + y(t) [x(t) &sdot; x(t))]*.\\n\\n\\n\\n------------------\\n\\n\\n\\nNote that:\\n\\n\\n\\n* By the algorithm's specification, the update is only applied if *x(t)* was misclassified.\\n\\n* By the definition of the dot product, *x(t) &sdot; x(t) &ge; 0*.\\n\\n\\n\\n-----------------\\n\\n\\n\\nHow does this move the boundary relative to *x(t)*?\\n\\n\\n\\n* If *x(t)* was correctly classified, then the algorithm does not apply the update rule, so nothing changes.\\n\\n* If *x(t)* was incorrectly classified as negative, then *y(t) = 1*. It follows that the new dot product increased by *x(t) &sdot; x(t)* (which is positive). The boundary moved in the right direction as far as *x(t)* is concerned, therefore. \\n\\n* Conversely, if *x(t)* was incorrectly classified as positive, then *y(t) = -1*. It follows that the new dot product decreased by *x(t) &sdot; x(t)* (which is positive). The boundary moved in the right direction as far as *x(t)* is concerned, therefore. \\n\\n\",\n",
       "  '<algorithm><machine-learning><perceptron>',\n",
       "  datetime.date(2015, 12, 27),\n",
       "  '2015-12-27 17:28:09',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '10',\n",
       "  '',\n",
       "  '4294.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1459',\n",
       "  '30768282',\n",
       "  'Answer',\n",
       "  'C++ std::vector of independent std::threads',\n",
       "  \"You need to use something like\\n\\n\\n\\n    readerThreads.push_back(move(th));\\n\\n\\n\\nThis will make ``th`` an rvalue, and cause the move ctor to be called. The copy ctor of ``thread`` was [disabled](http://www.cplusplus.com/reference/thread/thread/thread/) by design (see Anthony Williams' [C++ Concurrency In Action](http://www.manning.com/williams/)).\",\n",
       "  '<c++><multithreading><c++11><vector><stdthread>',\n",
       "  datetime.date(2015, 6, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '11',\n",
       "  '',\n",
       "  '10679.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1474',\n",
       "  '32257724',\n",
       "  'Answer',\n",
       "  'Automate standard jupyter/ipython notebook imports',\n",
       "  \"Yep, you can do it with customized startup scripts. Mines are in `/Users/user_name/.ipython/profile_default/startup`. The scripts should be `python` score files (`.py`) and the automatic imports should go there.\\n\\n\\n\\nThe doc is [here](http://ipython.org/ipython-doc/1/config/overview.html#startup-files).\\n\\n\\n\\nIf you want to have `%pylab inline` in your startup script, the script has to be stored with `.ipy` extension, to specify it is an ipython script not a regular python script. I don't think it will work right if you have the cell magics like `%pylab inline` in a regular python script.\\n\\n\\n\\n\\n\\n\",\n",
       "  '<ipython><ipython-notebook><jupyter>',\n",
       "  datetime.date(2015, 8, 27),\n",
       "  '2016-03-06 06:03:30',\n",
       "  'CT Zhu (2487184)',\n",
       "  '16',\n",
       "  '',\n",
       "  '5798.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1475',\n",
       "  '32301804',\n",
       "  'Answer',\n",
       "  'How to add legend on Seaborn facetgrid bar plot',\n",
       "  'Some how there is one legend item for each of the subplot. Looks like if we want to have legend corresponds to the bars in each of the subplot, we have to manually make them.\\n\\n\\n\\n    # Let\\'s just make a 1-by-2 plot\\n\\n    df = df.head(10)\\n\\n\\n\\n    # Initialize a grid of plots with an Axes for each walk\\n\\n    grid = sns.FacetGrid(df, col=\"walk\", hue=\"walk\", col_wrap=2, size=5,\\n\\n            aspect=1)\\n\\n\\n\\n    # Draw a bar plot to show the trajectory of each random walk\\n\\n    bp = grid.map(sns.barplot, \"step\", \"position\", palette=\"Set3\")\\n\\n\\n\\n    # The color cycles are going to all the same, doesn\\'t matter which axes we use\\n\\n    Ax = bp.axes[0]\\n\\n\\n\\n    # Some how for a plot of 5 bars, there are 6 patches, what is the 6th one?\\n\\n    Boxes = [item for item in Ax.get_children()\\n\\n             if isinstance(item, matplotlib.patches.Rectangle)][:-1]\\n\\n\\n\\n    # There is no labels, need to define the labels\\n\\n    legend_labels  = [\\'a\\', \\'b\\', \\'c\\', \\'d\\', \\'e\\']\\n\\n\\n\\n    # Create the legend patches\\n\\n    legend_patches = [matplotlib.patches.Patch(color=C, label=L) for\\n\\n                      C, L in zip([item.get_facecolor() for item in Boxes],\\n\\n                                  legend_labels)]\\n\\n\\n\\n    # Plot the legend\\n\\n    plt.legend(handles=legend_patches)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/AX5CF.png',\n",
       "  '<python><pandas><matplotlib><seaborn>',\n",
       "  datetime.date(2015, 8, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '10',\n",
       "  '',\n",
       "  '8814.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1491',\n",
       "  '36351470',\n",
       "  'Answer',\n",
       "  'Rolling up data frame along with count of rows in python',\n",
       "  \"Roman Pekar's fine answer is correct for this case. However, I saw it after trying to write a solution for the general case stated in the text of your question, not just the example with specific column names. So, for the general case, consider:\\n\\n\\n\\n    df.groupby([df[c] for c in df.columns]).size().reset_index().rename(columns={0: 'Count'})\\n\\n\\n\\nFor example:\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    df = pd.DataFrame({'Col1': ['a', 'a', 'a', 'b', 'c'], 'Value': [1, 2, 1, 3, 2]})\\n\\n\\n\\n    >>> df.groupby([df[c] for c in df.columns]).size().reset_index().rename(columns={0: 'Count'})\\n\\n        Col1 \\tValue \\tCount\\n\\n    0 \\ta \\t1 \\t2\\n\\n    1 \\ta \\t2 \\t1\\n\\n    2 \\tb \\t3 \\t1\\n\\n    3 \\tc \\t2 \\t1\",\n",
       "  '<python><pandas><dataframe><rollup>',\n",
       "  datetime.date(2016, 4, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1193.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1493',\n",
       "  '36376955',\n",
       "  'Question',\n",
       "  'Using KVO to tell when elements have been added to an array',\n",
       "  'I want to check if elements have been added to an array in swift using KVO, and I essentially copied the example from Apple\\'s documentation, but when the code runs, it does not catch when the size of the array updates.  Here is what I have now:\\n\\n\\n\\n    class ShowDirectory: NSObject {\\n\\n        var shows = [Show]()\\n\\n        dynamic var showCount = Int()\\n\\n        func updateDate(x: Int) {\\n\\n            showCount = x\\n\\n        }\\n\\n    }\\n\\n    \\n\\n    class MyObserver: NSObject {\\n\\n        var objectToObserve = ShowDirectory()\\n\\n        override init() {\\n\\n            super.init()\\n\\n            objectToObserve.addObserver(self, forKeyPath: \"showCount\", options: .New, context: &myContext)\\n\\n        }\\n\\n        \\n\\n        override func observeValueForKeyPath(keyPath: String?, ofObject object: AnyObject?, change: [String : AnyObject]?, context: UnsafeMutablePointer<Void>) {\\n\\n            if context == &myContext {\\n\\n                if let newValue = change?[NSKeyValueChangeNewKey] {\\n\\n                    print(\"\\\\(newValue) shows were added\")\\n\\n                }\\n\\n            } else {\\n\\n                super.observeValueForKeyPath(keyPath, ofObject: object, change: change, context: context)\\n\\n            }\\n\\n        }\\n\\n        \\n\\n        deinit {\\n\\n            objectToObserve.removeObserver(self, forKeyPath: \"myDate\", context: &myContext)\\n\\n        }\\n\\n    }\\n\\n\\n\\nAfter I add the shows to the array, I set showCount equal to the number of elements in the array, however, it does not print \"X shows were added\" to console.  My viewDidLoad() function simply calls the function that adds elements to the array, and nothing else at the moment.',\n",
       "  '<arrays><swift><key-value-observing>',\n",
       "  datetime.date(2016, 4, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '1.0',\n",
       "  '1516.0',\n",
       "  '1.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1496',\n",
       "  '30964709',\n",
       "  'Answer',\n",
       "  'multiply pandas dataframe column with a constant',\n",
       "  \"The problem in this case is pandas's auto alignment (ususally a good thing).  Because your 'constant' is actually in a dataframe, what pandas will try to do is create row 0 from each of the row 0s and then row 1 from each of the row 1s, but there is no row 1 in the second dataset, so you get NaN from there forward.\\n\\n\\n\\nSo what you need to do intentionally break the dataframe aspect of the second dataframe so that pandas will then 'broadcast' the constant to ALL rows.  One way to do this is with `values`, which in this case essentially just drops the index from a dataframe so that it becomes a numpy array with one element (really a scalar, but contained in a numpy array technically).  `to_list()` will also accomplish the same thing.\\n\\n\\n\\n    allcitations=pd.DataFrame({ 'ActualCitations':[54703.888410120424] })\\n\\n    \\n\\n    df['Percent'] * allcitations['ActualCitations'].values\\n\\n     \\n\\n    0    1485.374682\\n\\n    1     963.718402\\n\\n    2    1250.421481\\n\\n    3    1700.415667\\n\\n\\n\\n\",\n",
       "  '<python-2.7><pandas><dataframe>',\n",
       "  datetime.date(2015, 6, 21),\n",
       "  '2015-06-21 13:00:36',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '3692.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1498',\n",
       "  '31037243',\n",
       "  'Answer',\n",
       "  'Pandas data frame to dictionary with two keys from index and columns',\n",
       "  \"You simply need to [`stack`](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.stack.html) and then call [`to_dict`](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.to_dict.html):\\n\\n\\n\\n    import pandas as pd\\n\\n    df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\\n\\n    >> df\\n\\n     \\ta \\tb\\n\\n    0 \\t1 \\t3\\n\\n    1 \\t2 \\t4\\n\\n\\n\\n    >> df.stack().to_dict()\\n\\n    {(0, 'a'): 1, (0, 'b'): 3, (1, 'a'): 2, (1, 'b'): 4}\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 6, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1185.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1509',\n",
       "  '32358752',\n",
       "  'Answer',\n",
       "  'Pandas scatterplot categorical and timeseries axes',\n",
       "  \"I am not sure you can do this with `.plot` method. However, it is easy to do it straightly in `matplotlib`:\\n\\n\\n\\n    plt.plot(tst.index, tst, marker='|', lw=0, ms=10)\\n\\n    plt.ylim([-0.5, 5.5])\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/LEJ12.png\",\n",
       "  '<pandas><matplotlib><scatter-plot><categorical-data><timeserieschart>',\n",
       "  datetime.date(2015, 9, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1930.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1511',\n",
       "  '32414455',\n",
       "  'Answer',\n",
       "  'Count multiple letters in string Python',\n",
       "  'Use regular expression:\\n\\n\\n\\n    >>> import re\\n\\n    >>> len(re.findall(\\'[lo]\\', \"hello world\"))\\n\\n    5\\n\\n\\n\\nor `map`:\\n\\n\\n\\n    >>> sum(map(s.count, [\\'l\\',\\'o\\']))\\n\\n    5',\n",
       "  '<python><string><count>',\n",
       "  datetime.date(2015, 9, 5),\n",
       "  '2015-09-05 17:08:43',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '3729.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1512',\n",
       "  '33288683',\n",
       "  'Answer',\n",
       "  'Extracting URL parameters into Pandas DataFrame',\n",
       "  \"There is a [`urlparse`][1] library that I will recommend, the benefit of this approach is that you don't need to know the field names of the query in advance (`'param1'` etc.):\\n\\n\\n\\n    In [278]:\\n\\n\\n\\n    import urlparse\\n\\n    In [279]:\\n\\n\\n\\n    T = ['http://example.com/?param1=apple&param2=tomato&param3=carrot',\\n\\n         'http://sample.com/?param1=banana&param3=potato&param4=berry',\\n\\n         'http://example.org/?param2=apple&param3=tomato&param4=carrot']\\n\\n    In [280]:\\n\\n\\n\\n    df = pd.concat(map(lambda x: pd.DataFrame(urlparse.parse_qs(urlparse.urlparse(x).query)), T))\\n\\n    print df\\n\\n    #df['URL'] = T : add another column with the original URL's\\n\\n       param1  param2  param3  param4\\n\\n    0   apple  tomato  carrot     NaN\\n\\n    0  banana     NaN  potato   berry\\n\\n    0     NaN   apple  tomato  carrot\\n\\n\\n\\n\\n\\n  [1]: https://docs.python.org/2/library/urlparse.html\",\n",
       "  '<python><pandas><urlparse>',\n",
       "  datetime.date(2015, 10, 22),\n",
       "  '2015-10-22 20:22:24',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1305.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1520',\n",
       "  '36232190',\n",
       "  'Answer',\n",
       "  'return type of operator+= while overloading',\n",
       "  \"As @Ant already pointed out, it *can* be chained, but it's not the only consideration. Consider \\n\\n\\n\\n    cout << (a += b);\\n\\n\\n\\nfor example - this won't work if there is no return.\\n\\n\\n\\nThe arithmetic operators themselves are nothing more than a human convention. Technically, you can even have `+=` do `-=` - it will build and possibly run for you (as long as you follow your new private conventions). Within C++, you should follow the conventions of the language: the clients of your code will expect that `+=` self increments, and that \\n\\n\\n\\n    cout << (a += b);\\n\\n\\n\\nwill print out the result.\",\n",
       "  '<c++><c++11><operator-overloading>',\n",
       "  datetime.date(2016, 3, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1612.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1524',\n",
       "  '31050930',\n",
       "  'Answer',\n",
       "  \"pandas 'as_index' function doesn't work as expected\",\n",
       "  \"I believe that, irrespective of the `groupby` operation you've done, you just need to call [`reset_index`](http://pandas.pydata.org/pandas-docs/dev/generated/pandas.DataFrame.reset_index.html) to say that the index column should just be a regular column.\\n\\n\\n\\nStarting with a mockup of your data:\\n\\n\\n\\n    import pandas as pd\\n\\n    calls = pd.DataFrame({\\n\\n        'agent': ['orange', 'red'],\\n\\n        'phone_number': [2234, 1478],\\n\\n        'call_outcome': [2234, 1478],\\n\\n    })\\n\\n    >> calls\\n\\n     \\tagent \\tcall_outcome \\tphone_number\\n\\n    0 \\torange \\t2234 \\t2234\\n\\n    1 \\tred \\t1478 \\t1478\\n\\n\\n\\nhere is the operation you did with `reset_index()` appended:\\n\\n\\n\\n    >> calls.groupby('agent').count().sort('phone_number', ascending=False).reset_index()\\n\\n     \\tagent \\tcall_outcome \\tphone_number\\n\\n    0 \\torange \\t1 \\t1\\n\\n    1 \\tred \\t1 \\t1\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 6, 25),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1514.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1532',\n",
       "  '35062974',\n",
       "  'Answer',\n",
       "  'How can I analyze a confusion matrix?',\n",
       "  'IIUC, your question is undefined. \"False positives\", \"true negatives\" - these are terms that are defined only for binary classification. Read more about the definition of a [confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix).\\n\\n\\n\\nIn this case, the confusion matrix is of dimension *N X N*. Each diagonal represents, for entry *(i, i)* the case where the prediction is *i* and the outcome is *i* too. Any other off-diagonal entry indicates some mistake where the prediction was *i* and the outcome is *j*. There is no meaning to \"positive\" and \"negative in this case. \\n\\n\\n\\nYou can find the diagnoal elements easily using [`np.diagonal`](http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.diagonal.html), and, following that, it is easy to sum them. The sum of wrong cases is the sum of the matrix minus the sum of the diagonal. ',\n",
       "  '<python><matrix><scikit-learn><confusion-matrix>',\n",
       "  datetime.date(2016, 1, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '2806.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1535',\n",
       "  '36296150',\n",
       "  'Answer',\n",
       "  'Find The Longest Consecutive Subsequence Of Distinct Items In A Sequence',\n",
       "  'Suppose you use two data structures:\\n\\n\\n\\n* a `std::list` of integers\\n\\n\\n\\n* a `std::unordered_map` mapping integers into iterators of the list\\n\\n\\n\\nNow operate as follows. When you encounter the next integer, check if it is in the unordered map. \\n\\n\\n\\n* If it is, find the corresponding iterator in the list, and remove it and all items left of it (from both list and unordered map)\\n\\n\\n\\n* If it is not, add it to the list, and set the unordered map to point at it\\n\\n\\n\\nFurthermore, at each step, track both the step index, and the size of the unordered map (or the list). Output the index of the largest size, at the end.\\n\\n\\n\\nThis algorithm is [amortized](https://en.wikipedia.org/wiki/Amortized_analysis) expected linear time: although there might be a step that removes more than a single list item (from each of the data structures), each item enters and leaves them at most once.\\n\\n\\n\\n\\n\\n----------------------------\\n\\n\\n\\n**Example**\\n\\n\\n\\nHere is an example of the two data structures at some step. The grey map references items in the blue linked list.\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/X2YR2.jpg\\n\\n\\n\\nThe distinct length is now 4. If 2 is now encountered, however, then it and the link(s) left of it (in this case 3) will be purged before it is added, and the length will be reduced to 3.',\n",
       "  '<c++><algorithm><data-structures><data-stream>',\n",
       "  datetime.date(2016, 3, 29),\n",
       "  '2016-03-29 22:28:08',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1252.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1541',\n",
       "  '37364796',\n",
       "  'Answer',\n",
       "  'Pandas: rolling mean by time interval',\n",
       "  \"This example seems to call for a weighted mean as suggested in @andyhayden's comment.  For example, there are two polls on 10/25 and one each on 10/26 and 10/27.  If you just resample and then take the mean, this effectively gives twice as much weighting to the polls on 10/26 and 10/27 compared to the ones on 10/25.\\n\\n\\n\\nTo give equal weight to each **poll** rather than equal weight to each **day**, you could do something like the following.\\n\\n\\n\\n    >>> wt = df.resample('D',limit=5).count()\\n\\n\\n\\n                favorable  unfavorable  other\\n\\n    enddate                                  \\n\\n    2012-10-25          2            2      2\\n\\n    2012-10-26          1            1      1\\n\\n    2012-10-27          1            1      1\\n\\n\\n\\n    >>> df2 = df.resample('D').mean()\\n\\n\\n\\n                favorable  unfavorable  other\\n\\n    enddate                                  \\n\\n    2012-10-25      0.495        0.485  0.025\\n\\n    2012-10-26      0.560        0.400  0.040\\n\\n    2012-10-27      0.510        0.470  0.020\\n\\n\\n\\nThat gives you the raw ingredients for doing a poll-based mean instead of a day-based mean.  As before, the polls are averaged on 10/25, but the weight for 10/25 is also stored and is double the weight on 10/26 or 10/27 to reflect that two polls were taken on 10/25.\\n\\n\\n\\n    >>> df3 = df2 * wt\\n\\n    >>> df3 = df3.rolling(3,min_periods=1).sum()\\n\\n    >>> wt3 = wt.rolling(3,min_periods=1).sum()\\n\\n\\n\\n    >>> df3 = df3 / wt3  \\n\\n\\n\\n                favorable  unfavorable     other\\n\\n    enddate                                     \\n\\n    2012-10-25   0.495000     0.485000  0.025000\\n\\n    2012-10-26   0.516667     0.456667  0.030000\\n\\n    2012-10-27   0.515000     0.460000  0.027500\\n\\n    2012-10-28   0.496667     0.465000  0.041667\\n\\n    2012-10-29   0.484000     0.478000  0.042000\\n\\n    2012-10-30   0.488000     0.474000  0.042000\\n\\n    2012-10-31   0.530000     0.450000  0.020000\\n\\n    2012-11-01   0.500000     0.465000  0.035000\\n\\n    2012-11-02   0.490000     0.470000  0.040000\\n\\n    2012-11-03   0.490000     0.465000  0.045000\\n\\n    2012-11-04   0.500000     0.448333  0.035000\\n\\n    2012-11-05   0.501429     0.450000  0.032857\\n\\n    2012-11-06   0.503333     0.450000  0.028333\\n\\n    2012-11-07   0.510000     0.435000  0.010000\\n\\n\\n\\nNote that the rolling mean for 10/27 is now 0.51500 (poll-weighted) rather than 52.1667 (day-weighted).\\n\\n\\n\\nAlso note that there have been changes to the APIs for `resample` and `rolling` as of version 0.18.0.\\n\\n\\n\\n[rolling (what's new in pandas 0.18.0)][1]\\n\\n\\n\\n[resample (what's new in pandas 0.18.0)][2]\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/version/0.18.0/whatsnew.html#window-functions-are-now-methods\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/version/0.18.0/whatsnew.html#resample-api\",\n",
       "  '<python><pandas><time-series>',\n",
       "  datetime.date(2016, 5, 21),\n",
       "  '2016-05-25 13:00:40',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '70854.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1542',\n",
       "  '37390422',\n",
       "  'Answer',\n",
       "  'Face recognition using raspberry pi, pi camera,python and Open Cv',\n",
       "  \"You can reformat it as follows and see if you are getting any frames.\\n\\n\\n\\n    import cv2.cv as cv\\n\\n            \\n\\n    cv.NamedWindow('w1', cv.CV_WINDOW_AUTOSIZE)\\n\\n    camera_index = 0\\n\\n    capture = cv.CaptureFromCAM(camera_index)\\n\\n\\n\\n    def repeat():\\n\\n        global capture #declare as globals since we are assigning to them now\\n\\n        global camera_index\\n\\n        frame = cv.QueryFrame(capture)\\n\\n        if frame:\\n\\n            cv.ShowImage('w1', frame)\\n\\n            c = cv.WaitKey(10)\\n\\n            if(c=='n'): #in “n” key is pressed while the popup window is in focus\\n\\n                camera_index += 1 #try the next camera index\\n\\n                capture = cv.CaptureFromCAM(camera_index)\\n\\n                if not capture: #if the next camera index didn’t work, reset to 0.\\n\\n                    camera_index = 0\\n\\n                    capture = cv.CaptureFromCAM(camera_index)\\n\\n\\n\\n     while True:\\n\\n         repeat()\",\n",
       "  '<python><python-2.7><opencv><raspberry-pi2><raspberry-pi3>',\n",
       "  datetime.date(2016, 5, 23),\n",
       "  '2016-05-23 13:09:21',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '3287.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1544',\n",
       "  '31061265',\n",
       "  'Answer',\n",
       "  'Pandas DataFrame to Excel Problems',\n",
       "  \"On my CentOS, I got your same exact problem. This was easily addressed with\\n\\n\\n\\n    pip install xlsxwriter\\n\\n\\n\\n(on your system you might have to do something a bit different; nevertheless, install this package). \\n\\n\\n\\n-------------------------\\n\\n\\n\\nFollowing that, the problem changed to \\n\\n\\n\\n    AttributeError: 'list' object has no attribute 'rfind'\\n\\n\\n\\n\\n\\nHowever,\\n\\n\\n\\n    df.to_excel('data.xls', sheet_name='Sheet1', index=False, engine='xlsxwriter')\\n\\n\\n\\nworks.\\n\\n\\n\\n\",\n",
       "  '<python><excel><pandas>',\n",
       "  datetime.date(2015, 6, 25),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '6581.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1548',\n",
       "  '36481484',\n",
       "  'Answer',\n",
       "  'How can a pandas groupby .sum return wrong values?',\n",
       "  \"Do you have any `NaN` values in column A?  This can produce the behavior that you're describing, because `NaN` values get dropped when they're being grouped.  Consider the DataFrame below:\\n\\n\\n\\n         A    B     C      D\\n\\n    0    x  1.0   NaN  100.0\\n\\n    1    x  2.0  21.0  105.0\\n\\n    2    y  NaN  22.0  110.0\\n\\n    3  NaN  4.0  23.0  115.0\\n\\n    4    z  5.0  24.0  120.0\\n\\n    5    z  6.0  25.0    NaN \\n\\n\\n\\nThen `df.sum()` produces:\\n\\n\\n\\n    B     18.0\\n\\n    C    115.0\\n\\n    D    550.0\\n\\n    dtype: float64\\n\\n\\n\\nBut `df.groupby('A')['B', 'C', 'D'].sum().sum()` produces:\\n\\n\\n\\n    B     14.0\\n\\n    C     92.0\\n\\n    D    435.0\\n\\n    dtype: float64 \",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 4, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '2041.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1549',\n",
       "  '36543588',\n",
       "  'Answer',\n",
       "  \"What's the difference between predict_proba and decision_function in scikit-learn?\",\n",
       "  \"The latter, [`predict_proba`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.libsvm.predict_proba.html) is a method of a (soft) classifier outputting the probability of the instance being in each of the classes.\\n\\n\\n\\nThe former, [`decision_function`](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC.decision_function), finds the distance to the separating hyperplane. For example, a(n) SVM classifier finds hyperplanes separating the space into areas associated with classification outcomes. This function, given a point, finds the distance to the separators.\\n\\n\\n\\nI'd guess that `predict_prob` is more useful in your case, in general - the other method is more specific to the algorithm.\",\n",
       "  '<scikit-learn>',\n",
       "  datetime.date(2016, 4, 11),\n",
       "  '2016-04-11 12:00:05',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '30',\n",
       "  '',\n",
       "  '17828.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1552',\n",
       "  '37746073',\n",
       "  'Answer',\n",
       "  'Using plotly without online plotly account',\n",
       "  'Yes, it can be done. The only purpose of having a `plotly` account is to host the graphs in your `plotly` account. \\n\\n\\n\\n[`Plotly Offline`][1] allows you to create graphs offline and save them locally. Instead of saving the graphs to a server, your data and graphs will remain in your local system. \\n\\n\\n\\n[1]:https://plot.ly/python/offline/',\n",
       "  '<python><charts><plotly>',\n",
       "  datetime.date(2016, 6, 10),\n",
       "  '2016-06-10 10:44:57',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '20',\n",
       "  '',\n",
       "  '13938.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1553',\n",
       "  '37767149',\n",
       "  'Answer',\n",
       "  \"boto3 TypeError: 'NoneType' object is not iterable\",\n",
       "  'You could try doing this and check:\\n\\n\\n\\n    def tag_to_dict(ec2):\\n\\n        tag_dict = {}\\n\\n        if ec2.tags is None: \\n\\n            return \"No Value\"    ## Replace \"No Value\" with tag_dict to get an empty dict\\n\\n        for tag in ec2.tags:\\n\\n            tag_dict[tag[\\'Key\\']] = tag[\\'Value\\']  \\n\\n        return tag_dict\\n\\n\\n\\nYou end up getting a dictionary with keys as keys and tag values as values.\\n\\n\\n\\n\\n\\n----------\\n\\n**Edit:**\\n\\n\\n\\nTry and see if you can get anything here:\\n\\n\\n\\n    for i in ec2.instances.all():\\n\\n        if i.tags is None:\\n\\n            continue\\n\\n        for tag in i.tags:\\n\\n            if tag[\\'Key\\'] == \\'Function\\':\\n\\n                print(tag[\\'Value\\'])\\n\\n\\n\\nIt continues with the next iteration of the loop and ignores `NoneType` values.\\n\\n\\n\\n',\n",
       "  '<python><amazon-web-services><amazon-ec2><boto>',\n",
       "  datetime.date(2016, 6, 11),\n",
       "  '2016-06-15 12:21:39',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1084.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1554',\n",
       "  '34094132',\n",
       "  'Answer',\n",
       "  'Matplotlib, horizontal bar chart (barh) is upside-down',\n",
       "  \"I will consider this to be a bug, i.e., the y position of the bars are not assigned correctly. The patch is however relatively simple:\\n\\n\\n\\nThis is only one right order of bars, and that is called..., the right order.  Anything that is not the right order, is thus a buggy order.  :p\\n\\n\\n\\n    In [63]:\\n\\n\\n\\n    print df\\n\\n          Total_beef_cattle  Total_dairy_cattle  Total_sheep  Total_deer  \\\\\\n\\n    1994           0.000000            0.000000     0.000000    0.000000   \\n\\n    2002         -11.025827           34.444950   -20.002034   33.858009   \\n\\n    2003          -8.344764           32.882482   -20.041908   37.229441   \\n\\n    2004         -11.895128           34.207998   -20.609926   42.707754   \\n\\n    2005         -12.366101           32.506699   -19.379727   38.499840   \\n\\n\\n\\n          Total_pigs  Total_horses  \\n\\n    1994    0.000000      0.000000  \\n\\n    2002  -19.100637     11.811093  \\n\\n    2003  -10.766476     18.504488  \\n\\n    2004   -8.072078     13.376472  \\n\\n    2005  -19.230733   -100.000000  \\n\\n    In [64]:\\n\\n\\n\\n    ax = df.plot(kind='barh', sort_columns=True)\\n\\n\\n\\n    #Get the actual bars\\n\\n    bars = [item for item in ax.get_children() if isinstance(item, matplotlib.patches.Rectangle)]\\n\\n    bars = bars[:df.size]\\n\\n\\n\\n    #Reset the y positions for each bar\\n\\n    bars_y = [plt.getp(item, 'y') for item in bars]\\n\\n    for B, Y in zip(bars, np.flipud(np.array(bars_y).reshape(df.shape[::-1])).ravel()):\\n\\n        B.set_y(Y)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/CkZ4r.png\",\n",
       "  '<python><pandas><matplotlib><bar-chart>',\n",
       "  datetime.date(2015, 12, 4),\n",
       "  '2015-12-04 17:41:42',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '13076.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1555',\n",
       "  '34208506',\n",
       "  'Answer',\n",
       "  'Formatting of DateTimeIndex in plot pandas',\n",
       "  \"If you use the `plot` method in `pandas`, the `set_major_locator` and `set_major_formatter` methods of `matplotlib` is likely to [fail][1].  It might just be easier to  manually adjust the ticks, if you want to stay with `pandas``plot` methods.\\n\\n\\n\\n    #True if it is the first month of a quarter, False otherwise\\n\\n    xtick_idx = np.hstack((True, \\n\\n                           np.diff(rets_m.index.quarter)!=0))\\n\\n\\n\\n    #Year-Quarter string for the tick labels.\\n\\n    xtick     = ['{0:d} quarter {1:d}'.format(*item) \\n\\n                 for item in zip(rets_m.index.year, rets_m.index.quarter)]\\n\\n    ax        =rets_m.plot(kind='bar')\\n\\n\\n\\n    #Only put ticks on the 1st months of each quarter\\n\\n    ax.xaxis.set_ticks(np.arange(len(xtick))[xtick_idx])\\n\\n\\n\\n    #Adjust the ticklabels\\n\\n    ax.xaxis.set_ticklabels(np.array(xtick)[xtick_idx])\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/questions/24665990/time-series-plotting-inconsistencies-in-pandas/24690145#24690145\\n\\n  [2]: http://i.stack.imgur.com/JO9S0.png\",\n",
       "  '<date><numpy><pandas><matplotlib><format>',\n",
       "  datetime.date(2015, 12, 10),\n",
       "  '2017-05-23 12:33:06',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1049.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1561',\n",
       "  '31124983',\n",
       "  'Answer',\n",
       "  'pyplot x-axis being sorted',\n",
       "  \"If you use `.plot` method of `pandas.DataFrame`, just grab the resultant `axis` and `set_xticklables`:\\n\\n\\n\\n    a = pd.DataFrame({'Bike ID': [5454, 3432, 4432, 3314],\\n\\n                      'Number of Uses': [11, 23, 5, 9]})\\n\\n    a.sort(columns='Number of Uses', inplace=True)\\n\\n    ax = a.plot(y='Number of Uses', kind='bar')\\n\\n    _ = ax.set_xticklabels(a['Bike ID'])\\n\\n\\n\\n![enter image description here][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/5BBkO.png\",\n",
       "  '<python><csv><python-3.x><matplotlib>',\n",
       "  datetime.date(2015, 6, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '11425.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1564',\n",
       "  '31194933',\n",
       "  'Answer',\n",
       "  'Use Multiple Character Delimiter in Python Pandas read_csv',\n",
       "  'As Padraic Cunningham writes in the comment above, it\\'s unclear why you want this. The [Wiki entry for the CSV Spec](https://en.wikipedia.org/wiki/Comma-separated_values#Specification) states about delimiters:\\n\\n\\n\\n> ... separated by delimiters (typically a single reserved character such as comma, semicolon, or tab; sometimes the delimiter may include optional spaces),\\n\\n\\n\\nIt\\'s unsurprisingly that both the [`csv` module](https://docs.python.org/2/library/csv.html) and pandas don\\'t support what you\\'re asking. \\n\\n\\n\\nHowever, if you really want to do so, you\\'re pretty much down to using Python\\'s string manipulations. The following example shows how to turn the dataframe to a \"csv\" with `$$` separating lines, and `%%` separating columns.\\n\\n\\n\\n    \\'$$\\'.join(\\'%%\\'.join(str(r) for r in rec) for rec in df.to_records())\\n\\n\\n\\nOf course, you don\\'t have to turn it into a string like this prior to writing it into a file.',\n",
       "  '<python><python-2.7><csv><pandas>',\n",
       "  datetime.date(2015, 7, 2),\n",
       "  '2015-07-02 21:38:39',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '4954.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1566',\n",
       "  '37890629',\n",
       "  'Answer',\n",
       "  'How to include end date in pandas date_range method?',\n",
       "  \"You can use [`.union`][1] to add the next logical value after initializing the `date_range`.  It should work as written for any frequency:\\n\\n\\n\\n    d = pd.date_range('2016-01', '2016-05', freq='M')\\n\\n    d = d.union([d[-1] + 1]).strftime('%Y-%m')\\n\\n\\n\\nAlternatively, you can use [`period_range`][2] instead of `date_range`.  Depending on what you intend to do, this might not be the right thing to use, but it satisfies your question:\\n\\n\\n\\n    pd.period_range('2016-01', '2016-05', freq='M').strftime('%Y-%m')\\n\\n\\n\\nIn either case, the resulting output is as expected:\\n\\n\\n\\n    ['2016-01' '2016-02' '2016-03' '2016-04' '2016-05']\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DatetimeIndex.union.html#pandas.DatetimeIndex.union\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.period_range.html\",\n",
       "  '<python><string><datetime><pandas><range>',\n",
       "  datetime.date(2016, 6, 17),\n",
       "  '2016-06-17 22:08:03',\n",
       "  'root (3339965)',\n",
       "  '5',\n",
       "  '',\n",
       "  '2092.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1568',\n",
       "  '38208600',\n",
       "  'Answer',\n",
       "  'How to stream in and manipulate a large data file in python',\n",
       "  \"You can use [`dask.dataframe`][1], which is syntactically similar to `pandas`, but performs manipulations out-of-core, so memory shouldn't be an issue:\\n\\n\\n\\n    import dask.dataframe as dd\\n\\n    \\n\\n    df = dd.read_csv('my_file.csv')\\n\\n    df = df.groupby('Geography')['Count'].sum().to_frame()\\n\\n    df.to_csv('my_output.csv')\\n\\n\\n\\nAlternatively, if `pandas` is a requirement you can use chunked reads, as mentioned by @chrisaycock.  You may want to experiment with the `chunksize` parameter.\\n\\n\\n\\n    # Operate on chunks.\\n\\n    data = []\\n\\n    for chunk in pd.read_csv('my_file.csv', chunksize=10**5):\\n\\n        chunk = chunk.groupby('Geography', as_index=False)['Count'].sum()\\n\\n        data.append(chunk)\\n\\n    \\n\\n    # Combine the chunked data.\\n\\n    df = pd.concat(data, ignore_index=True)\\n\\n    df = df.groupby('Geography')['Count'].sum().to_frame()\\n\\n    df.to_csv('my_output.csv')\\n\\n\\n\\n  [1]: http://dask.pydata.org/en/latest/dataframe.html\",\n",
       "  '<python><pandas><dataframe><itertools>',\n",
       "  datetime.date(2016, 7, 5),\n",
       "  '2016-07-05 16:56:44',\n",
       "  'root (3339965)',\n",
       "  '10',\n",
       "  '',\n",
       "  '6733.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1578',\n",
       "  '36998498',\n",
       "  'Question',\n",
       "  'Python script to store Json files in real time to amazon S3',\n",
       "  'I have a python code that gives me tweets in real time using Twitter Streaming API. I have stored the output to a json file which keeps on updating dynamically as new tweets arrive.However, I would like to save this json to amazon **s3** which I could use to trigger events using amazon lambda service.Can somebody suggest me a way to solve this problem?',\n",
       "  '<python><amazon-web-services><amazon-s3><aws-lambda>',\n",
       "  datetime.date(2016, 5, 3),\n",
       "  '2018-12-12 12:46:21',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '0',\n",
       "  '2.0',\n",
       "  '4937.0',\n",
       "  '1.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1581',\n",
       "  '32105754',\n",
       "  'Answer',\n",
       "  'pandas dataframe reshaping/stacking of multiple value variables into seperate columns',\n",
       "  \"This might be a shorter approach:\\n\\n\\n\\n    [72]:\\n\\n\\n\\n    df.columns = pd.MultiIndex.from_tuples(map(lambda x: (x[:-1], x), df.columns))\\n\\n    In [73]:\\n\\n\\n\\n    print pd.DataFrame({key:df[key].stack().values for key in set(df.columns.get_level_values(0))},\\n\\n                       index = df['des'].stack().index.get_level_values(0))\\n\\n          des interval\\n\\n    value             \\n\\n    aaa     a      ##1\\n\\n    aaa     b      ##2\\n\\n    aaa     c      ##3\\n\\n    bbb     d      ##4\\n\\n    bbb     e      ##5\\n\\n    bbb     f      ##6\\n\\n    ccc     g      ##7\\n\\n    ccc     h      ##8\\n\\n    ccc     i      ##9\\n\\n\\n\\nOr preserve the 1,2,3 info:\\n\\n\\n\\n    [73]:\\n\\n\\n\\n    df.columns = pd.MultiIndex.from_tuples(map(lambda x: (x[:-1], x[-1]), df.columns))\\n\\n    Keys = set(df.columns.get_level_values(0))\\n\\n    df2  = pd.concat([df[key].stack() for key in Keys], axis=1)\\n\\n    df2.columns = Keys\\n\\n    print df2\\n\\n            des interval\\n\\n    value               \\n\\n    aaa   1   a      ##1\\n\\n          2   b      ##2\\n\\n          3   c      ##3\\n\\n    bbb   1   d      ##4\\n\\n          2   e      ##5\\n\\n          3   f      ##6\\n\\n    ccc   1   g      ##7\\n\\n          2   h      ##8\\n\\n          3   i      ##9\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 8, 19),\n",
       "  '2015-08-19 21:29:53',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1647.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1582',\n",
       "  '32149940',\n",
       "  'Answer',\n",
       "  'TypeError: cannot concatenate a non-NDFrame object, when time series mungling',\n",
       "  \"I think you want to `agg` (aggregate), not `apply`, as for each of your group, you want 1 returning value:\\n\\n\\n\\n    In [185]:\\n\\n\\n\\n    print ts.groupby(pd.TimeGrouper(freq='10Min')).agg(my_func)\\n\\n                          latitude  longitude      speed\\n\\n    2014-10-20 15:20:00  36.567360  36.567360  36.567360\\n\\n    2014-10-20 15:30:00  36.567232  36.567232  36.567232\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 8, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '21709.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1589',\n",
       "  '38254895',\n",
       "  'Answer',\n",
       "  'how to append a row from one dataframe to a new dataframe',\n",
       "  \"IIUC, `append` shouldn't be necessary for what you're trying to do.  You should be able to do it with [boolean indexing][1]:\\n\\n\\n\\n    dfnew = df1[df1.datecompare == yesterday].copy()\\n\\n\\n\\nIn general, iterating over a DataFrame will be much slower than doing a vectorized operation like what I've done above.\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing\",\n",
       "  '<python><pandas><dataframe><append>',\n",
       "  datetime.date(2016, 7, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1492.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1593',\n",
       "  '38413780',\n",
       "  'Answer',\n",
       "  'What is the difference between sklearn LabelEncoder and pd.get_dummies?',\n",
       "  'These are just convenience functions falling naturally into the way these two libraries tend to do things, respectively. The first one \"condenses\" the information by changing things to integers, and the second one \"expands\" the dimensions allowing (possibly) more convenient access.\\n\\n\\n\\n-----------------\\n\\n\\n\\n[`sklearn.preprocessing.LabelEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) simply transforms data, from whatever domain, so that its domain is *0, ..., k - 1*, where *k* is the number of classes.\\n\\n\\n\\nSo, for example\\n\\n\\n\\n    [\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]\\n\\n\\n\\ncould become\\n\\n\\n\\n    [0, 0, 1, 2]\\n\\n\\n\\n-----------------\\n\\n\\n\\n[`pandas.get_dummies`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html) also takes a Series with elements from some domain, but expands it into a DataFrame whose columns correspond to the entries in the series, and the values are 0 or 1 depending on what they originally were. So, for example, the same\\n\\n\\n\\n    [\"paris\", \"paris\", \"tokyo\", \"amsterdam\"]\\n\\n\\n\\nwould become a DataFrame with labels\\n\\n\\n\\n    [\"paris\", \"tokyo\", \"amsterdam\"]\\n\\n\\n\\nand whose `\"paris\"` entry would be the series \\n\\n\\n\\n    [1, 1, 0, 0]\\n\\n\\n\\n\\n\\n------------------------------------\\n\\n\\n\\nThe main advantage of the first method is that it conserves space. Conversely, encoding things as integers might give the impression (to you or to some machine learning algorithm) that the order means something. Is \"amsterdam\" closer to \"tokyo\" than to \"paris\" just because of the integer encoding? probably not. The second representation is a bit clearer on that.',\n",
       "  '<python><pandas><scikit-learn>',\n",
       "  datetime.date(2016, 7, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '5397.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1596',\n",
       "  '32322948',\n",
       "  'Answer',\n",
       "  'Numpy matrix binarization using only one expression',\n",
       "  'We may consider [`np.where`][1]:\\n\\n\\n\\n    np.where(a>threshold, upper, lower)\\n\\n    Out[6]: \\n\\n    array([[0, 1, 1, 1],\\n\\n           [1, 1, 0, 1],\\n\\n           [0, 1, 0, 1],\\n\\n           [1, 0, 0, 1]])\\n\\n\\n\\n\\n\\n  [1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html',\n",
       "  '<python><numpy>',\n",
       "  datetime.date(2015, 9, 1),\n",
       "  '2015-09-01 03:20:34',\n",
       "  'CT Zhu (2487184)',\n",
       "  '23',\n",
       "  '',\n",
       "  '9314.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1599',\n",
       "  '32431547',\n",
       "  'Answer',\n",
       "  'Given two strings, find if they are one edit away from each other',\n",
       "  'In the dynamic programming method, frequently a matrix is used. The rows correspond to one string, and the columns to the other. The point is to find the cheapest path from the top-left cell to the bottom right. At any point, a horizontal or vertical transition stands for an insertion.\\n\\n\\n\\nYour problem is the same, but the paths are restricted. With *k* insertions/deletions, the path is restricted to be in the *k*-diagonal. Other than that, the classical DP algorithm should work. The complexity is linear.',\n",
       "  '<string><algorithm><edit-distance>',\n",
       "  datetime.date(2015, 9, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '5256.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1601',\n",
       "  '37757496',\n",
       "  'Answer',\n",
       "  'Best way to join two large datasets in Pandas',\n",
       "  'This seems like a task that [`dask`][1] was designed for.  Essentially, `dask` can do `pandas` operations out-of-core, so you can work with datasets that don\\'t fit into memory.  The `dask.dataframe` API is a subset of the `pandas` API, so there shouldn\\'t be much of a learning curve.  See the [Dask DataFrame Overview][2] page for some additional DataFrame specific details.\\n\\n\\n\\n    import dask.dataframe as dd\\n\\n    \\n\\n    # Read in the csv files.\\n\\n    df1 = dd.read_csv(\\'file1.csv\\')\\n\\n    df2 = dd.read_csv(\\'file2.csv\\')\\n\\n    \\n\\n    # Merge the csv files.\\n\\n    df = dd.merge(df1, df2, how=\\'outer\\', on=[\\'product\\',\\'version\\'])\\n\\n    \\n\\n    # Write the output.\\n\\n    df.to_csv(\\'file3.csv\\', index=False)\\n\\n\\n\\nAssuming that `\\'product\\'` and `\\'version\\'` are the only columns, it may be more efficient to replace the `merge` with:\\n\\n\\n\\n    df = dd.concat([df1, df2]).drop_duplicates()\\n\\n\\n\\nI\\'m not entirely sure if that will be better, but apparently merges that aren\\'t done on the index are \"slow-ish\" in `dask`, so it could be worth a try.\\n\\n\\n\\n\\n\\n  [1]: http://dask.pydata.org/en/latest/index.html\\n\\n  [2]: http://dask.pydata.org/en/latest/dataframe-overview.html',\n",
       "  '<python><pandas><memory-management>',\n",
       "  datetime.date(2016, 6, 10),\n",
       "  '2016-06-10 22:58:57',\n",
       "  'root (3339965)',\n",
       "  '9',\n",
       "  '',\n",
       "  '6245.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1622',\n",
       "  '35270177',\n",
       "  'Question',\n",
       "  'Passing arguments (for argparse) with unittest discover',\n",
       "  '`foo` is a Python project with deep directory nesting, including ~30 `unittest` files in various subdirectories. Within `foo`\\'s `setup.py`, I\\'ve [added a custom \"test\" command](https://stackoverflow.com/questions/1710839/custom-distutils-commands) internally running\\n\\n\\n\\n     python -m unittest discover foo \\'*test.py\\'\\n\\n\\n\\nNote that this uses [`unittest`\\'s discovery](https://docs.python.org/2/library/unittest.html#test-discovery) mode.\\n\\n\\n\\n-----\\n\\n\\n\\nSince some of the tests are extremely slow, I\\'ve recently decided that tests should have \"levels\". The answer to [this question](https://stackoverflow.com/questions/1029891/python-unittest-is-there-a-way-to-pass-command-line-options-to-the-app) explained very well how to get `unittest` and `argparse` to work well with each other. So now, I can run an *individual* unittest file, say `foo/bar/_bar_test.py`, with\\n\\n\\n\\n    python foo/bar/_bar_test.py --level=3\\n\\n\\n\\nand only level-3 tests are run.\\n\\n\\n\\nThe problem is that I can\\'t figure out how to pass the custom flag (in this case \"--level=3\" using discover. Everything I try fails, e.g.:\\n\\n\\n\\n    $ python -m unittest discover --level=3 foo \\'*test.py\\'\\n\\n    Usage: python -m unittest discover [options]\\n\\n\\n\\n    python -m unittest discover: error: no such option: --level\\n\\n\\n\\n    $ python -m --level=3 unittest discover foo \\'*test.py\\'\\n\\n    /usr/bin/python: No module named --level=3\\n\\n\\n\\nHow can I pass `--level=3` to the individual unittests? If possible, I\\'d like to avoid dividing different-level tests to different files.\\n\\n\\n\\n**Bounty Edit**\\n\\n\\n\\nThe pre-bounty (fine) solution suggests using system environment variables. This is not bad, but I\\'m looking for something cleaner. \\n\\n\\n\\nChanging the multiple-file test runner (i.e., python -m unittest discover foo \\'*test.py\\') to something else is fine, as long as: \\n\\n\\n\\n1. It allows generating a single report for multiple-file unittests. \\n\\n2. It can somehow support multiple test levels (either using the technique in the question, or using some other different mechanism).',\n",
       "  '<python><command-line><argparse><python-unittest>',\n",
       "  datetime.date(2016, 2, 8),\n",
       "  '2017-05-23 12:01:33',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '13',\n",
       "  '4.0',\n",
       "  '3187.0',\n",
       "  '3.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1629',\n",
       "  '38673651',\n",
       "  'Answer',\n",
       "  'Non deterministic Polynomial(NP) vs Polynomial(P)?',\n",
       "  'Your question touches several points.\\n\\n\\n\\nFirst, in the sense relevant to your question, the size of a problem is defined to be the size of *the representation of the problem*. So, for example, when you write about the problem of a divisor of *n*. What is the representation of *n*? It is a series of characters of length *q* (I don\\'t want to be more specific than that). In general, *n* is exponential in *q*. So when you talk about a simple loop from 1 to *n*, you\\'re talking about something that is exponential in the size of the input. For example, the number \"999999999999999\" represents the number 999999999999999. That is quite a large number, but it is represented by 12 characters here.\\n\\n\\n\\nSecond, while there is more than a single way to define the class NP, perhaps the simplest one for *decision problems* (which is the type you raise in your question, namely is something true or not) is that if the answer is true, then there is an \"certificate\" that can be verified in polynomial time. For example, consider the [Hamilton Path Problem](https://en.wikipedia.org/wiki/Hamiltonian_path_problem). This is (probably) a hard problem to solve, but, if you are given a hamilton path as an answer, it is very easy to verify that it is so; specifically, it can be done in polynomial time. For the Hamilton Path Problem, the path is a polynomial-time verifiable certificate, and therefore this problem is NP.',\n",
       "  '<algorithm><np>',\n",
       "  datetime.date(2016, 7, 30),\n",
       "  '2016-07-30 12:28:44',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1219.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1631',\n",
       "  '38446637',\n",
       "  'Answer',\n",
       "  'Filling dict with NA values to allow conversion to pandas dataframe',\n",
       "  \"Another option is to use `from_dict` with `orient='index'` and then take the tranpose:\\n\\n\\n\\n    my_dict = {'a' : [1, 2, 3, 4, 5], 'b': [1, 2, 3]}\\n\\n    df = pd.DataFrame.from_dict(my_dict, orient='index').T\\n\\n\\n\\nNote that you could run into problems with `dtype` if your columns have different types, e.g. floats in one column, strings in another.\\n\\n\\n\\nResulting output:\\n\\n\\n\\n         a    b\\n\\n    0  1.0  1.0\\n\\n    1  2.0  2.0\\n\\n    2  3.0  3.0\\n\\n    3  4.0  NaN\\n\\n    4  5.0  NaN\",\n",
       "  '<python><pandas><dictionary><dataframe><na>',\n",
       "  datetime.date(2016, 7, 18),\n",
       "  '2016-07-18 22:06:12',\n",
       "  'root (3339965)',\n",
       "  '11',\n",
       "  '',\n",
       "  '1090.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1632',\n",
       "  '38573317',\n",
       "  'Answer',\n",
       "  'Pandas - Modify string values in each cell',\n",
       "  \"Use [`str.slice_replace`][1]:\\n\\n\\n\\n    df['B'] = df['B'].str.slice_replace(1, 3, 'AAA')\\n\\n\\n\\nSample Input:\\n\\n\\n\\n       A         B\\n\\n    0  w   abcdefg\\n\\n    1  x   bbbbbbb\\n\\n    2  y   ccccccc\\n\\n    3  z  zzzzzzzz\\n\\n\\n\\nSample Output:\\n\\n\\n\\n       A          B\\n\\n    0  w   aAAAdefg\\n\\n    1  x   bAAAbbbb\\n\\n    2  y   cAAAcccc\\n\\n    3  z  zAAAzzzzz\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.slice_replace.html\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 7, 25),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1555.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1633',\n",
       "  '38597491',\n",
       "  'Answer',\n",
       "  'Pandas: groupby forward fill with datetime index',\n",
       "  \"You can add `'company'` to the index, making it unique, and do a simple `ffill` via `groupby`:\\n\\n\\n\\n    a = a.set_index('company', append=True)\\n\\n    a = a.groupby(level=1).ffill()\\n\\n\\n\\nFrom here, you can use `reset_index` to revert the index back to the just the date, if necessary.  I'd recommend keeping `'company'` as part of the the index (or just adding it to the index to begin with), so your index remains unique:\\n\\n\\n\\n    a = a.reset_index(level=1)\",\n",
       "  '<python><datetime><pandas><group-by><missing-data>',\n",
       "  datetime.date(2016, 7, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2625.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1635',\n",
       "  '31362884',\n",
       "  'Answer',\n",
       "  'Total number of chunks in pandas',\n",
       "  \"CSV, being row-based, does not allow a process to know how many lines there are in it until after it has all been scanned.\\n\\n\\n\\nVery minimal scanning is necessary, though, assuming the CSV file is well formed:\\n\\n\\n\\n    sum(1 for row in open('data.txt', 'r'))\\n\\n\\n\\nThis might prove useful in case you need to calculate in advance how many chunks there are. A full CSV reader is an overkill for this. The above line has very low memory requirements, and does minimal parsing.\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 7, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '1546.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1641',\n",
       "  '32750237',\n",
       "  'Answer',\n",
       "  \"pandas equivalent of Stata's encode\",\n",
       "  \"Stata's `encode` command starts with a string variable and creates a new integer variable with labels mapped to the original string variable.  The direct analog of this in pandas would now be the categorical variable type which became a full-fledged part of pandas starting in 0.15 (which was released after this question was originally asked and answered).\\n\\n\\n\\nSee documentation [here][1].\\n\\n\\n\\nTo demonstrate for this example, the Stata command would be something like:\\n\\n\\n\\n    encode cat, generate(cat2)\\n\\n\\n\\nwhereas the pandas command would be:\\n\\n\\n\\n    x['cat2'] = x['cat'].astype('category')\\n\\n    \\n\\n      cat  val cat2\\n\\n    0   A   10    A\\n\\n    1   A   20    A\\n\\n    2   B   30    B\\n\\n    \\n\\nJust as Stata does with `encode`, the data are stored as integers, but display as strings in the default output.\\n\\n\\n\\nYou can verify this by using the categorical accessor `cat` to see the underlying integer.  (And for that reason you probably don't want to use 'cat' as a column name.)\\n\\n\\n\\n    x['cat2'].cat.codes\\n\\n    \\n\\n    0    0\\n\\n    1    0\\n\\n    2    1\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/version/0.15.2/categorical.html\\n\\n\\n\\n\",\n",
       "  '<python><pandas><stata>',\n",
       "  datetime.date(2015, 9, 23),\n",
       "  '2015-09-23 22:33:12',\n",
       "  'JohnE (3877338), Nick Cox (1820446)',\n",
       "  '8',\n",
       "  '',\n",
       "  '1202.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1642',\n",
       "  '33239929',\n",
       "  'Answer',\n",
       "  'Seaborn: countplot() with frequencies',\n",
       "  'I think you can first set the y major ticks manually and then modify each label\\n\\n\\n\\n    dfWIM = pd.DataFrame({\\'AXLES\\': np.random.randint(3, 10, 1000)})\\n\\n    total = len(dfWIM)*1.\\n\\n    plt.figure(figsize=(12,8))\\n\\n    ax = sns.countplot(x=\"AXLES\", data=dfWIM, order=[3,4,5,6,7,8,9,10,11,12])\\n\\n    plt.title(\\'Distribution of Truck Configurations\\')\\n\\n    plt.xlabel(\\'Number of Axles\\')\\n\\n    plt.ylabel(\\'Frequency [%]\\')\\n\\n\\n\\n    for p in ax.patches:\\n\\n            ax.annotate(\\'{:.1f}%\\'.format(100*p.get_height()/total), (p.get_x()+0.1, p.get_height()+5))\\n\\n\\n\\n    #put 11 ticks (therefore 10 steps), from 0 to the total number of rows in the dataframe\\n\\n    ax.yaxis.set_ticks(np.linspace(0, total, 11))\\n\\n\\n\\n    #adjust the ticklabel to the desired format, without changing the position of the ticks. \\n\\n    _ = ax.set_yticklabels(map(\\'{:.1f}%\\'.format, 100*ax.yaxis.get_majorticklocs()/total))\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/bZsBM.png',\n",
       "  '<python><pandas><matplotlib><data-visualization><seaborn>',\n",
       "  datetime.date(2015, 10, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '25291.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1650',\n",
       "  '35315784',\n",
       "  'Answer',\n",
       "  'error: assignment of member in read-only object',\n",
       "  'Angelika Langer once wrote a piece about this: [Are Set Iterators Mutable or Immutable?](http://www.angelikalanger.com/Articles/Cuj/01.SetIterators/SetIterators.html).\\n\\n\\n\\nYou can solve this by defining the `Node` members immaterial for the `set` ordering as `mutable`:\\n\\n\\n\\n    mutable bool left, right;\\n\\n\\n\\n(see a [building version in ideone](https://ideone.com/G7ieCg).)\\n\\n\\n\\n\\n\\nPersonally, I would consider a design mapping the immutable part to mutable parts using a `map`.',\n",
       "  '<c++>',\n",
       "  datetime.date(2016, 2, 10),\n",
       "  '2016-04-05 15:30:09',\n",
       "  'Ami Tavory (3510736), anukul (4077900)',\n",
       "  '5',\n",
       "  '',\n",
       "  '2916.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1651',\n",
       "  '35318894',\n",
       "  'Answer',\n",
       "  'RuntimeWarning: invalid value encountered in arccos',\n",
       "  \"Well, if you do \\n\\n\\n\\n    np.arccos(90)\\n\\n\\n\\n(which is your first element), you'll get the same warning - simplifying your example considerably.\\n\\n\\n\\nWhy is that? The [arccos function](https://en.wikipedia.org/wiki/Inverse_trigonometric_functions#arccos) is the *x* for which *cos(x) = 90*. From basic trignometry, you can tell there's [no such value](https://en.wikipedia.org/wiki/Trigonometric_functions#cosine).\",\n",
       "  '<python><numpy><trigonometry>',\n",
       "  datetime.date(2016, 2, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '5700.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1661',\n",
       "  '35485100',\n",
       "  'Answer',\n",
       "  \"What's the time complexity of indexing a numpy array directly\",\n",
       "  'On the one hand\\n\\n\\n\\n> must be using a hash-table which will give a time complexity close to O(1). Is that right?\\n\\n\\n\\nis not quite true. Numpy `array`s are basically [contiguous blocks of homogeneous memory](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html), with some extra info on the side on dimensions and such. Therefore, the access is *O(1)*, and just involves some trivial math to determine the position within the memory. \\n\\n\\n\\nOn the other hand\\n\\n\\n\\n> indexing must be pretty efficient.\\n\\n\\n\\nis unfortunately not true at all. Asides from bounds checking (which arrays do), everything involving pure python is extremely inefficient (and accesses involve pure-python calls). Numpy array access is [no exception](http://docs.cython.org/src/tutorial/numpy.html). You should try to use vector operations whenever possible.',\n",
       "  '<python><numpy><big-o>',\n",
       "  datetime.date(2016, 2, 18),\n",
       "  '2016-02-18 15:09:06',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '6',\n",
       "  '',\n",
       "  '3249.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1662',\n",
       "  '38676483',\n",
       "  'Answer',\n",
       "  'A transition from CountVectorizer to TfidfTransformer in sklearn',\n",
       "  \"You're probably looking for a [*pipeline*](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), perhaps something like this:\\n\\n\\n\\n    pipeline = Pipeline([\\n\\n        ('vect', CountVectorizer()),\\n\\n        ('tfidf', TfidfTransformer()),\\n\\n    ])\\n\\n\\n\\nor \\n\\n\\n\\n    pipeline = make_pipeline(CountVectorizer(), TfidfTransformer())\\n\\n\\n\\nOn this pipeline, perform the regular operations (e.g., `fit`, `fit_transform`, and so forth).\\n\\n\\n\\nSee [this example](http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html#example-model-selection-grid-search-text-feature-extraction-py) also.\\n\\n\",\n",
       "  '<python><scikit-learn><vectorization><tf-idf>',\n",
       "  datetime.date(2016, 7, 30),\n",
       "  '2016-10-03 23:31:54',\n",
       "  'maxymoo (839957)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2110.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1663',\n",
       "  '38677336',\n",
       "  'Answer',\n",
       "  'Multiply numpy int and float arrays: Cannot cast ufunc multiply output from dtype',\n",
       "  \"You could use [`broadcasting`][1] to multiply the two arrays and take only the integer part as follows:\\n\\n\\n\\n    In [2]: (A*B).astype(int)\\n\\n    Out[2]: array([ 0,  4,  9, 16])\\n\\n\\n\\n**Timing Constraints:**\\n\\n\\n\\n    In [8]: %timeit (A*B).astype(int)\\n\\n    1000000 loops, best of 3: 1.65 µs per loop\\n\\n    \\n\\n    In [9]: %timeit np.multiply(A, B, out=A, casting='unsafe')\\n\\n    100000 loops, best of 3: 2.01 µs per loop\\n\\n\\n\\n[1]: http://docs.scipy.org/doc/numpy/user/basics.broadcasting.html\",\n",
       "  '<python><arrays><numpy><floating-point><int>',\n",
       "  datetime.date(2016, 7, 30),\n",
       "  '2016-07-30 18:49:23',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '7757.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1664',\n",
       "  '38709590',\n",
       "  'Answer',\n",
       "  'How to Format a Date Column in Pandas?',\n",
       "  'Use the [`.dt`][1] accessor:\\n\\n\\n\\n    df[\"Date\"] = df[\"Date\"].dt.strftime(\"%m-%d-%Y\")\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/basics.html#dt-accessor',\n",
       "  '<python><datetime><pandas><dataframe><format>',\n",
       "  datetime.date(2016, 8, 1),\n",
       "  '2016-08-01 23:37:07',\n",
       "  'root (3339965)',\n",
       "  '8',\n",
       "  '',\n",
       "  '5300.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1668',\n",
       "  '34114333',\n",
       "  'Answer',\n",
       "  'Pandas, Bar Chart Annotations',\n",
       "  'It appears your `autolabel` function is expecting a list of `patches`, sssuming your plot only those bars as its `patches`, we could do:\\n\\n\\n\\n\\n\\n    df = pd.DataFrame({\\'score\\':np.random.randn(6),\\n\\n                       \\'person\\':[x*3 for x in list(\\'ABCDEF\\')]})\\n\\n\\n\\n    def autolabel(rects):\\n\\n        x_pos = [rect.get_x() + rect.get_width()/2. for rect in rects]\\n\\n        y_pos = [rect.get_y() + 1.05*rect.get_height() for rect in rects]\\n\\n        #if height constant: hbars, vbars otherwise\\n\\n        if (np.diff([plt.getp(item, \\'width\\') for item in rects])==0).all():\\n\\n            scores = [plt.getp(item, \\'height\\') for item in rects]\\n\\n        else:\\n\\n            scores = [plt.getp(item, \\'width\\') for item in rects]\\n\\n        # attach some text labels\\n\\n        for rect, x, y, s in zip(rects, x_pos, y_pos, scores):\\n\\n            ax.text(x, \\n\\n                    y,\\n\\n                    \\'%s\\'%s,\\n\\n                    ha=\\'center\\', va=\\'bottom\\')\\n\\n\\n\\n    ax = df.set_index([\\'person\\']).plot(kind=\\'barh\\', figsize=(10,7), \\n\\n                  color=[\\'dodgerblue\\', \\'slategray\\'], fontsize=13)\\n\\n\\n\\n    ax.set_alpha(0.8)\\n\\n    ax.set_title(\"BarH\")#,fontsize=18)\\n\\n    autolabel(ax.patches)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n    ax = df.set_index([\\'person\\']).plot(kind=\\'bar\\', figsize=(10,7), \\n\\n                  color=[\\'dodgerblue\\', \\'slategray\\'], fontsize=13)\\n\\n\\n\\n    ax.set_alpha(0.8)\\n\\n    ax.set_title(\"Bar\")#,fontsize=18)\\n\\n    autolabel(ax.patches)\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/KYOZ5.png\\n\\n  [2]: http://i.stack.imgur.com/gYuus.png',\n",
       "  '<pandas><matplotlib><plot><charts>',\n",
       "  datetime.date(2015, 12, 6),\n",
       "  '2015-12-06 18:24:39',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1138.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1671',\n",
       "  '38682179',\n",
       "  'Answer',\n",
       "  'Reshape pandas dataframe from rows to columns',\n",
       "  \"Say you start by unstacking:\\n\\n\\n\\n    df2 = df2.set_index(['Name', 'Job']).unstack()\\n\\n    >>> df2\\n\\n        Job Eff Date\\n\\n    Job\\tAnalyst\\tDirector\\tManager\\n\\n    Name\\t\\t\\t\\n\\n    Jane\\t1/1/2015\\tNone\\t1/1/2016\\n\\n    Joe\\t1/1/2015\\t7/1/2016\\t1/1/2016\\n\\n    In [29]:\\n\\n\\n\\n    df2\\n\\n\\n\\nNow, to make things easier, flatten the multi-index:\\n\\n\\n\\n    df2.columns = df2.columns.get_level_values(1)\\n\\n    >>> df2\\n\\n    Job\\tAnalyst\\tDirector\\tManager\\n\\n    Name\\t\\t\\t\\n\\n    Jane\\t1/1/2015\\tNone\\t1/1/2016\\n\\n    Joe\\t1/1/2015\\t7/1/2016\\t1/1/2016\\n\\n\\n\\nNow, just manipulate the columns:\\n\\n\\n\\n    cols = []\\n\\n    for i, c in enumerate(df2.columns):\\n\\n        col = 'Job %d' % i\\n\\n        df2[col] = c\\n\\n        cols.append(col)\\n\\n        col = 'Eff Date %d' % i\\n\\n        df2[col] = df2[c]\\n\\n        cols.append(col)\\n\\n    >>> df2[cols]\\n\\n    Job\\tJob 0\\tEff Date 0\\tJob 1\\tEff Date 1\\tJob 2\\tEff Date 2\\n\\n    Name\\t\\t\\t\\t\\t\\t\\n\\n    Jane\\tAnalyst\\t1/1/2015\\tDirector\\tNone\\tManager\\t1/1/2016\\n\\n    Joe\\tAnalyst\\t1/1/2015\\tDirector\\t7/1/2016\\tManager\\t1/1/2016\\n\\n\\n\\n**Edit**\\n\\n\\n\\nJane was never a director (alas). The above code states that Jane became Director at `None` date. To change the result so that it specifies that Jane became `None` at `None` date (which is a matter of taste), replace\\n\\n\\n\\n    df2[col] = c\\n\\n\\n\\nby\\n\\n\\n\\n    df2[col] = [None if d is None else c for d in df2[c]]\\n\\n\\n\\nThis gives\\n\\n\\n\\n    Job\\tJob 0\\tEff Date 0\\tJob 1\\tEff Date 1\\tJob 2\\tEff Date 2\\n\\n    Name\\t\\t\\t\\t\\t\\t\\n\\n    Jane\\tAnalyst\\t1/1/2015\\tNone\\tNone\\tManager\\t1/1/2016\\n\\n    Joe\\tAnalyst\\t1/1/2015\\tDirector\\t7/1/2016\\tManager\\t1/1/2016\\n\\n\\n\\n\\u200b\",\n",
       "  '<python><pandas><dataframe><reshape><pandas-groupby>',\n",
       "  datetime.date(2016, 7, 31),\n",
       "  '2016-07-31 11:13:17',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1059.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1673',\n",
       "  '38716682',\n",
       "  'Answer',\n",
       "  'Scikit Learn - Extract word tokens from a string delimiter using CountVectorizer',\n",
       "  'You could split on your separator(`#`) at most once and take the first part of the split.\\n\\n\\n\\n    from sklearn.feature_extraction.text import CountVectorizer\\n\\n\\n\\n    def tokenize(text):\\n\\n        return([text.split(\\'#\\', 1)[0].strip()])\\n\\n        \\n\\n    text = [\"first ques # on stackoverflow\", \"please help\"]\\n\\n\\n\\n    vec = CountVectorizer(tokenizer=tokenize)\\n\\n    data = vec.fit_transform(text).toarray()\\n\\n    vocab = vec.get_feature_names()\\n\\n\\n\\n    required_list = []\\n\\n    for word in vocab:\\n\\n        required_list.extend(word.split())\\n\\n    print(required_list)\\n\\n\\n\\n    #[\\'first\\', \\'ques\\', \\'please\\', \\'help\\']',\n",
       "  '<python><machine-learning><scikit-learn>',\n",
       "  datetime.date(2016, 8, 2),\n",
       "  '2016-08-02 09:35:27',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2690.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1675',\n",
       "  '35513727',\n",
       "  'Answer',\n",
       "  'Pandas limit Series/DataFrame to range of values of one column',\n",
       "  'Once you have it in a DataFrame `df`, with columns `Name`, and `Age`, you can simply use\\n\\n\\n\\n    df[(min_val <= df.Age) & (df.Age <= max_val)]\\n\\n\\n\\nNote that you need to use the seemingly-redundant parentheses in the above expression, due to operator precedence.\\n\\n\\n\\n-------------------\\n\\n\\n\\nYou can create this into a function like so:\\n\\n\\n\\n    def df_limited(df, min_val, max_val):\\n\\n        return df[(min_val <= df.Age) & (df.Age <= max_val)]\\n\\n',\n",
       "  '<pandas>',\n",
       "  datetime.date(2016, 2, 19),\n",
       "  '2016-02-19 20:50:36',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1351.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1680',\n",
       "  '35585807',\n",
       "  'Answer',\n",
       "  'Filter pandas dataframe using multiple conditions defined in list',\n",
       "  'You can use [`DataFrame.isin()`][1].\\n\\n\\n\\n    df2 = df1[df1[0].isin(conditions)]\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isin.html',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 2, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '1773.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1682',\n",
       "  '34418310',\n",
       "  'Answer',\n",
       "  'Pandas compare two dataframes and remove what matches in one column',\n",
       "  'As you asked, you can do this efficiently using ``isin`` (without resorting to expensive ``merge``s).\\n\\n\\n\\n    >>> df2[~df2.text.isin(df1.text.values)]\\n\\n    C\\tD\\ttext\\n\\n    0\\t0.5\\t2\\tshot\\n\\n    1\\t0.3\\t2\\tshot\\n\\n',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 12, 22),\n",
       "  '2015-12-22 14:52:32',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '24',\n",
       "  '',\n",
       "  '12571.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1693',\n",
       "  '38723651',\n",
       "  'Answer',\n",
       "  'Python pandas plot scatter datetime error',\n",
       "  \"*Scatter plot can be drawn by using the DataFrame.plot.scatter()  method. Scatter plot requires **numeric** columns for x and y axis. These\\n\\ncan be specified by x and y keywords each.*\\n\\n\\n\\nAlternative Approach:\\n\\n\\n\\n    In [71]: df['day'] = df['datetime'].dt.day\\n\\n    \\n\\n    In [72]: df.plot.scatter(x='day', y='value')\\n\\n    Out[72]: <matplotlib.axes._subplots.AxesSubplot at 0x25440a1bc88>\\n\\n   ![Image][1]\\n\\n    ￼\\n\\n[1]: http://i.stack.imgur.com/g5Xfv.png\",\n",
       "  '<python><pandas><plot><scatter-plot>',\n",
       "  datetime.date(2016, 8, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1655.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1700',\n",
       "  '35170149',\n",
       "  'Answer',\n",
       "  'Ensemble of different kinds of regressors using scikit-learn (or any other python framework)',\n",
       "  'This is a known interesting (and often painful!) problem with hierarchical predictions. A problem with training a number of predictors over the train data, then training a higher predictor over them, again using the train data - has to do with the bias-variance decomposition. \\n\\n\\n\\nSuppose you have two predictors, one essentially an overfitting version of the other, then the former will appear over the train set to be better than latter. The combining predictor will favor the former for no true reason, just because it cannot distinguish overfitting from true high-quality prediction.\\n\\n\\n\\nThe known way of dealing with this is to prepare, for each row in the train data, for each of the predictors, a prediction for the row, based on a model *not* fit for this row. For the overfitting version, e.g., this won\\'t produce a good result for the row, on average. The combining predictor will then be able to better assess a fair model for combining the lower-level predictors.\\n\\n\\n\\nShahar Azulay & I wrote a transformer stage for dealing with this:\\n\\n\\n\\n    class Stacker(object):\\n\\n        \"\"\"\\n\\n        A transformer applying fitting a predictor `pred` to data in a way\\n\\n            that will allow a higher-up predictor to build a model utilizing both this \\n\\n            and other predictors correctly.\\n\\n\\n\\n        The fit_transform(self, x, y) of this class will create a column matrix, whose \\n\\n            each row contains the prediction of `pred` fitted on other rows than this one. \\n\\n            This allows a higher-level predictor to correctly fit a model on this, and other\\n\\n            column matrices obtained from other lower-level predictors.\\n\\n\\n\\n        The fit(self, x, y) and transform(self, x_) methods, will fit `pred` on all \\n\\n            of `x`, and transform the output of `x_` (which is either `x` or not) using the fitted \\n\\n            `pred`.\\n\\n\\n\\n        Arguments:    \\n\\n            pred: A lower-level predictor to stack.\\n\\n\\n\\n            cv_fn: Function taking `x`, and returning a cross-validation object. In `fit_transform`\\n\\n                th train and test indices of the object will be iterated over. For each iteration, `pred` will\\n\\n                be fitted to the `x` and `y` with rows corresponding to the\\n\\n                train indices, and the test indices of the output will be obtained\\n\\n                by predicting on the corresponding indices of `x`.\\n\\n        \"\"\"\\n\\n        def __init__(self, pred, cv_fn=lambda x: sklearn.cross_validation.LeaveOneOut(x.shape[0])):\\n\\n            self._pred, self._cv_fn  = pred, cv_fn\\n\\n\\n\\n        def fit_transform(self, x, y):\\n\\n            x_trans = self._train_transform(x, y)\\n\\n\\n\\n            self.fit(x, y)\\n\\n\\n\\n            return x_trans\\n\\n\\n\\n        def fit(self, x, y):\\n\\n            \"\"\"\\n\\n            Same signature as any sklearn transformer.\\n\\n            \"\"\"\\n\\n            self._pred.fit(x, y)\\n\\n\\n\\n            return self\\n\\n\\n\\n        def transform(self, x):\\n\\n            \"\"\"\\n\\n            Same signature as any sklearn transformer.\\n\\n            \"\"\"\\n\\n            return self._test_transform(x)\\n\\n\\n\\n        def _train_transform(self, x, y):\\n\\n            x_trans = np.nan * np.ones((x.shape[0], 1))\\n\\n\\n\\n            all_te = set()\\n\\n            for tr, te in self._cv_fn(x):\\n\\n                all_te = all_te | set(te)\\n\\n                x_trans[te, 0] = self._pred.fit(x[tr, :], y[tr]).predict(x[te, :]) \\n\\n            if all_te != set(range(x.shape[0])):\\n\\n                warnings.warn(\\'Not all indices covered by Stacker\\', sklearn.exceptions.FitFailedWarning)\\n\\n\\n\\n            return x_trans\\n\\n\\n\\n        def _test_transform(self, x):\\n\\n            return self._pred.predict(x)\\n\\n\\n\\n------------------------------\\n\\n\\n\\nHere is an example of the improvement for the setting described in @MaximHaytovich\\'s answer.\\n\\n\\n\\nFirst, some setup:\\n\\n\\n\\n        from sklearn import linear_model\\n\\n        from sklearn import cross_validation\\n\\n        from sklearn import ensemble\\n\\n        from sklearn import metrics\\n\\n\\n\\n        y = np.random.randn(100)\\n\\n        x0 = (y + 0.1 * np.random.randn(100)).reshape((100, 1)) \\n\\n        x1 = (y + 0.1 * np.random.randn(100)).reshape((100, 1)) \\n\\n        x = np.zeros((100, 2)) \\n\\n\\n\\nNote that `x0` and `x1` are just noisy versions of `y`. We\\'ll use the first 80 rows for train, and the last 20 for test.\\n\\n\\n\\nThese are the two predictors: a higher-variance gradient booster, and a linear predictor:\\n\\n\\n\\n        g = ensemble.GradientBoostingRegressor()\\n\\n        l = linear_model.LinearRegression()\\n\\n\\n\\nHere is the methodology suggested in the answer:\\n\\n\\n\\n        g.fit(x0[: 80, :], y[: 80])\\n\\n        l.fit(x1[: 80, :], y[: 80])\\n\\n\\n\\n        x[:, 0] = g.predict(x0)\\n\\n        x[:, 1] = l.predict(x1)\\n\\n\\n\\n        >>> metrics.r2_score(\\n\\n            y[80: ],\\n\\n            linear_model.LinearRegression().fit(x[: 80, :], y[: 80]).predict(x[80: , :]))\\n\\n        0.940017788444\\n\\n\\n\\nNow, using stacking:\\n\\n\\n\\n        x[: 80, 0] = Stacker(g).fit_transform(x0[: 80, :], y[: 80])[:, 0]\\n\\n        x[: 80, 1] = Stacker(l).fit_transform(x1[: 80, :], y[: 80])[:, 0]\\n\\n\\n\\n        u = linear_model.LinearRegression().fit(x[: 80, :], y[: 80])\\n\\n\\n\\n        x[80: , 0] = Stacker(g).fit(x0[: 80, :], y[: 80]).transform(x0[80:, :])\\n\\n        x[80: , 1] = Stacker(l).fit(x1[: 80, :], y[: 80]).transform(x1[80:, :])\\n\\n\\n\\n        >>> metrics.r2_score(\\n\\n            y[80: ],\\n\\n            u.predict(x[80:, :]))\\n\\n        0.992196564279\\n\\n\\n\\nThe stacking prediction does better. It realizes that the gradient booster is not that great.\\n\\n\\n\\n',\n",
       "  '<machine-learning><scikit-learn><ensemble-learning>',\n",
       "  datetime.date(2016, 2, 3),\n",
       "  '2016-02-07 07:43:22',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '14',\n",
       "  '',\n",
       "  '6743.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1701',\n",
       "  '35217243',\n",
       "  'Answer',\n",
       "  'Pandas Dataframe automatically shorten strings?',\n",
       "  \"You seem to be confusing the *content* of the pandas cell with its *display*. If you want to change the latter, try using [`display.max_colwidth`](http://pandas.pydata.org/pandas-docs/stable/options.html), like so:\\n\\n\\n\\n    pd.set_option('max_colwidth',40)\\n\\n\\n\\n------\\n\\n\\n\\nAlso, if your DataFrame is `df` and the column name is `'c'`, you can access the contents of a cell using:\\n\\n\\n\\n    df['c'].values[1]\\n\\n\\n\\n(for the second cell, e.g.). If you `print` this, for example, you should see your Python interpreter's rendition of the string.\",\n",
       "  '<python><dictionary><pandas>',\n",
       "  datetime.date(2016, 2, 5),\n",
       "  '2016-02-05 06:36:22',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '6',\n",
       "  '',\n",
       "  '1777.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1707',\n",
       "  '35640008',\n",
       "  'Answer',\n",
       "  'Python Pandas Distance matrix using jaccard similarity',\n",
       "  \"Looking at the docs, the implementation of [`jaccard`][1] in `scipy.spatial.distance` is jaccard *dissimilarity*, not similarity. This is the usual way in which distance is computed when using jaccard as a metric.  The reason for this is because in order to be a metric, the distance between the identical points must be zero.\\n\\n\\n\\nIn your code, the dissimilarity between 0 and 1 should be minimized, which it is.  The other values look correct in the context of dissimilarity as well.\\n\\n\\n\\nIf you want similarity instead of dissimilarity, just subtract the dissimilarity from 1.\\n\\n\\n\\n    res = 1 - pdist(df[['category1','category2','category3']], 'jaccard')\\n\\n\\n\\n\\n\\n  [1]: http://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jaccard.html\",\n",
       "  '<python><pandas><matrix><scipy>',\n",
       "  datetime.date(2016, 2, 25),\n",
       "  '2016-02-25 23:07:47',\n",
       "  'root (3339965)',\n",
       "  '5',\n",
       "  '',\n",
       "  '3878.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1712',\n",
       "  '32003358',\n",
       "  'Answer',\n",
       "  'Interpolation differences on polar contour plots in Matplotlib',\n",
       "  \"Polar plots in `matplotlib` can get tricky. When that happens, a quick solution is to convert radii and angle to x,y,  plot in a normal projection. Then make a empty polar axis to superimpose on it:\\n\\n\\n\\n    from scipy.interpolate import griddata\\n\\n\\n\\n\\n\\n    Angles = [-180, -90, 0 , 90, 180, -135, \\n\\n              -45,45, 135, 180,-90, 0, 90, 180 ]\\n\\n\\n\\n    Radii = [0,0.33,0.33,0.33,0.33,0.5,0.5,\\n\\n             0.5,0.5,0.5,0.6,0.6,0.6,0.6]\\n\\n\\n\\n    Angles = np.array(Angles)/180.*np.pi\\n\\n    x = np.array(Radii)*np.sin(Angles)\\n\\n    y = np.array(Radii)*np.cos(Angles)\\n\\n\\n\\n    Values = [30.42,24.75, 32.23, 34.26, 26.31, 20.58, \\n\\n              23.38, 34.15,27.21, 22.609, 16.013, 22.75, 27.062, 18.27]\\n\\n\\n\\n    Xi = np.linspace(-1,1,100)\\n\\n    Yi = np.linspace(-1,1,100)\\n\\n\\n\\n    #make the axes\\n\\n    f = plt.figure()\\n\\n    left, bottom, width, height= [0,0, 1, 0.7]\\n\\n    ax  = plt.axes([left, bottom, width, height])\\n\\n    pax = plt.axes([left, bottom, width, height], \\n\\n                    projection='polar',\\n\\n                    axisbg='none')\\n\\n    cax = plt.axes([0.8, 0, 0.05, 1])\\n\\n    ax.set_aspect(1)\\n\\n    ax.axis('Off')\\n\\n\\n\\n\\n\\n    # grid the data.\\n\\n    Vi = griddata((x, y), Values, (Xi[None,:], Yi[:,None]), method='cubic')\\n\\n    cf = ax.contour(Xi,Yi,Vi, 15, cmap=plt.cm.jet)\\n\\n\\n\\n    #make a custom colorbar, because the default is ugly\\n\\n    gradient = np.linspace(1, 0, 256)\\n\\n    gradient = np.vstack((gradient, gradient))\\n\\n    cax.xaxis.set_major_locator(plt.NullLocator())\\n\\n    cax.yaxis.tick_right()\\n\\n    cax.imshow(gradient.T, aspect='auto', cmap=plt.cm.jet)\\n\\n    cax.set_yticks(np.linspace(0,256,len(cf1.get_array())))\\n\\n    cax.set_yticklabels(map(str, cf.get_array())[::-1])\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/wCkSB.png\",\n",
       "  '<python><matlab><matplotlib><plot>',\n",
       "  datetime.date(2015, 8, 14),\n",
       "  '2015-08-14 15:18:11',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1028.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1715',\n",
       "  '35743968',\n",
       "  'Answer',\n",
       "  'std::tuple vs std::array as items of a std::vector',\n",
       "  \"For this specific case, I'd have to disagree with the comments. For homogeneous type containers - as is the case here (all `int`s) - `array` is superior.\\n\\n\\n\\nWhen looking at the interface of [`std::tuple`](http://en.cppreference.com/w/cpp/utility/tuple) vs. [`std::array`](http://en.cppreference.com/w/cpp/container/array), it is very clear that the latter is a container (with iterators, e.g.), while the former is not. This means that the rest of the standard library will be much more naturally applicable to the latter.\\n\\n\\n\\nIf the types weren't homogeneous, there wouldn't be a question - it would have to be `std::tuple`.\",\n",
       "  '<c++><arrays><c++11><vector><tuples>',\n",
       "  datetime.date(2016, 3, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2162.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1725',\n",
       "  '35003880',\n",
       "  'Answer',\n",
       "  'Ordering functions by Asymptotic Growth Rate',\n",
       "  \"There's a very simple rule that can help out with these problems. It's very easy to prove using the basic definitions of calculus and complexity (and it might be a good exercise to boot).\\n\\n\\n\\nGiven two functions, *f(n)* and *g(n)*:\\n\\n\\n\\n* if *lim<sub>n &rarr; &infin;</sub>f(n) / g(n) = 0*, then *f(n) = o(g(n)*.\\n\\n\\n\\n* if *lim<sub>n &rarr; &infin;</sub>f(n) / g(n) = &infin;*, then *f(n) = w(g(n)* (this follows from the previous point).\\n\\n\\n\\n* if *lim<sub>n &rarr; &infin;</sub>f(n) / g(n) = c*, *0 < c < &infin;*, then *f(n) = &Theta;(g(n)*.\\n\\n\\n\\nLooking at your examples here, they can each and every one of which be solved using these. E.g., *lim<sub>n &rarr; &infin;</sub> g1(n) / g2(n) = 0*, so *g1(n) = o(g2(n))*.\\n\\n\",\n",
       "  '<algorithm><math>',\n",
       "  datetime.date(2016, 1, 25),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1475.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1728',\n",
       "  '35247301',\n",
       "  'Answer',\n",
       "  'C++ Input Iterator',\n",
       "  'Suppose you want to read integers from the standard input. One way to do so would be using\\n\\n\\n\\n    #include <iterator>\\n\\n    #include <iostream>\\n\\n    #include <vector>\\n\\n\\n\\n    using namespace std;\\n\\n\\n\\n    const vector<int> v{istream_iterator<int>{cin}, istream_iterator<int>{}};\\n\\n\\n\\nThis will read integers into `v` (see [`istream_iterator`](http://en.cppreference.com/w/cpp/iterator/istream_iterator)).\\n\\n\\n\\nIn this case, it makes a lot of sense that the iterators will be single pass, since the range is something that is just created on the fly, as the user types in things, and each element is consumed and disappeared. You can\\'t reiterate this range.\\n\\n\\n\\n------\\n\\n\\n\\nOther than that, not so sure what you mean by \\n\\n\\n\\n> how to make iterator to become a single pass?\\n\\n\\n\\n* If you mean \"how to signify that an iterator class you\\'re writing has this restriction?\", then set its `iterator_category` to [`input_iterator_tag`](http://en.cppreference.com/w/cpp/iterator/iterator_tags)\\n\\n\\n\\n        struct my_iterator\\n\\n        {\\n\\n            using iterator_category = input_iterator_tag;\\n\\n        };\\n\\n\\n\\n* If you want to query (at compile time) whether an iterator class has this restriction, use [`iterator_traits`](http://en.cppreference.com/w/cpp/iterator/iterator_traits).\\n\\n\\n\\n\\n\\n',\n",
       "  '<c++><iterator><input-iterator>',\n",
       "  datetime.date(2016, 2, 6),\n",
       "  '2016-02-07 20:44:10',\n",
       "  'Ami Tavory (3510736), Orion (1593817)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1711.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1729',\n",
       "  '38925156',\n",
       "  'Answer',\n",
       "  'how to compare two columns in pandas to make a third column ?',\n",
       "  \"Use [`numpy.where`][1]:\\n\\n\\n\\n    df['col3'] = np.where(df['age'] <= 9, 'child', df['sex'])\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n       age sex   col3\\n\\n    0   16   m      m\\n\\n    1   15   f      f\\n\\n    2   14   m      m\\n\\n    3    9   f  child\\n\\n    4    8   f  child\\n\\n    5    2   f  child\\n\\n    6   56   f      f\\n\\n\\n\\n**Timings**\\n\\n\\n\\nUsing the following setup to get a larger sample DataFrame:\\n\\n\\n\\n    np.random.seed([3,1415])\\n\\n    n = 10**5\\n\\n    df = pd.DataFrame({'sex': np.random.choice(['m', 'f'], size=n), 'age': np.random.randint(0, 100, size=n)})\\n\\n\\n\\nI get the following timings:\\n\\n\\n\\n    %timeit np.where(df['age'] <= 9, 'child', df['sex'])\\n\\n    1000 loops, best of 3: 1.26 ms per loop\\n\\n    \\n\\n    %timeit df['sex'].where(df['age'] > 9, 'child')\\n\\n    100 loops, best of 3: 3.25 ms per loop\\n\\n    \\n\\n    %timeit df.apply(lambda x: 'child' if x['age'] <= 9 else x['sex'], axis=1)\\n\\n    100 loops, best of 3: 3.92 ms per loop\\n\\n\\n\\n  [1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 8, 12),\n",
       "  '2016-08-12 19:41:17',\n",
       "  'root (3339965)',\n",
       "  '13',\n",
       "  '',\n",
       "  '19523.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1733',\n",
       "  '38875721',\n",
       "  'Answer',\n",
       "  'Generic method for flattening 2d vectors',\n",
       "  \"Your code contains several problems. To name a few:\\n\\n\\n\\n1. An identifier cannot start with a number.\\n\\n2. Your template should be parameterized by a single parameter - the basic value type of the returned vector\\n\\n3. Your code internally assumes that the vectors are same-sized, where it is just as easy to accommodate a ragged array\\n\\n4. There are more efficient ways to append a vector to the end of a vector.\\n\\n\\n\\nI'd suggest the following alternative:\\n\\n\\n\\n    #include <vector>\\n\\n\\n\\n    template<typename T>\\n\\n    std::vector<T> flatten(const std::vector<std::vector<T>> &orig)\\n\\n    {   \\n\\n        std::vector<T> ret;\\n\\n        for(const auto &v: orig)\\n\\n            ret.insert(ret.end(), v.begin(), v.end());                                                                                         \\n\\n        return ret;\\n\\n    }   \\n\\n\\n\\n    int main() \\n\\n    {   \\n\\n        std::vector<std::vector<int>> vv; \\n\\n        vv.push_back(std::vector<int>{1, 2, 3});\\n\\n        vv.push_back(std::vector<int>{10, 20});\\n\\n        flatten(vv);\\n\\n    }   \\n\\n\",\n",
       "  '<c++><templates>',\n",
       "  datetime.date(2016, 8, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1154.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1735',\n",
       "  '35299041',\n",
       "  'Question',\n",
       "  'g++ \"warning: iteration ... invokes undefined behavior\" for Seemingly Unrelated Variable',\n",
       "  \"Consider the following code in `strange.cpp`:\\n\\n\\n\\n    #include <vector> \\n\\n\\n\\n\\n\\n    using namespace std;\\n\\n\\n\\n\\n\\n    int i = 0;\\n\\n\\n\\n\\n\\n    int *bar()\\n\\n    {   \\n\\n        ++i;\\n\\n        return &i; \\n\\n    }   \\n\\n\\n\\n\\n\\n    int main()\\n\\n    {   \\n\\n        for(size_t j = 0; j < 99999999999; ++j) // (*)\\n\\n        {   \\n\\n            const auto p = bar();\\n\\n            if(!p) // (**)\\n\\n                return -1; \\n\\n        }   \\n\\n    }   \\n\\n\\n\\nCompiling this with g++ gives a warning:\\n\\n\\n\\n    $ g++ --std=c++11 -O3 strange.cpp \\n\\n    strange.cpp: In function ‘int main()’:\\n\\n    strange.cpp:12:12: warning: iteration 4294967296ul invokes undefined behavior [-Waggressive-loop-optimizations]\\n\\n             ++i;\\n\\n                ^\\n\\n    strange.cpp:19:9: note: containing loop\\n\\n             for(size_t j = 0; j < 99999999999; ++j) // (*)\\n\\n             ^\\n\\n\\n\\nI don't understand why the increment invokes undefined behavior. Moreover, there are two changes, each of which makes the warning disappear:\\n\\n\\n\\n1. changing the line `(*)` to `for(int j...`\\n\\n2. changing the line `(**)` to `if(!*p)`\\n\\n\\n\\nWhat is the meaning of this warning, and why are the changes relevant to it?\\n\\n\\n\\n**Note**\\n\\n\\n\\n    $ g++ --version\\n\\n    g++ (Ubuntu 4.8.4-2ubuntu1~14.04) 4.8.4\\n\\n\",\n",
       "  '<c++><g++><compiler-warnings>',\n",
       "  datetime.date(2016, 2, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '0.0',\n",
       "  '2821.0',\n",
       "  '1.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1742',\n",
       "  '39014192',\n",
       "  'Answer',\n",
       "  'Multiple stacked bar plot with pandas',\n",
       "  'You could do it by shifting the `position` parameter of a `bar-plot` so that they are adjacent to each other as shown:\\n\\n\\n\\n    matplotlib.style.use(\\'ggplot\\')\\n\\n\\n\\n    fig, ax = plt.subplots()\\n\\n    df[[\\'a\\', \\'c\\']].plot.bar(stacked=True, width=0.1, position=1.5, colormap=\"bwr\", ax=ax, alpha=0.7)\\n\\n    df[[\\'b\\', \\'d\\']].plot.bar(stacked=True, width=0.1, position=-0.5, colormap=\"RdGy\", ax=ax, alpha=0.7)\\n\\n    df[[\\'a\\', \\'d\\']].plot.bar(stacked=True, width=0.1, position=0.5, colormap=\"BrBG\", ax=ax, alpha=0.7)\\n\\n    plt.legend(loc=\"upper center\")\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/MKwjf.png',\n",
       "  '<python><pandas><matplotlib>',\n",
       "  datetime.date(2016, 8, 18),\n",
       "  '2016-08-18 09:16:18',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '5',\n",
       "  '',\n",
       "  '2402.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1743',\n",
       "  '39017677',\n",
       "  'Answer',\n",
       "  'Minimize total distance between two sets of points in Python',\n",
       "  \"There's a known algorithm for this, [The Hungarian Method For Assignment](http://www.math.harvard.edu/archive/20_spring_05/handouts/assignment_overheads.pdf), which works in time *O(n<sup>3</sup>)*. \\n\\n\\n\\nIn SciPy, you can find an implementation in [`scipy.optimize.linear_sum_assignment`](http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html)\",\n",
       "  '<python><scipy><euclidean-distance>',\n",
       "  datetime.date(2016, 8, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1284.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1744',\n",
       "  '32040000',\n",
       "  'Answer',\n",
       "  'Find target values in pandas dataframe',\n",
       "  \"These are somewhat of a hack, let's see if there are better solutions:\\n\\n\\n\\n    In [36]:\\n\\n    targets['t']=0\\n\\n\\n\\n    In [37]:\\n\\n    df2 = df.reset_index().set_index('case') - targets\\n\\n\\n\\n    In [38]:\\n\\n    df3 = df2.groupby(df2.index).transform(lambda x: x.abs()==np.min(x.abs()))\\n\\n\\n\\n    In [39]:\\n\\n    df4 = pd.DataFrame({'1': df2.t[df3[1]],\\n\\n                        '2': df2.t[df3[2]],\\n\\n                        '3': df2.t[df3[3]]})\\n\\n\\n\\n    print df4\\n\\n\\n\\n               1       2       3\\n\\n    case                        \\n\\n    1014  0.3991  0.3991  0.3991\\n\\n    1015  0.1991  0.3991  0.3991\\n\\n    1016  0.0991  0.3991  0.3991\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 8, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '3062.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1745',\n",
       "  '32105387',\n",
       "  'Answer',\n",
       "  'How can I sample a multivariate log-normal distribution in Python?',\n",
       "  'A multivariate lognormal distributed random variable `Rv` should have this property: `log(Rv)` should follow a normal distribution.  Therefore, the problem is really just to generation a random variable of multivariate normal distribution and `np.exp` it.\\n\\n\\n\\n    In [1]: import numpy.random as nr\\n\\n\\n\\n    In [2]: cov = np.array([[1.0, 0.2, 0.3,],\\n\\n                            [0.2, 1.0, 0.3,],\\n\\n                            [0.3, 0.3, 1.0]])\\n\\n\\n\\n    In [3]: mu  = np.log([0.3, 0.4, 0.5])\\n\\n\\n\\n    In [4]: mvn = nr.multivariate_normal(mu, cov, size=5)\\n\\n\\n\\n    In [5]: mvn   # This is multivariate normal\\n\\n    Out[5]:\\n\\n    array([[-1.36808854, -1.32562291, -1.9706876 ],\\n\\n           [-2.13119289,  1.28146425,  0.66000019],\\n\\n           [-2.82590272, -1.22500654, -0.32635701],\\n\\n           [-0.4967589 , -0.34469589, -2.04084115],\\n\\n           [-0.85590235, -1.27133544, -0.70959595]])\\n\\n\\n\\n    In [6]: mvln = np.exp(mvn)\\n\\n\\n\\n    In [7]: mvln   # This is multivariate log-normal\\n\\n    Out[7]:\\n\\n    array([[ 0.25459314,  0.26563744,  0.139361  ],\\n\\n           [ 0.11869562,  3.60190996,  1.9347927 ],\\n\\n           [ 0.05925514,  0.29375578,  0.72154754],\\n\\n           [ 0.60849968,  0.70843576,  0.12991938],\\n\\n           [ 0.42489961,  0.28045684,  0.49184289]])',\n",
       "  '<python><numpy><statistics><scipy><probability>',\n",
       "  datetime.date(2015, 8, 19),\n",
       "  '2017-07-28 00:45:34',\n",
       "  'CT Zhu (2487184), Nathaniel J. Smith (1925449)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2089.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1746',\n",
       "  '32106115',\n",
       "  'Answer',\n",
       "  'How can I write a binary array as an image in Python?',\n",
       "  \"The `numpy` and `matplotlib` way of doing it would be:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import matplotlib.cm as cm\\n    import numpy as np\\n\\n    plt.imsave('filename.png', np.array(data).reshape(50,50), cmap=cm.gray)\\n\\n\\n\\nSee [this](http://matplotlib.org/api/image_api.html#matplotlib.image.imsave)\",\n",
       "  '<python><python-imaging-library>',\n",
       "  datetime.date(2015, 8, 19),\n",
       "  '2015-08-19 23:19:15',\n",
       "  'CT Zhu (2487184)',\n",
       "  '6',\n",
       "  '',\n",
       "  '15709.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1747',\n",
       "  '32122734',\n",
       "  'Answer',\n",
       "  'Pandas Percentage count on a DataFrame groupby',\n",
       "  \"Could be just this:\\n\\n\\n\\n    In [73]:\\n\\n\\n\\n    print pd.DataFrame({'Percentage': df.groupby(('ID', 'Feature')).size() / len(df)})\\n\\n                Percentage\\n\\n    ID Feature            \\n\\n    0  False           0.2\\n\\n       True            0.3\\n\\n    1  False           0.3\\n\\n       True            0.2\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 8, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2548.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1749',\n",
       "  '35050464',\n",
       "  'Answer',\n",
       "  'Simpson rule integration,Python',\n",
       "  'Interestingly enough, you can find it in the [Wikipedia entry](https://en.wikipedia.org/wiki/Simpson%27s_rule#Python): \\n\\n\\n\\n    from __future__ import division  # Python 2 compatibility\\n\\n\\n\\n    def simpson(f, a, b, n):\\n\\n        \"\"\"Approximates the definite integral of f from a to b by the\\n\\n        composite Simpson\\'s rule, using n subintervals (with n even)\"\"\"\\n\\n    \\n\\n        if n % 2:\\n\\n            raise ValueError(\"n must be even (received n=%d)\" % n)\\n\\n    \\n\\n        h = (b - a) / n\\n\\n        s = f(a) + f(b)\\n\\n\\n\\n        for i in range(1, n, 2):\\n\\n            s += 4 * f(a + i * h)\\n\\n        for i in range(2, n-1, 2):\\n\\n            s += 2 * f(a + i * h)\\n\\n\\n\\n        return s * h / 3\\n\\n\\n\\nwhere you use it as:\\n\\n\\n\\n    simpson(lambda x:x**4, 0.0, 10.0, 100000)\\n\\n\\n\\n-----------\\n\\n\\n\\nNote how it bypasses your parity problem by requiring a function and `n`. \\n\\n\\n\\nIn case you need it for a list of values, though, then after adapting the code (which should be easy), I suggest you also raise a `ValueError` in case its length is not even.',\n",
       "  '<python><integral><simpsons-rule>',\n",
       "  datetime.date(2016, 1, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2269.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1751',\n",
       "  '35069243',\n",
       "  'Answer',\n",
       "  'Pandas DataFrame step plot: where=\"post\"',\n",
       "  'You just need to specify `drawstyle=\"steps-post\"`:\\n\\n\\n\\n    df = pd.DataFrame(np.random.randn(36, 3))\\n\\n    df.plot(drawstyle=\"steps\", linewidth=2)\\n\\n    df.plot(drawstyle=\"steps-post\", linewidth=2)\\n\\n\\n\\nCompare the result:\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/MCCLi.png\\n\\n  [2]: http://i.stack.imgur.com/tIiLY.png',\n",
       "  '<python><pandas><matplotlib>',\n",
       "  datetime.date(2016, 1, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '16',\n",
       "  '',\n",
       "  '4793.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1755',\n",
       "  '35803235',\n",
       "  'Answer',\n",
       "  'How to filter columns in numpy ndarray',\n",
       "  'Once you actually use `numpy.array`s, it all works:\\n\\n\\n\\n    import numpy as np\\n\\n\\n\\n    selections = np.array([True, False, True])\\n\\n    data = np.array([[ 1, 2, 3 ],\\n\\n            [ 4, 5, 6 ]])\\n\\n\\n\\n    >>> data[:, selections]\\n\\n    array([[1, 3],\\n\\n           [4, 6]])',\n",
       "  '<python><arrays><numpy>',\n",
       "  datetime.date(2016, 3, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1984.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1756',\n",
       "  '35082270',\n",
       "  'Answer',\n",
       "  'Preprocessing in scikit learn - single sample - Depreciation warning',\n",
       "  \"Well, it actually looks like the warning is telling you what to do.\\n\\n\\n\\nAs part of [`sklearn.pipeline` stages' uniform interfaces](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html), as a rule of thumb:\\n\\n\\n\\n* when you see `X`, it should be an `np.array` with two dimensions\\n\\n\\n\\n* when you see `y`, it should be an `np.array` with a single dimension. \\n\\n\\n\\nHere, therefore, you should consider the following:\\n\\n\\n\\n    temp = [1,2,3,4,5,5,6,....................,7]\\n\\n    # This makes it into a 2d array\\n\\n    temp = np.array(temp).reshape((len(temp), 1))\\n\\n    temp = scaler.transform(temp)\",\n",
       "  '<python><scikit-learn><deprecation-warning>',\n",
       "  datetime.date(2016, 1, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '29',\n",
       "  '',\n",
       "  '56776.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1757',\n",
       "  '35091895',\n",
       "  'Answer',\n",
       "  'std::atomic as a value of std::map',\n",
       "  \"I don't have access to Visual C++ compilers, but I'm guessing the following  might work. Use an indirection, utilizing a map to (smart) pointers of `atomic`s:\\n\\n\\n\\n    #include <atomic>\\n\\n    #include <map>\\n\\n    #include <memory>\\n\\n\\n\\n\\n\\n    using atomic_ptr_t = std::shared_ptr<std::atomic<int64_t>>;\\n\\n    typedef std::map<uint64_t, atomic_ptr_t> value_map_t;\\n\\n\\n\\n\\n\\n    int main()\\n\\n    {\\n\\n        value_map_t map;\\n\\n        map[1] = atomic_ptr_t(new std::atomic<int64_t>(0));\\n\\n\\n\\n        return 0;\\n\\n    }\\n\\n\",\n",
       "  '<c++11><dictionary><visual-studio-2012><gcc><atomic>',\n",
       "  datetime.date(2016, 1, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2214.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1760',\n",
       "  '35356443',\n",
       "  'Answer',\n",
       "  'C++ template function inside a non-template class',\n",
       "  \"As people have noted in the comments, there's no problem doing so.\\n\\n\\n\\nOne aspect to watch out for is where to put the definition of the method `templatefunction`. For the time being (see the [ISO cpp FAQ](https://isocpp.org/wiki/faq/templates#separate-template-class-defn-from-decl)), you should consider placing it in the header file, which is different than what you'd probably do with the definition of the other methods. Thus you'd have `example.hpp`:\\n\\n\\n\\n    class example\\n\\n    {\\n\\n     public:\\n\\n     example();\\n\\n     ~example();\\n\\n     template<typename T> void templatefunction(T);\\n\\n     void nontemplatefunction(string x);\\n\\n    };\\n\\n\\n\\n    template<typename T> void example::templatefunction(T)\\n\\n    {\\n\\n\\n\\n    }\\n\\n\\n\\nand then `example.cpp`:\\n\\n\\n\\n\\n\\n    example::example(){}\\n\\n\\n\\n    void example::nontemplatefunction(string x)\\n\\n    {\\n\\n\\n\\n    }\\n\\n\",\n",
       "  '<c++><templates>',\n",
       "  datetime.date(2016, 2, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1005.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1766',\n",
       "  '38892640',\n",
       "  'Answer',\n",
       "  'Append data to data frame in a loop - function only returns last row of data frame',\n",
       "  'Instead of `for (i in id) {`, try `for (i in 1:322) {` or `for (i in 1:length(id) {` at the beginning of your loop',\n",
       "  '<r>',\n",
       "  datetime.date(2016, 8, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '5862.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1767',\n",
       "  '38894053',\n",
       "  'Answer',\n",
       "  'LabelEncoder specify classes in DataFrame',\n",
       "  \"You could [`fit`][1] the label encoder and later [`transform`][2] the labels to their normalized encoding as follows:\\n\\n\\n\\n    In [4]: from sklearn import preprocessing\\n\\n       ...: import numpy as np\\n\\n    \\n\\n    In [5]: le = preprocessing.LabelEncoder()\\n\\n    \\n\\n    In [6]: le.fit(np.unique(df.values))\\n\\n    Out[6]: LabelEncoder()\\n\\n    \\n\\n    In [7]: list(le.classes_)\\n\\n    Out[7]: ['A', 'B', 'C', 'D', 'E']\\n\\n    \\n\\n    In [8]: df.apply(le.transform)\\n\\n    Out[8]: \\n\\n       Feat1  Feat2  Feat3  Feat4  Feat5\\n\\n    0      0      0      0      0      4\\n\\n    1      1      1      2      2      4\\n\\n    2      2      3      2      2      4\\n\\n    3      3      0      2      3      4\\n\\n\\n\\n\\n\\n----------\\n\\nOne way to specify labels by default would be:\\n\\n\\n\\n    In [9]: labels = ['A', 'B', 'C', 'D', 'E']\\n\\n    \\n\\n    In [10]: enc = le.fit(labels)\\n\\n    \\n\\n    In [11]: enc.classes_                       # sorts the labels in alphabetical order\\n\\n    Out[11]: \\n\\n    array(['A', 'B', 'C', 'D', 'E'], \\n\\n          dtype='<U1')\\n\\n\\n\\n    In [12]: enc.transform('E')\\n\\n    Out[12]: 4\\n\\n\\n\\n\\n\\n[1]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder.fit\\n\\n[2]: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html#sklearn.preprocessing.LabelEncoder.transform\",\n",
       "  '<python><pandas><machine-learning><scikit-learn>',\n",
       "  datetime.date(2016, 8, 11),\n",
       "  '2016-08-11 11:54:05',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '13',\n",
       "  '',\n",
       "  '5942.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1772',\n",
       "  '39058046',\n",
       "  'Answer',\n",
       "  'handling exception in reading data with pandas.read_csv()',\n",
       "  \"You could try:\\n\\n\\n\\n    df = pd.read_csv(path, chunksize=6)\\n\\n    for chunk in df:\\n\\n        print(chunk)\\n\\n\\n\\n\\n\\n----------\\n\\nIncase you want to carry out operations inside each chunk, you can do:\\n\\n\\n\\n    for chunk in df:\\n\\n        chunk['d'] = chunk[['a', 'b']].mean(axis=1)    # Average of columns 'a' and 'b'\\n\\n        print(chunk)\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 8, 20),\n",
       "  '2016-08-20 20:05:06',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1828.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1774',\n",
       "  '39092415',\n",
       "  'Answer',\n",
       "  'pandas dataframe convert column type to string or categorical',\n",
       "  \"To convert a column into a string type (that will be an *object* column per se in pandas), use `astype`:\\n\\n\\n\\n    df.zipcode = zipcode.astype(str)\\n\\n\\n\\nIf you want to get a `Categorical` column, you can pass the parameter `'category'` to the function:\\n\\n\\n\\n    df.zipcode = zipcode.astype('category')\\n\\n\",\n",
       "  '<pandas><dataframe><type-conversion><categorical-data>',\n",
       "  datetime.date(2016, 8, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '26737.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1776',\n",
       "  '39101287',\n",
       "  'Answer',\n",
       "  'How do I release memory used by a pandas dataframe?',\n",
       "  \"As noted in the comments, there are some things to try: `gc.collect` (@EdChum) may clear stuff, for example. At least from my experience, these things sometimes work and often don't. \\n\\n\\n\\nThere is one thing that always works, however, because it is done at the OS, not language, level.\\n\\n\\n\\nSuppose you have a function that creates an intermediate huge DataFrame, and returns a smaller result (which might also be a DataFrame):\\n\\n\\n\\n    def huge_intermediate_calc(something):\\n\\n        ...\\n\\n        huge_df = pd.DataFrame(...)\\n\\n        ...\\n\\n        return some_aggregate\\n\\n\\n\\nThen if you do something like\\n\\n\\n\\n    import multiprocessing\\n\\n\\n\\n    result = multiprocessing.Pool(1).map(huge_intermediate_calc, [something_])[0]\\n\\n\\n\\nThen [the function is executed at a different process](https://docs.python.org/2/library/multiprocessing.html). When that process completes, the OS retakes all the resources it used. There's really nothing Python, pandas, the garbage collector, could do to stop that.\",\n",
       "  '<python><pandas><memory>',\n",
       "  datetime.date(2016, 8, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '16',\n",
       "  '',\n",
       "  '44411.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1777',\n",
       "  '35139819',\n",
       "  'Answer',\n",
       "  'Sorting in pandas for large datasets',\n",
       "  \"In the past, I've used Linux's pair of venerable [`sort`](http://man7.org/linux/man-pages/man1/sort.1.html) and [`split`](http://linux.die.net/man/1/split) utilities, to sort massive files that choked pandas.\\n\\n\\n\\nI don't want to disparage the other answer on this page. However, since your data is text format (as you indicated in the comments), I think it's a tremendous complication to start transferring it into other formats (HDF, SQL, etc.), for something that GNU/Linux utilities have been solving very efficiently for the last 30-40 years.\\n\\n\\n\\n\\n\\n-------\\n\\n\\n\\nSay your file is called `stuff.csv`, and looks like this:\\n\\n                                                                                                                                 \\n\\n    4.9,3.0,1.4,0.6\\n\\n    4.8,2.8,1.3,1.2\\n\\n\\n\\nThen the following command will sort it by the 3rd column:\\n\\n\\n\\n    sort --parallel=8 -t . -nrk3 stuff.csv\\n\\n\\n\\nNote that the number of threads here is set to 8.\\n\\n\\n\\n-----\\n\\n\\n\\nThe above will work with files that fit into the main memory. When your file is too large, you would first split it into a number of parts. So\\n\\n\\n\\n    split -l 100000 stuff.csv stuff\\n\\n\\n\\nwould split the file into files of length at most 100000 lines. \\n\\n\\n\\nNow you would sort each file individually, as above. Finally, you would use [mergesort](https://en.wikipedia.org/wiki/Merge_sort), again through (waith for it...) `sort`:\\n\\n\\n\\n    sort -m sorted_stuff_* > final_sorted_stuff.csv\\n\\n\\n\\n-------------------------------\\n\\n\\n\\nFinally, if your file is not in CSV (say it is a `tgz` file), then you should find a way to pipe a CSV version of it into `split`.\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 2, 1),\n",
       "  '2016-02-02 07:15:27',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '11',\n",
       "  '',\n",
       "  '5865.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1783',\n",
       "  '32162160',\n",
       "  'Answer',\n",
       "  'Matplotlib: how to have a transparent box plot face while a non-transparent line edge?',\n",
       "  \"`set_alpha` changes both face color and edge color, to avoid that, you may want to consider passing RGBA values directly to face color:\\n\\n\\n\\n    #cols = ['red', 'blue', 'green']\\n\\n    cols = [[1,0,0,0.5],\\n\\n            [0,1,0,0.5],\\n\\n            [0,0,1,0.5]]\\n\\n\\n\\n    controls = ['trt_a', 'trt_b', 'trt_c']\\n\\n\\n\\n    fig, ax = plt.subplots()\\n\\n\\n\\n    boxplot_dict = ax.boxplot([data[x] for x in ['a', 'b', 'c']], \\\\\\n\\n        positions = [1, 1.5, 2], labels = controls, \\\\\\n\\n        patch_artist = True, widths = 0.25)\\n\\n\\n\\n    for b, c in zip(boxplot_dict['boxes'], cols):\\n\\n        #b.set_alpha(0.6)\\n\\n        b.set_edgecolor('k') # or try 'black'\\n\\n        b.set_facecolor(c)\\n\\n        b.set_linewidth(1)\\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/OawSz.png\",\n",
       "  '<python><matplotlib><boxplot><alpha-transparency>',\n",
       "  datetime.date(2015, 8, 23),\n",
       "  '2015-08-23 02:29:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1339.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1795',\n",
       "  '35397892',\n",
       "  'Answer',\n",
       "  'What are the rules for comparing numpy arrays using ==?',\n",
       "  \"It's possible that what's confusing you is that:\\n\\n\\n\\n1. some [broadcasting](http://docs.scipy.org/doc/numpy-1.10.1/user/basics.broadcasting.html) is going on.\\n\\n\\n\\n2. you appear to have an older version of numpy.\\n\\n\\n\\n--------------\\n\\n\\n\\n    x == np.array([[1],[2]])\\n\\n\\n\\nis broadcasting. It compares `x` to each of the first and second arrays; as they are scalars, broadcasting implies that it compares each element of `x` to each of the scalars. \\n\\n\\n\\n---------------\\n\\n\\n\\nHowever, each of\\n\\n\\n\\n    x == np.array([1,2])\\n\\n\\n\\nand\\n\\n\\n\\n    x == np.array([[1,3],[2]])\\n\\n\\n\\ncan't be broadcast. By me, with `numpy` 1.10.4, this gives\\n\\n\\n\\n    /usr/local/bin/ipython:1: DeprecationWarning: elementwise == comparison failed; this will raise an error in the future.\\n\\n    #!/usr/bin/python\\n\\n    False\",\n",
       "  '<python><numpy><numpy-broadcasting>',\n",
       "  datetime.date(2016, 2, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '4443.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1799',\n",
       "  '35851327',\n",
       "  'Answer',\n",
       "  'Fixing inflexion point estimate using python',\n",
       "  \"Any reason not to use uni-variate spline directly on the gradient?\\n\\n\\n\\n    from scipy.interpolate import UnivariateSpline\\n\\n\\n\\n    #raw data\\n\\n    data = np.genfromtxt('ww.txt')\\n\\n\\n\\n    plt.plot(np.gradient(data), '+')\\n\\n\\n\\n    spl = UnivariateSpline(np.arange(len(data)), np.gradient(data), k=5)\\n\\n    spl.set_smoothing_factor(1000)\\n\\n    plt.plot(spl(np.arange(len(data))), label='Smooth Fct 1e3')\\n\\n    spl.set_smoothing_factor(10000)\\n\\n    plt.plot(spl(np.arange(len(data))), label='Smooth Fct 1e4')\\n\\n    plt.legend(loc='lower left')\\n\\n\\n\\n    max_idx = np.argmax(spl(np.arange(len(data))))\\n\\n    plt.vlines(max_idx, -5, 9, linewidth=5, alpha=0.3)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nAlso we can solve for the maximum numerically:\\n\\n\\n\\n    In [122]:\\n\\n\\n\\n    import scipy.optimize as so\\n\\n    F = lambda x: -spl(x)\\n\\n    so.fmin(F, 102)\\n\\n    Optimization terminated successfully.\\n\\n             Current function value: -3.339112\\n\\n             Iterations: 20\\n\\n             Function evaluations: 40\\n\\n    Out[122]:\\n\\n    array([ 124.91303558])\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/yjs2p.png\",\n",
       "  '<python><numpy><scipy>',\n",
       "  datetime.date(2016, 3, 7),\n",
       "  '2016-03-07 18:58:36',\n",
       "  'CT Zhu (2487184)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2177.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1801',\n",
       "  '35405096',\n",
       "  'Answer',\n",
       "  'std::vector::emplace_back and std::move',\n",
       "  'There is a point of doing so in the second case. Consider this code:\\n\\n\\n\\n    int main()\\n\\n    {\\n\\n        std::vector<std::string> bar;\\n\\n        std::string str(\"some_string\");\\n\\n        bar.emplace_back(std::move(str));\\n\\n        // bar.emplace_back(str);\\n\\n        std::cout << str << std::endl;\\n\\n    }\\n\\n\\n\\nIf you change the comment to the line above, you can see that you will end up with two copies of \"some_string\" (one in `bar` and one in `str`). So it does change something.\\n\\n\\n\\nOtherwise, the first one is moving a temporary, and the third is moving a constant string literal. It does nothing.',\n",
       "  '<c++><c++11><vector><move-semantics>',\n",
       "  datetime.date(2016, 2, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '5957.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1803',\n",
       "  '35467300',\n",
       "  'Answer',\n",
       "  'Using datetime as ticks in Matplotlib',\n",
       "  \"This is an alternative plotting method `plot_date`, which you might want to use if your independent variable are `datetime` like, instead of using the more general `plot` method:\\n\\n\\n\\n    import datetime\\n\\n    data = np.random.rand(24)\\n\\n\\n\\n    #a list of time: 00:00:00 to 23:00:00\\n\\n    times = [datetime.datetime.strptime(str(i), '%H') for i in range(24)]\\n\\n\\n\\n    #'H' controls xticklabel format, 'H' means only the hours is shown\\n\\n    #day, year, week, month, etc are not shown\\n\\n    plt.plot_date(times, data, fmt='H')\\n\\n    plt.setp(plt.gca().xaxis.get_majorticklabels(),\\n\\n             'rotation', 90)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nThe benefit of it is that now you can easily control the density of xticks, if we want to have a tick every hour, we will insert these lines after `plot_date`:\\n\\n    \\n\\n    ##import it if not already imported\\n\\n    #import matplotlib.dates as mdates\\n\\n    plt.gca().xaxis.set_major_locator(mdates.HourLocator())\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Q60Nf.png\\n\\n  [2]: http://i.stack.imgur.com/CM4xI.png\",\n",
       "  '<python><datetime><numpy><matplotlib>',\n",
       "  datetime.date(2016, 2, 17),\n",
       "  '2016-02-17 20:48:37',\n",
       "  'CT Zhu (2487184)',\n",
       "  '5',\n",
       "  '',\n",
       "  '8622.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1814',\n",
       "  '35475807',\n",
       "  'Answer',\n",
       "  'Python: 1d array circular convolution',\n",
       "  \"Since this is for homework, I'm leaving out a few details.\\n\\n\\n\\n\\n\\nBy the [definition of convolution](https://en.wikipedia.org/wiki/Convolution#Definition), if you append a signal *a* to itself, then the convolution between *aa* and *b* will contain inside the cyclic convolution of *a* and *b*.\\n\\n\\n\\nE.g., consider the following:\\n\\n\\n\\n    import numpy as np\\n\\n    from scipy import signal\\n\\n\\n\\n    %pylab inline\\n\\n\\n\\n    a = np.array([1] * 10)\\n\\n    b = np.array([1] * 10)\\n\\n\\n\\n    plot(signal.convolve(a, b));\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\nThat is the standard convolution. Now this, however\\n\\n\\n\\n    plot(signal.convolve(a, np.concatenate((b, b))));\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\nIn this last figure, try to see where is the result of the circular convolution, and how to generalize this.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/CjPNa.png\\n\\n  [2]: http://i.stack.imgur.com/osqWg.png\",\n",
       "  '<python><numpy><scipy><convolution>',\n",
       "  datetime.date(2016, 2, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '2813.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1821',\n",
       "  '32337220',\n",
       "  'Answer',\n",
       "  'In Python, how can an image stored as a NumPy array be scaled in size?',\n",
       "  'In `scikit-image`, we have [transform][1]\\n\\n\\n\\n\\n\\n    from skimage import transform as tf\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    data = np.random.random((1, 15, 3))*255\\n\\n    data = data.astype(np.uint8)\\n\\n    new_data = tf.resize(data, (600, 300, 3), order=0) # order=0, Nearest-neighbor interpolation\\n\\n    f, (ax1, ax2, ax3) = plt.subplots(1,3, figsize=(10, 10))\\n\\n    ax1.imshow(data)\\n\\n    ax2.imshow(new_data)\\n\\n    ax3.imshow(tf.resize(data, (600, 300, 3), order=1))\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://scikit-image.org/docs/dev/api/skimage.transform.html\\n\\n  [2]: http://i.stack.imgur.com/fgifx.png',\n",
       "  '<python><image><numpy><scale><shape>',\n",
       "  datetime.date(2015, 9, 1),\n",
       "  '2015-09-01 17:22:47',\n",
       "  'CT Zhu (2487184)',\n",
       "  '2',\n",
       "  '',\n",
       "  '3139.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1833',\n",
       "  '35945135',\n",
       "  'Answer',\n",
       "  'How to store scaling parameters for later use',\n",
       "  \"I think that the best way is to pickle it post `fit`, as this is the most generic option. Perhaps you'll later create a pipeline composed of both a feature extractor and scaler. By pickling a (possibly compound) stage, you're making things more generic. The [sklearn documentation on model persistence](http://scikit-learn.org/stable/modules/model_persistence.html) discusses how to do this.\\n\\n\\n\\nHaving said that, you can query [`sklearn.preprocessing.StandardScaler`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) for the fit parameters:\\n\\n\\n\\n> **scale_** : ndarray, shape (n_features,)\\n\\nPer feature relative scaling of the data.\\n\\nNew in version 0.17: scale_ is recommended instead of deprecated std_.\\n\\n**mean_** : array of floats with shape [n_features]\\n\\nThe mean value for each feature in the training set.\\n\\n\\n\\nThe following short snippet illustrates this:\\n\\n\\n\\n    from sklearn import preprocessing\\n\\n    import numpy as np\\n\\n\\n\\n    s = preprocessing.StandardScaler()\\n\\n    s.fit(np.array([[1., 2, 3, 4]]).T)\\n\\n    >>> s.mean_, s.scale_\\n\\n    (array([ 2.5]), array([ 1.11803399]))\",\n",
       "  '<scikit-learn><normalization><standardized>',\n",
       "  datetime.date(2016, 3, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '3183.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1837',\n",
       "  '35559262',\n",
       "  'Answer',\n",
       "  'Matplotlib normalize colorbar (Python)',\n",
       "  'Please user the `levels` parameter, a set of examples:\\n\\n\\n\\n    In [9]:\\n\\n\\n\\n    ndom\\n\\n    z = np.random.random((10,10))\\n\\n\\n\\n***Without `levels`, colorbar will be auto-scaled***\\n\\n\\n\\n    In [11]:\\n\\n    plt.contourf(z)\\n\\n    plt.colorbar()\\n\\n    Out[11]:\\n\\n    <matplotlib.colorbar.Colorbar at 0x120d47390>\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n    In [12]:\\n\\n    plt.contourf(z*2)\\n\\n    plt.colorbar()\\n\\n    Out[12]:\\n\\n    <matplotlib.colorbar.Colorbar at 0x120f6ac10>\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n***Control colorbar with explicit `levels`***\\n\\n\\n\\n    In [13]:\\n\\n    plt.contourf(z*2, levels=np.linspace(0,2,20))\\n\\n    plt.colorbar()\\n\\n    Out[13]:\\n\\n    <matplotlib.colorbar.Colorbar at 0x121b119d0>\\n\\n\\n\\n[![enter image description here][3]][3]\\n\\n\\n\\n    In [14]:\\n\\n    plt.contourf(z, levels=np.linspace(0,2,20))\\n\\n    plt.colorbar()\\n\\n    Out[14]:\\n\\n    <matplotlib.colorbar.Colorbar at 0x120dc3510>\\n\\n\\n\\n[![enter image description here][4]][4]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/X6Tpp.png\\n\\n  [2]: http://i.stack.imgur.com/adtsT.png\\n\\n  [3]: http://i.stack.imgur.com/VZL7R.png\\n\\n  [4]: http://i.stack.imgur.com/250NZ.png',\n",
       "  '<python><numpy><matplotlib><colorbar><contourf>',\n",
       "  datetime.date(2016, 2, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2838.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1841',\n",
       "  '32341089',\n",
       "  'Answer',\n",
       "  'Python Pandas Choosing Random Sample of Groups from Groupby',\n",
       "  \"You can take a randoms sample of the unique values of `df.some_key.unique()`, use that to slice the `df` and finally `groupby` on the resultant:\\n\\n\\n\\n    In [337]:\\n\\n\\n\\n    df = pd.DataFrame({'some_key': [0,1,2,3,0,1,2,3,0,1,2,3],\\n\\n                       'val':      [1,2,3,4,1,5,1,5,1,6,7,8]})\\n\\n    In [338]:\\n\\n\\n\\n    print df[df.some_key.isin(random.sample(df.some_key.unique(),2))].groupby('some_key').mean()\\n\\n                   val\\n\\n    some_key          \\n\\n    0         1.000000\\n\\n    2         3.666667\\n\\n\\n\\nIf there are more than one groupby keys:\\n\\n\\n\\n    In [358]:\\n\\n\\n\\n    df = pd.DataFrame({'some_key1':[0,1,2,3,0,1,2,3,0,1,2,3],\\n\\n                       'some_key2':[0,0,0,0,1,1,1,1,2,2,2,2],\\n\\n                       'val':      [1,2,3,4,1,5,1,5,1,6,7,8]})\\n\\n    In [359]:\\n\\n\\n\\n    gby = df.groupby(['some_key1', 'some_key2'])\\n\\n    In [360]:\\n\\n\\n\\n    print gby.mean().ix[random.sample(gby.indices.keys(),2)]\\n\\n                         val\\n\\n    some_key1 some_key2     \\n\\n    1         1            5\\n\\n    3         2            8\\n\\n\\n\\nBut if you are just going to get the values of each group, you don't even need to `groubpy`, `MultiIndex` will do:\\n\\n\\n\\n    In [372]:\\n\\n\\n\\n    idx = random.sample(set(pd.MultiIndex.from_product((df.some_key1, df.some_key2)).tolist()),\\n\\n                        2)\\n\\n    print df.set_index(['some_key1', 'some_key2']).ix[idx]\\n\\n                         val\\n\\n    some_key1 some_key2     \\n\\n    2         0            3\\n\\n    3         1            5\",\n",
       "  '<python><pandas><random><group-by>',\n",
       "  datetime.date(2015, 9, 1),\n",
       "  '2015-09-01 21:37:20',\n",
       "  'CT Zhu (2487184)',\n",
       "  '7',\n",
       "  '',\n",
       "  '4657.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1850',\n",
       "  '35582695',\n",
       "  'Answer',\n",
       "  'get function by its values in certain points',\n",
       "  \"Since the function you're trying to fit is a polynomial, you can use [`numpy.polyfit`](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.polyfit.html)\\n\\n\\n\\n    >>> numpy.polyfit(x, y, 2) # The 2 signifies a polynomial of degree 2\\n\\n    array([  -1.04978546,  115.16698544,  236.16191491])\\n\\n\\n\\nThis means that the best fit was *y ~ -1.05 x<sup>2</sup> + 115.157x + 236.16*.\\n\\n\\n\\nFor a general function, the more you know about it (e.g., is it convex, differentiable, twice differentiable, etc.), the better you can do with [`scipy.optimize.minimize`](http://docs.scipy.org/doc/scipy-0.13.0/reference/generated/scipy.optimize.minimize.html). E.g., if you hardly know anything about it, you can use it specifying to use the Nelder-Mead method. Other methods there (refer to the documentation) can make use of the Jacobian and the Hessian, if they are defined, and you can calculate them. \\n\\n\\n\\nPersonally, I find that using it with Nelder-Mead (requiring almost no parameters) gives me adequate results for my needs.\\n\\n\\n\\n------------------\\n\\n\\n\\n**Example**\\n\\n\\n\\nSuppose you're trying to fit *y = kx* with *k* as the parameter to optimize. You'd write a function\\n\\n\\n\\n    x = ...\\n\\n    y = ...\\n\\n\\n\\n    def ss(k):\\n\\n        # use numpy.linalg.norm to find the sum-of-squares error between y and kx\\n\\n\\n\\nThen you'd use `scipy.optimize.minimize` on the function `ss`.\",\n",
       "  '<python><numpy><machine-learning><scipy><scikit-learn>',\n",
       "  datetime.date(2016, 2, 23),\n",
       "  '2016-02-23 16:47:23',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2015.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1851',\n",
       "  '35612350',\n",
       "  'Answer',\n",
       "  'Why does std::list::reverse have O(n) complexity?',\n",
       "  'Hypothetically, `reverse` could have been *O(1)*. There (again hypothetically) could have been a boolean list member indicating whether the direction of the linked list is currently the same or opposite as the original one where the list was created.\\n\\n\\n\\nUnfortunately, that would reduce the performance of basically any other operation (albeit without changing the asymptotic runtime). In each operation, a boolean would need to be consulted to consider whether to follow a \"next\" or \"prev\" pointer of a link.\\n\\n\\n\\nSince this was presumably considered a relatively infrequent operation, the standard (which does not dictate implementations, only complexity), specified that the complexity could be linear. This allows \"next\" pointers to always mean the same direction unambiguously, speeding up common-case operations.',\n",
       "  '<c++><c++11><stl><linked-list>',\n",
       "  datetime.date(2016, 2, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '183',\n",
       "  '',\n",
       "  '8250.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1857',\n",
       "  '35674735',\n",
       "  'Answer',\n",
       "  'Python - calculate normal distribution',\n",
       "  'A distribution and the cumulative distribution are not the same - the latter is the integral of the former. If the normal distribution looks like a \"bell\", the cumulative normal distribution looks like a gentle \"step\" function.\\n\\n\\n\\nE.g., for the following \"bells\"\\n\\n[![enter image description here][1]][1]\\n\\nyou\\'d get the following \"steps\"\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\nIf you have an array `data`, the following will fit it to a normal distribution using [`scipy.stats.norm`](http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.norm.html):\\n\\n\\n\\n    import numpy as np\\n\\n    from scipy.stats import norm\\n\\n\\n\\n    mu, std = norm.fit(data)\\n\\n\\n\\nThis will return the mean and standard deviation, the combination of which define a normal distribution.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/1ZH7z.png\\n\\n  [2]: http://i.stack.imgur.com/9LNgK.png',\n",
       "  '<python><numpy><scipy><statistics><normal-distribution>',\n",
       "  datetime.date(2016, 2, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '5337.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1858',\n",
       "  '35679163',\n",
       "  'Answer',\n",
       "  'Normalize rows of pandas data frame by their sums',\n",
       "  \"You can use\\n\\n\\n\\n    df.div(df.sum(axis=1), axis=0)\\n\\n\\n\\n`df.sum(axis=1)` sums up each row; `df.div(..., axis=0)` then divides.\\n\\n\\n\\nExample:\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    df = pd.DataFrame({'a': [1, 2], 'b': [3, 4]})\\n\\n    >>> df.div(df.sum(axis=1), axis=0)\\n\\n        a\\tb\\n\\n    0\\t0.250000\\t0.750000\\n\\n    1\\t0.333333\\t0.666667\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 2, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '28',\n",
       "  '',\n",
       "  '9380.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1859',\n",
       "  '35680477',\n",
       "  'Answer',\n",
       "  'Missing Value in Data Analysis',\n",
       "  'It depends a lot on the specific case. However, some general methods are:\\n\\n\\n\\n1. Removing the rows where some of the data is missing.\\n\\n\\n\\n2. [Imputing missing values](https://en.wikipedia.org/wiki/Imputation_(statistics)). Basically, you can consider the gender column as something you must predict (using, possibly, the other columns). Train your predictor using the rows that have all values, and predict the missing ones.\\n\\n\\n\\n3. Creating a third category of \"missing\", and letting the machine learning algorithm deal with it.',\n",
       "  '<machine-learning><missing-data><data-analysis><method-missing>',\n",
       "  datetime.date(2016, 2, 28),\n",
       "  '2016-02-28 16:00:34',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1385.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1862',\n",
       "  '32430628',\n",
       "  'Answer',\n",
       "  'Exponential Smoothing Average',\n",
       "  'I guess the main issue here is what `bwd[::-1]` means? See additional comments added.\\n\\n\\n\\n    # take EWMA in both directions with a smaller span term\\n\\n\\n\\n    fwd = ewma( x, span=15 ) # take EWMA in fwd direction\\n\\n    ## This part should not be a problem, right?\\n\\n\\n\\n    bwd = ewma( x[::-1], span=15 ) # take EWMA in bwd direction\\n\\n    ## x[::-1] means to go thr x, from end to beginning(!), with a step of -1\\n\\n    ## (hence it is going from the back to the front)\\n\\n\\n\\n    c = np.vstack(( fwd, bwd[::-1] )) # lump fwd and bwd together\\n\\n    c = np.mean( c, axis=0 ) # average\\n\\n    ## Then we reverse ewma into a beginning-to-end order\\n\\n    ## and take the average of fwd and bwd\\n\\n    ## IMO, better written just as:\\n\\n\\n\\n    #c =  0.5*(fwd + bwd[::-1])\\n\\n\\n\\nThe idea is, in the forward EWMA, the current value is affected, but increasingly less and less so, by earlier values.  The backward EWMA, on the other hand, are affected by the later values.  Finally, by taking the average of both the forward and backward EWMA, you create something that is affected by the surrounding values (if we call them so), but increasingly less and less so as you moving away from the current position.\\n\\n',\n",
       "  '<python><pandas><average>',\n",
       "  datetime.date(2015, 9, 7),\n",
       "  '2015-09-07 03:27:11',\n",
       "  'CT Zhu (2487184)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1352.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1864',\n",
       "  '36021151',\n",
       "  'Answer',\n",
       "  'Python Pandas: pivot only certain columns in the DataFrame while keeping others',\n",
       "  \"Your first attempt was nearly correct, just use `columns='value_id'` instead of including it in the index.\\n\\n\\n\\n    # Perform the pivot.\\n\\n    df = df.pivot_table(\\n\\n        values='value',\\n\\n        index=['stream_name', 'preferred_timestamp', 'internal_timestamp'],\\n\\n        columns='value_id'\\n\\n        )\\n\\n    \\n\\n    # Formatting.\\n\\n    df.reset_index(inplace=True)\\n\\n    df.columns.name = None\\n\\n\\n\\n\\n\\nThis isn't an issue in your example data, but keep in mind that `pivot_table` will aggregate values if multiple values are pivoted to the same position (taking the mean by default).\",\n",
       "  '<python><pandas><pivot-table>',\n",
       "  datetime.date(2016, 3, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '4240.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1866',\n",
       "  '36085278',\n",
       "  'Answer',\n",
       "  'How to deal with duplicates in red-black trees?',\n",
       "  'If you look at the pseudo-code you wrote here, it is completely agnostic to the question of whether keys are duplicate or not. The code here only looks at the result of *comparing* keys, and doesn\\'t care if they are identical or not. In fact, unique-key implementations need to go out of their way to make `RB-Insert` detect duplicate keys. The data structure doesn\\'t care about this naturally, and the algorithms and proofs hold whether there are duplicate keys or not. If you implemented these functions correctly, it should work as is.\\n\\n\\n\\nI also disagree with the comments advising you to hold what you call \"fat nodes\". Holding multiple keys is the common implementation of C++\\'s [`std::multimap`](http://en.cppreference.com/w/cpp/container/multimap), for example. Not that from a computational complexity point of view, say that you have altogether *n* keys, but each *k* are a multiple. Using the \"efficient\" fat node version, the complexity of the basic find operation will be *&Theta;(log(n / k)) = &Theta;(log(n) - log(k))*; using the multiple key version, the complexity will be *&Theta;(log(n))*. In real life cases, probably *k << n*, which means that the relative difference is negligible. \\n\\n\\n\\n',\n",
       "  '<algorithm><binary-tree><red-black-tree>',\n",
       "  datetime.date(2016, 3, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1235.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1871',\n",
       "  '39240436',\n",
       "  'Answer',\n",
       "  'create matrix structure using pandas',\n",
       "  \"You want to do the math between a vector and its tranposition. Transpose with `.T` and apply the matrix `dot` function between the two dataframes.\\n\\n\\n\\n    df = df.set_index('CODE')\\n\\n    \\n\\n    df.T\\n\\n    Out[10]: \\n\\n    CODE             A    B    C\\n\\n    COEFFICIENT    0.5  0.4  0.3\\n\\n    \\n\\n    df.dot(df.T)\\n\\n    Out[11]: \\n\\n    CODE     A     B     C\\n\\n    CODE                  \\n\\n    A     0.25  0.20  0.15\\n\\n    B     0.20  0.16  0.12\\n\\n    C     0.15  0.12  0.09\\n\\n\\n\\n\",\n",
       "  '<python><pandas><numpy><dataframe>',\n",
       "  datetime.date(2016, 8, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2638.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1874',\n",
       "  '39275726',\n",
       "  'Answer',\n",
       "  'Select row from a DataFrame based on the type of the object(i.e. str)',\n",
       "  \"You can do something *similar* to what you're asking with\\n\\n\\n\\n    In [14]: df[pd.to_numeric(df.A, errors='coerce').isnull()]\\n\\n    Out[14]: \\n\\n           A  B\\n\\n    2  Three  3\\n\\n\\n\\nWhy only similar? Because Pandas stores things in homogeneous columns (all entries in a column are of the same type). Even though you constructed the DataFrame from heterogeneous types, they are all made into columns each of the lowest common denominator:\\n\\n\\n\\n    In [16]: df.A.dtype\\n\\n    Out[16]: dtype('O')\\n\\n\\n\\n\\n\\nConsequently, you can't ask which rows are of what type - they will all be of the same type. What you *can* do is to try to convert the entries to numbers, and check where the conversion failed (this is what the code above does).\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 9, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '5002.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1887',\n",
       "  '35747071',\n",
       "  'Answer',\n",
       "  'sum values of columns starting with the same string in pandas dataframe',\n",
       "  \"I'd suggest that you do something different, which is to perform a transpose, groupby the prefix of the rows (your original columns), sum, and transpose again.\\n\\n\\n\\nConsider the following:\\n\\n\\n\\n    df = pd.DataFrame({\\n\\n            'a_a': [1, 2, 3, 4],\\n\\n            'a_b': [2, 3, 4, 5],\\n\\n            'b_a': [1, 2, 3, 4],\\n\\n            'b_b': [2, 3, 4, 5],\\n\\n        })\\n\\n\\n\\nNow \\n\\n\\n\\n    [s.split('_')[0] for s in df.T.index.values]\\n\\n\\n\\nis the prefix of the columns. So\\n\\n\\n\\n    >>> df.T.groupby([s.split('_')[0] for s in df.T.index.values]).sum().T\\n\\n        a\\tb\\n\\n    0\\t3\\t3\\n\\n    1\\t5\\t5\\n\\n    2\\t7\\t7\\n\\n    3\\t9\\t9\\n\\n\\n\\ndoes what you want.\\n\\n\\n\\nIn your case, make sure to split using the `'-'` character.\",\n",
       "  '<python><pandas><dataframe><startswith>',\n",
       "  datetime.date(2016, 3, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '2814.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1889',\n",
       "  '35759036',\n",
       "  'Answer',\n",
       "  'Most efficient way to construct similarity matrix',\n",
       "  'There are two useful function within `scipy.spatial.distance` that you can use for this: [`pdist`][1] and [`squareform`][2].  Using `pdist` will give you the pairwise distance between observations as a one-dimensional array, and `squareform` will convert this to a distance matrix.\\n\\n\\n\\nOne catch is that `pdist` uses distance measures by default, and not similarity, so you\\'ll need to manually specify your similarity function. Judging by the commented output in your code, your DataFrame is also not in the orientation `pdist` expects, so I\\'ve undone the transpose you did in your code.\\n\\n\\n\\n    import pandas as pd\\n\\n    from scipy.spatial.distance import euclidean, pdist, squareform\\n\\n    \\n\\n    \\n\\n    def similarity_func(u, v):\\n\\n        return 1/(1+euclidean(u,v))\\n\\n    \\n\\n    DF_var = pd.DataFrame.from_dict({\"s1\":[1.2,3.4,10.2],\"s2\":[1.4,3.1,10.7],\"s3\":[2.1,3.7,11.3],\"s4\":[1.5,3.2,10.9]})\\n\\n    DF_var.index = [\"g1\",\"g2\",\"g3\"]\\n\\n\\n\\n    dists = pdist(DF_var, similarity_func)\\n\\n    DF_euclid = pd.DataFrame(squareform(dists), columns=DF_var.index, index=DF_var.index)\\n\\n\\n\\n\\n\\n\\n\\n  [1]: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.pdist.html#scipy.spatial.distance.pdist\\n\\n  [2]: http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.spatial.distance.squareform.html#scipy.spatial.distance.squareform',\n",
       "  '<python><numpy><pandas><matrix><scipy>',\n",
       "  datetime.date(2016, 3, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '8',\n",
       "  '',\n",
       "  '7009.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1898',\n",
       "  '36235180',\n",
       "  'Question',\n",
       "  'Efficiently Creating A Pandas DataFrame From A Numpy 3d array',\n",
       "  \"Suppose we start with\\n\\n\\n\\n    import numpy as np\\n\\n    a = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])\\n\\n\\n\\nHow can this be efficiently be made into a pandas DataFrame equivalent to\\n\\n\\n\\n    import pandas as pd\\n\\n    >>> pd.DataFrame({'a': [0, 0, 1, 1], 'b': [1, 3, 5, 7], 'c': [2, 4, 6, 8]})\\n\\n\\n\\n       a  b  c\\n\\n    0  0  1  2\\n\\n    1  0  3  4\\n\\n    2  1  5  6\\n\\n    3  1  7  8\\n\\n\\n\\nThe idea is to have the `a` column have the index in the first dimension in the original array, and the rest of the columns be a vertical concatenation of the 2d arrays in the latter two dimensions in the original array.\\n\\n\\n\\n\\n\\n(This is easy to do with loops; the question is how to do it without them.)\\n\\n\\n\\n-------------------\\n\\n\\n\\n**Longer Example**\\n\\n\\n\\nUsing @Divakar's excellent suggestion:\\n\\n\\n\\n    >>> np.random.randint(0,9,(4,3,2))\\n\\n    array([[[0, 6],\\n\\n        [6, 4],\\n\\n        [3, 4]],\\n\\n\\n\\n       [[5, 1],\\n\\n        [1, 3],\\n\\n        [6, 4]],\\n\\n\\n\\n       [[8, 0],\\n\\n        [2, 3],\\n\\n        [3, 1]],\\n\\n\\n\\n       [[2, 2],\\n\\n        [0, 0],\\n\\n        [6, 3]]])\\n\\n\\n\\nShould be made to something like:\\n\\n\\n\\n    >>> pd.DataFrame({\\n\\n        'a': [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3], \\n\\n        'b': [0, 6, 3, 5, 1, 6, 8, 2, 3, 2, 0, 6], \\n\\n        'c': [6, 4, 4, 1, 3, 4, 0, 3, 1, 2, 0, 3]})\\n\\n        a  b  c\\n\\n    0   0  0  6\\n\\n    1   0  6  4\\n\\n    2   0  3  4\\n\\n    3   1  5  1\\n\\n    4   1  1  3\\n\\n    5   1  6  4\\n\\n    6   2  8  0\\n\\n    7   2  2  3\\n\\n    8   2  3  1\\n\\n    9   3  2  2\\n\\n    10  3  0  0\\n\\n    11  3  6  3\\n\\n\\n\\n\",\n",
       "  '<numpy><pandas><multidimensional-array><vectorization>',\n",
       "  datetime.date(2016, 3, 26),\n",
       "  '2016-03-26 12:44:42',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '3.0',\n",
       "  '4266.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1902',\n",
       "  '39323924',\n",
       "  'Answer',\n",
       "  'matrices are not aligned error message',\n",
       "  \"You have two columns in your weight matrix:\\n\\n\\n\\n    df3.shape\\n\\n    Out[38]: (4, 2)\\n\\n\\n\\nSet the index to `Symbol` on that matrix to get the proper `dot`:\\n\\n\\n\\n    ret.dot(df3.set_index('Symbol'))\\n\\n    Out[39]:\\n\\n                  Weight\\n\\n    Date\\n\\n    2010-03-02  0.007938\\n\\n    2010-03-03  0.006430\\n\\n    2010-03-04 -0.004278\\n\\n    2010-03-05  0.009902\",\n",
       "  '<python><pandas><matrix><np>',\n",
       "  datetime.date(2016, 9, 5),\n",
       "  '2016-12-05 19:28:59',\n",
       "  'Boud (624829)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2637.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1909',\n",
       "  '39053391',\n",
       "  'Answer',\n",
       "  'groupby python TypeError: unorderable types: tuple() < str()',\n",
       "  'I think you need to use [`groupby.agg`][1] and pass a function to aggregate the sum of each group as shown:\\n\\n\\n\\n\\n\\n    df = pd.DataFrame({\\'PROD_TAG\\':[\"P_1\", \"P_2\", \"P_3\", \"P_3\", \"P_4\", \"P_5\"],\\n\\n                       \\'BRAND\\':[\"A\", \"A\", \"B\", \"B\", \"C\", \"D\"],\\n\\n                       \\'Market\\':[\"Modern Trade\", \"Traditional Trade\",   \\\\\\n\\n                       \"Modern Trade\", \"Traditional Trade\", \"Modern Trade\", \"Modern Trade\"],\\n\\n                       (\\'VAL\\',\\'Per1\\'):[4.3, 5.7, 10.0, 8.7, 12.1, 8.0],\\n\\n                       (\\'VAL\\',\\'Per2\\'):[0.155, 0, 11.2, 6.3, 12.3, 7.0]})\\n\\n \\n\\n    type(df[(\\'VAL\\',\\'Per1\\')].name)\\n\\n    #<class \\'tuple\\'>\\n\\n\\n\\n    df.groupby([\\'BRAND\\'])[(\\'VAL\\',\\'Per1\\'), (\\'VAL\\',\\'Per2\\')].agg(lambda x: x.sum())\\n\\n \\n\\n           (VAL, Per1)  (VAL, Per2)\\n\\n    BRAND                          \\n\\n    A             10.0        0.155\\n\\n    B             18.7       17.500\\n\\n    C             12.1       12.300\\n\\n    D              8.0        7.000\\n\\n\\n\\nAlternatively, the index is not reset and the grouper columns are transformed. So, you could get rid of the *`TypeError`* due to the name mismatch of the columns[`tuple/str`].\\n\\n\\n\\n    df.groupby([\\'BRAND\\'], as_index=False)[(\\'VAL\\',\\'Per1\\'), (\\'VAL\\',\\'Per2\\')].sum()\\n\\n    \\n\\n      BRAND  (VAL, Per1)  (VAL, Per2)\\n\\n    0     A         10.0        0.155\\n\\n    1     B         18.7       17.500\\n\\n    2     C         12.1       12.300\\n\\n    3     D          8.0        7.000\\n\\n\\n\\nBut if you [`rename`][2] the `tuple` columns to `string`, you can continue as before without using `agg` functions:\\n\\n\\n\\n    df.rename(index=str, columns={(\\'VAL\\',\\'Per1\\'): \"(\\'VAL\\',\\'Per1\\')\",      \\\\\\n\\n                                  (\\'VAL\\',\\'Per2\\'): \"(\\'VAL\\',\\'Per2\\')\"}, inplace=True)\\n\\n    \\n\\n    type(df[\"(\\'VAL\\',\\'Per1\\')\"].name)\\n\\n    #<class \\'str\\'>\\n\\n\\n\\n    df.groupby([\\'BRAND\\'])[\"(\\'VAL\\',\\'Per1\\')\",\"(\\'VAL\\',\\'Per2\\')\"].sum()\\n\\n    \\n\\n           (\\'VAL\\',\\'Per1\\')  (\\'VAL\\',\\'Per2\\')\\n\\n    BRAND                                \\n\\n    A                10.0           0.155\\n\\n    B                18.7          17.500\\n\\n    C                12.1          12.300\\n\\n    D                 8.0           7.000\\n\\n\\n\\n\\n\\n**Note:** Tested in `Python 3.5`\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.DataFrameGroupBy.agg.html\\n\\n[2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.rename.html',\n",
       "  '<python-3.x><pandas>',\n",
       "  datetime.date(2016, 8, 20),\n",
       "  '2016-08-20 12:01:37',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1009.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1913',\n",
       "  '35942257',\n",
       "  'Answer',\n",
       "  'DataFrame of DataFrames in Python (Pandas)',\n",
       "  \"I think that pandas offers better alternatives to what you're suggesting (rationale below).\\n\\n\\n\\nFor one, there's the [`pandas.Panel`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Panel.html) data structure, which was meant for things like you're doing here.\\n\\n\\n\\nHowever, as Wes McKinney (the Pandas author) noted in his book [Python for Data Analysis: Data Wrangling with Pandas, NumPy, and IPython](http://www.amazon.com/Python-Data-Analysis-Wrangling-IPython-ebook/dp/B009NLMB8Q), multi-dimensional indices, to a large extent, offer a better alternative.\\n\\n\\n\\nConsider the following alternative to your code:\\n\\n\\n\\n    dfs = []\\n\\n    for year in range(1967,2014):\\n\\n        ....some codes that allow me to generate df1, df2 and df3 \\n\\n        df1['year'] = year\\n\\n        df1['origin'] = 'df1'\\n\\n        df2['year'] = year\\n\\n        df2['origin'] = 'df2'\\n\\n        df3['year'] = year\\n\\n        df3['origin'] = 'df3'\\n\\n        dfs.extend([df1, df2, df3])\\n\\n    df = pd.concat(dfs)\\n\\n\\n\\nThis gives you a DataFrame with 4 columns: `'firm'`, `'price'`, `'year'`, and `'origin'`. \\n\\n\\n\\nThis gives you the flexibility to:\\n\\n\\n\\n* Organize hierarchically by, say, `'year'` and `'origin'`: `df.set_index(['year', 'origin'])`, by, say, `'origin'` and `'price'`: `df.set_index(['origin', 'price'])` \\n\\n\\n\\n* Do `groupby`s according to different levels\\n\\n\\n\\n* In general, slice and dice the data along many different ways.\\n\\n\\n\\nWhat you're suggesting in the question makes one dimension (origin) arbitrarily different, and it's hard to think of an advantage to this. If a split along some dimension is necessary due, to, e.g., performance, you can combine DataFrames better with standard Python data structures:\\n\\n\\n\\n* A dictionary mapping each year to a Dataframe with the other three dimensions.\\n\\n\\n\\n* Three DataFrames, one for each origin, each having three dimensions.\\n\\n\\n\\n\",\n",
       "  '<python><pandas><input>',\n",
       "  datetime.date(2016, 3, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1837.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1918',\n",
       "  '36268312',\n",
       "  'Answer',\n",
       "  'How is a Pandas crosstab different from a Pandas pivot_table?',\n",
       "  \"The main difference between the two is the `pivot_table` expects your input data to already be a DataFrame; you pass a DataFrame to `pivot_table` and specify the `index`/`columns`/`values` by passing the column names as strings.  With `cross_tab`, you  don't necessarily need to have a DataFrame going in, as you just pass array-like objects for `index`/`columns`/`values`.\\n\\n\\n\\nLooking at the [source code][1] for `crosstab`, it essentially takes the array-like objects you pass, creates a DataFrame, then calls `pivot_table` as appropriate.\\n\\n\\n\\nIn general, use `pivot_table` if you already have a DataFrame, so you don't have the additional overhead of creating the same DataFrame again.  If you're starting from array-like objects and are only concerned with the pivoted data, use `crosstab`.  In most cases, I don't think it will really make a difference which function you decide to use.\\n\\n\\n\\n\\n\\n  [1]: https://github.com/pandas-dev/pandas/blob/b5dd6a38b18b3da8736a64ce3ce9b80bbe44f35f/pandas/core/reshape/pivot.py#L344-L470\",\n",
       "  '<pandas><numpy><scipy><pivot-table><crosstab>',\n",
       "  datetime.date(2016, 3, 28),\n",
       "  '2018-03-28 19:28:00',\n",
       "  'root (3339965)',\n",
       "  '14',\n",
       "  '',\n",
       "  '6029.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1920',\n",
       "  '39373401',\n",
       "  'Answer',\n",
       "  'Rolling sum in subgroups of a dataframe (pandas)',\n",
       "  \"Say you start with\\n\\n\\n\\n    In [58]: df = pd.DataFrame({'E-Mail': ['foo'] * 3 + ['bar'] * 3 + ['foo'] * 3, 'Session': range(9)})\\n\\n\\n\\n    In [59]: df\\n\\n    Out[59]: \\n\\n      E-Mail  Session\\n\\n    0    foo        0\\n\\n    1    foo        1\\n\\n    2    foo        2\\n\\n    3    bar        3\\n\\n    4    bar        4\\n\\n    5    bar        5\\n\\n    6    foo        6\\n\\n    7    foo        7\\n\\n    8    foo        8\\n\\n\\n\\n    In [60]: df[['Session']].groupby(df['E-Mail']).apply(pd.rolling_sum, 3)\\n\\n    Out[60]: \\n\\n              Session\\n\\n    E-Mail           \\n\\n    bar    3      NaN\\n\\n           4      NaN\\n\\n           5     12.0\\n\\n    foo    0      NaN\\n\\n           1      NaN\\n\\n           2      3.0\\n\\n           6      9.0\\n\\n           7     15.0\\n\\n           8     21.0\\n\\n\\n\\nIncidentally, note that I just rearranged your `rolling_sum`, but it has been deprecated - you should now use [`rolling`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rolling.html):\\n\\n\\n\\n    df[['Session']].groupby(df['E-Mail']).apply(lambda g: g.rolling(3).sum())\\n\\n\",\n",
       "  '<python><performance><pandas>',\n",
       "  datetime.date(2016, 9, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1602.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1925',\n",
       "  '36044484',\n",
       "  'Answer',\n",
       "  'Saving dict as JSON so that they are human readable',\n",
       "  'You should utilize the `indent` and `separators` parameters in the [`json` module](https://docs.python.org/2/library/json.html). From the docs there:\\n\\n\\n\\n    >>> import json\\n\\n    >>> print json.dumps({\\'4\\': 5, \\'6\\': 7}, sort_keys=True,\\n\\n    ...                  indent=4, separators=(\\',\\', \\': \\'))\\n\\n    {\\n\\n        \"4\": 5,\\n\\n        \"6\": 7\\n\\n    }\\n\\n\\n\\n',\n",
       "  '<python><json><human-readable>',\n",
       "  datetime.date(2016, 3, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '1450.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1927',\n",
       "  '33576709',\n",
       "  'Answer',\n",
       "  'Pandas dataframe: how to apply describe() to each group and add to new columns?',\n",
       "  \"Nothing beats one-liner:\\n\\n\\n\\n    In [145]:\\n\\n\\n\\n    print df.groupby('name').describe().reset_index().pivot(index='name', values='score', columns='level_1')\\n\\n\\n\\n    level_1  25%  50%  75%  count  max  mean  min       std\\n\\n    name                                                   \\n\\n    A        2.0    3  4.0      5    5     3    1  1.581139\\n\\n    B        3.5    5  6.5      4    8     5    2  2.581989\\n\\n\",\n",
       "  '<python><numpy><pandas><dataframe>',\n",
       "  datetime.date(2015, 11, 6),\n",
       "  '',\n",
       "  '',\n",
       "  '9',\n",
       "  '',\n",
       "  '8310.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1929',\n",
       "  '33637642',\n",
       "  'Answer',\n",
       "  'Pandas Grouper by frequency with completeness requirement',\n",
       "  \"You may want to write your own aggergate function, 1, if there are `nan`, return a `nan`; 2, if the period is too short, also return `nan`; 3, otherwise, return the sum:\\n\\n\\n\\n    In [43]:\\n\\n\\n\\n    gpy = df.groupby(pd.Grouper(level='Date', freq='Q'))\\n\\n\\n\\n    print gpy.agg(lambda x: np.nan if (np.isnan(x).any() or len(x)<3) else x.sum())\\n\\n\\n\\n                 Value\\n\\n    Date              \\n\\n    2014-03-31     NaN\\n\\n    2014-06-30     NaN\\n\\n    2014-09-30  1240.7\\n\\n    2014-12-31  1012.4\\n\\n    2015-03-31     NaN\\n\\n    2015-06-30  1056.2\\n\\n    2015-09-30     NaN\\n\\n\",\n",
       "  '<python><pandas><python-datetime>',\n",
       "  datetime.date(2015, 11, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2760.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1931',\n",
       "  '35860670',\n",
       "  'Answer',\n",
       "  'How do i change milliseconds to seconds in python?',\n",
       "  'Out of the many ways to address this, one way would be to specify that one of the operands is a float.\\n\\n\\n\\n    >>> 1762 / 1000 # Both integers\\n\\n    1\\n\\n\\n\\n    >>> 1762 / 1000.0 # One float\\n\\n    1.762',\n",
       "  '<python><date>',\n",
       "  datetime.date(2016, 3, 8),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '3406.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1932',\n",
       "  '35899285',\n",
       "  'Answer',\n",
       "  'Creating a unique value per row in pandas?',\n",
       "  \"If `'employee_id'+'customer_id'+'timestamp'` is long, and you are interested in something that is *unlikely* to have collisions, you can replace it with a hash. The range and quality of the hash will determine the probability of collisions. Perhaps the simplest is to use the [builtin `hash`](https://docs.python.org/2/library/functions.html#hash).  Assuming your DataFrame is `df`, and the columns are strings, this is \\n\\n\\n\\n    (df.employee_id + df.customer_id + df.timestamp).apply(hash)\\n\\n\\n\\nIf you want greater control of the size and collision probability, see [this piece on non-crypotgraphic hash functions in Python](http://www.peterbe.com/plog/best-hashing-function-in-python).\\n\\n\\n\\n-----------------------\\n\\n\\n\\n**Edit** \\n\\n\\n\\nBuilding on [an answer to this question](https://stackoverflow.com/questions/2510716/short-python-alphanumeric-hash-with-minimal-collisions), you could build 10-character hashes like this:\\n\\n\\n\\n    import hashlib\\n\\n    df['survey_id'] = (df.employee_id + df.customer_id + df.timestamp).apply(\\n\\n        lambda s: hashlib.md5(s).digest().encode('base64')[: 10])\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 3, 9),\n",
       "  '2017-05-23 12:24:48',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1297.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1937',\n",
       "  '36296246',\n",
       "  'Answer',\n",
       "  'Pandas finding max value in rolling window of time',\n",
       "  \"Here's a way with resample/rolling.  I get a weird warning using pandas version 0.18.0 and python 3.5.  I don't think it's a concern but not sure why it is generated.\\n\\n\\n\\nThis assumes index is 'timestamp', if not, precede the following with `df = df.set_index('timestamp')`:\\n\\n\\n\\n    >>> df2 = df.resample('30min').sort_index(ascending=False).fillna(np.nan)\\n\\n    >>> df2 = df2.rolling(48,min_periods=1).max()\\n\\n    >>> df.join(df2,rsuffix='2')\\n\\n\\n\\n                         Y   Y2\\n\\n    timestamp                  \\n\\n    2016-03-29 12:00:00  1  3.0\\n\\n    2016-03-29 13:00:00  2  4.0\\n\\n    2016-03-30 11:00:00  3  4.0\\n\\n    2016-03-30 12:30:00  4  4.0\\n\\n    2016-03-30 13:30:00  3  3.0\\n\\n    2016-03-30 14:00:00  2  2.0\\n\\n\\n\\nOn this tiny dataframe it seems to be about twice as fast, but you'd have to test it on a larger dataframe to get a reasonable idea of relative speed.\\n\\n\\n\\nHopefully this is somewhat self expanatory.  The ascending sort is necessary because rolling only allows a backwards or centered window as far as I can tell.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 3, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2532.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1944',\n",
       "  '39132123',\n",
       "  'Answer',\n",
       "  'How can I plot a pandas multiindex dataframe as 3d',\n",
       "  \"You could plot a 3D Bar graph using `Pandas` as shown:\\n\\n\\n\\n\\n\\n**Setup:**\\n\\n\\n\\n    arrays = [[2010, 2010, 2010, 2011, 2011, 2011],['A', 'B', 'C', 'A', 'B', 'C']]\\n\\n    tuples = list(zip(*arrays))\\n\\n    index = pd.MultiIndex.from_tuples(tuples, names=['Year', 'Product'])         \\n\\n    \\n\\n    df = pd.DataFrame({'Sales': [111, 20, 150, 10, 28, 190]}, index=index)\\n\\n    print (df)\\n\\n\\n\\n                  Sales\\n\\n    Year Product       \\n\\n    2010 A          111\\n\\n         B           20\\n\\n         C          150\\n\\n    2011 A           10\\n\\n         B           28\\n\\n         C          190\\n\\n    \\n\\n**Data Wrangling:**\\n\\n\\n\\n    import numpy as np\\n\\n    import pandas as pd\\n\\n    from mpl_toolkits.mplot3d import axes3d\\n\\n    import matplotlib.pyplot as plt\\n\\n\\n\\n    # Set plotting style\\n\\n    plt.style.use('seaborn-white')\\n\\n\\n\\nGrouping similar entries (*get_group*) occuring in the Sales column and iterating through them and later appending them to a `list`. This gets stacked horizontally using [`np.hstack`][1] which forms the `z` dimension of the 3d plot.\\n\\n\\n\\n    L = []\\n\\n    for i, group in df.groupby(level=1)['Sales']:\\n\\n        L.append(group.values)\\n\\n    z = np.hstack(L).ravel()\\n\\n \\n\\nLetting the labels on both the x and y dimensions take unique values of the respective levels of the Multi-Index Dataframe. The x and y dimensions then take the range of these values.\\n\\n   \\n\\n    xlabels = df.index.get_level_values('Year').unique()\\n\\n    ylabels = df.index.get_level_values('Product').unique()\\n\\n    x = np.arange(xlabels.shape[0])\\n\\n    y = np.arange(ylabels.shape[0])\\n\\n \\n\\nReturning coordinate matrices from coordinate vectors using [`np.meshgrid`][2]\\n\\n\\n\\n    x_M, y_M = np.meshgrid(x, y, copy=False)\\n\\n\\n\\n**3-D plotting:**\\n\\n    \\n\\n    fig = plt.figure(figsize=(10, 10))\\n\\n    ax = fig.add_subplot(111, projection='3d')\\n\\n    \\n\\n    # Making the intervals in the axes match with their respective entries\\n\\n    ax.w_xaxis.set_ticks(x + 0.5/2.)\\n\\n    ax.w_yaxis.set_ticks(y + 0.5/2.)\\n\\n\\n\\n    # Renaming the ticks as they were before\\n\\n    ax.w_xaxis.set_ticklabels(xlabels)\\n\\n    ax.w_yaxis.set_ticklabels(ylabels)\\n\\n\\n\\n    # Labeling the 3 dimensions\\n\\n    ax.set_xlabel('Year')\\n\\n    ax.set_ylabel('Product')\\n\\n    ax.set_zlabel('Sales')\\n\\n\\n\\n    # Choosing the range of values to be extended in the set colormap\\n\\n    values = np.linspace(0.2, 1., x_M.ravel().shape[0])\\n\\n\\n\\n    # Selecting an appropriate colormap\\n\\n    colors = plt.cm.Spectral(values)\\n\\n    ax.bar3d(x_M.ravel(), y_M.ravel(), z*0, dx=0.5, dy=0.5, dz=z, color=colors)\\n\\n    plt.show()\\n\\n\\n\\n[![Image][3]][3]\\n\\n\\n\\n\\n\\n----------\\n\\n**Note:**\\n\\n\\n\\nIncase of unbalanced `groupby` objects, you could still do it by `unstacking`\\n\\nand filling `Nans` with 0's and then `stacking` it back as follows:\\n\\n\\n\\n    df = df_multi_index.unstack().fillna(0).stack()\\n\\n\\n\\nwhere `df_multi_index.unstack` is your original multi-index dataframe.\\n\\n\\n\\nFor the new values added to the Multi-index Dataframe, following plot is obtained:\\n\\n\\n\\n[![Image2][4]][4]\\n\\n\\n\\n\\n\\n  [1]: http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.hstack.html\\n\\n  [2]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.meshgrid.html\\n\\n  [3]: http://i.stack.imgur.com/avlF3.png\\n\\n  [4]: http://i.stack.imgur.com/B5EpL.png\",\n",
       "  '<python><pandas><matplotlib><3d><seaborn>',\n",
       "  datetime.date(2016, 8, 24),\n",
       "  '2016-08-25 14:54:22',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1235.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1956',\n",
       "  '39457992',\n",
       "  'Answer',\n",
       "  'Pandas: how to compute the rolling sum of a variable over the last few days but only at a given hour?',\n",
       "  'IIUC, what you want is to perform a rolling sum, but only on the observations grouped by the exact same time of day. This can be done by\\n\\n\\n\\n    df.X.groupby([df.index.hour, df.index.minute]).apply(lambda g: g.rolling(window=5).sum())\\n\\n\\n\\n(Note that your question alternates between 5 and 10 periods.) For example:\\n\\n\\n\\n    In [43]: df.X.groupby([df.index.hour, df.index.minute]).apply(lambda g: g.rolling(window=5).sum()).tail()\\n\\n    Out[43]: \\n\\n    2000-02-04 17:15:00   -2.135887\\n\\n    2000-02-04 17:16:00   -3.056707\\n\\n    2000-02-04 17:17:00    0.813798\\n\\n    2000-02-04 17:18:00   -1.092548\\n\\n    2000-02-04 17:19:00   -0.997104\\n\\n    Freq: T, Name: X, dtype: float64\\n\\n',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 9, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1106.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1957',\n",
       "  '33813309',\n",
       "  'Answer',\n",
       "  'Histogram fitting with python',\n",
       "  'What you described is a form of [exponential distribution][1], and you want to estimate the parameters of the exponential distribution, given the probability density observed in your data.  Instead of using non-linear regression method (which assumes the residue errors are Gaussian distributed), one correct way is arguably a MLE (maximum likelihood estimation).\\n\\n\\n\\n`scipy` provides a large number of continuous distributions in its `stats` library, and the MLE is implemented with the `.fit()` method. Of course, exponential distribution is [there][2]:\\n\\n\\n\\n    In [1]:\\n\\n\\n\\n    import numpy as np\\n\\n    import scipy.stats as ss\\n\\n    import matplotlib.pyplot as plt\\n\\n    %matplotlib inline\\n\\n    In [2]:\\n\\n    #generate data \\n\\n    X = ss.expon.rvs(loc=0.5, scale=1.2, size=1000)\\n\\n\\n\\n    #MLE\\n\\n    P = ss.expon.fit(X)\\n\\n    print P\\n\\n    (0.50046056920696858, 1.1442947648425439)\\n\\n    #not exactly 0.5 and 1.2, due to being a finite sample\\n\\n\\n\\n    In [3]:\\n\\n    #plotting\\n\\n    rX = np.linspace(0,10, 100)\\n\\n    rP = ss.expon.pdf(rX, *P)\\n\\n    #Yup, just unpack P with *P, instead of scale=XX and shape=XX, etc.\\n\\n    In [4]:\\n\\n\\n\\n    #need to plot the normalized histogram with `normed=True`\\n\\n    plt.hist(X, normed=True)\\n\\n    plt.plot(rX, rP)\\n\\n    Out[4]:\\n\\n[![enter image description here][3]][3]\\n\\n\\n\\nYour `distance` will replace `X` here.\\n\\n\\n\\n\\n\\n  [1]: https://en.wikipedia.org/wiki/Exponential_distribution\\n\\n  [2]: http://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.expon.html\\n\\n  [3]: http://i.stack.imgur.com/94u4y.png',\n",
       "  '<python><pandas><matplotlib><scipy><data-analysis>',\n",
       "  datetime.date(2015, 11, 19),\n",
       "  '2015-11-19 20:59:12',\n",
       "  'CT Zhu (2487184)',\n",
       "  '6',\n",
       "  '',\n",
       "  '3608.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1962',\n",
       "  '36411337',\n",
       "  'Answer',\n",
       "  'Python Pandas dataframe: Collect values of a column',\n",
       "  \"If your DataFrame is `df`, then you can use\\n\\n\\n\\n    import itertools\\n\\n\\n\\n    itertools.chain.from_iterable(df.item_list)\\n\\n\\n\\nto create an iterable of all the items. If you do \\n\\n\\n\\n    list(itertools.chain.from_iterable(df.item_list))\\n\\n\\n\\nthen it will become a list.\\n\\n\\n\\n-------------------\\n\\n\\n\\n**Example**\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    df = pd.DataFrame({'item_list': [[1, 2], [3, 4]]})\\n\\n\\n\\n    import itertools\\n\\n\\n\\n    >>> list(itertools.chain.from_iterable(df.item_list.values))\\n\\n    [1, 2, 3, 4]\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 4, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1427.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1963',\n",
       "  '39163700',\n",
       "  'Answer',\n",
       "  'python return empty list from function',\n",
       "  \"Well, at least one case where your function will return an empty list, is if the topmost directory contains only directories. Here's your code (after removing the redundant loop):\\n\\n\\n\\n    for file in os.listdir(currdir):\\n\\n        path = os.path.join(currdir,file)\\n\\n        if not os.path.isdir(path):\\n\\n            myfiles.append(path)\\n\\n        else:\\n\\n            getfiles(path)\\n\\n\\n\\nIf the `else` part is reached, then you recursively call `getfiles(path)`. Unfortunately, you don't do anything with the result, and actually just throw it away. You probably meant the last line to be something like \\n\\n\\n\\n            myfiles.extend(getfiles(path))\\n\\n\\n\\n--------------------\\n\\n\\n\\nAdditionally, you might want to check out [`os.walk`](https://docs.python.org/2/library/os.html), as it does what it seems you're trying to do here.\",\n",
       "  '<python>',\n",
       "  datetime.date(2016, 8, 26),\n",
       "  '2016-08-26 10:36:58',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '0',\n",
       "  '',\n",
       "  '2038.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1967',\n",
       "  '34447448',\n",
       "  'Question',\n",
       "  'StringIO and pandas read_csv',\n",
       "  'I\\'m trying to mix StringIO and BytesIO with pandas and struggling with some basic stuff.  For example, I can\\'t get \"output\" below to work, whereas \"output2\" below does work.  But \"output\" is closer to the real world example I\\'m trying to do.  The way in \"output2\" is from an old pandas example but not really a useful way for me to do it.\\n\\n\\n\\n    import io   # note for python 3 only\\n\\n                # in python2 need to import StringIO\\n\\n    \\n\\n    output = io.StringIO()\\n\\n    output.write(\\'x,y\\\\n\\')\\n\\n    output.write(\\'1,2\\\\n\\')\\n\\n    \\n\\n    output2 = io.StringIO(\"\"\"x,y\\n\\n    1,2\\n\\n    \"\"\")\\n\\n    \\n\\nThey seem to be the same in terms of type and contents:\\n\\n\\n\\n    type(output) == type(output2)\\n\\n    Out[159]: True\\n\\n    \\n\\n    output.getvalue() == output2.getvalue()\\n\\n    Out[160]: True\\n\\n    \\n\\nBut no, not the same:\\n\\n\\n\\n    output == output2\\n\\n    Out[161]: False\\n\\n    \\n\\nMore to the point of the problem I\\'m trying to solve:\\n\\n\\n\\n    pd.read_csv(output)   # ValueError: No columns to parse from file\\n\\n    pd.read_csv(output2)  # works fine, same as reading from a file',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2015, 12, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '8249.0',\n",
       "  '1.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1968',\n",
       "  '35962557',\n",
       "  'Answer',\n",
       "  'Elegant way to replace values in pandas.DataFrame from another DataFrame',\n",
       "  \"This is a little cleaner if you already have the indexes set to id, but if not you can still do in one line:\\n\\n\\n\\n    >>> (dfReplace.set_index('id2').rename( columns = {'value2':'value1'} )\\n\\n                                   .combine_first(df.set_index('id1')))\\n\\n    \\n\\n         value1 value3\\n\\n    1001   rep1    yes\\n\\n    1001   rep1    yes\\n\\n    1002   rep2     no\\n\\n    1002   rep2    yes\\n\\n    1003      d     no\\n\\n    1004      e     no\\n\\n    1005      f     no\\n\\n    1006      h     no\\n\\n\\n\\nIf you separate into three lines and do the renaming and re-indexing separately, you can see that the `combine_first()` by itself is actually very simple:\\n\\n\\n\\n    >>> df = df.set_index('id1')\\n\\n    >>> dfReplace = dfReplace.set_index('id2').rename( columns={'value2':'value1'} )\\n\\n    \\n\\n    >>> dfReplace.combine_first(df)\",\n",
       "  '<python><pandas><apply>',\n",
       "  datetime.date(2016, 3, 12),\n",
       "  '2016-03-12 20:08:53',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '3035.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1973',\n",
       "  '39231251',\n",
       "  'Answer',\n",
       "  'Add a value to the end of a pandas index object',\n",
       "  \"You need to pass a collection of index values as parameter while appending to the given `index` object.\\n\\n\\n\\n    indx.append(pd.Index([20]))   # Pass the values inside the list \\n\\n    Int64Index([11, 12, 13, 14, 15, 20], dtype='int64')\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 8, 30),\n",
       "  '2016-08-30 15:21:21',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '3',\n",
       "  '',\n",
       "  '3023.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1977',\n",
       "  '36587770',\n",
       "  'Answer',\n",
       "  'Easiest way to read csv files with multiprocessing in Pandas',\n",
       "  '[`dask`][1] library is designed to address not only but certainly your issue.\\n\\n\\n\\n\\n\\n  [1]: http://dask.pydata.org/en/latest/',\n",
       "  '<python><csv><pandas><multiprocessing>',\n",
       "  datetime.date(2016, 4, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '5843.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['1992',\n",
       "  '39468002',\n",
       "  'Answer',\n",
       "  'Pandas: outer product of row and col sums',\n",
       "  'A Complete solution using only Pandas built-in methods:\\n\\n\\n\\n    def outer_product(row):\\n\\n        numerator = df.sum(1).mul(row.sum(0))\\n\\n        denominator = df.sum(0).sum(0)\\n\\n        return (numerator.floordiv(denominator))\\n\\n\\n\\n    df.apply(outer_product)\\n\\n\\n\\n  [![Image][1]][1]\\n\\n\\n\\nTimings: For 1 million rows of DF.\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/PVKfO.png\\n\\n  [2]: http://i.stack.imgur.com/NypQY.png',\n",
       "  '<python><r><pandas><outer-join><chi-squared>',\n",
       "  datetime.date(2016, 9, 13),\n",
       "  '2016-09-13 10:45:06',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1240.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2008',\n",
       "  '37171887',\n",
       "  'Answer',\n",
       "  'Pandas Keyerror',\n",
       "  \"This is your original dataframe:\\n\\n\\n\\n    >>> df\\n\\n\\n\\n       a   b   c  cac\\n\\n    0  1  43  65  mns\\n\\n    1  2  34  67   ab\\n\\n    2  3  65  78   cd\\n\\n    3  4  56  65   cd\\n\\n    4  5  29  45   ab\\n\\n    5  6  76  52    k\\n\\n\\n\\n\\n\\n    >>> df.set_index(['cac'], inplace=True)\\n\\n    >>> df\\n\\n\\n\\n         a   b   c\\n\\n    cac           \\n\\n    mns  1  43  65\\n\\n    ab   2  34  67\\n\\n    cd   3  65  78\\n\\n    cd   4  56  65\\n\\n    ab   5  29  45\\n\\n    k    6  76  52\\n\\n\\n\\nSo, setting the index in pandas is simply replacing the before counter values(0,1,2,...,5) to the new row values i.e (mns, ab,...,k) of `cac` column name.\\n\\n\\n\\n\\n\\n    >>> df.ix['mns']\\n\\n\\n\\n    a     1\\n\\n    b    43\\n\\n    c    65\\n\\n\\n\\nThis command specifically searches for row in the index column, `cac` whose value is equal to `mns` and retrieves it's corresponding elements.\\n\\n\\n\\n***Note:***  As `mns` is not a column name of the dataframe, `df['mns']` throws a key error.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 5, 11),\n",
       "  '2016-12-09 09:32:18',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '4022.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2010',\n",
       "  '37228009',\n",
       "  'Answer',\n",
       "  'Cannot import a package installed with pip',\n",
       "  \"Your `pip` is catching the `3.5` version. You must specifically install `pip` for `2.7` version for your code to work. This is how it's done:\\n\\n\\n\\n    $ sudo apt-get install python2-pip\\n\\n    $ sudo pip2 install crc16\",\n",
       "  '<python><pip>',\n",
       "  datetime.date(2016, 5, 14),\n",
       "  '2016-12-09 09:36:37',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2762.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2027',\n",
       "  '39591864',\n",
       "  'Answer',\n",
       "  'minimum number of steps to reduce number to 1',\n",
       "  \"I like the idea by squeamish ossifrage of greedily looking (for the case of odd numbers) whether *n + 1* or *n - 1* looks more promising, but think deciding what looks more promising can be done a bit better than looking at the total number of set bits.\\n\\n\\n\\nFor a number `x`,  \\n\\n\\n\\n    bin(x)[:: -1].index('1')\\n\\n\\n\\nindicates the number of least-significant 0s until the first 1. The idea, then, is to see whether this number is higher for *n + 1* or *n - 1*, and choose the higher of the two (many consecutive least-significant 0s indicate more consecutive halving).\\n\\n\\n\\nThis leads to \\n\\n\\n\\n    def min_steps_back(n):\\n\\n        count_to_1 = lambda x: bin(x)[:: -1].index('1')\\n\\n\\n\\n        if n in [0, 1]:\\n\\n            return 1 - n\\n\\n\\n\\n        if n % 2 == 0:\\n\\n            return 1 + min_steps_back(n / 2)\\n\\n\\n\\n        return 1 + (min_steps_back(n + 1) if count_to_1(n + 1) > count_to_1(n - 1) else min_steps_back(n - 1))\\n\\n\\n\\nTo compare the two, I ran\\n\\n\\n\\n    num = 10000\\n\\n    ms, msb = 0., 0.\\n\\n    for i in range(1000):\\n\\n        n =  random.randint(1, 99999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999)\\n\\n\\n\\n        ms += min_steps(n)\\n\\n        msb += min_steps_back(n)\\n\\n\\n\\n    print ms / num, msb / num\\n\\n\\n\\nWhich outputs\\n\\n\\n\\n    57.4797 56.5844\\n\\n\\n\\nshowing that, on average, this does use fewer operations (albeit not by that much).\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  '<algorithm><math><dynamic-programming>',\n",
       "  datetime.date(2016, 9, 20),\n",
       "  '2016-09-20 12:42:55',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '8874.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2033',\n",
       "  '38150142',\n",
       "  'Answer',\n",
       "  'Delete column(s) from very large CSV file using pandas or blaze',\n",
       "  \"You can use [`dask.dataframe`][1], which is syntactically similar to pandas, but does manipulations out-of-core so memory shouldn't be an issue.  It also parallelizes the process automatically, so it should be fast.\\n\\n\\n\\n    import dask.dataframe as dd\\n\\n    \\n\\n    df = dd.read_csv('myfile.csv', usecols=['col1', 'col2', 'col3'])\\n\\n    df.to_csv('output.csv', index=False)\\n\\n\\n\\n**Timings**\\n\\n\\n\\nI've timed each method posted so far on a 1.4 GB csv file.  I kept four columns, leaving the output csv file at 250 MB.\\n\\n\\n\\nUsing Dask:\\n\\n\\n\\n    %%timeit\\n\\n    df = dd.read_csv(f_in, usecols=cols_to_keep)\\n\\n    df.to_csv(f_out, index=False)\\n\\n    \\n\\n    1 loop, best of 3: 41.8 s per loop\\n\\n\\n\\nUsing Pandas:\\n\\n \\n\\n    %%timeit\\n\\n    chunksize = 10**5\\n\\n    for chunk in pd.read_csv(f_in, chunksize=chunksize, usecols=cols_to_keep):\\n\\n        chunk.to_csv(f_out, mode='a', index=False)\\n\\n    \\n\\n    1 loop, best of 3: 44.2 s per loop\\n\\n\\n\\nUsing Python/CSV:\\n\\n   \\n\\n    %%timeit\\n\\n    inc_f = open(f_in, 'r')\\n\\n    csv_r = csv.reader(inc_f)\\n\\n    out_f = open(f_out, 'w')\\n\\n    csv_w = csv.writer(out_f, delimiter=',', lineterminator='\\\\n')\\n\\n    for row in csv_r:\\n\\n        new_row = [row[1], row[5], row[6], row[8]]\\n\\n        csv_w.writerow(new_row)\\n\\n    inc_f.close()\\n\\n    out_f.close()\\n\\n    \\n\\n    1 loop, best of 3:  1min 1s per loop\\n\\n\\n\\n  [1]: http://dask.pydata.org/en/latest/dataframe.html\",\n",
       "  '<python><csv><pandas><blaze>',\n",
       "  datetime.date(2016, 7, 1),\n",
       "  '2016-07-01 17:09:33',\n",
       "  'root (3339965)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1422.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2041',\n",
       "  '39301020',\n",
       "  'Answer',\n",
       "  'python pandas dtypes detection from sql',\n",
       "  \"It is difficult to dig into your issue without some data samples. However, you probably face either of the two cases:\\n\\n\\n\\n- Some of the rows you select in your second case contain `NULL` values, which stops pandas interpret automatically your column as a datetime\\n\\n- You have a different MDY convention in your database and some dates lower than 13rd of the month are parsed as dates while others aren't and are kept as strings until you convert manually in DMY\",\n",
       "  '<python><sql><csv><pandas><dataframe>',\n",
       "  datetime.date(2016, 9, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1640.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2062',\n",
       "  '38577709',\n",
       "  'Answer',\n",
       "  'Neo4j Bolt StatementResult to Pandas DataFrame',\n",
       "  'The best I can come up with is a list comprehension similar to yours, but less verbose:\\n\\n\\n\\n    df = pd.DataFrame([r.values() for r in result], columns=result.keys())\\n\\n\\n\\nThe [`py2neo`][1] package seems to be more suitable for DataFrames, as it\\'s fairly straightforward to return a list of dictionaries.  Here\\'s the equivalent code using `py2neo`:\\n\\n\\n\\n    import py2neo\\n\\n    \\n\\n    # Some of these keyword arguments are unnecessary, as they are the default values.\\n\\n    graph = py2neo.Graph(bolt=True, host=\\'localhost\\', user=\\'neo4j\\', password=\\'neo4j\\')\\n\\n    \\n\\n    graph.run(\"CREATE (a:Person {name:\\'Arthur\\', title:\\'King\\'})\")\\n\\n    \\n\\n    query = \"MATCH (a:Person) WHERE a.name = \\'Arthur\\' RETURN a.name AS name, a.title AS title\"\\n\\n    df = pd.DataFrame(graph.data(query))\\n\\n\\n\\n\\n\\n  [1]: http://py2neo.org/v3/',\n",
       "  '<python><pandas><neo4j>',\n",
       "  datetime.date(2016, 7, 25),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1680.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2067',\n",
       "  '39619291',\n",
       "  'Answer',\n",
       "  'Pandas convert columns type from list to np.array',\n",
       "  \"Use `apply` to convert each element to it's equivalent array:\\n\\n\\n\\n    df['col1'] = df['col1'].apply(lambda x: np.array(x))\\n\\n\\n\\n    type(df['col1'].iloc[0])\\n\\n    numpy.ndarray\\n\\n\\n\\nData:\\n\\n\\n\\n    df = pd.DataFrame({'col1': [[1,2,3],[0,0,0]]})\\n\\n    df\\n\\n\\n\\n  [![Image][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/F2F0B.png\",\n",
       "  '<python><pandas><numpy><dataframe><casting>',\n",
       "  datetime.date(2016, 9, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '9',\n",
       "  '',\n",
       "  '4221.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2070',\n",
       "  '39647635',\n",
       "  'Answer',\n",
       "  'Pandas merging rows with the same value and same index',\n",
       "  \"You can use [`groupby.first`][1] or [`groupby.last`][2] to get the first/last non-null value for each column within the group.  For the example data, the output would be the same for either method:\\n\\n\\n\\n    df = df.groupby(['SubjectID', 'Visit']).first().reset_index()\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n      SubjectID  Visit  Value1  Value2\\n\\n    0        B1      1    1.57    1.75\\n\\n    1        B1      2     NaN    1.56\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.first.html\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.core.groupby.GroupBy.last.html\",\n",
       "  '<pandas><indexing><merge><rows>',\n",
       "  datetime.date(2016, 9, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '3605.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2073',\n",
       "  '36271919',\n",
       "  'Answer',\n",
       "  'Odo a CSV file into PostgreSQL database',\n",
       "  'I have used `odo` to insert a csv file into postgresql, so it is supported.  Your connection URI and code are essentially the same as mine.  Do you have the [`sqlalchemy`](http://www.sqlalchemy.org/) package installed?  I believe `odo` uses both `sqlalchemy` and `psycopg2` in the background.  I was able to replicate your error on a system without `sqlalchemy` installed, and installing `sqlalchemy` got me past the error.',\n",
       "  '<python><database><postgresql><csv><odo>',\n",
       "  datetime.date(2016, 3, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1511.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2077',\n",
       "  '36603002',\n",
       "  'Answer',\n",
       "  'Number of Fibonacci numbers smaller than number k. Sub O(n)',\n",
       "  \"I think it's fairly easy to see the growth of this number, at least. By the [Binet / De-Moivre formula](https://en.wikipedia.org/wiki/Fibonacci_number#Closed-form_expression),\\n\\n\\n\\n*f<sub>n</sub> = (&phi;<sup>n</sup> - &psi;<sup>n</sup>) / 5*\\n\\n\\n\\nSince *|&psi;| < 1 < &phi;*, then \\n\\n\\n\\n*f<sub>n</sub> &sim; &phi;<sup>n</sup> / 5*. \\n\\n\\n\\nFrom this it follows that the number of Fibonacci numbers smaller than *x* grows like *log<sub>&phi;</sub>(5x)*.\",\n",
       "  '<algorithm><fibonacci>',\n",
       "  datetime.date(2016, 4, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1483.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2081',\n",
       "  '35063786',\n",
       "  'Answer',\n",
       "  'Is numpy slower than c++ linear algebra libraries like eigen?',\n",
       "  \"I have to say that I think that the other answers here are missing things. \\n\\n\\n\\nFirst, as @Mike Muller correctly points out, Python's numerical libraries have C or Fortran (or both) backends, so the performance of pure-Python is almost irrelevant (as opposed to the performance of the backend, which can be significant). In this respect, whether you're manipulating something like [`MKL`](https://software.intel.com/en-us/intel-mkl) through Python or C++ - hardly makes a difference.\\n\\n\\n\\nThere are two differences, though:\\n\\n\\n\\n* On the plus side for Python - it is interactive. This means that, especially in conjunction with something like the [IPython Notebook](http://ipython.org/notebook.html), you can perform an operation and plot the result, perform another operation and plot the result, etc. It's hard to get this effect for exploratory analysis with a compiled language like   C++ or Java.\\n\\n\\n\\n* On the minus side for Python - it, and its scientific ecosystem, handle multicores imperfectly, to say the least. This is a fundamental problem of the language itself  (read about the [GIL](https://wiki.python.org/moin/GlobalInterpreterLock)).\",\n",
       "  '<c++><numpy><machine-learning><neural-network><linear-algebra>',\n",
       "  datetime.date(2016, 1, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2565.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2083',\n",
       "  '35107933',\n",
       "  'Answer',\n",
       "  'scikit-learn: One hot encoding of string categorical features',\n",
       "  \"Very nice question.\\n\\n\\n\\nHowever, in some sense, it is a private case of something that comes up (at least for me) rather often - given `sklearn` stages applicable to subsets of the `X` matrix, I'd like to apply (possibly several) given the entire matrix. Here, for example, you have a stage which knows to run on a single column, and you'd like to apply it thrice - once per column.\\n\\n\\n\\nThis is a classic case for using the [Composite Design Pattern](https://sourcemaking.com/design_patterns/composite).\\n\\n\\n\\nHere is a (sketch of a) reusable stage that accepts a dictionary mapping a column index into the transformation to apply to it:\\n\\n\\n\\n    class ColumnApplier(object):\\n\\n        def __init__(self, column_stages):\\n\\n            self._column_stages = column_stages\\n\\n\\n\\n        def fit(self, X, y):\\n\\n            for i, k in self._column_stages.items():\\n\\n                k.fit(X[:, i])\\n\\n\\n\\n            return self\\n\\n\\n\\n        def transform(self, X):\\n\\n            X = X.copy()\\n\\n            for i, k in self._column_stages.items():\\n\\n                X[:, i] = k.transform(X[:, i])\\n\\n\\n\\n            return X\\n\\n\\n\\nNow, to use it in this context, starting with\\n\\n\\n\\n    X = np.array([['a', 'dog', 'red'], ['b', 'cat', 'green']])\\n\\n    y = np.array([1, 2])\\n\\n    X\\n\\n\\n\\nyou would just use it to map each column index to the transformation you want:\\n\\n\\n\\n    multi_encoder = \\\\\\n\\n        ColumnApplier(dict([(i, preprocessing.LabelEncoder()) for i in range(3)]))\\n\\n    multi_encoder.fit(X, None).transform(X)\\n\\n\\n\\nOnce you develop such a stage (I can't post the one I use), you can use it over and over for various settings.\",\n",
       "  '<python><encoding><scikit-learn><one-hot>',\n",
       "  datetime.date(2016, 1, 30),\n",
       "  '2016-01-30 22:33:58',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '10910.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2086',\n",
       "  '39425708',\n",
       "  'Answer',\n",
       "  'SKlearn (scikit-learn) multivariate feature selection for regression',\n",
       "  '> I want to use a feature selection method where \"combinations\" of features or \"between features\" interactions are considered for a simple linear regression. \\n\\n\\n\\nFor this case, you might consider using [Lasso](https://en.wikipedia.org/wiki/Lasso_(statistics)) (or, actually, the [elastic net refinement](https://en.wikipedia.org/wiki/Elastic_net_regularization)). Lasso attempts to minimize linear least squares, but with absolute-value penalties on the coefficients. Some results from convex-optimization theory (mainly on duality), show that this constraint takes into account \"between feature\" interactions, and removes the more inferior of correlated features. Since Lasso is know to have some shortcomings (it is constrained in the number of features it can pick, for example), a newer variant is elastic net, which penalizes both absolute-value terms and square terms of the coefficients.\\n\\n\\n\\nIn sklearn, [`sklearn.linear_model.ElasticNet`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) implements this. Note that this algorithm requires you to tune the penalties, which you\\'d typically do using cross validation. Fortunately, sklearn also contains [`sklearn.linear_model.ElasticNetCV`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html#sklearn.linear_model.ElasticNetCV), which allows very efficient and convenient searching for the values of these penalty terms.',\n",
       "  '<machine-learning><scikit-learn><feature-selection>',\n",
       "  datetime.date(2016, 9, 10),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1356.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2090',\n",
       "  '37056748',\n",
       "  'Answer',\n",
       "  'Convert a dataframe with multiple rows into one row using pandas?',\n",
       "  \"Use `stack` followed by a transpose to get the DataFrame in the right shape, then format the column names as appropriate.\\n\\n\\n\\n    df = df.stack().to_frame().T\\n\\n    df.columns = ['{}_{}'.format(*c) for c in df.columns]\\n\\n\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2016, 5, 5),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '1895.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2099',\n",
       "  '38699433',\n",
       "  'Answer',\n",
       "  'How to convert std::queue to std::vector',\n",
       "  \"`std::vector` has a [constructor taking a pair of iterators](http://en.cppreference.com/w/cpp/container/vector/vector), so if you would be able to iterate over the queue, you would be set.\\n\\n\\n\\nBorrowing from an answer to [this question](https://stackoverflow.com/questions/1259099/stdqueue-iteration), you can indeed do this by subclassing `std::queue`:\\n\\n\\n\\n    template<typename T, typename Container=std::deque<T> >\\n\\n    class iterable_queue : public std::queue<T,Container>\\n\\n    {\\n\\n    public:\\n\\n        typedef typename Container::const_iterator const_iterator;\\n\\n\\n\\n        const_iterator begin() const { return this->c.begin(); }                                                                               \\n\\n        const_iterator end() const { return this->c.end(); }\\n\\n    };\\n\\n\\n\\n(Note we're allowing only `const` iteration; for the purpose in the question, we don't need iterators allowing modifying elements.)\\n\\n\\n\\nWith this, it's easy to construct a `vector`:\\n\\n\\n\\n    #include <queue>\\n\\n    #include <vector>\\n\\n\\n\\n    using namespace std;\\n\\n\\n\\n    template<typename T, typename Container=std::deque<T> >\\n\\n    class iterable_queue : public std::queue<T,Container>\\n\\n    {\\n\\n    public:\\n\\n        typedef typename Container::const_iterator const_iterator;\\n\\n\\n\\n        const_iterator begin() const { return this->c.begin(); }                                                                               \\n\\n        const_iterator end() const { return this->c.end(); }\\n\\n    };\\n\\n\\n\\n    int main() {\\n\\n        iterable_queue<int> int_queue;\\n\\n        for(int i=0; i<10; ++i)\\n\\n            int_queue.push(i);\\n\\n\\n\\n        vector<int> v(int_queue.begin(), int_queue.end());\\n\\n        return 0;\\n\\n    }\\n\\n\",\n",
       "  '<c++><queue><std><stdvector>',\n",
       "  datetime.date(2016, 8, 1),\n",
       "  '2017-05-23 12:26:09',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '5',\n",
       "  '',\n",
       "  '8486.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2100',\n",
       "  '38753069',\n",
       "  'Answer',\n",
       "  'Stream object directly into a std::string',\n",
       "  \"This can be solved by a new type of [`streambuf`](https://gcc.gnu.org/onlinedocs/libstdc++/manual/streambufs.html) (see [Standard C++ IOStreams and Locales: Advanced Programmer's Guide and Reference](https://www.amazon.com/Standard-IOStreams-Locales-Programmers-Reference/dp/0321585585)).\\n\\n\\n\\nHere is a sketch of how it can look:\\n\\n\\n\\n    #include <streambuf>\\n\\n\\n\\n    class existing_string_buf : public std::streambuf\\n\\n    {\\n\\n    public:\\n\\n        // Store a pointer to to_append.\\n\\n        explicit existing_string_buf(std::string &to_append); \\n\\n\\n\\n\\t    virtual int_type overflow (int_type c) {\\n\\n            // Push here to the string to_append.\\n\\n        }\\n\\n    };\\n\\n\\n\\nOnce you flesh out the details here, you could use it as follows:\\n\\n\\n\\n    #include <iostream>\\n\\n\\n\\n    std::string s;\\n\\n    // Create a streambuf of the string s\\n\\n    existing_string_buf b(s);\\n\\n    // Create an ostream with the streambuf\\n\\n    std::ostream o(&b);\\n\\n\\n\\nNow you just write to `o`, and the result should appear as appended to `s`.\\n\\n\\n\\n    // This will append to s\\n\\n    o << 22;\\n\\n\\n\\n**Edit** \\n\\n\\n\\nAs @rustyx correctly notes, overriding `xsputn` is required for improving performance.\\n\\n\\n\\n\\n\\n**Full Example**\\n\\n\\n\\nThe following prints `22`:\\n\\n\\n\\n    #include <streambuf>\\n\\n    #include <string>\\n\\n    #include <ostream> \\n\\n    #include <iostream>\\n\\n\\n\\n    class existing_string_buf : public std::streambuf\\n\\n    {\\n\\n    public:\\n\\n        // Somehow store a pointer to to_append.\\n\\n        explicit existing_string_buf(std::string &to_append) : \\n\\n            m_to_append(&to_append){}\\n\\n\\n\\n        virtual int_type overflow (int_type c) {\\n\\n            if (c != EOF) {\\n\\n                m_to_append->push_back(c);\\n\\n            }\\n\\n            return c;\\n\\n        }\\n\\n\\n\\n        virtual std::streamsize xsputn (const char* s, std::streamsize n) {\\n\\n            m_to_append->insert(m_to_append->end(), s, s + n);                                                                                 \\n\\n            return n;\\n\\n        }\\n\\n\\n\\n    private:\\n\\n        std::string *m_to_append;\\n\\n    };\\n\\n\\n\\n\\n\\n    int main()\\n\\n    {   \\n\\n        std::string s;\\n\\n        existing_string_buf b(s);\\n\\n        std::ostream o(&b);\\n\\n\\n\\n        o << 22; \\n\\n\\n\\n        std::cout << s << std::endl;\\n\\n    }   \\n\\n\\n\\n\\n\\n\",\n",
       "  '<c++><string>',\n",
       "  datetime.date(2016, 8, 3),\n",
       "  '2016-08-03 20:59:48',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '15',\n",
       "  '',\n",
       "  '1517.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2101',\n",
       "  '39453317',\n",
       "  'Answer',\n",
       "  'How to fillna() with value 0 after calling resample?',\n",
       "  \"The only workaround close to using `fillna` directly would be to call it after performing `.head(len(df.index))`. \\n\\n\\n\\nI'm presuming [`DF.head`][1] to be useful in this case mainly because when resample function is applied to a groupby object, it will act as a filter on the input, returning a reduced shape of the original due to elimination of groups.\\n\\n\\n\\nCalling `DF.head()` does not get affected by this transformation and returns the entire `DF`.\\n\\n\\n\\n**Demo:**\\n\\n\\n\\n    np.random.seed(42)\\n\\n    \\n\\n    df = pd.DataFrame(np.random.randn(10, 2),\\n\\n                  index=pd.date_range('1/1/2016', freq='10D', periods=10),\\n\\n                  columns=['A', 'B']).reset_index()\\n\\n    \\n\\n    df\\n\\n           index         A         B\\n\\n    0 2016-01-01  0.496714 -0.138264\\n\\n    1 2016-01-11  0.647689  1.523030\\n\\n    2 2016-01-21 -0.234153 -0.234137\\n\\n    3 2016-01-31  1.579213  0.767435\\n\\n    4 2016-02-10 -0.469474  0.542560\\n\\n    5 2016-02-20 -0.463418 -0.465730\\n\\n    6 2016-03-01  0.241962 -1.913280\\n\\n    7 2016-03-11 -1.724918 -0.562288\\n\\n    8 2016-03-21 -1.012831  0.314247\\n\\n    9 2016-03-31 -0.908024 -1.412304\\n\\n\\n\\n**Operations:**\\n\\n\\n\\n    resampled_group = df[['index', 'A']].groupby(['index'])['A'].agg('count').resample('2D')\\n\\n    resampled_group.head(len(resampled_group.index)).fillna(0).head(20)\\n\\n\\n\\n    index\\n\\n    2016-01-01    1.0\\n\\n    2016-01-03    0.0\\n\\n    2016-01-05    0.0\\n\\n    2016-01-07    0.0\\n\\n    2016-01-09    0.0\\n\\n    2016-01-11    1.0\\n\\n    2016-01-13    0.0\\n\\n    2016-01-15    0.0\\n\\n    2016-01-17    0.0\\n\\n    2016-01-19    0.0\\n\\n    2016-01-21    1.0\\n\\n    2016-01-23    0.0\\n\\n    2016-01-25    0.0\\n\\n    2016-01-27    0.0\\n\\n    2016-01-29    0.0\\n\\n    2016-01-31    1.0\\n\\n    2016-02-02    0.0\\n\\n    2016-02-04    0.0\\n\\n    2016-02-06    0.0\\n\\n    2016-02-08    0.0\\n\\n    Freq: 2D, Name: A, dtype: float64\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.head.html\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 9, 12),\n",
       "  '2016-09-12 16:54:03',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '3618.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2103',\n",
       "  '39502948',\n",
       "  'Answer',\n",
       "  'Pandas read_sql_query converting integer column to float',\n",
       "  \"As per the [documentation][1], the lack of NA representation in Numpy implies integer NA values can't be managed, so pandas promotes int columns into float.\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/gotchas.html#support-for-integer-na\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 9, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1038.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2104',\n",
       "  '37195394',\n",
       "  'Answer',\n",
       "  'Apply GZIP compression to a CSV in Python Pandas',\n",
       "  \"Using `df.to_csv()` with the keyword argument `compression='gzip'` should produce a gzip archive. I tested it using same keyword arguments as you, and it worked.\\n\\n\\n\\nYou may need to upgrade pandas, as gzip was not implemented until version 0.17.1, but trying to use it on prior versions will not raise an error, and just produce a regular csv.  You can determine your current version of pandas by looking at the output of `pd.__version__`.\",\n",
       "  '<python><csv><pandas><gzip><export-to-csv>',\n",
       "  datetime.date(2016, 5, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '19',\n",
       "  '',\n",
       "  '11616.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2112',\n",
       "  '35186529',\n",
       "  'Answer',\n",
       "  \"How do I delete rows not starting with 'x' in Pandas or keep rows starting with 'x'\",\n",
       "  \"I am a bit confused by your question. In any case, if you have a DataFrame `df` with a column `'c'`, and you would like to remove the items starting with `1`, then the safest way would be to use something like:\\n\\n\\n\\n    df = df[~df['c'].astype(str).str.startswith('1')]\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 2, 3),\n",
       "  '2017-09-18 17:59:39',\n",
       "  'FaCoffee (5110870)',\n",
       "  '12',\n",
       "  '',\n",
       "  '4546.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2117',\n",
       "  '38893248',\n",
       "  'Answer',\n",
       "  'Rounding up a column',\n",
       "  \"You can use [`numpy.ceil`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.ceil.html#numpy.ceil):\\n\\n\\n\\n    In [80]: import numpy as np\\n\\n\\n\\n    In [81]: np.ceil(df.Example)\\n\\n    Out[81]: \\n\\n    0    89.0\\n\\n    1    89.0\\n\\n    2    91.0\\n\\n    3    46.0\\n\\n    Name: Example, dtype: float64\\n\\n\\n\\ndepending on what you like, you could also change the type:\\n\\n\\n\\n    In [82]: np.ceil(df.Example).astype(int)\\n\\n    Out[82]: \\n\\n    0    89\\n\\n    1    89\\n\\n    2    91\\n\\n    3    46\\n\\n    Name: Example, dtype: int64\\n\\n\\n\\n------------------------\\n\\n\\n\\n**Edit**\\n\\n\\n\\nYour error message indicates you're trying just to round (not necessarily up), but are having a type problem. You can solve it like so:\\n\\n\\n\\n    In [84]: df.Example.astype(float).round()\\n\\n    Out[84]: \\n\\n    0    89.0\\n\\n    1    88.0\\n\\n    2    90.0\\n\\n    3    45.0\\n\\n    Name: Example, dtype: float64\\n\\n\\n\\nHere, too, you can cast at the end to an integer type:\\n\\n\\n\\n    In [85]: df.Example.astype(float).round().astype(int)\\n\\n    Out[85]: \\n\\n    0    89\\n\\n    1    88\\n\\n    2    90\\n\\n    3    45\\n\\n    Name: Example, dtype: int64\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 8, 11),\n",
       "  '2016-08-11 10:14:24',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '3286.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2118',\n",
       "  '38895008',\n",
       "  'Answer',\n",
       "  \"Dropping 'nan' with Pearson's r in scipy/pandas\",\n",
       "  \"You can use [`np.isnan`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.isnan.html) like this:\\n\\n\\n\\n    for i in range(len(frame3.columns)):    \\n\\n        x, y = frame3.iloc[ :,i].values, control['CONTROL'].values\\n\\n        nas = np.logical_or(x.isnan(), y.isnan())\\n\\n        corr = sp.pearsonr(x[~nas], y[~nas])\\n\\n        correlation.append(corr)\\n\\n\",\n",
       "  '<pandas><scipy><nan><pearson>',\n",
       "  datetime.date(2016, 8, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '4034.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2121',\n",
       "  '37663417',\n",
       "  'Answer',\n",
       "  'Saving Pandas dataframe indices to file',\n",
       "  \"The error in A) is likely because you have strings or someother type of `object` in your index.  The default format specifier in `np.savetxt` appears to assume the data is `float`-like. I got around this by setting `fmt='%s'`, though it likely isn't a reliable solution.\\n\\n\\n\\nB) doesn't yield any errors for me, using some basic examples of `Index` and `MultiIndex`.  Your error is probably due to the specific type of elements in your index.\\n\\n\\n\\nNote that there is an easier and more reliable way to save just the index.  You can set the `columns` parameter of [`to_csv`][1] as an empty list, which will suppress all columns from the output:\\n\\n\\n\\n    df.to_csv('file_name', columns=[], header=False)\\n\\n\\n\\nIf your index has a name and you want the name to appear in the output (similar to how column names appear), remove `header=False` from the code above.\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/version/0.18.1/generated/pandas.DataFrame.to_csv.html\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2016, 6, 6),\n",
       "  '2016-06-06 17:47:37',\n",
       "  'root (3339965)',\n",
       "  '6',\n",
       "  '',\n",
       "  '1578.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2128',\n",
       "  '38923436',\n",
       "  'Answer',\n",
       "  'Using the .loc function of the pandas dataframe',\n",
       "  \"You can use [`between`][1] to get Boolean values, then `astype` to convert from Boolean values to 0/1:\\n\\n\\n\\n    dtaframe['b'] = dtaframe['a'].between(0, 5, inclusive=False).astype(int)\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n       a  b\\n\\n    0  1  1\\n\\n    1  0  0\\n\\n    2  1  1\\n\\n    3  0  0\\n\\n    4  1  1\\n\\n    5  3  1\\n\\n    6  4  1\\n\\n    7  6  0\\n\\n    8  4  1\\n\\n    9  6  0\\n\\n\\n\\n**Edit**\\n\\n\\n\\nFor multiple ranges, you could use [`pandas.cut`][2]:\\n\\n\\n\\n    dtaframe['b'] = pd.cut(dtaframe['a'], bins=[0,1,6,9], labels=False, include_lowest=True)\\n\\n\\n\\nYou'll need to be careful about how you define `bins`.  Using `labels=False` will return integer indicators for each bin, which happens to correspond with the labels you provided.  You could also manually specify the labels for each bin, e.g. `labels=[0,1,2]`, `labels=[0,17,19]`, `labels=['a','b','c']`, etc.  You may need to use `astype` if you manually specify the labels, as they'll be returned as categories.\\n\\n\\n\\nAlternatively, you could combine `loc` and `between` to manually specify each range:\\n\\n\\n\\n    dtaframe.loc[dtaframe['a'].between(0,1), 'b'] = 0\\n\\n    dtaframe.loc[dtaframe['a'].between(2,6), 'b'] = 1\\n\\n    dtaframe.loc[dtaframe['a'].between(7,9), 'b'] = 2\\n\\n\\n\\n  \\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.between.html\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 8, 12),\n",
       "  '2016-08-12 19:18:28',\n",
       "  'root (3339965)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1548.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2131',\n",
       "  '35288267',\n",
       "  'Answer',\n",
       "  'C++ write to csv, performance',\n",
       "  \"You should note that [`endl`](http://en.cppreference.com/w/cpp/io/manip/endl) is more than a newline - it actually *flushes* data to the disk. \\n\\n\\n\\n> Inserts a newline character into the output sequence os and flushes it as if by calling os.put(os.widen('\\\\n')) followed by os.flush().\\n\\n\\n\\nThis might slow down things considerably. You should try replacing it with a newline.\",\n",
       "  '<c++><performance><csv>',\n",
       "  datetime.date(2016, 2, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '12',\n",
       "  '',\n",
       "  '1313.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2132',\n",
       "  '35301720',\n",
       "  'Answer',\n",
       "  'Remove extra white spaces in C++',\n",
       "  'There are plenty of ways of doing this (e.g., using regular expressions), but one way you could do this is using [`std::copy_if`](http://en.cppreference.com/w/cpp/algorithm/copy) with a stateful functor remembering whether the last character was a space:\\n\\n\\n\\n    #include <algorithm>\\n\\n    #include <string>\\n\\n    #include <iostream>\\n\\n\\n\\n    struct if_not_prev_space\\n\\n    {\\n\\n        // Is last encountered character space.\\n\\n        bool m_is = false;\\n\\n\\n\\n        bool operator()(const char c)\\n\\n        {                                      \\n\\n            // Copy if last was not space, or current is not space.                                                                                                                                                              \\n\\n            const bool ret = !m_is || c != \\' \\';\\n\\n            m_is = c == \\' \\';\\n\\n            return ret;\\n\\n        }\\n\\n    };\\n\\n\\n\\n\\n\\n    int main()\\n\\n    {\\n\\n        const std::string s(\"abc  sssd g g sdg    gg  gf into abc sssd g g sdg gg gf\");\\n\\n        std::string o;\\n\\n        std::copy_if(std::begin(s), std::end(s), std::back_inserter(o), if_not_prev_space());\\n\\n        std::cout << o << std::endl;\\n\\n    }\\n\\n',\n",
       "  '<c++><string><algorithm>',\n",
       "  datetime.date(2016, 2, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '12527.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2137',\n",
       "  '39667248',\n",
       "  'Answer',\n",
       "  'Python vectorizing nested for loops',\n",
       "  'Say you first build an `xyzy` array:\\n\\n\\n\\n    import itertools\\n\\n\\n\\n    xyz = [np.array(p) for p in itertools.product(range(volume.shape[0]), range(volume.shape[1]), range(volume.shape[2]))]\\n\\n\\n\\nNow, using [`numpy.linalg.norm`](http://docs.scipy.org/doc/numpy-1.10.4/reference/generated/numpy.linalg.norm.html), \\n\\n\\n\\n    np.linalg.norm(xyz - roi, axis=1) < radius\\n\\n\\n\\nchecks whether the distance for each tuple from `roi` is smaller than radius.\\n\\n\\n\\nFinally, just `reshape` the result to the dimensions you need.\\n\\n',\n",
       "  '<python><numpy><vectorization>',\n",
       "  datetime.date(2016, 9, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '3811.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2139',\n",
       "  '39687168',\n",
       "  'Answer',\n",
       "  'How to calculate a partial Area Under the Curve (AUC)',\n",
       "  \"Say we start with\\n\\n\\n\\n    import numpy as np\\n\\n    from sklearn import  metrics\\n\\n\\n\\nNow we set the true `y` and predicted `scores`:\\n\\n\\n\\n    y = np.array([0, 0, 1, 1])\\n\\n\\n\\n    scores = np.array([0.1, 0.4, 0.35, 0.8])\\n\\n\\n\\n(Note that `y` has shifted down by 1 from your problem. This is inconsequential: the exact same results (fpr, tpr, thresholds, etc.) are obtained whether predicting 1, 2 or 0, 1, but some `sklearn.metrics` functions are a drag if not using 0, 1.)\\n\\n\\n\\nLet's see the AUC here:\\n\\n\\n\\n    >>> metrics.roc_auc_score(y, scores)\\n\\n    0.75\\n\\n\\n\\nAs in your example:\\n\\n\\n\\n    fpr, tpr, thresholds = metrics.roc_curve(y, scores)\\n\\n    >>> fpr, tpr\\n\\n    (array([ 0. ,  0.5,  0.5,  1. ]), array([ 0.5,  0.5,  1. ,  1. ]))\\n\\n\\n\\nThis gives the following plot:\\n\\n\\n\\n    plot([0, 0.5], [0.5, 0.5], [0.5, 0.5], [0.5, 1], [0.5, 1], [1, 1]);\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nBy construction, the ROC for a finite-length *y* will be composed of rectangles:\\n\\n\\n\\n* For low enough threshold, everything will be classified as negative.\\n\\n\\n\\n* As the threshold increases continuously, at *discrete points*, some negative classifications will be changed to positive.\\n\\n\\n\\nSo, for a finite *y*, the ROC will always be characterized by a sequence of connected horizontal and vertical lines leading from *(0, 0)* to *(1, 1)*. \\n\\n\\n\\nThe AUC is the sum of these rectangles. Here, as shown above, the AUC is 0.75, as the rectangles have areas 0.5 * 0.5 + 0.5 * 1 = 0.75. \\n\\n\\n\\nIn some cases, people choose to calculate the AUC by linear interpolation. Say the length of *y* is much larger than the actual number of points calculated for the FPR and TPR. Then, in this case, a linear interpolation is an approximation of what the points in between *might* have been. In some cases people also follow the *conjecture* that, had *y* been large enough, the points in between would be interpolated linearly. `sklearn.metrics` does not use this conjecture, and to get results consistent with `sklearn.metrics`, it is necessary to use rectangle, not trapezoidal, summation.\\n\\n\\n\\nLet's write our own function to calculate the AUC directly from `fpr` and `tpr`:\\n\\n\\n\\n    import itertools\\n\\n    import operator\\n\\n\\n\\n    def auc_from_fpr_tpr(fpr, tpr, trapezoid=False):\\n\\n        inds = [i for (i, (s, e)) in enumerate(zip(fpr[: -1], fpr[1: ])) if s != e] + [len(fpr) - 1]\\n\\n        fpr, tpr = fpr[inds], tpr[inds]\\n\\n        area = 0\\n\\n        ft = zip(fpr, tpr)\\n\\n        for p0, p1 in zip(ft[: -1], ft[1: ]):\\n\\n            area += (p1[0] - p0[0]) * ((p1[1] + p0[1]) / 2 if trapezoid else p0[1])\\n\\n        return area\\n\\n\\n\\nThis function takes the FPR and TPR, and an optional parameter stating whether to use trapezoidal summation. Running it, we get:\\n\\n\\n\\n    >>> auc_from_fpr_tpr(fpr, tpr), auc_from_fpr_tpr(fpr, tpr, True)\\n\\n    (0.75, 0.875)\\n\\n\\n\\nWe get the same result as `sklearn.metrics` for the rectangle summation, and a different, higher, result for trapezoid summation.\\n\\n\\n\\nSo, now we just need to see what would happen to the FPR/TPR points if we would terminate at an FPR of 0.1. We can do this with the [`bisect` module](https://docs.python.org/2/library/bisect.html)\\n\\n\\n\\n    import bisect\\n\\n\\n\\n    def get_fpr_tpr_for_thresh(fpr, tpr, thresh):\\n\\n        p = bisect.bisect_left(fpr, thresh)\\n\\n        fpr = fpr.copy()\\n\\n        fpr[p] = thresh\\n\\n        return fpr[: p + 1], tpr[: p + 1]\\n\\n\\n\\nHow does this work? It simply checks where would be the insertion point of `thresh` in `fpr`. Given the properties of the FPR (it must start at 0), the insertion point must be in a horizontal line. Thus all rectangles before this one should be unaffected, all rectangles after this one should be removed, and this one should be possibly shortened. \\n\\n\\n\\nLet's apply it:\\n\\n\\n\\n    fpr_thresh, tpr_thresh = get_fpr_tpr_for_thresh(fpr, tpr, 0.1)\\n\\n    >>> fpr_thresh, tpr_thresh\\n\\n    (array([ 0. ,  0.1]), array([ 0.5,  0.5]))\\n\\n\\n\\nFinally, we just need to calculate the AUC from the updated versions:\\n\\n\\n\\n    >>> auc_from_fpr_tpr(fpr, tpr), auc_from_fpr_tpr(fpr, tpr, True)\\n\\n    0.050000000000000003, 0.050000000000000003)\\n\\n\\n\\nIn this case, both the rectangle and trapezoid summations give the same results. Note that in general, they will not. For consistency with `sklearn.metrics`, the first one should be used.\\n\\n\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/A142Q.png\",\n",
       "  '<python><machine-learning><statistics><scikit-learn>',\n",
       "  datetime.date(2016, 9, 25),\n",
       "  '2016-09-26 17:03:49',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '7',\n",
       "  '',\n",
       "  '5439.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2141',\n",
       "  '37686724',\n",
       "  'Answer',\n",
       "  'Convert rows of pandas Dataframe into an iterable list of strings',\n",
       "  \"You can perform `groupby` followed by finding intersection between the two lists as shown:  \\n\\n\\n\\n    >>>df2 = df.groupby('USER_ID')['PRODUCT'].apply(list).reset_index()\\n\\n    >>>df2\\n\\n\\n\\n       USER_ID    PRODUCT\\n\\n    0        1  [a, b, c]\\n\\n    1        2  [d, a, k]\\n\\n\\n\\n    >>>list(set(df2['PRODUCT'].loc[0]).intersection(df2['PRODUCT'].loc[1]))\\n\\n    ['a']\\n\\n\\n\\nOr in a more shorter way:\\n\\n\\n\\n    df2 = df.groupby('USER_ID')['PRODUCT'].apply(list)\\n\\n    >>>list(set(df2.loc[1]).intersection(df2.loc[2]))\\n\\n    ['a']\\n\\n\",\n",
       "  '<python><list><pandas><intersection><iterable>',\n",
       "  datetime.date(2016, 6, 7),\n",
       "  '2017-02-02 11:02:02',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1210.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2150',\n",
       "  '35368169',\n",
       "  'Answer',\n",
       "  'Python fast DataFrame concatenation',\n",
       "  \"There are a couple of things that stand out.\\n\\n\\n\\n----------------\\n\\n\\n\\nTo begin with, the loop\\n\\n\\n\\n    i = 0\\n\\n    while i < (max_count // int(counts[tag])):\\n\\n        array = pandas.concat([array, group])\\n\\n        i += 1\\n\\n\\n\\nis going to be very slow. Pandas is not built for these dynamic concatenations, and I suspect the performance is quadratic for what you're doing. \\n\\n\\n\\nInstead, perhaps you could try\\n\\n\\n\\n    pandas.concat([group] * (max_count // int(counts[tag]))\\n\\n\\n\\nwhich just creates a list first, and then calls `concat` for a one-shot concatenation on the entire list. This should bring the complexity to being linear, and I suspect it will have lower constants in any case.\\n\\n\\n\\n------------------\\n\\n\\n\\nAnother thing which would reduce these small `concats` is calling [`groupby-apply`](http://pandas.pydata.org/pandas-docs/stable/groupby.html). Instead of iterating over the result of `groupby`, write the loop body as a function, and call `apply` on it. Let Pandas figure out best how to concat all of the results into a single DataFrame.\\n\\n\\n\\nHowever, even if you prefer to keep the loop, I'd just append things into a list, and just `concat` everything at the end:\\n\\n\\n\\n    stuff = []\\n\\n    for tag, group in data.groupby(expectation, sort=False):\\n\\n        # Call stuff.append for any DataFrame you were going to concat.\\n\\n    pandas.concat(stuff)\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 2, 12),\n",
       "  '2016-02-12 17:05:14',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1461.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2152',\n",
       "  '38949542',\n",
       "  'Answer',\n",
       "  'pandas DataFrame sort rows by duplicate',\n",
       "  \"You can formulate your need as taking the first row of each group, then the second line, then the thrid, etc. So this is equivalent to group your results by row per group and then per `'A'` .\\n\\n\\n\\nYou can number your rows per key in  `'A'` with the function [`rank`][1]. Apply this function on each group and you are done:\\n\\n\\n\\n    df['C'] = df.groupby('A')['B'].rank()\\n\\n    \\n\\n    df\\n\\n    Out[8]: \\n\\n        A  B    C\\n\\n    0  r1  0  1.0\\n\\n    1  r1  1  2.0\\n\\n    2  r2  2  1.0\\n\\n    3  r2  3  2.0\\n\\n    4  r3  4  1.0\\n\\n    5  r3  5  2.0\\n\\n\\n\\n    df.sort_values(['C', 'A'])\\n\\n    Out[9]: \\n\\n        A  B    C\\n\\n    0  r1  0  1.0\\n\\n    2  r2  2  1.0\\n\\n    4  r3  4  1.0\\n\\n    1  r1  1  2.0\\n\\n    3  r2  3  2.0\\n\\n    5  r3  5  2.0\\n\\n\\n\\nYou drop `'C'` if you don't need it.\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n**Edit to follow up on a comment**\\n\\n\\n\\nI take for granted in your sample `'B'` is your index column. If it's not then you need to work on the index itself:\\n\\n\\n\\n    df['C'] = df.reset_index().groupby('A')['index'].rank()\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.rank.html\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><sorting><pandas><dataframe><duplicates>',\n",
       "  datetime.date(2016, 8, 15),\n",
       "  '2016-08-15 05:26:20',\n",
       "  'Boud (624829)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1123.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2157',\n",
       "  '37755179',\n",
       "  'Answer',\n",
       "  'How to get the indices list of all NaN value in numpy array?',\n",
       "  'You can use [`np.where`][1] to match the boolean conditions corresponding to `Nan` values of the array and `map` each outcome to generate a list of `tuples`.\\n\\n   \\n\\n    >>>list(map(tuple, np.where(np.isnan(x))))\\n\\n    [(1, 2), (2, 0)]\\n\\n\\n\\n[1]:http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.where.html',\n",
       "  '<python><numpy><scipy>',\n",
       "  datetime.date(2016, 6, 10),\n",
       "  '2017-02-02 10:48:08',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '11',\n",
       "  '',\n",
       "  '45255.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2171',\n",
       "  '38383282',\n",
       "  'Answer',\n",
       "  'Pandas pd.isnull() function',\n",
       "  \"This is the expected behavior for [`where`][1]. According to the docs, `where` keeps values that are `True` and replaces values that are `False`, and `pd.isnull` will return `True` only for the `None` entries, which is why they were the only ones that were kept.\\n\\n\\n\\nYou either want to use the [`mask`][2] function with `pd.isnull`:\\n\\n\\n\\n    mydf.mask(pd.isnull(mydf), 0, inplace=True)\\n\\n\\n\\nOr you want to use `where` with `pd.notnull`:\\n\\n\\n\\n    mydf.where(pd.notnull(mydf), 0, inplace=True)\\n\\n\\n\\nRegardless, @piRSquared's method is probably better than either of the above. \\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.where.html\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mask.html\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 7, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '8147.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2172',\n",
       "  '38492356',\n",
       "  'Answer',\n",
       "  'Grouping and ungrouping based on a column',\n",
       "  'To perform the grouping, you can `groupby` on `\\'groupId\\'`, and then within each group perform a join with your given delimiter on each column:\\n\\n\\n\\n    def group_delim(grp, delim=\\'|\\'):\\n\\n        \"\"\"Join each columns within a group by the given delimiter.\"\"\"\\n\\n        return grp.apply(lambda col: delim.join(col))\\n\\n    \\n\\n    # Make sure the DataFrame consists of strings, then apply grouping function.\\n\\n    grouped = df.astype(str).groupby(\\'groupId\\').apply(group_delim)\\n\\n    \\n\\n    # Drop the grouped groupId column, and replace it with the index groupId.\\n\\n    grouped = grouped.drop(\\'groupId\\', axis=1).reset_index()\\n\\n\\n\\nThe grouped output:\\n\\n\\n\\n      groupId uniqueId                                   feature_1 feature_2\\n\\n    0     100    1|2|5  text of 1|some text of 2|another text of 5  10|20|50\\n\\n    1     200      3|4                    text of 3|more text of 4     30|40 \\n\\n\\n\\nSimilar idea for the inverse process, but since each row is a unique group you can just use a regular `apply`, no need for a `groupby`:\\n\\n\\n\\n    def ungroup_delim(col, delim=\\'|\\'):\\n\\n        \"\"\"Split elements in a column by the given delimiter, stacking columnwise\"\"\"\\n\\n        return col.str.split(delim, expand=True).stack()\\n\\n    \\n\\n    # Apply the ungrouping function, and forward fill elements that aren\\'t grouped.\\n\\n    ungrouped = grouped.apply(ungroup_delim).ffill()\\n\\n    \\n\\n    # Drop the unwieldy altered index for a new one.\\n\\n    ungrouped = ungrouped.reset_index(drop=True)\\n\\n\\n\\nAnd ungrouping yields the original data:\\n\\n\\n\\n      groupId uniqueId          feature_1 feature_2\\n\\n    0     100        1          text of 1        10\\n\\n    1     100        2     some text of 2        20\\n\\n    2     100        5  another text of 5        50\\n\\n    3     200        3          text of 3        30\\n\\n    4     200        4     more text of 4        40\\n\\n\\n\\nTo use different delimiters, you\\'d just pass `delim` as an argument to `apply`:\\n\\n\\n\\n    foo.apply(group_delim, delim=\\';\\')\\n\\n\\n\\nAs a side note, in general iterating over DataFrames is quite slow.  Whenever possible you\\'ll want to use a vectorized approach like what I\\'ve done above.',\n",
       "  '<python><r><csv><pandas>',\n",
       "  datetime.date(2016, 7, 20),\n",
       "  '2016-07-20 23:21:53',\n",
       "  'root (3339965)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1227.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2174',\n",
       "  '35405356',\n",
       "  'Answer',\n",
       "  'Pandas: How to drop self correlation from correlation matrix',\n",
       "  \"Say you have\\n\\n\\n\\n    corrs = df.corr()\\n\\n\\n\\nThen the problem is with the diagonal elements, IIUC. You can easily set them to some negative value, say -2 (which will necessarily be lower than all correlations) with\\n\\n\\n\\n    np.fill_diagonal(corrs.values, -2)\\n\\n\\n\\n-----\\n\\n\\n\\n**Example**\\n\\n\\n\\n(Many thanks to @Fabian Rost for the improvement & @jezrael for the DataFrame)\\n\\n\\n\\n    import numpy as np\\n\\n    df=pd.DataFrame( {\\n\\n        'one':[0.1, .32, .2, 0.4, 0.8], \\n\\n        'two':[.23, .18, .56, .61, .12], \\n\\n        'three':[.9, .3, .6, .5, .3], \\n\\n        'four':[.34, .75, .91, .19, .21], \\n\\n        'zive': [0.1, .32, .2, 0.4, 0.8], \\n\\n        'six':[.9, .3, .6, .5, .3],\\n\\n        'drive':[.9, .3, .6, .5, .3]})\\n\\n    corrs = df.corr()\\n\\n    np.fill_diagonal(corrs.values, -2)\\n\\n    >>> corrs\\n\\n        drive\\tfour\\tone\\tsix\\tthree\\ttwo\\tzive\\n\\n    drive\\t-2.000000\\t-0.039607\\t-0.747365\\t1.000000\\t1.000000\\t0.238102\\t-0.747365\\n\\n    four\\t-0.039607\\t-2.000000\\t-0.489177\\t-0.039607\\t-0.039607\\t0.159583\\t-0.489177\\n\\n    one\\t-0.747365\\t-0.489177\\t-2.000000\\t-0.747365\\t-0.747365\\t-0.351531\\t1.000000\\n\\n    six\\t1.000000\\t-0.039607\\t-0.747365\\t-2.000000\\t1.000000\\t0.238102\\t-0.747365\\n\\n    three\\t1.000000\\t-0.039607\\t-0.747365\\t1.000000\\t-2.000000\\t0.238102\\t-0.747365\\n\\n    two\\t0.238102\\t0.159583\\t-0.351531\\t0.238102\\t0.238102\\t-2.000000\\t-0.351531\\n\\n    zive\\t-0.747365\\t-0.489177\\t1.000000\\t-0.747365\\t-0.747365\\t-0.351531\\t-2.000000\",\n",
       "  '<python><numpy><pandas><correlation>',\n",
       "  datetime.date(2016, 2, 15),\n",
       "  '2016-02-15 09:24:18',\n",
       "  'Fabian Rost (5142797), Ami Tavory (3510736)',\n",
       "  '8',\n",
       "  '',\n",
       "  '2448.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2175',\n",
       "  '35411880',\n",
       "  'Answer',\n",
       "  'Efficiently ploting a table in csv format using Python',\n",
       "  \"You might consider using [Pandas](http://pandas.pydata.org/) for this munging + plotting of data.\\n\\n\\n\\nI didn't follow through your logic all the way (i.e., the mask), but here is the output of the following two lines (on part of your data):\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    df = pd.read_csv('stuff.csv', delimiter=',', index_col='year').T.plot();\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nThe more stuff you have (e.g., handling missing data, etc.) - the longer the difference in lines of code will become. Numpy is great, but you should probably use higher-level libraries (built over it!) - for this sort of stuff.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/Vn9Jp.png\",\n",
       "  '<python><csv><numpy><matplotlib><plot>',\n",
       "  datetime.date(2016, 2, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2283.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2180',\n",
       "  '38622421',\n",
       "  'Answer',\n",
       "  'How can I use sklearn.naive_bayes with (multiple) categorical features?',\n",
       "  '> Some of the features are boolean, but other features are categorical and can take on a small number of values (~5).\\n\\n\\n\\nThis is an interesting question, but it is actually more than a single one:\\n\\n\\n\\n1. How to deal with a categorical feature in NB.\\n\\n2. How to deal with non-homogeneous features in NB (and, as I\\'ll point out in the following, even two categorical features are non-homogeneous).\\n\\n3. How to do this in `sklearn`.\\n\\n\\n\\n----------------------------------\\n\\n\\n\\nConsider first a *single categorical feature*. NB assumes/simplifies that the features are independent. Your idea of transforming this into several binary variables is exactly that of [dummy variables](http://www.psychstat.missouristate.edu/multibook/mlt08m.html). Clearly, these dummy variables are anything but independent. Your idea of then running a Bernoulli NB on the result implicitly assumes independence. While it is known that, in practice, NB does not necessarily break when faced with dependent variables, there is no reason to try to transform the problem into the worst configuration for NB, especially as multinomial NB is a very easy alternative.\\n\\n\\n\\nConversely, suppose that after transforming the single categorical variable into a multi-column dataset using the dummy variables, you use a multinomial NB. The theory for multinomial NB [states](https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Multinomial_naive_Bayes):\\n\\n\\n\\n> With a multinomial event model, samples (feature vectors) represent the frequencies with which certain events have been generated by a multinomial ... where p i is the probability that event i occurs. A feature vector ... is then a histogram, with x i {\\\\displaystyle x_{i}} x_{i} counting the number of times event i was observed in a particular instance. This is the event model typically used for document classification, with events representing the occurrence of a word in a single document (see bag of words assumption). \\n\\n\\n\\nSo, here, each instance of your single categorical variable is a \"length-1 paragraph\", and the distribution is exactly multinomial. Specifically, each row has 1 in one position and 0 in all the rest because a length-1 paragraph must have exactly one word, and so those will be the frequencies.\\n\\n\\n\\nNote that from the point of view of `sklearn`\\'s multinomial NB, the fact that the dataset is 5-columned, does not now imply an assumption of independence.\\n\\n\\n\\n-------------------------\\n\\n\\n\\nNow consider the case where you have a dataset consisting of several features:\\n\\n\\n\\n1. Categorical\\n\\n2. Bernoulli\\n\\n3. Normal\\n\\n\\n\\nUnder the very assumption of using NB, these variables are independent. Consequently, you can do the following:\\n\\n\\n\\n1. Build a NB classifier for *each* of the categorical data separately, using your dummy variables and a multinomial NB.\\n\\n2. Build a NB classifier for *all* of the Bernoulli data at once - this is because `sklearn`\\'s Bernoulli NB is simply a shortcut for several single-feature Bernoulli NBs.\\n\\n3. Same as 2 for all the normal features.\\n\\n\\n\\nBy the definition of independence, the probability for an instance, is the product of the probabilities of instances by these classifiers.',\n",
       "  '<machine-learning><statistics><scikit-learn><naivebayes>',\n",
       "  datetime.date(2016, 7, 27),\n",
       "  '2016-07-29 06:59:39',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '12',\n",
       "  '',\n",
       "  '3515.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2183',\n",
       "  '37193152',\n",
       "  'Answer',\n",
       "  'Extract date from string in python',\n",
       "  '  With minor tweaks in the aforementioned post, you can get it to work.\\n\\n\\n\\n    import re\\n\\n    from datetime import datetime\\n\\n    \\n\\n    text = \"Campaign on 01.11.2015\"\\n\\n    \\n\\n    match = re.search(r\\'\\\\d{2}.\\\\d{2}.\\\\d{4}\\', text)\\n\\n    date = datetime.strptime(match.group(), \\'%d.%m.%Y\\').date()\\n\\n    print str(date).replace(\"-\", \"\")\\n\\n    20151101\\n\\n',\n",
       "  '<python><string><date><slice>',\n",
       "  datetime.date(2016, 5, 12),\n",
       "  '2016-12-09 09:35:24',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '7',\n",
       "  '',\n",
       "  '3433.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2184',\n",
       "  '37230878',\n",
       "  'Answer',\n",
       "  'Understanding settings of Birch clustering in Scikit Learn',\n",
       "  'Yes, you are right. The default value should be 3 instead of None.\\n\\n\\n\\nWhen `n_clusters = integer`, the model fit becomes [Agglomerative Clustering][1] whose `n_clusters` is set to the value of that `integer`.\\n\\n\\n\\nWhen `n_clusters = None`, the further clustering step is not performed and the subclusters are returned as they were before. \\n\\n\\n\\n[***See #6635 github issue***][2]\\n\\n[2]: https://github.com/scikit-learn/scikit-learn/issues/6635\\n\\n\\n\\n  [1]: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html',\n",
       "  '<python><scipy><scikit-learn><cluster-analysis><hierarchical-clustering>',\n",
       "  datetime.date(2016, 5, 14),\n",
       "  '2016-12-18 19:18:07',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1220.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2191',\n",
       "  '39042831',\n",
       "  'Answer',\n",
       "  'Is there a random letter generator with a range?',\n",
       "  \"The function `choice` takes a general sequence.\\n\\n\\n\\n> Return a random element from the non-empty sequence seq.\\n\\n\\n\\nIn particular\\n\\n\\n\\n    random.choice(['A', 'B', 'C', 'D'])\\n\\n\\n\\nwill do what you want.\\n\\n\\n\\nYou can easily generate the range programatically:\\n\\n\\n\\n    random.choice([chr(c) for c in xrange(ord('A'), ord('D')+1)])\",\n",
       "  '<python>',\n",
       "  datetime.date(2016, 8, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '5176.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2196',\n",
       "  '38647581',\n",
       "  'Answer',\n",
       "  'Weighted correlation coefficient with pandas',\n",
       "  'I don\\'t know of any Python packages that implement this, but it should be fairly straightforward to roll your own implementation.  Using the naming conventions of the wikipedia article:\\n\\n\\n\\n    def m(x, w):\\n\\n        \"\"\"Weighted Mean\"\"\"\\n\\n        return np.sum(x * w) / np.sum(w)\\n\\n    \\n\\n    def cov(x, y, w):\\n\\n        \"\"\"Weighted Covariance\"\"\"\\n\\n        return np.sum(w * (x - m(x, w)) * (y - m(y, w))) / np.sum(w)\\n\\n    \\n\\n    def corr(x, y, w):\\n\\n        \"\"\"Weighted Correlation\"\"\"\\n\\n        return cov(x, y, w) / np.sqrt(cov(x, x, w) * cov(y, y, w))\\n\\n\\n\\nI tried to make the functions above match the formulas in the wikipedia as closely as possible, but there are some potential simplifications and performance improvements.  For example, as pointed out by @Alberto Garcia-Raboso, `m(x, w)` is really just `np.average(x, weights=w)`, so there\\'s no need to actually write a function for it.\\n\\n\\n\\nThe functions are pretty bare-bones, just doing the calculations.  You may want to consider forcing inputs to be arrays prior to doing the calculations, i.e. `x = np.asarray(x)`, as these functions will not work if lists are passed.  Additional checks to verify all inputs have equal length, non-null values, etc. could also be implemented.\\n\\n\\n\\nExample usage:\\n\\n\\n\\n    # Initialize a DataFrame.\\n\\n    np.random.seed([3,1415])\\n\\n    n = 10**6\\n\\n    df = pd.DataFrame({\\n\\n        \\'x\\': np.random.choice(3, size=n),\\n\\n        \\'y\\': np.random.choice(4, size=n),\\n\\n        \\'w\\': np.random.random(size=n)\\n\\n        })\\n\\n    \\n\\n    # Compute the correlation.\\n\\n    r = corr(df[\\'x\\'], df[\\'y\\'], df[\\'w\\'])\\n\\n\\n\\nThere\\'s a discussion [here][1] regarding the p-value.  It doesn\\'t look like there\\'s a generic calculation, and it depends on how you\\'re actually getting the weights.\\n\\n\\n\\n\\n\\n  [1]: https://stats.stackexchange.com/q/94569',\n",
       "  '<python><pandas><correlation><pearson-correlation>',\n",
       "  datetime.date(2016, 7, 28),\n",
       "  '2017-04-13 12:44:13',\n",
       "  'root (3339965), URL Rewriter Bot (n/a)',\n",
       "  '8',\n",
       "  '',\n",
       "  '3227.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2197',\n",
       "  '38671195',\n",
       "  'Answer',\n",
       "  'type does not provide a call operator',\n",
       "  \"It's a bit hard to tell without your posting more complete code, but consider the following:\\n\\n\\n\\n    int order(int j, int k)\\n\\n    {   \\n\\n        return 3;\\n\\n    }   \\n\\n\\n\\n    int main(int argc, char *argv[])\\n\\n    {   \\n\\n        char order;\\n\\n\\n\\n        // order(2, 3);                                                \\n\\n    }\\n\\n\\n\\nThis code builds fine. However, uncommenting\\n\\n\\n\\n        // order(2, 3);                     \\n\\n\\n\\ncauses it to fail, as within `main`, `order` is a character, not a function. From the error message, it looks like you might have some similar problem.\\n\\n\\n\\n\",\n",
       "  '<c++><c++11>',\n",
       "  datetime.date(2016, 7, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '12725.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2199',\n",
       "  '39594999',\n",
       "  'Answer',\n",
       "  'Convert a dict to a pandas DataFrame',\n",
       "  'As the values are strings, you can use the [`json` module](https://docs.python.org/2/library/json.html) and list comprehension:\\n\\n\\n\\n\\n\\n    In [20]: d =     {u\\'\"57e01311817bc367c030b390\"\\': u\\'{\"ad_since\": 2016, \"indoor_swimming_pool\": \"No\", \"seaside\": \"No\", \"handicapped_access\": \"Yes\"}\\', u\\'\"57e01311817bc367c030b3a8\"\\': u\\'{\"ad_since\": 2012, \"indoor_swimming_pool\": \"No\", \"seaside\": \"No\", \"handicapped_access\": \"Yes\"}\\'}\\n\\n\\n\\n    In [21]: import json\\n\\n\\n\\n    In [22]: pd.DataFrame(dict([(k, [json.loads(e)[k] for e in d.values()]) for k in json.loads(d.values()[0])]), index=d.keys())Out[22]: \\n\\n                                ad_since handicapped_access indoor_swimming_pool  \\\\\\n\\n    \"57e01311817bc367c030b390\"      2016                Yes                   No   \\n\\n    \"57e01311817bc367c030b3a8\"      2012                Yes                   No   \\n\\n\\n\\n                           seaside  \\n\\n    \"57e01311817bc367c030b390\"      No  \\n\\n    \"57e01311817bc367c030b3a8\"      No  \\n\\n',\n",
       "  '<python><json><pandas>',\n",
       "  datetime.date(2016, 9, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1792.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2201',\n",
       "  '39622282',\n",
       "  'Answer',\n",
       "  \"pd.read_csv ignores columns that don't have headers\",\n",
       "  \"You could do it as shown:\\n\\n\\n\\n    col_name = list('ABCDEFGHIJK')\\n\\n    data = 'path.csv'\\n\\n    pd.read_csv(data, delim_whitespace=True, header=None, names=col_name, usecols=col_name[5:])\\n\\n    \\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\nTo read all the columns from A → K, simply omit the `usecols` parameter.\\n\\n\\n\\n\\n\\n----------\\n\\nData:\\n\\n\\n\\n    data = StringIO(\\n\\n    '''\\n\\n    %m/%d/%Y,49.78,85,6,15                      \\n\\n    03/01/1984,6.63368,82,7,9.8,34.29056405,2.79984079,2.110346498,0.014652412,2.304545521,0.004732732\\n\\n    03/02/1984,6.53368,68,0,0.2,44.61471002,3.21623666,2.990408898,0.077444779,2.793385466,0.02661873\\n\\n    03/03/1984,4.388344,55,6,0,61.14463457,3.637231063,3.484310818,0.593098236,3.224973641,0.214360796\\n\\n    ''')\\n\\n\\n\\n    col_name = list('ABCDEFGHIJK')\\n\\n    pd.read_csv(data, header=None, names=col_name, usecols=col_name[5:])\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/QqBJJ.png\",\n",
       "  '<python><csv><pandas>',\n",
       "  datetime.date(2016, 9, 21),\n",
       "  '2016-09-21 17:21:26',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1094.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2210',\n",
       "  '38681726',\n",
       "  'Answer',\n",
       "  'How to remove common rows in two dataframes in Pandas?',\n",
       "  'You can use [`pandas.concat`][1] to concatenate the two dataframes rowwise, followed by [`drop_duplicates`][2] to remove all the duplicated rows in them.\\n\\n\\n\\n    In [1]: import pandas as pd\\n\\n    df_1 = pd.DataFrame({\"A\":[\"foo\", \"foo\", \"foo\", \"bar\"], \"B\":[0,1,1,1], \"C\":[\"A\",\"A\",\"B\",\"A\"]})\\n\\n    df_2 = pd.DataFrame({\"A\":[\"foo\", \"bar\", \"foo\", \"bar\"], \"B\":[1,0,1,0], \"C\":[\"A\",\"B\",\"A\",\"B\"]})\\n\\n    \\n\\n    In [2]: df = pd.concat([df_1, df_2])\\n\\n    \\n\\n    In [3]: df\\n\\n    Out[3]: \\n\\n         A  B  C\\n\\n    0  foo  0  A\\n\\n    1  foo  1  A\\n\\n    2  foo  1  B\\n\\n    3  bar  1  A\\n\\n    0  foo  1  A\\n\\n    1  bar  0  B\\n\\n    2  foo  1  A\\n\\n    3  bar  0  B\\n\\n    \\n\\n    In [4]: df.drop_duplicates(keep=False)\\n\\n    Out[4]: \\n\\n         A  B  C\\n\\n    0  foo  0  A\\n\\n    2  foo  1  B\\n\\n    3  bar  1  A\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html\\n\\n[2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop_duplicates.html',\n",
       "  '<python-2.7><pandas><scikit-learn>',\n",
       "  datetime.date(2016, 7, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '4239.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2219',\n",
       "  '37335656',\n",
       "  'Answer',\n",
       "  'How to get the length of a cell value in pandas dataframe?',\n",
       "  'You can use `.str.len` to get the length of a list, even though lists aren\\'t strings:\\n\\n\\n\\n    df[\\'EventCount\\'] = df[\\'Event\\'].str.split(\"/\").str.len()\\n\\n\\n\\nAlternatively, the count you\\'re looking for is just 1 more than the count of `\"/\"`\\'s in the string, so you could add 1 to the result of `.str.count`:\\n\\n\\n\\n    df[\\'EventCount\\'] = df[\\'Event\\'].str.count(\"/\") + 1\\n\\n\\n\\nThe resulting output for either method:\\n\\n\\n\\n             Event  EventCount\\n\\n    0      abc/def           2\\n\\n    1          abc           1\\n\\n    2  abc/def/hij           3\\n\\n\\n\\nTimings on a slightly larger DataFrame:\\n\\n\\n\\n    %timeit df[\\'Event\\'].str.count(\"/\") + 1\\n\\n    100 loops, best of 3: 3.18 ms per loop\\n\\n    \\n\\n    %timeit df[\\'Event\\'].str.split(\"/\").str.len()\\n\\n    100 loops, best of 3: 4.28 ms per loop\\n\\n    \\n\\n    %timeit df[\\'Event\\'].str.split(\"/\").apply(len)\\n\\n    100 loops, best of 3: 4.08 ms per loop\\n\\n\\n\\n',\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 5, 19),\n",
       "  '2016-05-20 01:37:19',\n",
       "  'root (3339965)',\n",
       "  '12',\n",
       "  '',\n",
       "  '10121.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2220',\n",
       "  '37397166',\n",
       "  'Answer',\n",
       "  'Color histogram for multiple images in a directory using cv2.calcHist in python',\n",
       "  'There are some minor modifications to your above code and I have plotted the histograms for 2 dummy images inside the `test` folder.  \\n\\n\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import cv2\\n\\n    import os\\n\\n     \\n\\n    images = []\\n\\n    path = \"../Mission Begins/test/\"\\n\\n    for image in os.listdir(path):\\n\\n        images.append(image)\\n\\n            \\n\\n    for image in images:\\n\\n         img = cv2.imread(\"%s%s\"%(path, image))    # Load the image \\n\\n         channels = cv2.split(img)       # Set the image channels\\n\\n         colors = (\"b\", \"g\", \"r\")        # Initialize tuple \\n\\n         plt.figure()    \\n\\n         plt.title(\"Color Histogram\")\\n\\n         plt.xlabel(\"Bins\")\\n\\n         plt.ylabel(\"Number of Pixels\")\\n\\n         \\n\\n         for (i, col) in zip(channels, colors):       # Loop over the image channels\\n\\n              hist = cv2.calcHist([i], [0], None, [256], [0, 256])   # Create a histogram for current channel\\n\\n              plt.plot(hist, color = col)      # Plot the histogram\\n\\n              plt.xlim([0, 256])\\n\\n\\n\\n## Input Image ##\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n## Histogram ##\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n## Input Image ##\\n\\n[![enter image description here][3]][3]\\n\\n\\n\\n## Histogram ##\\n\\n[![enter image description here][4]][4]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/uOKhp.jpg\\n\\n  [2]: http://i.stack.imgur.com/GC4nj.jpg\\n\\n  [3]: http://i.stack.imgur.com/t6S4f.jpg\\n\\n  [4]: http://i.stack.imgur.com/50xr5.png',\n",
       "  '<python><opencv><matplotlib><computer-vision><histogram>',\n",
       "  datetime.date(2016, 5, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1342.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2223',\n",
       "  '35528185',\n",
       "  'Answer',\n",
       "  'Pandas recalculate index after a concatenation',\n",
       "  \"After vertical concatenation, if you get an index of *[0, n)* followed by *[0, m)*, all you need to do is call [`reset_index`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html):\\n\\n\\n\\n    train_df.reset_index(drop=True)\\n\\n\\n\\n(you can do this in place using `inplace=True`).\\n\\n\\n\\n------\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    >>> pd.concat([\\n\\n        pd.DataFrame({'a': [1, 2]}), \\n\\n        pd.DataFrame({'a': [1, 2]})]).reset_index(drop=True)\\n\\n        a\\n\\n    0\\t1\\n\\n    1\\t2\\n\\n    2\\t1\\n\\n    3\\t2\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 2, 20),\n",
       "  '2016-02-20 19:53:27',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '18',\n",
       "  '',\n",
       "  '12750.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2227',\n",
       "  '38775038',\n",
       "  'Answer',\n",
       "  'Pandas Datetime Formatting',\n",
       "  'You could convert the `Timestamp` object to `datetime.datetime` object and extract the `datetime.date` part as shown:\\n\\n\\n\\n    In [7]: import pandas as pd\\n\\n    \\n\\n    In [8]: print(pd.Timestamp(\\'2015-02-26 16:45:36.0\\').to_datetime().date())\\n\\n    2015-02-26\\n\\n    <class \\'datetime.date\\'>\\n\\n\\n\\nYour desired format:\\n\\n\\n\\n    In [11]: print(pd.Timestamp(\\'2015-02-26 16:45:36.0\\').to_datetime().date().strftime(\"%Y%m%d\"))\\n\\n    20150226\\n\\n    <class \\'str\\'>',\n",
       "  '<python><datetime><pandas>',\n",
       "  datetime.date(2016, 8, 4),\n",
       "  '2016-08-04 18:46:00',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1108.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2232',\n",
       "  '35538530',\n",
       "  'Answer',\n",
       "  'How can I split a DataFrame column with datetimes into two columns: one with dates and one with times of the day?',\n",
       "  \"If your series is `s`, then this will create such a DataFrame:\\n\\n\\n\\n    pd.DataFrame({\\n\\n        'date': pd.to_datetime(s).dt.date,\\n\\n        'time': pd.to_datetime(s).dt.time})\\n\\n\\n\\n\\n\\nas once you convert the series using [`pd.to_datetime`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html), then the `dt` member can be used to extract the parts.\\n\\n\\n\\n---------------\\n\\n\\n\\n**Example**\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    s = pd.Series(['2015-05-13 23:53:00', '2015-05-13 23:53:00'])\\n\\n    >>> pd.DataFrame({\\n\\n        'date': pd.to_datetime(s).dt.date,\\n\\n        'time': pd.to_datetime(s).dt.time})\\n\\n        date\\ttime\\n\\n    0\\t2015-05-13\\t23:53:00\\n\\n    1\\t2015-05-13\\t23:53:00\\n\\n\",\n",
       "  '<python><pandas><datetime><dataframe><series>',\n",
       "  datetime.date(2016, 2, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1519.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2234',\n",
       "  '39657077',\n",
       "  'Answer',\n",
       "  'Rename a single column header in a pandas dataframe',\n",
       "  \"A much faster implementation would be to use `list-comprehension` if you need to rename a single column.\\n\\n\\n\\n    df.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]\\n\\n\\n\\nIf the need arises to rename multiple columns, either use conditional expressions like:\\n\\n\\n\\n    df.columns = ['log(gdp)' if x=='gdp' else 'cap_mod' if x=='cap' else x for x in df.columns]\\n\\n\\n\\nOr, construct a mapping using a `dictionary` and perform the `list-comprehension` with it's `get` operation by setting default value as the old name:\\n\\n\\n\\n    col_dict = {'gdp': 'log(gdp)', 'cap': 'cap_mod'}   ## key→old name, value→new name\\n\\n\\n\\n    df.columns = [col_dict.get(x, x) for x in df.columns]\\n\\n\\n\\n**Timings:**\\n\\n\\n\\n\\n\\n    %%timeit\\n\\n    df.rename(columns={'gdp':'log(gdp)'}, inplace=True)\\n\\n    10000 loops, best of 3: 168 µs per loop\\n\\n\\n\\n    %%timeit\\n\\n    df.columns = ['log(gdp)' if x=='gdp' else x for x in df.columns]\\n\\n    10000 loops, best of 3: 58.5 µs per loop\\n\\n\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><pandas><dataframe><rename>',\n",
       "  datetime.date(2016, 9, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '15',\n",
       "  '',\n",
       "  '108639.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2239',\n",
       "  '39816012',\n",
       "  'Answer',\n",
       "  'Handling multiple clients with async_accept',\n",
       "  'It\\'s a bit hard to understand the specifics of your question, since the code is incomplete (e.g., there\\'s a `return` in your block, but it\\'s unclear what is that block part of).\\n\\n\\n\\nNotwithstanding, the documentation contains an [example of a TCP echo server using coroutines](http://www.boost.org/doc/libs/1_55_0/doc/html/boost_asio/example/cpp11/spawn/echo_server.cpp). It seems you basically need to add SSL support to it, to adapt it to your needs.\\n\\n\\n\\nIf you look at `main`, it has the following chunk:\\n\\n\\n\\n    boost::asio::spawn(io_service,\\n\\n        [&](boost::asio::yield_context yield)\\n\\n        {\\n\\n          tcp::acceptor acceptor(io_service,\\n\\n            tcp::endpoint(tcp::v4(), std::atoi(argv[1])));\\n\\n\\n\\n          for (;;)\\n\\n          {\\n\\n            boost::system::error_code ec;\\n\\n            tcp::socket socket(io_service);\\n\\n            acceptor.async_accept(socket, yield[ec]);\\n\\n            if (!ec) std::make_shared<session>(std::move(socket))->go();\\n\\n          }\\n\\n        });\\n\\n\\n\\nThis loops endlessly, and, following each (successful) call to `async_accept`, handles accepting the next connection (while this connection and others might still be active).\\n\\n\\n\\nAgain, I\\'m not sure about your code, but it contains exits from the loop like\\n\\n\\n\\n    return; //connection closed cleanly by peer\\n\\n\\n\\n----------------------------------------------------------\\n\\n\\n\\nTo illustrate the point, here are two applications. \\n\\n\\n\\nThe first is a Python multiprocessing echo client, adapted from [PMOTW](https://pymotw.com/2/socket/tcp.html):\\n\\n\\n\\n    import socket\\n\\n    import sys\\n\\n    import multiprocessing\\n\\n\\n\\n    def session(i):\\n\\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n\\n\\n\\n        server_address = (\\'localhost\\', 5000)\\n\\n        print \\'connecting to %s port %s\\' % server_address\\n\\n        sock.connect(server_address)\\n\\n        print \\'connected\\'\\n\\n\\n\\n        for _ in range(300):\\n\\n            try:\\n\\n\\n\\n                # Send data\\n\\n                message = \\'client \\' + str(i) + \\' message\\'\\n\\n                print \\'sending \"%s\"\\' % message\\n\\n                sock.sendall(message)\\n\\n\\n\\n                # Look for the response\\n\\n                amount_received = 0\\n\\n                amount_expected = len(message)\\n\\n\\n\\n                while amount_received < amount_expected:\\n\\n                    data = sock.recv(16)\\n\\n                    amount_received += len(data)\\n\\n                    print \\'received \"%s\"\\' % data\\n\\n\\n\\n            except:\\n\\n                print >>sys.stderr, \\'closing socket\\'\\n\\n                sock.close()\\n\\n\\n\\n    if __name__ == \\'__main__\\':\\n\\n        pool = multiprocessing.Pool(8)\\n\\n        pool.map(session, range(8))\\n\\n\\n\\nThe details are not that important (although it\\'s Python, and therefore easy to read), but the point is that it opens up 8 processes, and each engages the same asio echo server (below) with 300 messages. \\n\\n\\n\\nWhen run, it outputs \\n\\n\\n\\n    ...\\n\\n    received \"client 1 message\"\\n\\n    sending \"client 1 message\"\\n\\n    received \"client 2 message\"\\n\\n    sending \"client 2 message\"\\n\\n    received \"client 3 message\"\\n\\n    received \"client 0 message\"\\n\\n    sending \"client 3 message\"\\n\\n    sending \"client 0 message\"\\n\\n    ...\\n\\n\\n\\nshowing that the echo sessions are indeed interleaved.\\n\\n\\n\\nNow for the echo server. I\\'ve slightly adapted the [example from the docs](http://www.boost.org/doc/libs/1_55_0/doc/html/boost_asio/example/cpp11/echo/async_tcp_echo_server.cpp):\\n\\n\\n\\n    #include <cstdlib>\\n\\n    #include <iostream>\\n\\n    #include <memory>\\n\\n    #include <utility>\\n\\n    #include <boost/asio.hpp>\\n\\n\\n\\n    using boost::asio::ip::tcp;\\n\\n\\n\\n    class session :\\n\\n        public std::enable_shared_from_this<session> {\\n\\n\\n\\n    public:\\n\\n        session(tcp::socket socket) : socket_(std::move(socket)) {}\\n\\n\\n\\n        void start() { do_read(); }\\n\\n\\n\\n    private:\\n\\n        void do_read() {\\n\\n            auto self(\\n\\n                shared_from_this());\\n\\n            socket_.async_read_some(\\n\\n                boost::asio::buffer(data_, max_length),\\n\\n                [this, self](boost::system::error_code ec, std::size_t length) {\\n\\n                     if(!ec)\\n\\n                         do_write(length);\\n\\n                });\\n\\n        }\\n\\n\\n\\n        void do_write(std::size_t length) {\\n\\n            auto self(shared_from_this());\\n\\n            socket_.async_write_some(\\n\\n                boost::asio::buffer(data_, length),\\n\\n                [this, self](boost::system::error_code ec, std::size_t /*length*/) {\\n\\n                    if (!ec)\\n\\n                        do_read();\\n\\n                });\\n\\n        }\\n\\n\\n\\n    private:\\n\\n        tcp::socket socket_;\\n\\n        enum { max_length = 1024 };\\n\\n        char data_[max_length];\\n\\n    };\\n\\n\\n\\n    class server {\\n\\n    public:\\n\\n        server(boost::asio::io_service& io_service, short port) :\\n\\n                acceptor_(io_service, tcp::endpoint(tcp::v4(), port)),\\n\\n                socket_(io_service) {\\n\\n            do_accept();\\n\\n        }\\n\\n\\n\\n    private:\\n\\n        void do_accept() {\\n\\n            acceptor_.async_accept(\\n\\n                socket_,\\n\\n                [this](boost::system::error_code ec) {\\n\\n                    if(!ec)\\n\\n                        std::make_shared<session>(std::move(socket_))->start();\\n\\n\\n\\n                    do_accept();\\n\\n                });\\n\\n        }\\n\\n\\n\\n        tcp::acceptor acceptor_;\\n\\n        tcp::socket socket_;\\n\\n    };\\n\\n\\n\\n    int main(int argc, char* argv[]) {\\n\\n        const int port = 5000;\\n\\n        try {\\n\\n            boost::asio::io_service io_service;\\n\\n\\n\\n            server s{io_service, port};\\n\\n\\n\\n            io_service.run();\\n\\n        }\\n\\n        catch (std::exception& e) {\\n\\n            std::cerr << \"Exception: \" << e.what() << \"\\\\n\";\\n\\n        }\\n\\n    }\\n\\n\\n\\nThis shows that this server indeed interleaves. \\n\\n\\n\\nNote that this is *not* the coroutine version. While I once played with the coroutine version a bit, I just couldn\\'t get it to build on my current box (also, as sehe notes in the comments below, you might anyway prefer this more mainstream version for now).\\n\\n\\n\\nHowever, this is not a fundamental difference, w.r.t. your question. The non-coroutine version has callbacks explicitly explicitly launching new operations supplying the next callback; the coroutine version uses a more sequential-looking paradigm. Each call returns to `asio`\\'s control loop in both versions, which monitors *all* the current operations which can proceed. \\n\\n\\n\\nFrom the [`asio` coroutine docs](http://theboostcpplibraries.com/boost.asio-coroutines):\\n\\n\\n\\n> Coroutines let you create a structure that mirrors the actual program logic. Asynchronous operations don’t split functions, because there are no handlers to define what should happen when an asynchronous operation completes. Instead of having handlers call each other, the program can use a sequential structure.\\n\\n\\n\\nIt\\'s not that the sequential structure makes all operations sequential - that would eradicate the entire need for `asio`.\\n\\n                                                   ',\n",
       "  '<c++><boost-asio>',\n",
       "  datetime.date(2016, 10, 2),\n",
       "  '2016-10-04 14:01:07',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1078.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2243',\n",
       "  '39122332',\n",
       "  'Answer',\n",
       "  'Explicit constructor taking multiple arguments',\n",
       "  \"The excellent answers by @StoryTeller and @Sneftel are the main reason. However, IMHO, this makes sense (at least I do it), as part of future proofing later changes to the code. Consider your example:\\n\\n\\n\\n    class A {\\n\\n        public:\\n\\n            explicit A( int b, int c ); \\n\\n    };\\n\\n\\n\\nThis code doesn't directly benefit from `explicit`. \\n\\n\\n\\nSome time later, you decide to add a default value for `c`, so it becomes this:\\n\\n\\n\\n    class A {\\n\\n        public:\\n\\n            A( int b, int c=0 ); \\n\\n    };\\n\\n\\n\\nWhen doing this, you're focussing on the `c` parameter - in retrospect, it should have a default value. You're not necessarily focussing on whether `A` itself should be implicitly constructed. Unfortunately, this change makes `explicit` relevant again.\\n\\n\\n\\nSo, in order to convey that a ctor is `explicit`, it might pay to do so when first writing the method.\\n\\n\\n\\n\",\n",
       "  '<c++><explicit-constructor>',\n",
       "  datetime.date(2016, 8, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '17',\n",
       "  '',\n",
       "  '4216.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2244',\n",
       "  '39154679',\n",
       "  'Answer',\n",
       "  'Modifying python colormaps to single value beyond a specific point',\n",
       "  \"You could create the colormap for the given range (0 →100) by stacking two different colormaps on top of each other as shown:\\n\\n\\n\\n**Illustration:**\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    import palettable\\n\\n    import matplotlib.colors as mcolors\\n\\n    \\n\\n    # Set random seed\\n\\n    np.random.seed(42)\\n\\n\\n\\n    # Create random values of shape 10x10\\n\\n    data = np.random.rand(10,10) * 100 \\n\\n\\n\\n    # Given colormap which takes values from 0→50\\n\\n    colors1 = palettable.colorbrewer.sequential.YlGn_9.mpl_colormap(np.linspace(0, 1, 256))\\n\\n    # Red colormap which takes values from 50→100\\n\\n    colors2 = plt.cm.Reds(np.linspace(0, 1, 256))\\n\\n    \\n\\n    # stacking the 2 arrays row-wise\\n\\n    colors = np.vstack((colors1, colors2))\\n\\n\\n\\n    # generating a smoothly-varying LinearSegmentedColormap\\n\\n    cmap = mcolors.LinearSegmentedColormap.from_list('colormap', colors)\\n\\n    \\n\\n    plt.pcolor(data, cmap=cmap)\\n\\n    plt.colorbar()\\n\\n    # setting the lower and upper limits of the colorbar\\n\\n    plt.clim(0, 100)\\n\\n\\n\\n    plt.show()\\n\\n\\n\\n[![Image1][1]][1]\\n\\n\\n\\nIncase you want the upper portion to be of the same color and not spread over the length of the colormap, you could make the following modification:\\n\\n\\n\\n    colors2 = plt.cm.Reds(np.linspace(1, 1, 256))\\n\\n\\n\\n[![Image2][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/568SD.png\\n\\n  [2]: http://i.stack.imgur.com/edQh2.png\",\n",
       "  '<python><matplotlib><seaborn>',\n",
       "  datetime.date(2016, 8, 25),\n",
       "  '2016-08-26 07:57:34',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '7',\n",
       "  '',\n",
       "  '2065.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2253',\n",
       "  '35582977',\n",
       "  'Answer',\n",
       "  'Find and access the element in a Priority Queue in C++11',\n",
       "  \"For this usage, I'd suggest you use a combination of `std::priority_queue` and an `std::unordered_map`.\\n\\n\\n\\nLet's restructure your data as follows:\\n\\n\\n\\n    struct PersonInfo {\\n\\n        std::string y;\\n\\n    };\\n\\n\\n\\nThis contains the mutable information about a person.\\n\\n\\n\\nNow you have two containers:\\n\\n\\n\\n* an `std::priority_queue<int>` of the values which were previously the `val`s in your `Person` class objects.\\n\\n\\n\\n* an `std::unordered_map<int, PersonInfo>` mapping these values into `PersonInfo`s.\\n\\n\\n\\nFor your stated intent\\n\\n\\n\\n>  in which I have to check whether a particular object has been inserted into priority queue or not. \\n\\n\\n\\nSimply check if things were inserted using the map; make sure to update it when pushing and popping the priority queue, though.\\n\\n\\n\\n> If it is being inserted, then I need to access that particular object and possibly update it.\\n\\n\\n\\nJust use the unordered map.\\n\\n\",\n",
       "  '<c++><c++11><priority-queue>',\n",
       "  datetime.date(2016, 2, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1304.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2255',\n",
       "  '35652058',\n",
       "  'Answer',\n",
       "  'std::map with iterators to std::list from iterators of std::map',\n",
       "  'For the purpose of a FIFO with the ability to remove by key, I suggest you use [`unordered_map`](http://en.cppreference.com/w/cpp/container/unordered_map), as you have no need for order in the map.\\n\\n\\n\\nFollowing that, perhaps you could change your cross-referencing scheme. Use a list of strings, and a map mapping strings to iterators of such a list:\\n\\n\\n\\n    #include <unordered_map>                                                                                                                                                                                     \\n\\n    #include <list>\\n\\n    #include <string>\\n\\n\\n\\n\\n\\n    using map_t = unordered_map<string, list<string>::iterator>;\\n\\n    using list_t = list<string>;\\n\\n\\n\\nFor the direction of finding a key in the map once you have an iterator in the list, you need to perform a redundant hash on the name relative to your full iterator-to-iterator scheme, but it is still *O(1)* (expected). Conversely, your original scheme required logarithmic operations for removal by key, so you\\'re probably still ahead.\\n\\n\\n\\n\\n\\nTo insert a new element, you could do something like this:\\n\\n\\n\\n    map_t map;\\n\\n    list_t list;\\n\\n\\n\\n    list.push_back(\"koko\");\\n\\n    auto it = --list.end();\\n\\n    map[\"koko\"] = it;\\n\\n\\n\\n-----\\n\\n\\n\\n**Example**\\n\\n\\n\\n    #include <unordered_map>                                                                                                                                                                                     \\n\\n    #include <list>\\n\\n    #include <string>\\n\\n\\n\\n\\n\\n    using namespace std;\\n\\n\\n\\n\\n\\n    int main()\\n\\n    {\\n\\n        using map_t = unordered_map<string, list<string>::iterator>;\\n\\n        using list_t = list<string>;\\n\\n\\n\\n        map_t map;\\n\\n        list_t list;\\n\\n\\n\\n        list.push_back(\"koko\");\\n\\n        auto it = --list.end();\\n\\n        map[\"koko\"] = it;\\n\\n    }\\n\\n',\n",
       "  '<c++><templates><stl><iterator>',\n",
       "  datetime.date(2016, 2, 26),\n",
       "  '2016-02-26 12:53:27',\n",
       "  'Lightness Races in Orbit (560648)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1189.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2259',\n",
       "  '38901797',\n",
       "  'Answer',\n",
       "  'Pandas swap columns based on condition',\n",
       "  \"You can use `loc` to do the swap:\\n\\n\\n\\n    df.loc[df['Col3'].isnull(), ['Col2', 'Col3']] = df.loc[df['Col3'].isnull(), ['Col3', 'Col2']].values\\n\\n\\n\\nNote that `.values` is required to make sure the swap is done properly, otherwise Pandas would try to align based on index and column names, and no swap would occur.\\n\\n\\n\\nYou can also just reassign each row individually, if you feel the code is cleaner:\\n\\n\\n\\n    null_idx = df['Col3'].isnull()\\n\\n    df.loc[null_idx, 'Col3'] = df['Col2']\\n\\n    df.loc[null_idx, 'Col2'] = np.nan\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n      Col1  Col2  Col3\\n\\n    0    A   NaN   7.0\\n\\n    1    B   NaN  16.0\\n\\n    2    B  16.0  15.0\\n\\n\\n\\n\",\n",
       "  '<python><pandas><swap>',\n",
       "  datetime.date(2016, 8, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '1479.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2260',\n",
       "  '38914112',\n",
       "  'Answer',\n",
       "  'Make the size of a heatmap bigger with seaborn',\n",
       "  'You could alter the [`figsize`][1] by passing a `tuple` showing the `width, height` parameters you would like to keep. \\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n\\n\\n    fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\\n\\n    sns.heatmap(df1.iloc[:, 1:6:], annot=True, linewidths=.5, ax=ax)\\n\\n\\n\\n**EDIT**\\n\\n\\n\\nI remember answering a similar question of yours where you had to set the index as `TIMESTAMP`. So, you could then do something like below:\\n\\n\\n\\n    df = df.set_index(\\'TIMESTAMP\\')\\n\\n    df.resample(\\'30min\\').mean()\\n\\n    fig, ax = plt.subplots()\\n\\n    ax = sns.heatmap(df.iloc[:, 1:6:], annot=True, linewidths=.5)\\n\\n    ax.set_yticklabels([i.strftime(\"%Y-%m-%d %H:%M:%S\") for i in df.index], rotation=0)\\n\\n\\n\\nFor the `head` of the dataframe you posted, the plot would look like:\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://matplotlib.org/api/figure_api.html#matplotlib.figure.Figure\\n\\n  [2]: http://i.stack.imgur.com/RqGom.png',\n",
       "  '<python><heatmap><seaborn>',\n",
       "  datetime.date(2016, 8, 12),\n",
       "  '2016-08-12 13:49:11',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '27',\n",
       "  '',\n",
       "  '29623.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2261',\n",
       "  '39179715',\n",
       "  'Answer',\n",
       "  'value counts of group by in pandas',\n",
       "  'You could use [`value_counts`][1] after you have binned them into bins corresponding to the groups specified in [`cut`][2].\\n\\n\\n\\n    sns.set_style(\\'darkgrid\\')\\n\\n\\n\\n    df = pd.DataFrame(datasets.load_iris().data[:,0], columns=[\\'SL\\'])\\n\\n    df[\\'target\\'] = datasets.load_iris().target\\n\\n\\n\\n    # Total number of bins to be grouped under\\n\\n    grps = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\\n\\n    \\n\\n    # Empty list to append later\\n\\n    grouped_list = []\\n\\n\\n\\n    # Iterating through grouped by target variable\\n\\n    for label, key in df.groupby(\\'target\\'):\\n\\n        grouped_list.append(pd.cut(key[\\'SL\\'], bins = grps).value_counts())\\n\\n\\n\\n    # Concatenate column-wise and create a stacked-bar plot\\n\\n    pd.concat(grouped_list, axis=1).add_prefix(\\'class_\\').plot(kind=\\'bar\\', stacked=True, rot=0, \\n\\n                                                            figsize=(6,6), cmap=plt.cm.rainbow)\\n\\n    \\n\\n    # Aesthetics\\n\\n    plt.title(\"Sepal Length binned counts\")\\n\\n    plt.xlabel(\"Buckets\")\\n\\n    plt.ylabel(\"Occurences\")\\n\\n    sns.plt.show()\\n\\n\\n\\n[![Image][3]][3]\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.cut.html\\n\\n  [3]: http://i.stack.imgur.com/itYfa.png',\n",
       "  '<python><pandas><matplotlib><dataframe>',\n",
       "  datetime.date(2016, 8, 27),\n",
       "  '2016-08-27 15:01:22',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '2041.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2269',\n",
       "  '39708191',\n",
       "  'Answer',\n",
       "  'Pandas - Alternative to rank() function that gives unique ordinal ranks for a column',\n",
       "  \"I think the way you were trying to use the `method=first` to rank them after sorting were causing problems. \\n\\n\\n\\nYou could simply use the rank method with `first` arg on the grouped object itself giving you the desired unique ranks per group.\\n\\n\\n\\n    df['new_rank'] = df.groupby(['weeks','device'])['ranking'].rank(method='first').astype(int)\\n\\n    print (df['new_rank'])\\n\\n\\n\\n    0     2\\n\\n    1     3\\n\\n    2     1\\n\\n    3     4\\n\\n    4     3\\n\\n    5     1\\n\\n    6     2\\n\\n    7     4\\n\\n    8     2\\n\\n    9     3\\n\\n    10    1\\n\\n    11    4\\n\\n    12    2\\n\\n    13    3\\n\\n    14    1\\n\\n    15    4\\n\\n    Name: new_rank, dtype: int32\\n\\n\\n\\nPerform pivot operation:\\n\\n\\n\\n    df = df.pivot_table(index=['weeks', 'device'], columns=['new_rank'],\\n\\n                        values=['adtext'], aggfunc=lambda x: ' '.join(x))\\n\\n\\n\\nChoose the second level of the multiindex columns which pertain to the rank numbers:\\n\\n\\n\\n    df.columns = ['rank_' + str(i) for i in df.columns.get_level_values(1)]\\n\\n    df\\n\\n\\n\\n[![Image_2][1]][1]\\n\\n\\n\\n\\n\\n----------\\n\\n**Data:**(to replicate)\\n\\n\\n\\n    df = pd.DataFrame({'weeks': ['wk 1', 'wk 1', 'wk 1', 'wk 1', 'wk 1', 'wk 1', 'wk 1', 'wk 1',\\n\\n                                 'wk 2', 'wk 2', 'wk 2', 'wk 2', 'wk 2', 'wk 2', 'wk 2', 'wk 2'],\\n\\n                      'device': ['mobile', 'mobile', 'mobile', 'mobile', 'desktop', 'desktop', 'desktop', 'desktop',\\n\\n                                 'mobile', 'mobile', 'mobile', 'mobile', 'desktop', 'desktop', 'desktop', 'desktop'],\\n\\n                      'website': ['url1', 'url2', 'url3', 'url4', 'url5', 'url2', 'url3', 'url4',\\n\\n                                 'url1', 'url16', 'url3', 'url4', 'url5', 'url2', 'url3', 'url4'],\\n\\n                      'ranking': [2.1, 2.1, 1.0, 2.9, 2.1, 1.5, 1.5, 2.9, \\n\\n                                  2.0, 2.1, 1.0, 2.9, 2.1, 2.9, 1.0, 2.9],\\n\\n                      'adtext': ['string', 'string', 'string', 'string', 'string', 'string', 'string', 'string',\\n\\n                                 'string', 'string', 'string', 'string', 'string', 'string', 'string', 'string']})\\n\\n\\n\\nNote: `method=first` assigns ranks in the order they appear in the array/series.\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/iMT88.png\\n\\n\",\n",
       "  '<python><pandas><ranking><rank><ordinal>',\n",
       "  datetime.date(2016, 9, 26),\n",
       "  '2016-09-27 14:10:09',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '3748.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2273',\n",
       "  '38466941',\n",
       "  'Answer',\n",
       "  'Pandas: replace column values based on match from another column',\n",
       "  \"You can convert `df2` into a Series indexed by `'ItemType2'`, and then use [`replace`][1] on `df1`:\\n\\n\\n\\n    # Make df2 a Series indexed by 'ItemType'.\\n\\n    df2 = df2.set_index('ItemType2')['newType'].dropna()\\n\\n    \\n\\n    # Replace values in df1.\\n\\n    df1['ItemType1'] = df1['ItemType1'].replace(df2)\\n\\n\\n\\nOr in a single line, if you don't want to alter `df2`:\\n\\n\\n\\n    df1['ItemType1'] = df1['ItemType1'].replace(df2.set_index('ItemType2')['newType'].dropna())\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.replace.html\",\n",
       "  '<python><python-2.7><pandas><dataframe>',\n",
       "  datetime.date(2016, 7, 19),\n",
       "  '2016-07-19 19:37:40',\n",
       "  'root (3339965)',\n",
       "  '4',\n",
       "  '',\n",
       "  '4190.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2288',\n",
       "  '39233913',\n",
       "  'Answer',\n",
       "  'Split and merge pandas dataframe',\n",
       "  \"You could get all your `Record2` values under the `Record1` columns as follows:\\n\\n\\n\\n**Data Setup:**   \\n\\n\\n\\n    data = StringIO(\\n\\n    '''\\n\\n    dataTime Record1Field1 Record1Field2 Record1Field3 Record2Field1 Record2Field2 Record2Field3\\n\\n    01-01-2015 1 2 3 4 5 6 \\n\\n    ''')\\n\\n\\n\\n    df = pd.read_csv(data, delim_whitespace=True, parse_dates=['dataTime'])\\n\\n    print (df)\\n\\n\\n\\n        dataTime  Record1Field1  Record1Field2  Record1Field3  Record2Field1  \\\\\\n\\n    0 2015-01-01              1              2              3              4   \\n\\n    \\n\\n       Record2Field2  Record2Field3  \\n\\n    0              5              6 \\n\\n\\n\\n \\n\\n\\n\\n**Operations:**\\n\\n\\n\\n    df.set_index('dataTime', inplace=True)\\n\\n\\n\\n    # Filter column names corresponding to Record2\\n\\n    tempdf = df[[col for col in list(df) if col.startswith('Record2')]]\\n\\n    \\n\\n    # Drop those columns after assigning to tempdf\\n\\n    df.drop(tempdf.columns, inplace=True, axis=1)\\n\\n    \\n\\n    # Rename the column names for appending\\n\\n    tempdf.columns = [col for col in list(df) if col.startswith('Record1')]\\n\\n    \\n\\n    # Concatenate row-wise\\n\\n    print (df.append(tempdf))\\n\\n\\n\\n                Record1Field1  Record1Field2  Record1Field3\\n\\n    dataTime                                               \\n\\n    2015-01-01              1              2              3\\n\\n    2015-01-01              4              5              6\\n\\n\\n\\n\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 8, 30),\n",
       "  '2016-08-30 18:12:31',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1024.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2295',\n",
       "  '38467566',\n",
       "  'Answer',\n",
       "  'How to get particular Column of DataFrame in pandas?',\n",
       "  'You might want to read the \\n\\n[documentation on indexing](http://pandas-docs.github.io/pandas-docs-travis/indexing.html). \\n\\n\\n\\nFor what you specified in the question, you can use\\n\\n\\n\\n    x, y = df.iloc[:, [0]], df.iloc[:, [1]]',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 7, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2770.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2298',\n",
       "  '39003074',\n",
       "  'Answer',\n",
       "  'Binning time series with pandas',\n",
       "  \"IIUC, you could use [`TimeGrouper`][1] along with [`groupby`][2] on the index level to calculate the averages for the `Value` column as shown:\\n\\n\\n\\n    df.set_index('Time', inplace=True)\\n\\n    # Taking mean values for a frequency of 2 minutes\\n\\n    df_group = df.groupby(pd.TimeGrouper(level='Time', freq='2T'))['Value'].agg('mean')   \\n\\n    df_group.dropna(inplace=True)\\n\\n    df_group = df_group.to_frame().reset_index()\\n\\n    print(df_group)\\n\\n    \\n\\n                     Time     Value\\n\\n    0 2015-04-24 06:38:00  0.021459\\n\\n    1 2015-04-24 06:42:00  0.023844\\n\\n    2 2015-04-24 06:44:00  0.020665\\n\\n    3 2015-04-24 06:46:00  0.023844\\n\\n    4 2015-04-24 06:48:00  0.019075\\n\\n    5 2015-04-24 06:50:00  0.022254\\n\\n    6 2015-04-24 06:52:00  0.020665\\n\\n    7 2015-04-24 06:54:00  0.023844\\n\\n    8 2015-04-24 07:00:00  0.020665\\n\\n\\n\\nYou could also use [`resample`][3] as pointed out by @Paul H which is rather concise for this situation.\\n\\n\\n\\n    print(df.set_index('Time').resample('2T').mean().dropna().reset_index())\\n\\n\\n\\n                     Time     Value\\n\\n    0 2015-04-24 06:38:00  0.021459\\n\\n    1 2015-04-24 06:42:00  0.023844\\n\\n    2 2015-04-24 06:44:00  0.020665\\n\\n    3 2015-04-24 06:46:00  0.023844\\n\\n    4 2015-04-24 06:48:00  0.019075\\n\\n    5 2015-04-24 06:50:00  0.022254\\n\\n    6 2015-04-24 06:52:00  0.020665\\n\\n    7 2015-04-24 06:54:00  0.023844\\n\\n    8 2015-04-24 07:00:00  0.020665\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.Grouper.html\\n\\n[2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\n\\n[3]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#resampling\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 8, 17),\n",
       "  '2016-08-17 18:03:46',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2093.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2302',\n",
       "  '39283657',\n",
       "  'Answer',\n",
       "  'how to understand axis = 0 or 1 in pandas (Python)?',\n",
       "  'Interpret axis=0 to apply the algorithm down each column, or to the row labels (the index).. A more detailed schema [here][1].\\n\\n\\n\\nIf you apply that general interpretation to your case, the algorithm here is `concat`. Thus for axis=0, it means: \\n\\n\\n\\n*for each column, take all the rows down (across all the dataframes for `concat`) , and do contact them when they are in common (because you selected `join=inner`).* \\n\\n\\n\\nSo the meaning would be to take all columns `x` and concat them down the rows which would stack each chunk of rows one after another. However, here `x` is not present everywhere, so it is not kept for the final result. The same applies for `z`. For `y` the result is kept as `y` is in all dataframes. This is the result you have.\\n\\n\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/a/25774395/624829',\n",
       "  '<python><pandas><axis>',\n",
       "  datetime.date(2016, 9, 2),\n",
       "  '2017-05-23 10:29:11',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '23736.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2304',\n",
       "  '39906235',\n",
       "  'Answer',\n",
       "  'How to flatten a pandas dataframe with some columns as json?',\n",
       "  \"Here's a solution using [`json_normalize()`][2] again by using a custom function to get the data in the correct format understood by `json_normalize` function.\\n\\n \\n\\n    import ast\\n\\n    from pandas.io.json import json_normalize\\n\\n\\n\\n    def only_dict(d):\\n\\n        '''\\n\\n        Convert json string representation of dictionary to a python dict\\n\\n        '''\\n\\n        return ast.literal_eval(d)\\n\\n\\n\\n    def list_of_dicts(ld):\\n\\n        '''\\n\\n        Create a mapping of the tuples formed after \\n\\n        converting json strings of list to a python list   \\n\\n        '''\\n\\n        return dict([(list(d.values())[1], list(d.values())[0]) for d in ast.literal_eval(ld)])\\n\\n\\n\\n    A = json_normalize(df['columnA'].apply(only_dict).tolist()).add_prefix('columnA.')\\n\\n    B = json_normalize(df['columnB'].apply(list_of_dicts).tolist()).add_prefix('columnB.pos.') \\n\\n\\n\\nFinally, join the `DFs` on the common index to get:\\n\\n\\n\\n    df[['id', 'name']].join([A, B])\\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n\\n\\n----------\\n\\n***EDIT:-*** As per the comment by @MartijnPieters, the recommended way of decoding the json strings would be to use [`json.loads()`][3] which is much faster when compared to using [`ast.literal_eval()`][4] if you know that the data source is JSON.\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/SpBIg.png\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\\n\\n[3]: https://docs.python.org/3/library/json.html#json.loads\\n\\n[4]: https://docs.python.org/3/library/ast.html#ast.literal_eval\",\n",
       "  '<python><json><pandas><dataframe><flatten>',\n",
       "  datetime.date(2016, 10, 6),\n",
       "  '2017-11-12 08:22:58',\n",
       "  'gaborous (1121352), Nickil Maveli (6207849)',\n",
       "  '17',\n",
       "  '',\n",
       "  '8137.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2306',\n",
       "  '39296352',\n",
       "  'Answer',\n",
       "  'groupby, sum and count to one table',\n",
       "  \"You can do this using a single `groupby` with\\n\\n\\n\\n    res = df.groupby(df.C).agg({'A': 'sum', 'B': {'sum': 'sum', 'count': 'count'}})\\n\\n\\n\\n    res.columns = ['A_sum', 'B_sum', 'count']\\n\\n\\n\\n\",\n",
       "  '<python><pandas><numpy>',\n",
       "  datetime.date(2016, 9, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '3053.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2309',\n",
       "  '39308115',\n",
       "  'Answer',\n",
       "  'How to one-hot-encode from a csv file input',\n",
       "  'Say you start with\\n\\n\\n\\n    In [31]: df = pd.DataFrame({\\'col\\': [\\'foo\\', \\'foo\\', \\'bar\\', \\'bar\\'], \\'num\\': [1, 1, 3, 213]})\\n\\n\\n\\n    In [32]: df\\n\\n    Out[32]: \\n\\n       col  num\\n\\n    0  foo    1\\n\\n    1  foo    1\\n\\n    2  bar    3\\n\\n    3  bar  213\\n\\n\\n\\nFirst, let\\'s take care of `col`:\\n\\n\\n\\nIf we define\\n\\n\\n\\n    In [33]: d = dict([e[:: -1] for e in enumerate(df.col.unique())])\\n\\n\\n\\nThen we can use it to \"numerify\" `col`:\\n\\n\\n\\n    In [34]: df.col = df.col.map(d)\\n\\n\\n\\n    In [35]: df\\n\\n    Out[35]: \\n\\n       col  num\\n\\n    0    0    1\\n\\n    1    0    1\\n\\n    2    1    3\\n\\n    3    1  213\\n\\n\\n\\nNow let\\'s deal with `num`:\\n\\n\\n\\n    In [36]: import numpy as np\\n\\n\\n\\nWe\\'ll just make everything over 100 into 100:\\n\\n\\n\\n    In [37]: df.num = np.minimum(df.num.values, 100)\\n\\n\\n\\n    In [38]: df\\n\\n    Out[38]: \\n\\n       col  num\\n\\n    0    0    1\\n\\n    1    0    1\\n\\n    2    1    3\\n\\n    3    1  100\\n\\n\\n\\nNow for the encoding:\\n\\n\\n\\n    In [49]: from sklearn import preprocessing\\n\\n\\n\\n    In [50]: enc = preprocessing.OneHotEncoder()\\n\\n\\n\\n    In [51]: enc.fit(df.as_matrix()).transform(df.as_matrix()).toarray()\\n\\n    Out[51]: \\n\\n    array([[ 1.,  0.,  1.,  0.,  0.],\\n\\n           [ 1.,  0.,  1.,  0.,  0.],\\n\\n           [ 0.,  1.,  0.,  1.,  0.],\\n\\n           [ 0.,  1.,  0.,  0.,  1.]])\\n\\n\\n\\nTwo things to note:\\n\\n\\n\\n1. `toarray()` makes the matrix dense again; its use is optional, of course.\\n\\n\\n\\n2. By construction, the last column is necessarily the \"100 and over\" category of `num`. You can retain it or drop this column, as needed.\\n\\n',\n",
       "  '<python><csv><pandas><scikit-learn>',\n",
       "  datetime.date(2016, 9, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1255.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2318',\n",
       "  '35797361',\n",
       "  'Answer',\n",
       "  'Using pandas to select specific seasons from a dataframe whose values are over a defined threshold',\n",
       "  \"Here's an example for selecting a normal spring followed by a warm summer (just using 1 std dev, not 2, for this example).\\n\\n\\n\\n    >>> seasdif[ (abs(seasdif) < seasdif.std()) &                     # within 1 std dev\\n\\n                 (seasdif.index.get_level_values('Season') == '1') &  # spring \\n\\n                 (seasdif.shift(-1) > seasdif.std()) ]                # following summer\\n\\n\\n\\n    Year  Season\\n\\n    2036  1         0.064691\\n\\n    2038  1        -0.016453\\n\\n    2047  1         0.020691\\n\\n    2053  1         0.063338\\n\\n    2055  1        -0.045606\\n\\n    Name: A, dtype: float64\\n\\n    \\n\\nMy random data is different than yours, so here are my values for 2036 and the std dev below that so that you can verify what the code is doing.\\n\\n\\n\\n    >>> seasdif.loc[2036]\\n\\n    \\n\\n    Season\\n\\n    1    0.064691\\n\\n    2    0.165824\\n\\n    3   -0.043372\\n\\n    4    0.086788\\n\\n    Name: A, dtype: float64\\n\\n    \\n\\n    >>> seasdif.std()\\n\\n\\n\\n    0.09357005962032763\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 3, 4),\n",
       "  '2016-03-04 16:10:17',\n",
       "  'JohnE (3877338)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1311.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2320',\n",
       "  '35805166',\n",
       "  'Answer',\n",
       "  'Iterate through Pandas DataFrame, use condition and add column',\n",
       "  \"You can define a function that maps a time period to the string you want, and then use [`map`][1].\\n\\n\\n\\n    def get_periode(hour):\\n\\n        if 4 <= hour <= 7:\\n\\n            return 'morning'\\n\\n        elif 8 <= hour <= 11:\\n\\n            return 'before midday'\\n\\n    \\n\\n    basket_times['periode'] = basket_times['hour'].map(get_periode)\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.map.html\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 3, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1253.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2322',\n",
       "  '39028591',\n",
       "  'Answer',\n",
       "  \"TypeError: __init__() got an unexpected keyword argument 'columns'\",\n",
       "  \"If you look at [the documentation for the class's `__init__`](https://wxpython.org/Phoenix/docs/html/wx.lib.agw.ultimatelistctrl.UltimateListCtrl.html#wx.lib.agw.ultimatelistctrl.UltimateListCtrl.__init__), you can see that it has no [keyword argument](https://docs.python.org/2/tutorial/controlflow.html#keyword-arguments) `columns`, and yet you're trying to pass one:\\n\\n\\n\\n    self.view_listctrl = ULC.UltimateListCtrl(package_panel, id=-1,columns=2,selectionType=1)\\n\\n\\n\\nIt's basically exactly what the error message is telling you.\",\n",
       "  '<python><wxpython>',\n",
       "  datetime.date(2016, 8, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1320.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2328',\n",
       "  '39333319',\n",
       "  'Answer',\n",
       "  'How to iterate the rows of a DataFrame as Series in Pandas?',\n",
       "  \"> How can I iterate over rows in a DataFrame? For some reason iterrows() is returning tuples rather than Series.\\n\\n\\n\\nThe second entry in the tuple is a Series:\\n\\n\\n\\n    In [9]: df = pd.DataFrame({'a': range(4), 'b': range(2, 6)})\\n\\n\\n\\n    In [10]: for r in df.iterrows():\\n\\n        print r[1], type(r[1])\\n\\n       ....:     \\n\\n    a    0\\n\\n    b    2\\n\\n    Name: 0, dtype: int64 <class 'pandas.core.series.Series'>\\n\\n    a    1\\n\\n    b    3\\n\\n    Name: 1, dtype: int64 <class 'pandas.core.series.Series'>\\n\\n    a    2\\n\\n    b    4\\n\\n    Name: 2, dtype: int64 <class 'pandas.core.series.Series'>\\n\\n    a    3\\n\\n    b    5\\n\\n    Name: 3, dtype: int64 <class 'pandas.core.series.Series'>\\n\\n\\n\\n>  I also understand that this is not an efficient way of using Pandas.\\n\\n\\n\\nThat is true, in general, but the question is a bit too general. You'll need to specify why you're trying to iterate over the DataFrame.\",\n",
       "  '<loops><pandas><dataframe><series>',\n",
       "  datetime.date(2016, 9, 5),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '4484.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2330',\n",
       "  '39365089',\n",
       "  'Answer',\n",
       "  'pandas group by ALL functionality?',\n",
       "  \"You could add a dummy column:\\n\\n\\n\\n    df['dummy'] = 1\\n\\n\\n\\nThen groupby + agg on it:\\n\\n\\n\\n    df.groupby('dummy').agg(aggs_dict)\\n\\n\\n\\nand then [delete it](https://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe) when you're done.\",\n",
       "  '<python><pandas><group-by>',\n",
       "  datetime.date(2016, 9, 7),\n",
       "  '2017-05-23 10:29:11',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1123.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2334',\n",
       "  '39803077',\n",
       "  'Answer',\n",
       "  'how to set initial centers of K-means openCV c++',\n",
       "  'The function allows you to directly set the initial *labeling*, not *centers*. Fortunately, since [k-means alternates between assignment and update steps](https://en.wikipedia.org/wiki/K-means_clustering#Algorithms), you can get the effect you want indirectly.\\n\\n\\n\\n-----------------\\n\\n\\n\\nFrom [the docs](http://docs.opencv.org/2.4/modules/core/doc/clustering.html):\\n\\n\\n\\n> **labels** – Input/output integer array that stores the cluster indices for every sample.\\n\\n\\n\\n> **KMEANS_USE_INITIAL_LABELS** During the first (and possibly the only) attempt, use the user-supplied labels instead of computing them from the initial centers. For the second and further attempts, use the random or semi-random centers. Use one of KMEANS_*_CENTERS flag to specify the exact method.\\n\\n\\n\\nSo, what the docs say is that you can set the initial *labeling*. If you want to do this, in your code\\n\\n\\n\\n    kmeans(samples, 2, labels, TermCriteria(TermCriteria::EPS + TermCriteria::COUNT, 10, 1.0), 3, KMEANS_USE_INITIAL_LABELS, centers);\\n\\n\\n\\ninitialize the 3rd parameter to be the input labels (for use in the first iteration).\\n\\n\\n\\n---------------\\n\\n\\n\\nIf you want to get the effect of setting the initial centers, you can do the following:\\n\\n\\n\\n1. Decide what the centers are.\\n\\n\\n\\n2. Calculate the labeling like the algorithm does in [the assignment step](https://en.wikipedia.org/wiki/K-means_clustering#Algorithms).\\n\\n\\n\\n3. Pass the resulting labeling to the function.',\n",
       "  '<c++><algorithm><opencv><k-means><centroid>',\n",
       "  datetime.date(2016, 10, 1),\n",
       "  '2016-10-01 06:45:23',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1762.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2339',\n",
       "  '39129252',\n",
       "  'Answer',\n",
       "  'Pandas realization of leave one out encoding for categorical features',\n",
       "  'Replace each element of the Series with difference between the sum of the Series and the element, then divide by the length of the series minus 1.  Assuming `s` is your Series:\\n\\n\\n\\n    s = (s.sum() - s)/(len(s) - 1)\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n    159812     39.00\\n\\n    464556     25.25\\n\\n    717223     34.75\\n\\n    1043801    40.75\\n\\n    1152917    44.25',\n",
       "  '<python><pandas><categorical-data>',\n",
       "  datetime.date(2016, 8, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1353.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2343',\n",
       "  '35970444',\n",
       "  'Answer',\n",
       "  'csv to nested JSON?',\n",
       "  \"Here's the general way of doing so with `csv.DictReader`.\\n\\n\\n\\nStart by loading the data:\\n\\n\\n\\n    import csv\\n\\n    import itertools\\n\\n    with open('stuff.csv', 'rb') as csvfile:\\n\\n        all_ = list(csv.DictReader(csvfile))\\n\\n\\n\\nNow, you can use `itertools.groupby` to group and process each group. For example\\n\\n\\n\\n    d = []\\n\\n    for k, g in itertools.groupby(\\n\\n            all_, \\n\\n            key=lambda r: (r['PrimaryId'], r[' LastName'])):\\n\\n        d.append({\\n\\n            'PrimaryId': k[0],\\n\\n            'LastName': k[1],\\n\\n            'CarName': [e[' CarName'] for e in g]\\n\\n            })\\n\\n\\n\\nWill group by primary id and last name, and make a list of cars.\\n\\n\\n\\nOnce you have something like this, you can just use [`json.dumps()`](https://docs.python.org/2/library/json.html).\\n\\n\",\n",
       "  '<python><json><csv>',\n",
       "  datetime.date(2016, 3, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '5324.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2344',\n",
       "  '35971047',\n",
       "  'Answer',\n",
       "  'polynomial evaluation time complexity',\n",
       "  \"Yes, you are right. The form you show is often called [Horner's Method](https://en.wikipedia.org/wiki/Horner%27s_method#Description_of_the_algorithm). It is linear in the sense that the number of elementary operations (additions, multiplications) is *O(n)*, where *n* is the highest coefficient.\\n\\n\\n\\n---------------\\n\\n\\n\\nIncidentally, your code above seems to contain a mistake. It probably should be\\n\\n\\n\\n    for(int i = order ; i>=0 ; i--){\\n\\n\\n\\n(the original is an infinited loop). Otherwise, it can be considered an implementation of Horner.\",\n",
       "  '<algorithm><time-complexity><polynomials>',\n",
       "  datetime.date(2016, 3, 13),\n",
       "  '2016-03-13 13:56:44',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1409.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2345',\n",
       "  '35975986',\n",
       "  'Answer',\n",
       "  'Pythonic way of write if open is successful',\n",
       "  \"Since you're asking about what the Pythonic was of doing something, I think that you should consider the [Ask Forgiveness, Not Permission](https://stackoverflow.com/questions/12265451/ask-forgiveness-not-permission-explain) paradigm. Namely, just perform the operation, and catch the appropriate exception if it didn't work.\\n\\n\\n\\nFor example, \\n\\n\\n\\n    In [1]: open('/usr/tmp.txt', 'w').write('hello')\\n\\n    ---------------------------------------------------------------------------\\n\\n    IOError                                   Traceback (most recent call last)\\n\\n    <ipython-input-1-cc55d7c8e6f9> in <module>()\\n\\n    ----> 1 open('/usr/tmp.txt', 'w').write('hello')\\n\\n\\n\\n    IOError: [Errno 13] Permission denied: '/usr/tmp.txt'\\n\\n\\n\\nIf there was no permission to do the op, an `IOError` will be thrown. Just catch it, then.\\n\\n\\n\\n    try:\\n\\n        open('/usr/tmp.txt', 'w').write('hello')\\n\\n    except IOError:\\n\\n        ...\\n\\n\\n\\n-----------------\\n\\n\\n\\nAlex Martelli once talked about this, and described some inherent fallacies about checking permissions. There's an inherent race in these matters. You could always have permission to write when you opened the file, but not later when you attempted to write. You'll have to deal with exceptions anyway, so you might as well just build with them.\\n\\n\",\n",
       "  '<python>',\n",
       "  datetime.date(2016, 3, 13),\n",
       "  '2017-05-23 11:46:36',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '5077.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2350',\n",
       "  '39407213',\n",
       "  'Question',\n",
       "  \"Why doesn't std::lock_guard/std::unique_lock use type erasure?\",\n",
       "  \"Why do [`std::lock_guard`][1] and [`std::unique_lock`][2] necessitate specifying the lock type as a template parameter?\\n\\n\\n\\nConsider the following alternative. First, in a `detail` namespace, there are type erasure classes (a non-template abstract base class, and a template derived class):\\n\\n\\n\\n    #include <type_traits>\\n\\n    #include <mutex>\\n\\n    #include <chrono>\\n\\n    #include <iostream>\\n\\n\\n\\n    namespace detail {\\n\\n\\n\\n        struct locker_unlocker_base {\\n\\n            virtual void lock() = 0;\\n\\n            virtual void unlock() = 0;\\n\\n        };\\n\\n\\n\\n        template<class Mutex>\\n\\n        struct locker_unlocker : public locker_unlocker_base {\\n\\n            locker_unlocker(Mutex &m) : m_m{&m} {}\\n\\n            virtual void lock() { m_m->lock(); }\\n\\n            virtual void unlock() { m_m->unlock(); }\\n\\n            Mutex *m_m;\\n\\n        };\\n\\n    }\\n\\n\\n\\nNow `te_lock_guard`, the type erasure lock guard, simply placement-news an object of the correct type when constructed (without dynamic memory allocation):\\n\\n\\n\\n    class te_lock_guard {\\n\\n    public:\\n\\n        template<class Mutex>\\n\\n        te_lock_guard(Mutex &m) {\\n\\n            new (&m_buf) detail::locker_unlocker<Mutex>(m);\\n\\n            reinterpret_cast<detail::locker_unlocker_base *>(&m_buf)->lock();\\n\\n        }\\n\\n        ~te_lock_guard() {\\n\\n            reinterpret_cast<detail::locker_unlocker_base *>(&m_buf)->unlock();\\n\\n        }\\n\\n\\n\\n    private:\\n\\n        std::aligned_storage<sizeof(detail::locker_unlocker<std::mutex>), alignof(detail::locker_unlocker<std::mutex>)>::type m_buf;\\n\\n    };\\n\\n\\n\\nI've checked the performance vs. the standard library's classes:\\n\\n\\n\\n    int main() {\\n\\n        constexpr std::size_t num{999999};\\n\\n        {\\n\\n            std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();\\n\\n            for(size_t i = 0; i < num; ++i) {\\n\\n                std::mutex m;\\n\\n                te_lock_guard l(m);\\n\\n            }\\n\\n            std::chrono::steady_clock::time_point end= std::chrono::steady_clock::now();\\n\\n            std::cout << std::chrono::duration_cast<std::chrono::microseconds>(end - begin).count() << std::endl;\\n\\n        }\\n\\n        {\\n\\n            std::chrono::steady_clock::time_point begin = std::chrono::steady_clock::now();\\n\\n            for(size_t i = 0; i < num; ++i) {\\n\\n                std::mutex m;\\n\\n                std::unique_lock<std::mutex> l(m);\\n\\n            }\\n\\n            std::chrono::steady_clock::time_point end= std::chrono::steady_clock::now();\\n\\n            std::cout << std::chrono::duration_cast<std::chrono::microseconds>(end - begin).count() << std::endl;\\n\\n        }\\n\\n    }\\n\\n\\n\\nUsing g++ with `-O3`, there is no statistically-significant performance loss.\\n\\n\\n\\n  [1]: http://en.cppreference.com/w/cpp/thread/lock_guard\\n\\n  [2]: http://en.cppreference.com/w/cpp/thread/unique_lock\\n\\n\",\n",
       "  '<c++><multithreading><type-erasure>',\n",
       "  datetime.date(2016, 9, 9),\n",
       "  '2016-09-09 20:40:50',\n",
       "  'Peter Mortensen (63550)',\n",
       "  '20',\n",
       "  '2.0',\n",
       "  '1252.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2356',\n",
       "  '36068006',\n",
       "  'Answer',\n",
       "  'Pandas, Get count of a single value in a Column of a Dataframe',\n",
       "  'If you take the `value_counts` return, you can query it for multiple values:\\n\\n\\n\\n    import pandas as pd\\n\\n\\n\\n    a = pd.Series([1, 1, 1, 1, 2, 2])\\n\\n    counts = a.value_counts()\\n\\n    >>> counts[1], counts[2]\\n\\n    (4, 2)\\n\\n\\n\\nHowever, to count only a single item, it would be faster to use\\n\\n\\n\\n    import numpy as np\\n\\n    np.sum(a == 1)',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 3, 17),\n",
       "  '2016-03-17 17:55:56',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '12741.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2358',\n",
       "  '39184512',\n",
       "  'Answer',\n",
       "  'pandas.DataFrame set all string values to nan',\n",
       "  \"You can use [`pd.to_numeric`](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.to_numeric.html) with `errors='coerce'`. \\n\\n\\n\\n    In [30]: df = pd.DataFrame({'a': [1, 2, 'NaN', 'bob', 3.2]})\\n\\n\\n\\n    In [31]: pd.to_numeric(df.a, errors='coerce')\\n\\n    Out[31]: \\n\\n    0    1.0\\n\\n    1    2.0\\n\\n    2    NaN\\n\\n    3    NaN\\n\\n    4    3.2\\n\\n    Name: a, dtype: float64\\n\\n\\n\\nHere is one way to apply it to all columns:\\n\\n\\n\\n    for c in df.columns:\\n\\n        df[c] = pd.to_numeric(df[c], errors='coerce')\\n\\n\\n\\n(See comment by NinjaPuppy for a better way.)\",\n",
       "  '<python><string><pandas><dataframe>',\n",
       "  datetime.date(2016, 8, 27),\n",
       "  '2016-08-27 18:50:41',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1823.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2362',\n",
       "  '39806178',\n",
       "  'Answer',\n",
       "  'pandas remove seconds from datetime index',\n",
       "  \"You could use [`datetime.replace`][2] to alter the second's attribute as shown:\\n\\n\\n\\n    df.index = df.index.map(lambda x: x.replace(second=0))\\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/xO9gu.png\\n\\n  [2]: https://docs.python.org/2/library/datetime.html#datetime.datetime.replace\",\n",
       "  '<python><pandas><indexing><seconds>',\n",
       "  datetime.date(2016, 10, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '11',\n",
       "  '',\n",
       "  '3268.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2363',\n",
       "  '39808427',\n",
       "  'Answer',\n",
       "  'bbox_to_anchor and loc in matplotlib',\n",
       "  \"The explanation of @Gabriel is slightly misleading. `bbox_to_anchor=[x0, y0]` will create a bounding box with **lower left** corner at position `[x0, y0]`. The extend of the bounding box is zero - being equivalent to `bbox_to_anchor=[x0, y0, 0, 0]`. The legend will then be placed 'inside' this box and overlapp it according to the specified `loc` parameter. So `loc` specifies where inside the box the legend sits.\\n\\n\\n\\nAlso see this question https://stackoverflow.com/questions/39803385/what-does-a-4-element-tuple-argument-for-bbox-to-anchor-mean-in-matplotlib/39806180#39806180\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2016, 10, 1),\n",
       "  '2017-05-23 12:26:10',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '11',\n",
       "  '',\n",
       "  '27335.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2370',\n",
       "  '40077889',\n",
       "  'Answer',\n",
       "  'Is there a way to compile python application into static binary?',\n",
       "  'To freeze your python executable and ship it along your code, embed it in an empty shell app. Follow the instructions how to embed python in an application from the [official documentation][1]. You can start building a sample app directly from the C sample code they give on the web page. \\n\\n\\n\\nMake that program execute your python application through the embedded python. Ship the program, the embedded python you used and your python program. Execute that program.\\n\\n\\n\\n\\n\\n  [1]: https://docs.python.org/3/extending/embedding.html#embedding-python-in-another-application',\n",
       "  '<python><build>',\n",
       "  datetime.date(2016, 10, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '17518.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2375',\n",
       "  '38738209',\n",
       "  'Answer',\n",
       "  'TypeError: list indices must be integers, not str (boolean convertion actually)',\n",
       "  \"Only change that needs to be made is that `features` must be initialized to a `dict` (`{}`) rather than a `list` (`[]`) and then you could populate it's contents. \\n\\n\\n\\nThe `TypeError` was because `word_features` is a list of *strings* which you were trying to index using a list and lists can't have string indices.\\n\\n\\n\\n    features={}\\n\\n    for w in word_features:\\n\\n        features[w] = (w in words)\\n\\n\\n\\nHere, the elements present in `word_features` constitute the `keys` of dictionary, `features` holding boolean values, `True` based on whether the same element appears in `words` (which holds unique items due to calling of `set()`) and `False` for the vice-versa situation.\\n\\n\\n\\n   \",\n",
       "  '<python><find><movie><review>',\n",
       "  datetime.date(2016, 8, 3),\n",
       "  '2017-02-16 07:11:01',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '6',\n",
       "  '',\n",
       "  '31044.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2376',\n",
       "  '38768867',\n",
       "  'Answer',\n",
       "  'add a label to a plot with seaborn',\n",
       "  'You can specify the `label` names to each subplot axis and use `plt.legend` to add the appropriate legends to the center right corner. \\n\\n    \\n\\n    \\n\\n    fig = sns.plt.figure(figsize=(12, 5), dpi=100)\\n\\n    ax1 = fig.add_subplot(111)\\n\\n    x1 = pd.to_datetime(df_no_missing.TIMESTAMP)\\n\\n    y1 = df_no_missing.P_ACT_KW\\n\\n    y2 = df_no_missing.depassement\\n\\n    y3 = df_no_missing.P_SOUSCR\\n\\n    yearFmt = mdates.DateFormatter(\"%H:%M:%S\")\\n\\n    ax1.xaxis.set_major_formatter(yearFmt)\\n\\n    ax1.plot(x1, y1, \\'g-\\', label=\\'p_act_kw\\')\\n\\n    ax1.plot(x1, y3, \\'r-\\', label=\\'p_souscr\\')\\n\\n    ax2 = ax1.twinx()\\n\\n    ax2.plot(x1, y2, \\'b-\\', label=\\'depassement\\')\\n\\n    h1, l1 = ax1.get_legend_handles_labels()\\n\\n    h2, l2 = ax2.get_legend_handles_labels()\\n\\n    ax1.legend(h1+h2, l1+l2, loc=\\'center right\\')\\n\\n    ax1.set_xlabel(\\'temps\\')\\n\\n    ax1.set_ylabel(\\'puissance\\', color=\\'g\\')\\n\\n    ax2.set_ylabel(\\'dépassement\\', color=\\'b\\')\\n\\n    sns.plt.ylim(plt.ylim()[0], 1.0)\\n\\n    sns.plt.show()\\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/9mqqj.png',\n",
       "  '<python><seaborn>',\n",
       "  datetime.date(2016, 8, 4),\n",
       "  '2016-08-04 15:45:04',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1711.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2377',\n",
       "  '39276164',\n",
       "  'Answer',\n",
       "  'Sort by certain order (Situation: pandas DataFrame Groupby)',\n",
       "  'Set the `\\'day\\'` column as [categorical][1] dtype, just make sure when you set the category your list of days is sorted as you\\'d like it to be.  Performing the `groupby` will then automatically sort it for you, but if you otherwise tried to sort the column it will sort in the correct order that you specify.\\n\\n\\n\\n    # Initial setup.\\n\\n    np.random.seed([3,1415])\\n\\n    n = 100\\n\\n    days = [\\'Mon\\', \\'Tue\\', \\'Wed\\', \\'Thu\\', \\'Fri\\', \\'Sat\\', \\'Sun\\']\\n\\n    df = pd.DataFrame({\\n\\n        \\'device_id\\': np.random.randint(1,3,n),\\n\\n        \\'day\\': np.random.choice(days, n),\\n\\n        \\'dwell_time\\':np.random.random(n)\\n\\n        })\\n\\n    \\n\\n    \\n\\n    # Set as category, groupby, and sort.\\n\\n    df[\\'day\\'] = df[\\'day\\'].astype(\"category\", categories=days, ordered=True)\\n\\n    df = df.groupby([\\'device_id\\', \\'day\\']).sum()\\n\\n\\n\\n***Update***: astype no longer accepts categories, use:\\n\\n\\n\\n    category_day = pd.api.types.CategoricalDtype(categories=days, ordered=True)\\n\\n    df[\\'day\\'] = df[\\'day\\'].astype(category_day)\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n                   dwell_time\\n\\n    device_id day            \\n\\n    1         Mon    4.428626\\n\\n              Tue    3.259319\\n\\n              Wed    2.436024\\n\\n              Thu    0.909724\\n\\n              Fri    4.974137\\n\\n              Sat    5.583778\\n\\n              Sun    2.687258\\n\\n    2         Mon    3.117923\\n\\n              Tue    2.427154\\n\\n              Wed    1.943927\\n\\n              Thu    4.599547\\n\\n              Fri    2.628887\\n\\n              Sat    6.247520\\n\\n              Sun    2.716886\\n\\n\\n\\nNote that this method works for any type of customized sorting.  For example, if you had a column with entries `\\'a\\', \\'b\\', \\'c\\'`, and wanted it to be sorted in a non-standard order, e.g. `\\'c\\', \\'a\\', \\'b\\'`, you\\'d just do the same type of procedure: specify the column as categorical with your categories being in the non-standard order you want.\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/categorical.html\\n\\n\\n\\n',\n",
       "  '<python><sorting><pandas>',\n",
       "  datetime.date(2016, 9, 1),\n",
       "  '2018-03-27 12:13:16',\n",
       "  'root (3339965), Community (-1)',\n",
       "  '4',\n",
       "  '',\n",
       "  '3587.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2379',\n",
       "  '39815686',\n",
       "  'Answer',\n",
       "  'Pandas: append dataframe to another df',\n",
       "  \"If you look at [the documentation for `pd.DataFrame.append`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.append.html)\\n\\n\\n\\n> Append rows of other to the end of this frame, **returning a new object**. Columns not in this frame are added as new columns.\\n\\n\\n\\n(emphasis mine). \\n\\n\\n\\nTry \\n\\n\\n\\n    df_res = df_res.append(res)\\n\\n\\n\\n-----------------------------\\n\\n\\n\\nIncidentally, note that pandas isn't that efficient for creating a DataFrame by successive concatenations. You might try this, instead:\\n\\n\\n\\n    all_res = []\\n\\n    for df in df_all:\\n\\n        for i in substr:\\n\\n            res = df[df['url'].str.contains(i)]\\n\\n            all_res.append(res)\\n\\n\\n\\n    df_res = pd.concat(all_res)\\n\\n\\n\\nThis first creates a list of all the parts, then creates a DataFrame from all of them once at the end.\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 10, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '8195.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2380',\n",
       "  '39816998',\n",
       "  'Answer',\n",
       "  'Pandas populate new dataframe column based on matching columns in another dataframe',\n",
       "  \"**APPROACH 1:**\\n\\n\\n\\nYou could use [`concat`][1] instead and drop the duplicated values present in both `Index` and `AUTHOR_NAME` columns combined. After that, use [`isin`][2] for checking membership:\\n\\n\\n\\n    df_concat = pd.concat([df2, df]).reset_index().drop_duplicates(['Index', 'AUTHOR_NAME'])\\n\\n    df_concat.set_index('Index', inplace=True)\\n\\n    df_concat[df_concat.index.isin(df.index)]\\n\\n\\n\\n[![Image][3]][3]\\n\\n\\n\\nNote: The column `Index` is assumed to be set as the index column for both the `DF's`.\\n\\n\\n\\n\\n\\n----------\\n\\n**APPROACH 2:**\\n\\n\\n\\nUse [`join`][5] after setting the index column correctly as shown:\\n\\n\\n\\n    df2.set_index(['Index', 'AUTHOR_NAME'], inplace=True)\\n\\n    df.set_index(['Index', 'AUTHOR_NAME'], inplace=True)\\n\\n    \\n\\n    df.join(df2).reset_index()\\n\\n\\n\\n[![Image][4]][4]\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isin.html\\n\\n  [3]: http://i.stack.imgur.com/wjWdz.png\\n\\n  [4]: http://i.stack.imgur.com/vRkQP.png\\n\\n  [5]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html\",\n",
       "  '<python><pandas><merge><populate>',\n",
       "  datetime.date(2016, 10, 2),\n",
       "  '2016-10-02 14:40:02',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '20565.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2382',\n",
       "  '36104255',\n",
       "  'Answer',\n",
       "  'The right way to query a pandas MultiIndex',\n",
       "  \"I think what you did is fine, but there are alternative ways also.\\n\\n\\n\\n    >>> df = pd.DataFrame({ \\n\\n                  'stock':np.repeat( ['AAPL','GOOG','YHOO'], 3 ),\\n\\n                  'date':np.tile( pd.date_range('5/5/2015', periods=3, freq='D'), 3 ),\\n\\n                  'price':(np.random.randn(9).cumsum() + 10) })\\n\\n\\n\\n    >>> df = df.set_index(['stock','date'])\\n\\n    \\n\\n                          price\\n\\n    stock date                 \\n\\n    AAPL  2015-05-05   8.538459\\n\\n          2015-05-06   9.330140\\n\\n          2015-05-07   8.968898\\n\\n    GOOG  2015-05-05   8.964389\\n\\n          2015-05-06   9.828230\\n\\n          2015-05-07   9.992985\\n\\n    YHOO  2015-05-05   9.929548\\n\\n          2015-05-06   9.330295\\n\\n          2015-05-07  10.676468\\n\\n    \\n\\nA slightly more standard way of using loc twice\\n\\n\\n\\n    >>> df.loc['AAPL'].loc['2015-05-05']\\n\\n\\n\\nwould be to do \\n\\n\\n\\n    >>> df.loc['AAPL','2015-05-05']\\n\\n     \\n\\n    price    8.538459\\n\\n    Name: (AAPL, 2015-05-05 00:00:00), dtype: float64\\n\\n    \\n\\nAnd instead of `xs` you could use an IndexSlice.  I think for 2 levels `xs` is easier, but IndexSlice might be better past 2 levels.\\n\\n\\n\\n    >>> idx=pd.IndexSlice\\n\\n    \\n\\n    >>> df.loc[ idx[:,'2015-05-05'], : ]\\n\\n     \\n\\n                         price\\n\\n    stock date                \\n\\n    AAPL  2015-05-05  8.538459\\n\\n    GOOG  2015-05-05  8.964389\\n\\n    YHOO  2015-05-05  9.929548\\n\\n\\n\\nAnd to be honest, I think the absolute easiest way here is use either date or stock (or neither) as index and then most selections are very straightforward.  For example, if you remove the index completely you can effortlessly select by date:\\n\\n\\n\\n    >>> df = df.reset_index()\\n\\n    >>> df[ df['date']=='2015-05-05' ]\\n\\n \\n\\n       index stock       date      price\\n\\n    0      0  AAPL 2015-05-05   8.538459\\n\\n    3      3  GOOG 2015-05-05   8.964389\\n\\n    6      6  YHOO 2015-05-05   9.929548\\n\\n\\n\\nDoing some quickie timings with 3 stocks and 3000 dates (=9000 rows), I found that a simple boolean selection (no index) was about 35% faster than xs, and xs was about 35% faster than using IndexSlice.  But see Jeff's comment below, you should expect the boolean selection to perform relative worse with more rows.\\n\\n\\n\\nOf course, the best thing for you to do is test on your own data and see how it comes out.\",\n",
       "  '<pandas>',\n",
       "  datetime.date(2016, 3, 19),\n",
       "  '2016-03-19 17:26:45',\n",
       "  'JohnE (3877338)',\n",
       "  '4',\n",
       "  '',\n",
       "  '3211.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2383',\n",
       "  '36115594',\n",
       "  'Answer',\n",
       "  'Implementing the Alon-Matias-Szegedy Algorithm For The Second Moment Stream Approximation',\n",
       "  'This is an interesting question.\\n\\n\\n\\nSay we start with\\n\\n\\n\\n    import random\\n\\n    import string\\n\\n\\n\\n    size = 100000\\n\\n    seq = [random.choice(string.ascii_letters) for x in range(size)]\\n\\n\\n\\nThen the first implementation is similar to yours (note the use of [`collections.Counter`](https://pymotw.com/2/collections/counter.html), though):\\n\\n\\n\\n    from collections import Counter\\n\\n\\n\\n    def secondMoment(seq):\\n\\n        c = Counter(seq)\\n\\n        return sum(v**2 for v in c.values())\\n\\n\\n\\n    >>> secondMoment(seq)\\n\\n    192436972\\n\\n\\n\\nThe second implementation differs more significantly than yours, though. Note that first the random indices are found. Then, an element is counted only after its first occurrence (if any) at one of the indices:\\n\\n\\n\\n    from collections import defaultdict\\n\\n\\n\\n    def AMSestimate(seq, num_samples=10):\\n\\n        inds = list(range(len(seq)))\\n\\n        random.shuffle(inds)\\n\\n        inds = sorted(inds[: num_samples])\\n\\n\\n\\n        d = {}\\n\\n        for i, c in enumerate(seq):\\n\\n            if i in inds and c not in d:\\n\\n                d[c] = 0\\n\\n            if c in d:\\n\\n                d[c] += 1\\n\\n        return int(len(seq) / float(len(d)) * sum((2 * v - 1) for v in d.values()))\\n\\n\\n\\n    >>> AMSestimate(seq)\\n\\n    171020000\\n\\n\\n\\n----------------------------------------------------\\n\\n\\n\\n**Edit Regarding The Original Code In The Question**\\n\\n\\n\\nIn the code in the question, consider your loop\\n\\n\\n\\n    for el in vector:\\n\\n        if el in elements:\\n\\n            elements[el] += 1\\n\\n        elif random.choice(range(0, 10)) == 0:\\n\\n            elements[el] = 1\\n\\n\\n\\n(Minor) The sampling is problematic: it is hard-coded probabilistic at 0.1\\n\\n\\n\\nAlso consider:\\n\\n\\n\\n        estimateM2 += lenvect * ((2 * value) - 1)\\n\\n\\n\\nThis lacks a division by the number of sampled elements.',\n",
       "  '<python><random><data-mining><data-stream><bigdata>',\n",
       "  datetime.date(2016, 3, 20),\n",
       "  '2016-03-21 08:44:39',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1130.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2385',\n",
       "  '39298489',\n",
       "  'Answer',\n",
       "  'Python Pandas: Check if all columns in rows value is NaN',\n",
       "  \">  I need to check if in any particular row all the values are NaN so that I can drop them from my dataset. \\n\\n\\n\\nThat's exactly what [`pd.DataFrame.dropna(how='all')`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.dropna.html) does:\\n\\n\\n\\n    In [3]: df = pd.DataFrame({'a': [None, 1, None], 'b': [None, 1, 2]})\\n\\n\\n\\n    In [4]: df\\n\\n    Out[4]: \\n\\n         a    b\\n\\n    0  NaN  NaN\\n\\n    1  1.0  1.0\\n\\n    2  NaN  2.0\\n\\n\\n\\n    In [5]: df.dropna(how='all')\\n\\n    Out[5]: \\n\\n         a    b\\n\\n    1  1.0  1.0\\n\\n    2  NaN  2.0\\n\\n\\n\\nRegarding your second question, [`pd.DataFrame.boxplot`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.boxplot.html) will do that. You can specify the columns you want (if needed), with the `column` parameter. See [the example in the docs](http://pandas.pydata.org/pandas-docs/stable/visualization.html#box-plots) also.\",\n",
       "  '<python><pandas><nan>',\n",
       "  datetime.date(2016, 9, 2),\n",
       "  '2016-09-02 18:15:08',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2381.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2386',\n",
       "  '39301490',\n",
       "  'Answer',\n",
       "  'Appending one data frame into another',\n",
       "  \"Get a single column name for all your dataframes:\\n\\n\\n\\n    org_city_id.columns = pol_city_id.columns = pod_city_id.columns = 'Final Name'\\n\\n\\n\\nThen concat them:\\n\\n\\n\\n    pd.concat([org_city_id,pol_city_id,pod_city_id])\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 9, 2),\n",
       "  '2016-09-02 22:56:42',\n",
       "  'Boud (624829)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1866.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2392',\n",
       "  '39308809',\n",
       "  'Answer',\n",
       "  'TfidfVectorizer in scikit-learn : ValueError: np.nan is an invalid document',\n",
       "  \"You need to convert the dtype `object` to `unicode` string as is clearly mentioned in the traceback.\\n\\n\\n\\n    x = v.fit_transform(df['Review'].values.astype('U'))  ## Even astype(str) would work\\n\\n\\n\\nFrom the Doc page of TFIDF Vectorizer:\\n\\n> fit_transform(raw_documents, y=None) <br>\\n\\n\\n\\n>Parameters:\\t raw_documents : iterable <br>\\n\\n>an iterable which yields either *str*, *unicode* or *file objects*\",\n",
       "  '<python><pandas><machine-learning><scikit-learn><tf-idf>',\n",
       "  datetime.date(2016, 9, 3),\n",
       "  '2016-09-03 16:06:27',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '61',\n",
       "  '',\n",
       "  '15334.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2395',\n",
       "  '38797881',\n",
       "  'Answer',\n",
       "  'Applying strptime function to pandas series',\n",
       "  \"Use [`pd.to_datetime`][1]:\\n\\n\\n\\n    date_series = pd.to_datetime(date_string)\\n\\n\\n\\nIn general it's best have your dates as Pandas' `pd.Timestamp` instead of Python's `datetime.datetime` if you plan to do your work in Pandas.  You may also want to review the [Time Series / Date functionality documentation][2].\\n\\n\\n\\nAs to why your `apply` isn't working, `args` isn't being read as a tuple, but rather as a string that's being broken up into 17 characters, each being interpreted as a separate argument.  To make it be read as a tuple, add a comma: `args=('%Y-%m-%d %H:%M:%S',)`.\\n\\n\\n\\nThis is standard behaviour in Python.  Consider the following example:\\n\\n\\n\\n    x = ('a')\\n\\n    y = ('a',)\\n\\n    print('x info:', x, type(x))\\n\\n    print('y info:', y, type(y))\\n\\n    \\n\\n    x info: a <class 'str'>\\n\\n    y info: ('a',) <class 'tuple'>\\n\\n\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_datetime.html\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html\\n\\n\",\n",
       "  '<python><pandas><strptime>',\n",
       "  datetime.date(2016, 8, 5),\n",
       "  '2016-08-05 21:41:42',\n",
       "  'root (3339965)',\n",
       "  '3',\n",
       "  '',\n",
       "  '4707.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2396',\n",
       "  '38812096',\n",
       "  'Answer',\n",
       "  'python-pandas: dealing with NaT type values in a date columns of pandas dataframe',\n",
       "  \"Say you start with something like this:\\n\\n\\n\\n    df = pd.DataFrame({\\n\\n        'CUSTOMER_name': ['abc', 'def', 'abc', 'def', 'abc', 'fff'], \\n\\n        'DATE': ['NaT', 'NaT', '2010-04-15 19:09:08', '2011-01-25 15:29:37', '2010-04-10 12:29:02', 'NaT']})\\n\\n    df.DATE = pd.to_datetime(df.DATE)\\n\\n\\n\\n(note that the only difference is adding `fff` mapped to `NaT`).\\n\\n\\n\\nThen the following does what you ask:\\n\\n  \\n\\n    >>> pd.to_datetime(df.DATE.groupby(df.CUSTOMER_name).min())\\n\\n    CUSTOMER_name\\n\\n    abc   2010-04-10 12:29:02\\n\\n    def   2011-01-25 15:29:37\\n\\n    fff                   NaT\\n\\n    Name: DATE, dtype: datetime64[ns]\\n\\n\\n\\nThis is because `groupby`-`min` already excludes missing data where applicable (albeit changing the format of the results), and the final `pd.to_datetime` coerces the result again to a `datetime`.\\n\\n\\n\\n------------------------------\\n\\n\\n\\nTo get the date part of the result (which I think is a separate question), use `.dt.date`:\\n\\n\\n\\n    >>> pd.to_datetime(df.DATE.groupby(df.CUSTOMER_name).min()).dt.date\\n\\n    Out[19]: \\n\\n    CUSTOMER_name\\n\\n    abc    2010-04-10\\n\\n    def    2011-01-25\\n\\n    fff           NaN\\n\\n    Name: DATE, dtype: object\\n\\n\\n\\n\",\n",
       "  '<python><datetime><pandas>',\n",
       "  datetime.date(2016, 8, 7),\n",
       "  '2016-08-07 08:25:01',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '4157.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2398',\n",
       "  '40149233',\n",
       "  'Answer',\n",
       "  'TypeError: return arrays must be of ArrayType',\n",
       "  \"Presumably, you're using [`numpy.logical_and`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.logical_and.html), in the form of\\n\\n\\n\\n    np.logical_and(a, b, c)\\n\\n\\n\\nwith the meaning that you'd like to take the logical and of the three. If you check the documentation, though, that's not what it does. It's interpreting `c` as the array where you intend to store the results. \\n\\n\\n\\nYou probably mean here something like\\n\\n\\n\\n    np.logical_and(a, np.logical_and(b, c))\",\n",
       "  '<python><numpy><multidimensional-array><typeerror>',\n",
       "  datetime.date(2016, 10, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2172.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2406',\n",
       "  '39372386',\n",
       "  'Answer',\n",
       "  'Convert Date-Time to Milliseconds - C++ - cross platform',\n",
       "  \"Given the format of your string, it is fairly easy to parse it as follows (although a regex or `get_time` might be more elegant):\\n\\n\\n\\n    tm t;\\n\\n    t.tm_year = stoi(s.substr(0, 4));\\n\\n    t.tm_mon = stoi(s.substr(4, 2));\\n\\n    t.tm_mday = stoi(s.substr(6, 2));\\n\\n    t.tm_hour = stoi(s.substr(9, 2));\\n\\n    t.tm_min = stoi(s.substr(12, 2));\\n\\n    t.tm_sec = 0;\\n\\n    double sec = stod(s.substr(15));\\n\\n\\n\\nFinding the time since the epoch can be done with [`mktime`](http://linux.die.net/man/3/mktime):\\n\\n\\n\\n    mktime(&t) + sec * 1000\\n\\n\\n\\nNote that the fractional seconds need to be handled differently - unfortunately, [`tm` has only integer seconds](http://www.cplusplus.com/reference/ctime/tm/).\\n\\n\\n\\n(See the full code [here](https://ideone.com/WFNfYK).)\\n\\n\\n\\n\\n\\n-------------------\\n\\n\\n\\n**Edit**\\n\\n\\n\\nAs Mine and Panagiotis Kanavos correctly note in the comments, Visual C++ apparently supports `get_time` for quite a while, and it's much shorter with it (note that the fractional seconds need to be handled the same way, though).\\n\\n\\n\\n\",\n",
       "  '<c++><posix>',\n",
       "  datetime.date(2016, 9, 7),\n",
       "  '2016-09-07 14:46:13',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1155.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2409',\n",
       "  '39376979',\n",
       "  'Answer',\n",
       "  'append columns to pandas dataframe with duplicate rows',\n",
       "  \"You could use [`join`][1] to join the columns of the two dataframes on a common index, `id`. Then, drop the duplicated values along with dropping off `Nans` if present as shown:\\n\\n\\n\\n    data[['a', 'b']].join(q['id'])                           \\\\\\n\\n                    .drop_duplicates()                       \\\\ \\n\\n                    .dropna()                                \\\\ \\n\\n                    .sort_values('id', ascending=False)      \\\\\\n\\n                    .reset_index(drop=True)                  \\\\\\n\\n                    .astype(int)                              \\n\\n\\n\\n       a  b   id\\n\\n    0  3  4  333\\n\\n    1  5  6  111\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.join.html\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 9, 7),\n",
       "  '2016-09-07 18:44:06',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1164.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2419',\n",
       "  '40250694',\n",
       "  'Answer',\n",
       "  'Converting text in Matplotlib when exporting .eps files',\n",
       "  'As sebacastroh points out, one can save the matplotlib figure as `svg` using `plt.savefig()` and then use Inkscape to do the conversion between `svg` and `emf`. Enhanced Meta files (emf) are easily read by any Office programm.  \\n\\nThis can be automated, like so\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    from subprocess import call\\n\\n    \\n\\n    def saveEMF(filename):\\n\\n        path_to_inkscape = \"D:\\\\Path\\\\to\\\\Inkscape\\\\inkscape.exe\"\\n\\n        call([path_to_inkscape, \"--file\", filename,  \"--export-emf\",  filename[:-4]+\".emf\" ])\\n\\n    \\n\\n    axes = plt.gca()\\n\\n    data = np.random.random((2, 100))\\n\\n    axes.plot(data[0, :], data[1, :])\\n\\n    plt.title(\"some title\")\\n\\n    plt.xlabel(u\"some x label [µm]\")\\n\\n    plt.ylabel(\"some y label\")\\n\\n    \\n\\n    fn = \"data.svg\"\\n\\n    plt.savefig(fn)\\n\\n    saveEMF(fn)\\n\\n\\n\\nIt may also make sense to save the `saveEMF()` function externally in a module to always have it at hand.',\n",
       "  '<python><matplotlib><eps>',\n",
       "  datetime.date(2016, 10, 25),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1189.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2424',\n",
       "  '39563332',\n",
       "  'Answer',\n",
       "  'calling child methods from parent pointer with different child classes',\n",
       "  '> I\\'ve a parent class with 2 or more child class deriving from it... But I\\'ll loose control over particular execution of unique methods as their number rises.\\n\\n\\n\\nAnother option, useful when the number of methods is expected to increase, and the derived classes are expected to remain relatively stable, is to use the [visitor pattern](https://en.wikipedia.org/wiki/Visitor_pattern). The following uses [`boost::variant`](http://www.boost.org/doc/libs/1_55_0/doc/html/variant/tutorial.html).\\n\\n\\n\\nSay you start with your three classes:\\n\\n\\n\\n    #include <memory>\\n\\n    #include <iostream>\\n\\n\\n\\n    using namespace std;\\n\\n    using namespace boost;\\n\\n\\n\\n    class b{};\\n\\n    class c : public b{};\\n\\n    class d : public b{};\\n\\n\\n\\nInstead of using a (smart) pointer to the base class `b`, you use a variant type:\\n\\n\\n\\n    using variant_t = variant<c, d>;\\n\\n\\n\\nand variant variables:\\n\\n\\n\\n    variant_t v{c{}};\\n\\n\\n\\nNow, if you want to handle `c` and `d` methods differently, you can use:\\n\\n\\n\\n    struct unique_visitor : public boost::static_visitor<void> {\\n\\n        void operator()(c c_) const { cout << \"c\" << endl; };\\n\\n        void operator()(d d_) const { cout << \"d\" << endl; };\\n\\n    };\\n\\n\\n\\nwhich you would call with\\n\\n\\n\\n    apply_visitor(unique_visitor{}, v);\\n\\n\\n\\nNote that you can also use the same mechanism to uniformly handle all types, by using a visitor that accepts the base class:\\n\\n\\n\\n    struct common_visitor : public boost::static_visitor<void> {\\n\\n        void operator()(b b_) const { cout << \"b\" << endl; };\\n\\n    };\\n\\n\\n\\n    apply_visitor(common_visitor{}, v);\\n\\n\\n\\nNote that if the number of classes increases faster than the number of methods, this approach will cause maintenance problems.\\n\\n\\n\\n------------------------------------------------------\\n\\n\\n\\nFull code:\\n\\n\\n\\n    #include \"boost/variant.hpp\"\\n\\n    #include <iostream>\\n\\n\\n\\n    using namespace std;\\n\\n    using namespace boost;\\n\\n\\n\\n    class b{};\\n\\n    class c : public b{};\\n\\n    class d : public b{};\\n\\n\\n\\n    using variant_t = variant<c, d>;\\n\\n\\n\\n    struct unique_visitor : public boost::static_visitor<void> {\\n\\n        void operator()(c c_) const { cout << \"c\" << endl; };\\n\\n        void operator()(d d_) const { cout << \"d\" << endl; };\\n\\n    };\\n\\n\\n\\n    struct common_visitor : public boost::static_visitor<void> {\\n\\n        void operator()(b b_) const { cout << \"b\" << endl; };\\n\\n    };\\n\\n\\n\\n    int main() {\\n\\n        variant_t v{c{}};\\n\\n        apply_visitor(unique_visitor{}, v);\\n\\n        apply_visitor(common_visitor{}, v);\\n\\n    }\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "  '<c++><class><oop><c++11><inheritance>',\n",
       "  datetime.date(2016, 9, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '11',\n",
       "  '',\n",
       "  '1884.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2428',\n",
       "  '40016938',\n",
       "  'Answer',\n",
       "  'Momentum portfolio(trend following) quant simulation on pandas',\n",
       "  \"You could simplify further by storing the values corresponding to `p` in a `DF` rather than computing for each series separately as shown:\\n\\n\\n\\n    def fractal(a, p):\\n\\n        df = pd.DataFrame()\\n\\n        for count in range(1,p+1):\\n\\n            a['direction'] = np.where(a['price'].diff(count)>0,1,0)\\n\\n            a['abs'] = a['price'].diff(count).abs()\\n\\n            a['volatility'] = a.price.diff().abs().rolling(count).sum()\\n\\n            a['fractal'] = a['abs']/a['volatility']*a['direction']\\n\\n            df = pd.concat([df, a['fractal']], axis=1)\\n\\n        return df\\n\\n\\n\\nThen, you could assign the repeating operations to a variable which reduces the re-computation time.\\n\\n    \\n\\n    def meanfractal(a, l=12):\\n\\n        a['meanfractal']= pd.DataFrame(fractal(a, l)).sum(1,skipna=False)/l\\n\\n        mean_shift = a['meanfractal'].shift(1)\\n\\n        price_shift = a['price'].shift(1)\\n\\n        factor = 1.03**(1/l)\\n\\n        a['portfolio1'] = (a['price']/price_shift*mean_shift+(1-mean_shift)*factor).cumprod()\\n\\n        a['portfolio2'] = ((a['price']/price_shift*mean_shift+factor)/(1+mean_shift)).cumprod()\\n\\n        a.dropna(inplace=True)\\n\\n        a = a.div(a.ix[0])\\n\\n        return a[['price','portfolio1','portfolio2']].plot() \\n\\n\\n\\nResulting plot obtained:\\n\\n\\n\\n    meanfractal(a)\\n\\n\\n\\n[![Image][1]][1]\\n\\n  \\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/v4tp3.png\\n\\n\\n\\nNote: If speed is not a major concern, you could perform the operations via the built-in methods present in `pandas` instead of converting them into it's corresponding `numpy` array values.\",\n",
       "  '<python><pandas><quantitative-finance><momentum>',\n",
       "  datetime.date(2016, 10, 13),\n",
       "  '2016-10-13 09:27:33',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2767.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2434',\n",
       "  '39590240',\n",
       "  'Answer',\n",
       "  'Python pandas dataframe sort_values does not work',\n",
       "  \"Presumbaly, what you're trying to do is sort by the numerical value after `sso_`. You can do this as follows:\\n\\n\\n\\n    import numpy as np\\n\\n\\n\\n    df.ix[np.argsort(df.test_type.str.split('_').str[-1].astype(int).values)\\n\\n\\n\\nThis \\n\\n\\n\\n1. splits the strings at `_`\\n\\n\\n\\n2. converts what's after this character to the numerical value\\n\\n\\n\\n3. Finds the indices sorted according to the numerical values\\n\\n\\n\\n4. Reorders the DataFrame according to these indices\\n\\n\\n\\n**Example**\\n\\n\\n\\n\\n\\n    In [15]: df = pd.DataFrame({'test_type': ['sso_1000', 'sso_500']})\\n\\n\\n\\n    In [16]: df.sort_values(by=['test_type'], ascending=True)\\n\\n    Out[16]: \\n\\n      test_type\\n\\n    0  sso_1000\\n\\n    1   sso_500\\n\\n\\n\\n    In [17]: df.ix[np.argsort(df.test_type.str.split('_').str[-1].astype(int).values)]\\n\\n    Out[17]: \\n\\n      test_type\\n\\n    1   sso_500\\n\\n    0  sso_1000\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 9, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '6863.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2435',\n",
       "  '39590660',\n",
       "  'Answer',\n",
       "  'Python pandas dataframe sort_values does not work',\n",
       "  \"Alternatively, you could also extract the numbers from `test_type` and sort them. Followed by reindexing `DF` according to those indices.\\n\\n\\n\\n    df.reindex(df['test_type'].str.extract('(\\\\d+)', expand=False)    \\\\\\n\\n                              .astype(int).sort_values().index).reset_index(drop=True)\\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/7Xu95.png\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 9, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '6863.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2437',\n",
       "  '39616671',\n",
       "  'Answer',\n",
       "  'Initialize Derived class member variable before calling Base class constructor. Is this UB?',\n",
       "  '> I would like to initialize a member variable of a Derived class, and after that pass it to the Base class constructor.\\n\\n\\n\\nIn C++, the order of construction is of the base part(s) before the derived parts. This is because it is far more common for the derived parts to (potentially) be constructed in terms of the base parts. In order to make this well-defined, the base-then-derived order is specified. Using derived in the base is undefined, therefore.\\n\\n\\n\\nIf you want your base to use derived members, there\\'s a way to ensure the order is OK. Make the \"members\" base classes too. Note that [`boost::base_from_member`](http://www.boost.org/doc/libs/1_39_0/libs/utility/base_from_member.html) is built exactly for making this more convenient.\\n\\n\\n\\nSay you have some\\n\\n\\n\\n    class member_type{};\\n\\n\\n\\nAnd you\\'d like to have `derived` have a `member_type` member, and derive from `base`. Then you could use:\\n\\n\\n\\n    class derived : \\n\\n        private boost::base_from_member<member_type>,\\n\\n        public base {\\n\\n        using my_member_type = private boost::base_from_member<member_type>;\\n\\n\\n\\n    public:\\n\\n        derived();\\n\\n    };\\n\\n\\n\\nNote that now `derived` subclasses both `my_member_type` and `base` (in that order). Hence, the latter can use the former in its construction.\\n\\n\\n\\n    derived::derived() : \\n\\n        my_member_type{3},\\n\\n        base{my_member_type::member} {\\n\\n    }',\n",
       "  '<c++><object-construction>',\n",
       "  datetime.date(2016, 9, 21),\n",
       "  '2016-09-21 12:41:00',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1308.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2441',\n",
       "  '38878035',\n",
       "  'Answer',\n",
       "  'Converting pandas dataframe into list of tuples with index',\n",
       "  \"You can iterate over the result of `to_records(index=True)`.\\n\\n\\n\\nSay you start with this:\\n\\n\\n\\n    In [6]: df = pd.DataFrame({'a': range(3, 7), 'b': range(1, 5), 'c': range(2, 6)}).set_index('a')\\n\\n\\n\\n    In [7]: df\\n\\n    Out[7]: \\n\\n       b  c\\n\\n    a      \\n\\n    3  1  2\\n\\n    4  2  3\\n\\n    5  3  4\\n\\n    6  4  5\\n\\n\\n\\nthen this works, except that it does not include the index (`a`):\\n\\n\\n\\n    In [8]: [tuple(x) for x in df.to_records(index=False)]\\n\\n    Out[8]: [(1, 2), (2, 3), (3, 4), (4, 5)]\\n\\n\\n\\nHowever, if you pass `index=True`, then it does what you want:\\n\\n\\n\\n    In [9]: [tuple(x) for x in df.to_records(index=True)]\\n\\n    Out[9]: [(3, 1, 2), (4, 2, 3), (5, 3, 4), (6, 4, 5)]\\n\\n\",\n",
       "  '<python><pandas><numpy><tuples>',\n",
       "  datetime.date(2016, 8, 10),\n",
       "  '2016-08-10 16:06:29',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '4',\n",
       "  '',\n",
       "  '4028.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2442',\n",
       "  '38884502',\n",
       "  'Answer',\n",
       "  'How to select a range of values in a pandas dataframe column?',\n",
       "  \"Use [`between`][1] with `inclusive=False` for strict inequalities:\\n\\n\\n\\n    df['two'].between(-0.5, 0.5, inclusive=False)\\n\\n\\n\\n\\t\\n\\nThe `inclusive` parameter determines if the endpoints are included or not (`True`: `<=`, `False`: `<`).  This applies to both signs. If you want mixed inequalities, you'll need to code them explicitly:\\n\\n\\n\\n    (df['two'] >= -0.5) & (df['two'] < 0.5)\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.between.html\",\n",
       "  '<python><python-3.x><pandas><dataframe><range>',\n",
       "  datetime.date(2016, 8, 10),\n",
       "  '2016-08-10 22:38:13',\n",
       "  'root (3339965)',\n",
       "  '19',\n",
       "  '',\n",
       "  '25792.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2455',\n",
       "  '39643428',\n",
       "  'Question',\n",
       "  'How can I make a python script change itself?',\n",
       "  'How can I make a python script change itself?\\n\\n\\n\\nTo boil it down, I would like to have a python script (`run.py`)like this\\n\\n\\n\\n    a = 0\\n\\n    b = 1\\n\\n    print a + b\\n\\n    # do something here such that the first line of this script reads a = 1\\n\\n\\n\\nSuch that the next time the script is run it would look like\\n\\n\\n\\n    a = 1\\n\\n    b = 1\\n\\n    print a + b\\n\\n    # do something here such that the first line of this script reads a = 2\\n\\n\\n\\nIs this in any way possible? The script might use external resources; however, everything should work by just running the one `run.py`-file.\\n\\n\\n\\nEDIT:\\n\\nIt may not have been clear enough, but the script should update itself, not any other file. Sure, once you allow for a simple configuration file next to the script, this task is trivial.\\n\\n\\n\\n**Answer:**\\n\\n\\n\\nIt is actually much easier than thought. @khelwood \\'s suggestion works just fine, opening the script and writing it\\'s own content to it is completely unproblematic. @Gerrat\\'s solution also works nicely. This is how I\\'m having it:\\n\\n\\n\\n    # -*- coding: utf-8 -*-\\n\\n    a = 0\\n\\n    b = 1\\n\\n    print a + b\\n\\n    \\n\\n    content = []\\n\\n    with open(__file__,\"r\") as f:\\n\\n        for line in f:\\n\\n            content.append(line)\\n\\n    \\n\\n    with open(__file__,\"w\") as f:\\n\\n        content[1] = \"a = {n}\\\\n\".format(n=b)\\n\\n        content[2] = \"b = {n}\\\\n\".format(n=a+b)\\n\\n        for i in range(len(content)):\\n\\n            f.write(content[i])',\n",
       "  '<python><runtime><interpreter>',\n",
       "  datetime.date(2016, 9, 22),\n",
       "  '2016-09-28 21:42:47',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '5',\n",
       "  '2.0',\n",
       "  '3046.0',\n",
       "  '3.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2462',\n",
       "  '36271553',\n",
       "  'Answer',\n",
       "  'pandas - Merge nearly duplicate rows based on column value',\n",
       "  \"You can `groupby` and `apply` the `list` function:\\n\\n\\n\\n    >>> df['Use_Case'].groupby([df.Name, df.Sid, df.Revenue]).apply(list).reset_index()\\n\\n        Name \\tSid \\tRevenue \\t0\\n\\n    0 \\tA \\txx01 \\t$10.00 \\t[Voice, SMS]\\n\\n    1 \\tB \\txx02 \\t$5.00 \\t[Voice]\\n\\n    2 \\tC \\txx03 \\t$15.00 \\t[Voice, SMS, Video]\\n\\n\\n\\n(In case you are concerned about duplicates, use `set` instead of `list`.)\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 3, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '8049.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2466',\n",
       "  '40326052',\n",
       "  'Answer',\n",
       "  'daily data, resample every 3 days, calculate over trailing 5 days efficiently',\n",
       "  'Looks like a rolling centered window where you pick up data every n days:\\n\\n\\n\\n    def rolleach(df, ndays, window):\\n\\n        return df.rolling(window, center=True).sum()[ndays-1::ndays]\\n\\n    \\n\\n    rolleach(df, 3, 5)\\n\\n    Out[95]: \\n\\n                   A\\n\\n    2013-01-02  10.0\\n\\n    2013-01-05  25.0\\n\\n    2013-01-08  40.0',\n",
       "  '<python><pandas><numpy>',\n",
       "  datetime.date(2016, 10, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1601.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2468',\n",
       "  '40333040',\n",
       "  'Answer',\n",
       "  'Difference between df.where( ) and df [ (df [ ] == ) ] in pandas , python',\n",
       "  'As per the documentation of `where`:\\n\\n\\n\\n> Return an object of same shape as self and whose corresponding entries\\n\\n> are from self where cond is True and otherwise are from other.\\n\\n\\n\\nSo the purpose of `where` is slightly different than filtering with brackets, as it will give you the result with the same shape of the dataframe you run it against.\\n\\n\\n\\nThe goal is in the notes of the documentation:\\n\\n\\n\\n> The where method is an application of the if-then idiom. For each\\n\\n> element in the calling DataFrame, if ``cond`` is ``True`` the element\\n\\n> is used; otherwise the corresponding element from the DataFrame\\n\\n> ``other`` is used',\n",
       "  '<python-3.x><pandas>',\n",
       "  datetime.date(2016, 10, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '5177.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2469',\n",
       "  '38896618',\n",
       "  'Answer',\n",
       "  'Python Pandas : How to compile all lists in a column into one unique list',\n",
       "  'You can use [`str.concat`][1] followed by some `string` manipulations to obtain the desired `list`.\\n\\n\\n\\n    In [60]: import re\\n\\n        ...: from collections import OrderedDict\\n\\n    \\n\\n    In [62]: s = df[\\'val\\'].str.cat()\\n\\n    \\n\\n    In [63]: L = re.sub(\\'[[]|[]]\\',\\' \\', s).strip().replace(\"  \",\\',\\').split(\\',\\')\\n\\n    \\n\\n    In [64]: list(OrderedDict.fromkeys(L))\\n\\n    Out[64]: [\\'val1\\', \\'val2\\', \\'val33\\', \\'val9\\', \\'val6\\', \\'val7\\']\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.cat.html',\n",
       "  '<python><list><pandas><merge><unique>',\n",
       "  datetime.date(2016, 8, 11),\n",
       "  '2016-08-11 12:54:55',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '2666.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2471',\n",
       "  '38953942',\n",
       "  'Answer',\n",
       "  'understanding how to construct a higher order markov chain',\n",
       "  'Say the set of spaces is *S*. Typically, in the *n*th order,\\n\\n\\n\\n1. The transition matrix has dimensions *|S|<sup>n</sup> X |S|*. This is because given the current *n* history of states, we need the probability of the single next state. It is true that this single next state induces another compound state of history *n*, but the transition itself is to the single next state. See [this example in Wikipedia](https://en.wikipedia.org/wiki/Markov_chain#Music), e.g..\\n\\n\\n\\n2. The initial distribution is a distribution over *|S|<sup>n</sup>* elements (your second option).',\n",
       "  '<algorithm><markov-chains>',\n",
       "  datetime.date(2016, 8, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1878.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2481',\n",
       "  '39667343',\n",
       "  'Answer',\n",
       "  'Difference between numpy.float and numpy.float64',\n",
       "  \"`np.float` is an alias for python `float` type. \\n\\n`np.float32` and `np.float64` are numpy specific 32 and 64-bit float types.\\n\\n\\n\\n    float?\\n\\n    Init signature: float(self, /, *args, **kwargs)\\n\\n    Docstring:     \\n\\n    float(x) -> floating point number\\n\\n    \\n\\n    Convert a string or number to a floating point number, if possible.\\n\\n    Type:           type\\n\\n    \\n\\n    np.float?\\n\\n    Init signature: np.float(self, /, *args, **kwargs)\\n\\n    Docstring:     \\n\\n    float(x) -> floating point number\\n\\n    \\n\\n    Convert a string or number to a floating point number, if possible.\\n\\n    Type:           type\\n\\n    \\n\\n    np.float32?\\n\\n    Init signature: np.float32(self, /, *args, **kwargs)\\n\\n    Docstring:      32-bit floating-point number. Character code 'f'. C float compatible.\\n\\n    File:           c:\\\\python\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py\\n\\n    Type:           type\\n\\n    \\n\\n    np.float64?\\n\\n    Init signature: np.float64(self, /, *args, **kwargs)\\n\\n    Docstring:      64-bit floating-point number. Character code 'd'. Python float compatible.\\n\\n    File:           c:\\\\python\\\\lib\\\\site-packages\\\\numpy\\\\__init__.py\\n\\n    Type:           type\\n\\n\\n\\nThus, when you do `isinstance(2.0, np.float)`, it is equivalent to `isinstance(2.0, float)` as 2.0 is a plain python built-in float type... and not the numpy type.\\n\\n\\n\\n`isinstance(np.float64(2.0), np.float64)` would obviously be `True`.\",\n",
       "  '<python><numpy><types>',\n",
       "  datetime.date(2016, 9, 23),\n",
       "  '2016-09-23 18:33:59',\n",
       "  'Boud (624829)',\n",
       "  '8',\n",
       "  '',\n",
       "  '2841.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2486',\n",
       "  '40335118',\n",
       "  'Answer',\n",
       "  'How to change outliers to some other colors in a scatter plot',\n",
       "  'First, you need to find a criterion for \"outliers\". Once you have that, you could mask those unwanted points in your plot.\\n\\nSelecting a subset of an array based on a condition can be easily done in numpy, e.g. if `a` is a numpy array, `a[a <= 1] ` will return the array with all values bigger than 1 \"cut out\".\\n\\n\\n\\nPlotting could then be done as follows\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    num= 1000\\n\\n    x= np.linspace(0,100, num=num)\\n\\n    y= np.random.normal(size=num)\\n\\n    \\n\\n    fig=plt.figure()\\n\\n    ax=fig.add_subplot(111)\\n\\n    # plot points inside distribution\\'s width\\n\\n    ax.scatter(x[np.abs(y)<1], y[np.abs(y)<1], marker=\"s\", color=\"#2e91be\")\\n\\n    # plot points outside distribution\\'s width\\n\\n    ax.scatter(x[np.abs(y)>=1], y[np.abs(y)>=1], marker=\"d\", color=\"#d46f9f\")\\n\\n    plt.show()\\n\\n\\n\\nproducing \\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nHere, we plot points from a normal distribution, colorizing all points outside the distribution\\'s width differently.\\n\\n  [1]: https://i.stack.imgur.com/Hb15H.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2016, 10, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1536.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2487',\n",
       "  '40335630',\n",
       "  'Answer',\n",
       "  'Plotting markers on a map using Pandas & Folium',\n",
       "  'Use apply along the column axis:\\n\\n\\n\\n    df.apply(lambda row:folium.CircleMarker(location=[row[\"LAT\"], \\n\\n                                                      row[\"LONG\"]]).add_to(map_osm),\\n\\n             axis=1)',\n",
       "  '<python><pandas><data-visualization><folium>',\n",
       "  datetime.date(2016, 10, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '2202.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2488',\n",
       "  '40341544',\n",
       "  'Answer',\n",
       "  'How to update the value of DatetimeIndex of a single row in a pandas DataFrame?',\n",
       "  \"A Fast way would be a direct lookup if you already are aware of the index to be operated upon and then you can set it's value accordingly with the help of [`Index.set_value`][1]:\\n\\n\\n\\n    df.index.set_value(df.index, df.index[0], pd.Timestamp(2011,10,1,1,0,0))\\n\\n    #                  <-index-> <-row num->  <---value to be inserted--->\\n\\n\\n\\nThis is an inplace operation, so you don't need to assign back the result to itself.\\n\\n\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.set_value.html#pandas.Index.set_value\",\n",
       "  '<python><pandas><dataframe><datetimeindex>',\n",
       "  datetime.date(2016, 10, 31),\n",
       "  '2016-10-31 11:56:51',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1065.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2491',\n",
       "  '39076655',\n",
       "  'Answer',\n",
       "  'Color time-series based on column values in pandas',\n",
       "  'One way to achieve this would be to use [`DF.replace`][2] and create a nested `dictionary` to specify the color values for the `int/float` values to be mapped against.\\n\\n\\n\\n    plt.style.use(\\'seaborn-white\\')\\n\\n    df.replace({\\'colors\\':{0:\\'red\\',1:\\'orange\\',2:\\'green\\'}}, inplace=True)\\n\\n\\n\\nYou could then perform [`DF.groupby`][3] on it to keep the colors same for each subgroup of the `groupby` object on every iteration step.\\n\\n\\n\\n    for index, group in df.groupby(\\'colors\\'):\\n\\n        group[\\'data\\'].plot(style=\".\", x_compat=True, ms=10, color=index, grid=True)\\n\\n[![Image][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/7bell.png\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.replace.html\\n\\n  [3]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html',\n",
       "  '<python><pandas><matplotlib>',\n",
       "  datetime.date(2016, 8, 22),\n",
       "  '2016-08-22 10:11:58',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1128.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2492',\n",
       "  '39078825',\n",
       "  'Answer',\n",
       "  'Converting a dataframe to dictionary with multiple values',\n",
       "  \"You can use `groupby` with orient of [`to_dict`][1] as `list` and convert the resultant series to a `dictionary`.\\n\\n\\n\\n    df.set_index('Sr.No', inplace=True)\\n\\n    df.groupby('ID').apply(lambda x: x.to_dict('list')).reset_index(drop=True).to_dict()\\n\\n\\n\\n    {0: {'C': ['Mercedes', 'Audi'], 'ID': ['John', 'John'], 'A': ['Venus', nan],  \\n\\n         'B': ['Portugese', 'German'], 'D': ['Blue', 'Red']}, \\n\\n     1: {'C': ['Audi'], 'ID': ['Michael'], 'A': ['Mercury'], 'B': ['Hindi'], 'D': ['Yellow']}, \\n\\n     2: {'C': ['BMW', 'BMW'], 'ID': ['Tom', 'Tom'], 'A': ['Earth', 'Mars'], \\n\\n         'B': ['English', 'Spanish'], 'D': [nan, 'Green']}}\\n\\n\\n\\n\\n\\n----------\\n\\nInorder to remove `ID`, you can also do:\\n\\n\\n\\n    df.groupby('ID')['A','B','C','D'].apply(lambda x: x.to_dict('list'))  \\\\\\n\\n                                     .reset_index(drop=True).to_dict()\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.DataFrame.to_dict.html\",\n",
       "  '<python><pandas><dictionary><dataframe>',\n",
       "  datetime.date(2016, 8, 22),\n",
       "  '2016-08-22 12:34:36',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1367.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2494',\n",
       "  '36377091',\n",
       "  'Answer',\n",
       "  \"Swift 2.0 Format 1000's into a friendly K's\",\n",
       "  '    func formatPoints(num: Double) ->String{\\n\\n        let thousandNum = num/1000\\n\\n        let millionNum = num/1000000\\n\\n        if num >= 1000 && num < 1000000{\\n\\n            if(floor(thousandNum) == thousandNum){\\n\\n                return(\"\\\\(Int(thousandNum))k\")\\n\\n            }\\n\\n            return(\"\\\\(thousandNum.roundToPlaces(1))k\")\\n\\n        }\\n\\n        if num > 1000000{\\n\\n            if(floor(millionNum) == millionNum){\\n\\n                return(\"\\\\(Int(thousandNum))k\")\\n\\n            }\\n\\n            return (\"\\\\(millionNum.roundToPlaces(1))M\")\\n\\n        }\\n\\n        else{\\n\\n            if(floor(num) == num){\\n\\n                return (\"\\\\(Int(num))\")\\n\\n            }\\n\\n            return (\"\\\\(num)\")\\n\\n        }\\n\\n    \\n\\n    }\\n\\n\\n\\n    extension Double {\\n\\n        /// Rounds the double to decimal places value\\n\\n        func roundToPlaces(places:Int) -> Double {\\n\\n            let divisor = pow(10.0, Double(places))\\n\\n            return round(self * divisor) / divisor\\n\\n        }\\n\\n    }\\n\\nThe updated code should now not return a .0 if the number is whole.  Should now output 1k instead of 1.0k for example.  I just checked essentially if double and its floor were the same.\\n\\n\\n\\nI found the double extension in this question:\\n\\nhttps://stackoverflow.com/questions/27338573/rounding-a-double-value-to-x-number-of-decimal-places-in-swift',\n",
       "  '<ios><swift><numbers><swift2><format>',\n",
       "  datetime.date(2016, 4, 2),\n",
       "  '2017-05-23 11:46:30',\n",
       "  'user3483203 (3483203), URL Rewriter Bot (n/a)',\n",
       "  '17',\n",
       "  '',\n",
       "  '3210.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2502',\n",
       "  '39509360',\n",
       "  'Answer',\n",
       "  'pandas: rename axis in df',\n",
       "  \"This is kind of a hack to get you what you eventually wanted it to look like.\\n\\n\\n\\n    df = pd.read_csv(data, sep='\\\\s{2,}', index_col='Channel', engine='python')\\n\\n    df\\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n    df_excel_format = df.rename_axis('Channel', axis=1)\\n\\n    del(df_excel_format.index.name)\\n\\n    df_excel_format\\n\\n\\n\\n[![Image][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/CLcCU.png\\n\\n  [2]: http://i.stack.imgur.com/Fpif7.png\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 9, 15),\n",
       "  '2016-09-15 15:00:53',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1062.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2509',\n",
       "  '39526088',\n",
       "  'Answer',\n",
       "  'pandas sort lambda function',\n",
       "  \"There are at least two ways:\\n\\n\\n\\n**Method 1**\\n\\n\\n\\nSay you start with\\n\\n\\n\\n    In [175]: df = pd.DataFrame({'A': [1, 2], 'B': [1, -1], 'C': [1, 1]})\\n\\n\\n\\nYou can add a column which is your sort key\\n\\n\\n\\n    In [176]: df['sort_val'] = df.A * df.B\\n\\n\\n\\nFinally sort by it and drop it\\n\\n\\n\\n    In [190]: df.sort_values('sort_val').drop('sort_val', 1)\\n\\n    Out[190]: \\n\\n       A  B  C\\n\\n    1  2 -1  1\\n\\n    0  1  1  1\\n\\n\\n\\n**Method 2**\\n\\n\\n\\nUse [`numpy.argsort`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.argsort.html) and then use `.ix` on the resulting indices:\\n\\n\\n\\n    In [197]: import numpy as np\\n\\n\\n\\n    In [198]: df.ix[np.argsort(df.A * df.B).values]\\n\\n    Out[198]: \\n\\n       A  B  C\\n\\n    0  1  1  1\\n\\n    1  2 -1  1\\n\\n\",\n",
       "  '<sorting><pandas>',\n",
       "  datetime.date(2016, 9, 16),\n",
       "  '2016-09-16 07:38:45',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '15',\n",
       "  '',\n",
       "  '4087.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2514',\n",
       "  '36412132',\n",
       "  'Answer',\n",
       "  'How to create a random array in a certain range',\n",
       "  'You can just do (thanks user2357112!)\\n\\n\\n\\n    [np.random.uniform(1.5, 12.4), np.random.uniform(0, 5), ...]\\n\\n\\n\\nusing [`numpy.random.uniform`](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.random.uniform.html).',\n",
       "  '<python><arrays><numpy><random><numpy-random>',\n",
       "  datetime.date(2016, 4, 4),\n",
       "  '2016-04-04 20:20:51',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '10',\n",
       "  '',\n",
       "  '4895.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2516',\n",
       "  '40139416',\n",
       "  'Answer',\n",
       "  'Fast Live Plotting in Matplotlib / PyPlot',\n",
       "  'First of all, the code that is posted in the question runs with 7 fps on my machine, with QT4Agg as backend.\\n\\n\\n\\nNow, as has been suggested in many posts, like [here][1] or [here][2], using `blit` might be an option. Although [this article][3] mentionnes that blit causes strong memory leakage, I could not observe that.\\n\\n\\n\\nI have modified your code a bit and compared the frame rate with and without the use of blit. The code below gives \\n\\n\\n\\n - 18 fps when run without blit\\n\\n - 28 fps with blit\\n\\n\\n\\nCode:\\n\\n\\n\\n    import time\\n\\n    from matplotlib import pyplot as plt\\n\\n    import numpy as np\\n\\n    \\n\\n    def live_update_demo(blit = False):\\n\\n        x = np.linspace(0,50., num=100)\\n\\n        X,Y = np.meshgrid(x,x)\\n\\n        fig = plt.figure()\\n\\n        ax1 = fig.add_subplot(2, 1, 1)\\n\\n        ax2 = fig.add_subplot(2, 1, 2)\\n\\n        \\n\\n        fig.canvas.draw()   # note that the first draw comes before setting data \\n\\n        \\n\\n        h1 = ax1.imshow(X, vmin=-1, vmax=1, interpolation=\"None\", cmap=\"RdBu\")\\n\\n        \\n\\n        h2, = ax2.plot(x, lw=3)\\n\\n        text = ax2.text(0.8,1.5, \"\")\\n\\n        ax2.set_ylim([-1,1])\\n\\n        \\n\\n        \\n\\n        if blit:\\n\\n            # cache the background\\n\\n            axbackground = fig.canvas.copy_from_bbox(ax1.bbox)\\n\\n            ax2background = fig.canvas.copy_from_bbox(ax2.bbox)\\n\\n    \\n\\n        t_start = time.time()\\n\\n        k=0.\\n\\n        for i in np.arange(1000):\\n\\n            h1.set_data(np.sin(X/3.+k)*np.cos(Y/3.+k))\\n\\n            h2.set_ydata(np.sin(x/3.+k))\\n\\n            tx = \\'Mean Frame Rate:\\\\n {fps:.3f}FPS\\'.format(fps= ((i+1) / (time.time() - t_start)) ) \\n\\n            text.set_text(tx)\\n\\n            #print tx\\n\\n            k+=0.11\\n\\n            if blit:\\n\\n                # restore background\\n\\n                fig.canvas.restore_region(axbackground)\\n\\n                fig.canvas.restore_region(ax2background)\\n\\n    \\n\\n                # redraw just the points\\n\\n                ax1.draw_artist(h1)\\n\\n                ax2.draw_artist(h2)\\n\\n    \\n\\n                # fill in the axes rectangle\\n\\n                fig.canvas.blit(ax1.bbox)\\n\\n                fig.canvas.blit(ax2.bbox)\\n\\n                # in this post http://bastibe.de/2013-05-30-speeding-up-matplotlib.html\\n\\n                # it is mentionned that blit causes strong memory leakage. \\n\\n                # however, I did not observe that.\\n\\n    \\n\\n            else:\\n\\n                # redraw everything\\n\\n                fig.canvas.draw()\\n\\n                fig.canvas.flush_events()\\n\\n                \\n\\n            \\n\\n            plt.pause(0.000000000001) \\n\\n            #plt.pause calls canvas.draw(), as can be read here:\\n\\n            #http://bastibe.de/2013-05-30-speeding-up-matplotlib.html\\n\\n            #however with Qt4 (and TkAgg??) this is needed. It seems,using a different backend, \\n\\n            #one can avoid plt.pause() and gain even more speed.\\n\\n      \\n\\n    \\n\\n    live_update_demo(True) # 28 fps\\n\\n    #live_update_demo(False) # 18 fps\\n\\n\\n\\n\\n\\n----------\\n\\n**Update:**  \\n\\nFor faster plotting, one may consider using [pyqtgraph][4].  \\n\\nAs the [pyqtgraph documentation][5] puts it: *\"For plotting, pyqtgraph is not nearly as complete/mature as matplotlib, but runs much faster.\"*\\n\\n\\n\\n\\n\\nI ported the above example to pyqtgraph. And although it looks kind of ugly, it runs with 250 fps on my maschine.\\n\\n\\n\\nSumming that up, \\n\\n\\n\\n - matplotlib (without blitting): 18 fps\\n\\n - matplotlib (with blitting): 28 fps\\n\\n - pyqtgraph : **250 fps**\\n\\n\\n\\n   \\n\\npyqtgraph code:\\n\\n\\n\\n    import sys\\n\\n    import time\\n\\n    from pyqtgraph.Qt import QtCore, QtGui\\n\\n    import numpy as np\\n\\n    import pyqtgraph as pg\\n\\n    \\n\\n    \\n\\n    class App(QtGui.QMainWindow):\\n\\n        def __init__(self, parent=None):\\n\\n            super(App, self).__init__(parent)\\n\\n            \\n\\n            #### Create Gui Elements ###########\\n\\n            self.mainbox = QtGui.QWidget()\\n\\n            self.setCentralWidget(self.mainbox)\\n\\n            self.mainbox.setLayout(QtGui.QVBoxLayout())\\n\\n            \\n\\n            self.canvas = pg.GraphicsLayoutWidget()\\n\\n            self.mainbox.layout().addWidget(self.canvas)\\n\\n            \\n\\n            self.label = QtGui.QLabel()\\n\\n            self.mainbox.layout().addWidget(self.label)\\n\\n            \\n\\n            self.view = self.canvas.addViewBox()\\n\\n            self.view.setAspectLocked(True)\\n\\n            self.view.setRange(QtCore.QRectF(0,0, 100, 100))\\n\\n            \\n\\n            #  image plot\\n\\n            self.img = pg.ImageItem(border=\\'w\\')\\n\\n            self.view.addItem(self.img)\\n\\n            \\n\\n            self.canvas.nextRow()\\n\\n            #  line plot\\n\\n            self.otherplot = self.canvas.addPlot()\\n\\n            self.h2 = self.otherplot.plot(pen=\\'y\\')\\n\\n            \\n\\n            \\n\\n            #### Set Data  #####################\\n\\n            \\n\\n            self.x = np.linspace(0,50., num=100)\\n\\n            self.X,self.Y = np.meshgrid(self.x,self.x)\\n\\n            \\n\\n            self.counter = 0\\n\\n            self.fps = 0.\\n\\n            self.lastupdate = time.time()\\n\\n            \\n\\n            #### Start  #####################\\n\\n            self._update()\\n\\n            \\n\\n        def _update(self):\\n\\n            \\n\\n            self.data = np.sin(self.X/3.+self.counter/9.)*np.cos(self.Y/3.+self.counter/9.)\\n\\n            self.ydata = np.sin(self.x/3.+ self.counter/9.)\\n\\n            \\n\\n            self.img.setImage(self.data)\\n\\n            self.h2.setData(self.ydata)\\n\\n             \\n\\n            now = time.time()\\n\\n            dt = (now-self.lastupdate)\\n\\n            if dt <= 0:\\n\\n                dt = 0.000000000001\\n\\n            fps2 = 1.0 / dt\\n\\n            self.lastupdate = now\\n\\n            self.fps = self.fps * 0.9 + fps2 * 0.1\\n\\n            tx = \\'Mean Frame Rate:  {fps:.3f} FPS\\'.format(fps=self.fps )\\n\\n            self.label.setText(tx)\\n\\n            QtCore.QTimer.singleShot(1, self._update)\\n\\n            self.counter += 1\\n\\n            \\n\\n    \\n\\n    if __name__ == \\'__main__\\':\\n\\n    \\n\\n        app = QtGui.QApplication(sys.argv)\\n\\n        thisapp = App()\\n\\n        thisapp.show()\\n\\n        sys.exit(app.exec_())\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/a/15724978/4124317\\n\\n  [2]: https://stackoverflow.com/questions/8955869/why-is-plotting-with-matplotlib-so-slow\\n\\n  [3]: http://bastibe.de/2013-05-30-speeding-up-matplotlib.html\\n\\n  [4]: http://www.pyqtgraph.org\\n\\n  [5]: http://www.pyqtgraph.org/documentation/introduction.html',\n",
       "  '<matplotlib>',\n",
       "  datetime.date(2016, 10, 19),\n",
       "  '2017-05-23 11:47:21',\n",
       "  'ImportanceOfBeingErnest (4124317), URL Rewriter Bot (n/a)',\n",
       "  '29',\n",
       "  '',\n",
       "  '19766.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2520',\n",
       "  '40321723',\n",
       "  'Answer',\n",
       "  'daily data, resample every 3 days, calculate over trailing 5 days efficiently',\n",
       "  \"##For regularly-spaced dates only\\n\\n\\n\\nHere are two methods, first a pandas way and second a numpy function.\\n\\n\\n\\n    >>> n=5   # trailing periods for rolling sum\\n\\n    >>> k=3   # frequency of rolling sum calc\\n\\n\\n\\n    >>> df.rolling(n).sum()[-1::-k][::-1]\\n\\n    \\n\\n                   A\\n\\n    2013-01-01   NaN\\n\\n    2013-01-04  10.0\\n\\n    2013-01-07  25.0\\n\\n    2013-01-10  40.0\\n\\n\\n\\nAnd here's a numpy function (adapted from [Jaime's numpy moving_average][1]):\\n\\n\\n\\n    def rolling_sum(a, n=5, k=3):\\n\\n        ret = np.cumsum(a.values)\\n\\n        ret[n:] = ret[n:] - ret[:-n]\\n\\n        return pd.DataFrame( ret[n-1:][-1::-k][::-1], \\n\\n                             index=a[n-1:][-1::-k][::-1].index )\\n\\n             \\n\\n    rolling_sum(df,n=6,k=4)   # default n=5, k=3\\n\\n\\n\\n##For irregularly-spaced dates (or regularly-spaced)\\n\\n\\n\\nSimply precede with:\\n\\n\\n\\n    df.resample('D').sum().fillna(0)\\n\\n\\n\\nFor example, the above methods become:\\n\\n\\n\\n    df.resample('D').sum().fillna(0).rolling(n).sum()[-1::-k][::-1]\\n\\n\\n\\nand\\n\\n\\n\\n    rolling_sum( df.resample('D').sum().fillna(0) )\\n\\n\\n\\nNote that dealing with irregularly-spaced dates can be done simply and elegantly in pandas as this is a strength of pandas over almost anything else out there.  But you can likely find a numpy (or numba or cython) approach that will trade off some simplicity for an increase in speed.  Whether this is a good tradeoff will depend on your data size and performance requirements, of course.\\n\\n\\n\\nFor the irregularly spaced dates, I tested on the following example data and it seemed to work correctly.  This will produce a mix of missing, single, and multiple entries per date:\\n\\n\\n\\n    np.random.seed(12345)\\n\\n    per = 11\\n\\n    tidx = np.random.choice( pd.date_range('2012-12-31', periods=per, freq='D'), per )\\n\\n    df = pd.DataFrame(dict(A=np.arange(len(tidx))), tidx).sort_index()\\n\\n\\n\\n  [1]: https://stackoverflow.com/questions/14313510/how-to-calculate-moving-average-using-numpy/14314054#14314054\\n\\n\",\n",
       "  '<python><pandas><numpy>',\n",
       "  datetime.date(2016, 10, 29),\n",
       "  '2017-05-23 12:02:18',\n",
       "  'JohnE (3877338), URL Rewriter Bot (n/a)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1601.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2529',\n",
       "  '36461799',\n",
       "  'Answer',\n",
       "  'Reading in header information from csv file using Pandas',\n",
       "  'You have to parse your metadata header by yourself, yet you can do it in an elegant manner in one pass and even by using it on the fly so that you can extract data out it / control the correctness of the file etc.\\n\\n\\n\\nFirst, open the file yourself:\\n\\n\\n\\n    f = open(filename)\\n\\n\\n\\nThen, do the work to parse each metadata line to extract data out it. For the sake of the explanation, I\\'m just skipping these rows:\\n\\n\\n\\n    for i in range(13):  # skip the first 13 lines that are useless for the columns definition\\n\\n        f.readline()  # use the resulting string for metadata extraction\\n\\n    \\n\\nNow you have the file pointer ready on the unique header line you want to use to load the DataFrame. The cool thing is that `read_csv` accepts file objects! Thus you start loading your DataFrame right away now:\\n\\n\\n\\n    pandas.read_csv(f, sep=\",\") \\n\\n\\n\\nNote that I don\\'t use the header argument as I consider by your description you have only that one last line of header that is useful for your dataframe. You can build and adjust hearder parsing values / rows to skip from that example.',\n",
       "  '<python><csv><pandas>',\n",
       "  datetime.date(2016, 4, 6),\n",
       "  '2016-12-21 16:47:51',\n",
       "  'Community (-1)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1871.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2531',\n",
       "  '39755183',\n",
       "  'Answer',\n",
       "  'How to divide the sum with the size in a pandas groupby',\n",
       "  \"Use `groupby.apply` instead:\\n\\n\\n\\n    df.groupby(['ID_0', 'ID_1']).apply(lambda x: x['ID_2'].sum()/len(x))\\n\\n\\n\\n    ID_0  ID_1\\n\\n    a     b       0.500000\\n\\n          c       0.666667\\n\\n    d     c       0.000000\\n\\n    dtype: float64\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 9, 28),\n",
       "  '2016-09-28 18:35:27',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1257.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2533',\n",
       "  '39762771',\n",
       "  'Answer',\n",
       "  'Choose a random number from a list of integer',\n",
       "  'Since, as a substep, you need to generate random numbers, you might as well do it the C++11 way (instead of using modulo, which incidentally, is [known to have a slight bias toward low numbers](https://stackoverflow.com/questions/5008804/generating-random-integer-from-a-range)):\\n\\n\\n\\nSay you start with\\n\\n\\n\\n    #include <iostream>\\n\\n    #include <random>\\n\\n    #include <vector>\\n\\n\\n\\n    int main()\\n\\n    {\\n\\n        const std::vector<int> intList{1, 3, 5, 2};\\n\\n    \\n\\nNow you define the random generators:\\n\\n\\n\\n        std::random_device rd; \\n\\n        std::mt19937 eng(rd());\\n\\n        std::uniform_int_distribution<> distr(0, intList.size() - 1);\\n\\n\\n\\nWhen you need to generate a random element, you can do this:\\n\\n\\n\\n        intList[distr(eng)];\\n\\n    }\\n\\n',\n",
       "  '<c++>',\n",
       "  datetime.date(2016, 9, 29),\n",
       "  '2017-05-23 12:33:21',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1354.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2535',\n",
       "  '39776042',\n",
       "  'Answer',\n",
       "  'Pandas: how to draw a bar plot with two categories and four series each?',\n",
       "  \"You could simply perform `unstack` after calculating the `mean` of the `DF` to render the bar plot.\\n\\n\\n\\n    import seaborn as sns\\n\\n    sns.set_style('white')\\n\\n    \\n\\n    #color=0.75(grey)\\n\\n    df.mean().unstack().plot.bar(color=list('rbg')+['0.75'], rot=0, figsize=(8,8)) \\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n\\n\\n----------\\n\\n**Data:** (As per the edited post)\\n\\n\\n\\n    df\\n\\n\\n\\n[![Image][2]][2]\\n\\n\\n\\nPrepare the multiindex `DF` by creating an extra column by repeating the labels according to the selections of columns(Here, 4).\\n\\n\\n\\n    df_multi_col = df.T.reset_index()\\n\\n    df_multi_col['labels'] = np.concatenate((np.repeat('A', 4), np.repeat('B', 4)))\\n\\n    df_multi_col.set_index(['labels', 'index'], inplace=True)\\n\\n    df_multi_col\\n\\n\\n\\n[![Image][3]][3]\\n\\n\\n\\n    df_multi_col.mean(1).unstack().plot.bar(color=list('rbg')+['0.75'], rot=0, figsize=(6,6), width=2)\\n\\n\\n\\n [![Image][4]][4]\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/crBu3.png\\n\\n  [2]: http://i.stack.imgur.com/DTy03.png\\n\\n  [3]: http://i.stack.imgur.com/fMbiK.png\\n\\n  [4]: http://i.stack.imgur.com/uGYVO.png\",\n",
       "  '<python><pandas><matplotlib><bar-chart><categories>',\n",
       "  datetime.date(2016, 9, 29),\n",
       "  '2016-09-29 18:54:56',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '3',\n",
       "  '',\n",
       "  '3243.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2536',\n",
       "  '39089671',\n",
       "  'Answer',\n",
       "  'Why is numpy.prod() incorrectly returning negative results, or 0, for my long lists of natural numbers?',\n",
       "  'Note that [Python uses \"unlimited\" integers](http://www.gossamer-threads.com/lists/python/python/105568), but in numpy everything is typed, and so it is a \"C\"-style (probably 64-bit) integer here. You\\'re probably experiencing an overflow.\\n\\n\\n\\nIf you look at the documentation for [`numpy.prod`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.prod.html), you can see the `dtype` parameter:\\n\\n\\n\\n> The type of the returned array, as well as of the accumulator in which the elements are multiplied.\\n\\n\\n\\nThere are a few things you can do:\\n\\n\\n\\n1. Drop back to Python, and multiply using its \"unlimited integers\" (see [this question](https://stackoverflow.com/questions/2104782/returning-the-product-of-a-list) for how to do so).\\n\\n\\n\\n2. Consider whether you actually need to find the product of such huge numbers. Often, when you\\'re working with the product of very small or very large numbers, you switch to sums of logarithms. As @WarrenWeckesser notes, this is obviously imprecise (it\\'s not like taking the exponent at the end will give you the exact solution) - rather, it\\'s used to gauge whether one product is growing faster than another.',\n",
       "  '<python><list><numpy><math>',\n",
       "  datetime.date(2016, 8, 22),\n",
       "  '2017-05-23 10:28:47',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1447.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2539',\n",
       "  '39555211',\n",
       "  'Answer',\n",
       "  'AttributeError: instance has no attribute',\n",
       "  'Your init function has three underscores:\\n\\n\\n\\n    def ___init___(self):\\n\\n\\n\\nIt [should have only two](https://docs.python.org/2/reference/datamodel.html#basic-customization):\\n\\n\\n\\n    def __init__(self):\\n\\n\\n\\nAs it is written now, it is not being called when you create a new object.\\n\\n',\n",
       "  '<python><python-2.7><oop><attributeerror>',\n",
       "  datetime.date(2016, 9, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1301.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2546',\n",
       "  '39637027',\n",
       "  'Answer',\n",
       "  'Removing non alphabetic characters in a char array',\n",
       "  'You might want to consider a more C++ way to do things:\\n\\n\\n\\n    #include <iostream>\\n\\n    #include <string>\\n\\n    #include <algorithm>\\n\\n    #include <cctype>\\n\\n\\n\\n    using namespace std;\\n\\n\\n\\n    int main() {\\n\\n        string s;\\n\\n        getline(cin, s);\\n\\n        s.erase(remove_if(begin(s), end(s), [](char c){ return !isalpha(c); }));\\n\\n        cout << s << endl;\\n\\n    }\\n\\n\\n\\nNote the following:\\n\\n\\n\\n1. [`string` + `getline`](http://www.cplusplus.com/reference/string/string/getline/) remove the problems of length overruns on inputs.\\n\\n2. [`isalpha`](http://www.cplusplus.com/reference/cctype/isalpha/) checks whether a character is alphabetical.\\n\\n3. The [erase-remove](https://en.wikipedia.org/wiki/Erase%E2%80%93remove_idiom) idiom handles the tricky left-shifts for you.\\n\\n',\n",
       "  '<c><arrays>',\n",
       "  datetime.date(2016, 9, 22),\n",
       "  '2016-09-22 11:07:20',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1262.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2547',\n",
       "  '39653412',\n",
       "  'Answer',\n",
       "  'C++ reading a String character by character',\n",
       "  '>  I have a constraint to read the input strings character by character \\n\\n\\n\\nOne way of reading character by character, is via [`std::basic_istream::get`](http://en.cppreference.com/w/cpp/io/basic_istream/get).\\n\\n\\n\\nIf you define\\n\\n\\n\\n    char c;\\n\\n\\n\\nthen\\n\\n\\n\\n    std::cin.get(c);\\n\\n\\n\\nwill read the next character into `c`.\\n\\n\\n\\nIn a loop, you could use it as\\n\\n\\n\\n    while(std::cin.get(c))\\n\\n        <body>',\n",
       "  '<c++><iostream>',\n",
       "  datetime.date(2016, 9, 23),\n",
       "  '2016-09-23 06:00:43',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '4900.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2551',\n",
       "  '36539295',\n",
       "  'Answer',\n",
       "  'Merging dataframes on index with pandas',\n",
       "  \"You can do this with [`merge`][1]:\\n\\n\\n\\n    df_merged = df1.merge(df2, how='outer', left_index=True, right_index=True)\\n\\n\\n\\n\\n\\nThe keyword argument `how='outer'` keeps all indices from both frames, filling in missing indices with `NaN`.  The `left_index` and `right_index` keyword arguments have the merge be done on the indices. If you get all `NaN` in a column after doing a merge, another troubleshooting step is to verify that your indices have the same `dtypes`.\\n\\n\\n\\nThe `merge` code above produces the following output for me:\\n\\n\\n\\n                    V1    V2\\n\\n    A 2012-01-01  12.0  15.0\\n\\n      2012-02-01  14.0   NaN\\n\\n      2012-03-01   NaN  21.0\\n\\n    B 2012-01-01  15.0  24.0\\n\\n      2012-02-01   8.0   9.0\\n\\n    C 2012-01-01  17.0   NaN\\n\\n      2012-02-01   9.0   NaN\\n\\n    D 2012-01-01   NaN   7.0\\n\\n      2012-02-01   NaN  16.0\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.merge.html\",\n",
       "  '<python><pandas><merge><dataframe>',\n",
       "  datetime.date(2016, 4, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '19',\n",
       "  '',\n",
       "  '34387.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2555',\n",
       "  '40376667',\n",
       "  'Answer',\n",
       "  'Insert result of sklearn CountVectorizer in a pandas dataframe',\n",
       "  'Return term-document matrix after learning the vocab dictionary from the raw documents.\\n\\n\\n\\n    X = vect.fit_transform(docs) \\n\\n\\n\\nConvert sparse csr matrix to dense format and allow columns to contain the array mapping from feature integer indices to feature names.\\n\\n    \\n\\n    count_vect_df = pd.DataFrame(X.todense(), columns=vect.get_feature_names())\\n\\n\\n\\nConcatenate the original `df` and the `count_vect_df` columnwise.\\n\\n    \\n\\n    pd.concat([df, count_vect_df], axis=1)',\n",
       "  '<python><pandas><machine-learning><scikit-learn>',\n",
       "  datetime.date(2016, 11, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '13',\n",
       "  '',\n",
       "  '1976.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2556',\n",
       "  '40386509',\n",
       "  'Answer',\n",
       "  'Python: convert string array to int array in dataframe',\n",
       "  \"Use [`astype`][1]:\\n\\n\\n\\n    df['duration'] = df['duration'].astype(int)\\n\\n\\n\\n**Timings**\\n\\n\\n\\nUsing the following setup to produce a large sample dataset:\\n\\n\\n\\n    n = 10**5\\n\\n    data = list(map(str, np.random.randint(10**4, size=n)))\\n\\n    df = pd.DataFrame({'duration': data})\\n\\n\\n\\nI get the following timings:\\n\\n\\n\\n    %timeit -n 100 df['duration'].astype(int)\\n\\n    100 loops, best of 3: 10.9 ms per loop\\n\\n    \\n\\n    %timeit -n 100 df['duration'].apply(int)\\n\\n    100 loops, best of 3: 44.3 ms per loop\\n\\n    \\n\\n    %timeit -n 100 df['duration'].apply(lambda x: int(x))\\n\\n    100 loops, best of 3: 60.1 ms per loop\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.astype.html\",\n",
       "  '<python><pandas><numpy>',\n",
       "  datetime.date(2016, 11, 2),\n",
       "  '2016-11-02 17:54:25',\n",
       "  'root (3339965)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2003.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2557',\n",
       "  '40390704',\n",
       "  'Answer',\n",
       "  'Pandas groupby nlargest sum',\n",
       "  \"You can use `apply` after performing the `groupby`:\\n\\n\\n\\n    df.groupby('State')['Population'].apply(lambda grp: grp.nlargest(2).sum())\\n\\n\\n\\nI think this issue you're having is that `df.groupby('State')['Population'].nlargest(2)` will return a DataFrame, so you can no longer do group level operations.  In general, if you want to perform multiple operations in a group, you'll need to use `apply`/`agg`.\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n    State\\n\\n    Alabama    150\\n\\n    Wyoming    330\\n\\n\\n\\n\\n\\n**EDIT**\\n\\n\\n\\nA slightly cleaner approach, as suggested by @cᴏʟᴅsᴘᴇᴇᴅ:\\n\\n\\n\\n    df.groupby('State')['Population'].nlargest(2).sum(level=0)\\n\\n\\n\\nThis is slightly slower than using `apply` on larger DataFrames though.\\n\\n\\n\\nUsing the following setup:\\n\\n\\n\\n    import numpy as np\\n\\n    import pandas as pd\\n\\n    from string import ascii_letters\\n\\n    \\n\\n    n = 10**6\\n\\n    df = pd.DataFrame({'A': np.random.choice(list(ascii_letters), size=n),\\n\\n                       'B': np.random.randint(10**7, size=n)})\\n\\n\\n\\nI get the following timings:\\n\\n\\n\\n    In [3]: %timeit df.groupby('A')['B'].apply(lambda grp: grp.nlargest(2).sum())\\n\\n    103 ms ± 1.08 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\\n\\n    \\n\\n    In [4]: %timeit df.groupby('A')['B'].nlargest(2).sum(level=0)\\n\\n    147 ms ± 3.38 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\\n\\n\\n\\nThe slower performance is potentially caused by the `level` kwarg in `sum` performing a second `groupby` under the hood.\",\n",
       "  '<python><pandas><group-by><sum>',\n",
       "  datetime.date(2016, 11, 2),\n",
       "  '2017-12-28 19:14:22',\n",
       "  'root (3339965)',\n",
       "  '17',\n",
       "  '',\n",
       "  '11510.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2559',\n",
       "  '39257298',\n",
       "  'Answer',\n",
       "  'Convert Pandas DataFrame to JSON format',\n",
       "  \"The output that you get after [`DF.to_json`][1] is a `string`. So, you can simply slice it according to your requirement and remove the commas from it too.\\n\\n\\n\\n    out = df.to_json(orient='records')[1:-1].replace('},{', '} {')\\n\\n\\n\\nTo write the output to a text file, you could do:\\n\\n\\n\\n    with open('file_name.txt', 'w') as f:\\n\\n        f.write(out)\\n\\n\\n\\n\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_json.html\",\n",
       "  '<json><pandas><dataframe>',\n",
       "  datetime.date(2016, 8, 31),\n",
       "  '2016-08-31 19:48:23',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '21',\n",
       "  '',\n",
       "  '35974.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2561',\n",
       "  '39783805',\n",
       "  'Answer',\n",
       "  'Passing pandas DataFrame by reference',\n",
       "  \"Python doesn't have pass by value vs. pass by reference - there are just [bindings from names to objects](https://docs.python.org/2/reference/executionmodel.html#naming-and-binding). \\n\\n\\n\\nIf you change your function to\\n\\n\\n\\n    def foo(df1, df2):\\n\\n\\n\\n        res = df1.join(df2['C'], how='inner')\\n\\n        res['B'] = 1\\n\\n\\n\\n        return res\\n\\n\\n\\nThen `df1`, `df2`, in the function, are bound to the objects you sent. The result of the `join`, which is a new object in this case, is bound to the name `res`. You can manipulate it, and return it, without affecting any of the other objects or bindings. \\n\\n\\n\\nIn your calling code, you could just write\\n\\n\\n\\n    print foo(df1, df2)\\n\\n\\n\\n\",\n",
       "  '<python><pandas><dataframe><pass-by-reference><immutability>',\n",
       "  datetime.date(2016, 9, 30),\n",
       "  '2016-09-30 04:49:49',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2462.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2562',\n",
       "  '39793589',\n",
       "  'Answer',\n",
       "  'Python -Two figures in one plot',\n",
       "  'It depends what you want exactly:\\n\\n\\n\\n1. If you want the two figures overlayed, then you can call [`hold`](https://stackoverflow.com/questions/21465988/python-equivalent-to-hold-on-in-matlab)`(True)` after the first, then plot the second, then call `hold(False)`.\\n\\n\\n\\n2. If you want the two figures in a single figure, but side by side (or one over the other), then you can use [`subplot`](http://matplotlib.org/examples/pylab_examples/subplot_demo.html). E.g., call `subplot(2, 1, 1)` before plotting the first, then `subplot(2, 1, 2)` before the second.',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2016, 9, 30),\n",
       "  '2017-05-23 11:48:19',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1424.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2570',\n",
       "  '40442506',\n",
       "  'Answer',\n",
       "  'Unknown label type sklearn',\n",
       "  \"You are currently providing a dataframe and not it's numpy array representation as the training input to the `fit` method. Do this instead:\\n\\n\\n\\n    clf.fit(X=test.values, y=target.values)   \\n\\n    # Even .asmatrix() works but is not generally recommended\\n\\n\\n\\n\",\n",
       "  '<python><pandas><scikit-learn>',\n",
       "  datetime.date(2016, 11, 5),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '9733.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2583',\n",
       "  '40555808',\n",
       "  'Answer',\n",
       "  'How to keep column names when converting from pandas to numpy',\n",
       "  \"Consider a `DF` as shown below:\\n\\n\\n\\n    X = pd.DataFrame(dict(one=['Strawberry', 'Fields', 'Forever'], two=[1,2,3]))\\n\\n    X\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nProvide a *list of tuples* as data input to the structured array:\\n\\n\\n\\n    arr_ip = [tuple(i) for i in X.as_matrix()]\\n\\n\\n\\nOrdered list of field names:\\n\\n\\n\\n    dtyp = np.dtype(list(zip(X.dtypes.index, X.dtypes)))\\n\\n\\n\\nHere, `X.dtypes.index` gives you the column names and `X.dtypes` it's corresponding dtypes which are unified again into a *list of tuples* and fed as input to the dtype elements to be constructed.\\n\\n\\n\\n    arr = np.array(arr_ip, dtype=dtyp)\\n\\n\\n\\ngives:\\n\\n\\n\\n    arr\\n\\n    # array([('Strawberry', 1), ('Fields', 2), ('Forever', 3)], \\n\\n    #       dtype=[('one', 'O'), ('two', '<i8')])\\n\\nand\\n\\n\\n\\n    arr.dtype.names\\n\\n    # ('one', 'two')\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/pBXuB.png\",\n",
       "  '<python><pandas><numpy>',\n",
       "  datetime.date(2016, 11, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '4736.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2585',\n",
       "  '40570281',\n",
       "  'Answer',\n",
       "  'Pandas join on columns with different names',\n",
       "  \"When the names are different, use the `xxx_on` parameters instead of `on=`:\\n\\n\\n\\n    pd.merge(df1, df2, left_on=  ['userid', 'column1'],\\n\\n                       right_on= ['username', 'column1'], \\n\\n                       how = 'left')\",\n",
       "  '<python><sql><pandas><merge>',\n",
       "  datetime.date(2016, 11, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '3382.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2587',\n",
       "  '36938674',\n",
       "  'Answer',\n",
       "  'Sliding Window over Pandas Dataframe',\n",
       "  \"I think pandas rolling techniques are fine here.  Note that starting with version 0.18.0 of pandas, you would use `rolling().mean()` instead of `rolling_mean()`.\\n\\n\\n\\n    >>> df=pd.DataFrame({ 'x':range(30) })\\n\\n    >>> df = df.rolling(10).mean()           # version 0.18.0 syntax\\n\\n    >>> df[4::5]                             # take every 5th row\\n\\n    \\n\\n           x\\n\\n    4    NaN\\n\\n    9    4.5\\n\\n    14   9.5\\n\\n    19  14.5\\n\\n    24  19.5\\n\\n    29  24.5\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2016, 4, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '6789.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2588',\n",
       "  '37170309',\n",
       "  'Answer',\n",
       "  'BeagleBone Black OpenCV Python is too slow',\n",
       "  'The *\"secret\"* to obtaining higher FPS when processing video streams with OpenCV is to move the I/O (i.e., the reading of frames from the camera sensor) to a separate thread.\\n\\n\\n\\nWhen calling `read()` method along with `cv2.VideoCapture` function, it makes the entire process very slow as it has to wait for each I/O operation to be completed for it to move on to the next one ([Blocking Process][1]).\\n\\n\\n\\nIn order to accomplish this FPS increase/latency decrease, our goal is to move the reading of frames from a webcam or USB device to an entirely different thread, totally separate from our main Python script. \\n\\n\\n\\nThis will allow frames to be read continuously from the I/O thread, all while our root thread processes the current frame. Once the root thread has finished processing its frame, it simply needs to grab the current frame from the I/O thread. This is accomplished without having to wait for blocking I/O operations.\\n\\n\\n\\nYou can read [Increasing webcam FPS with Python and OpenCV][2] to know the steps in implementing threads.\\n\\n\\n\\n--------------------------------------------------------------------------------\\n\\n**EDIT**\\n\\n\\n\\nBased on the discussions in our comments, I feel you could rewrite the code as follows:\\n\\n\\n\\n    import cv2\\n\\n\\n\\n    cv2.namedWindow(\"output\")\\n\\n    cap = cv2.VideoCapture(0)\\n\\n    \\n\\n    if cap.isOpened():              # Getting the first frame\\n\\n        ret, frame = cap.read()\\n\\n    else:\\n\\n        ret = False\\n\\n    \\n\\n    while ret:\\n\\n        cv2.imshow(\"output\", frame)\\n\\n        ret, frame = cap.read()\\n\\n        key = cv2.waitKey(20)\\n\\n        if key == 27:                    # exit on Escape key\\n\\n            break\\n\\n    cv2.destroyWindow(\"output\")\\n\\n\\n\\n\\n\\n  [1]: https://en.wikipedia.org/wiki/Blocking_(computing)\\n\\n  [2]: http://www.pyimagesearch.com/2015/12/21/increasing-webcam-fps-with-python-and-opencv/',\n",
       "  '<python><opencv><beagleboneblack>',\n",
       "  datetime.date(2016, 5, 11),\n",
       "  '2016-08-08 07:44:04',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2112.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2590',\n",
       "  '40573071',\n",
       "  'Answer',\n",
       "  'Matplotlib/pyplot: Auto adjust unit of y Axis',\n",
       "  'In principle there is always the option to set custom labels via `plt.gca().yaxis.set_xticklabels()`.\\n\\n\\n\\nHowever, I\\'m not sure why there shouldn\\'t be the possibility to use [`matplotlib.ticker.FuncFormatter`][1] here. The `FuncFormatter` is designed for exactly the purpose of providing custom ticklabels depending on the ticklabel\\'s position and value. \\n\\nThere is actually a [nice example][2] in the matplotlib example collection.\\n\\n\\n\\nIn this case we can use the FuncFormatter as desired to provide unit prefixes as suffixes on the axes of a matplotlib plot. To this end, we iterate over the multiples of 1000 and check if the value to be formatted exceeds it. If the value is then a whole number, we can format it as integer with the respective unit symbol as suffix. On the other hand, if there is a remainder behind the decimal point, we check how many decimal places are needed to format this number.\\n\\n\\n\\nHere is a complete example:\\n\\n\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    from matplotlib.ticker import FuncFormatter\\n\\n        \\n\\n    def y_fmt(y, pos):\\n\\n        decades = [1e9, 1e6, 1e3, 1e0, 1e-3, 1e-6, 1e-9 ]\\n\\n        suffix  = [\"G\", \"M\", \"k\", \"\" , \"m\" , \"u\", \"n\"  ]\\n\\n        if y == 0:\\n\\n            return str(0)\\n\\n        for i, d in enumerate(decades):\\n\\n            if np.abs(y) >=d:\\n\\n                val = y/float(d)\\n\\n                signf = len(str(val).split(\".\")[1])\\n\\n                if signf == 0:\\n\\n                    return \\'{val:d} {suffix}\\'.format(val=int(val), suffix=suffix[i])\\n\\n                else:\\n\\n                    if signf == 1:\\n\\n                        print val, signf\\n\\n                        if str(val).split(\".\")[1] == \"0\":\\n\\n                           return \\'{val:d} {suffix}\\'.format(val=int(round(val)), suffix=suffix[i]) \\n\\n                    tx = \"{\"+\"val:.{signf}f\".format(signf = signf) +\"} {suffix}\"\\n\\n                    return tx.format(val=val, suffix=suffix[i])\\n\\n                    \\n\\n                    #return y\\n\\n        return y\\n\\n    \\n\\n    \\n\\n    fig, ax = plt.subplots(ncols=3, figsize=(10,5))\\n\\n    \\n\\n    x = np.linspace(0,349,num=350) \\n\\n    y = np.sinc((x-66.)/10.3)**2*1.5e6+np.sinc((x-164.)/8.7)**2*660000.+np.random.rand(len(x))*76000.  \\n\\n    width = 1\\n\\n    \\n\\n    ax[0].bar(x, y, width, align=\\'center\\', linewidth=2, color=\\'red\\', edgecolor=\\'red\\')\\n\\n    ax[0].yaxis.set_major_formatter(FuncFormatter(y_fmt))\\n\\n    \\n\\n    ax[1].bar(x[::-1], y*(-0.8e-9), width, align=\\'center\\', linewidth=2, color=\\'orange\\', edgecolor=\\'orange\\')\\n\\n    ax[1].yaxis.set_major_formatter(FuncFormatter(y_fmt))\\n\\n    \\n\\n    ax[2].fill_between(x, np.sin(x/100.)*1.7+100010, np.cos(x/100.)*1.7+100010, linewidth=2, color=\\'#a80975\\', edgecolor=\\'#a80975\\')\\n\\n    ax[2].yaxis.set_major_formatter(FuncFormatter(y_fmt))\\n\\n    \\n\\n    for axes in ax:\\n\\n        axes.set_title(\"TTL Distribution\")\\n\\n        axes.set_xlabel(\\'TTL Value\\')\\n\\n        axes.set_ylabel(\\'Number of Packets\\')\\n\\n        axes.set_xlim([x[0], x[-1]+1])\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\nwhich provides the following  plot:\\n\\n\\n\\n[![enter image description here][3]][3]\\n\\n\\n\\n\\n\\n  [1]: http://matplotlib.org/api/ticker_api.html#matplotlib.ticker.FuncFormatter\\n\\n  [2]: http://matplotlib.org/examples/pylab_examples/custom_ticker1.html\\n\\n  [3]: https://i.stack.imgur.com/r1xT4.png',\n",
       "  '<matplotlib>',\n",
       "  datetime.date(2016, 11, 13),\n",
       "  '2016-11-13 13:37:59',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2461.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2592',\n",
       "  '40515595',\n",
       "  'Answer',\n",
       "  'Plotting in python3 (histogram)',\n",
       "  'One needs to understand that the numbers cannot be in the middle of the bar, since they are at exactly those positions where they should be. (If you put 0 as close to -3 as to 2, the gods of mathematics will start crying.)\\n\\n\\n\\nSo what you are asking for here is a histogram of **categorial** values, not numerical ones. There are some questions related to categorial histograms already, see, e.g.  \\n\\n \\n\\n - https://stackoverflow.com/questions/28418988/how-to-make-a-histogram-from-a-list-of-strings-in-python\\n\\n - https://stackoverflow.com/questions/17451425/hist-in-matplotlib-bins-are-not-centered-and-proportions-not-correct-on-the-axi\\n\\n\\n\\nWhat you need to do, is to think of the grades `-3`, `0` ,`2` etc. as category (like `red`, `green`, `yellow`) and the question is now how often each category is represented in the list `finalGrades`. Since `matplotlib.hist` only understands numerical data, we would map the `n` categories to the first `n` integers, `-3 -> 0`, `0 -> 1`, `2 -> 2` and so forth.  \\n\\nNow instead of a list of grades  \\n\\n`[-3, -3, 10, 2, 10, 0, ...]`   \\n\\nwe have a list of category numbers   \\n\\n`[0, 0, 5, 2, 5, 1, ...]`  \\n\\nand those category numbers are equally spaced, such that the histogram will understand what we want.\\n\\nThe histogram can then be plotted with `bins = [0,1,2, ... , 6,7]` (we need 8 bin edges to get 7 bins). Finally, and funnily enough, `align=\\'left\\'` makes the bins centered at the tickmarks. \\n\\nThe tickmarks are then set to be the categorial values, i.e. the possible grades.\\n\\n\\n\\n\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    finalGrades = [-3, -3, 10, 2, 10, 0, 7, 7, 12, -3, 7, 0, 12, 12, 12 ,12, 12, 0, 0, 0, 4]\\n\\n    possibleGrades = [-3, 0, 2, 4, 7, 10, 12]\\n\\n    fin = [ possibleGrades.index(i) for i in finalGrades]\\n\\n    plt.hist(fin, bins=range(8), align=\"left\")\\n\\n    plt.xticks(range(7), possibleGrades)\\n\\n\\n\\n    plt.title(\"Final Grades plot\")\\n\\n    plt.xlabel(\"All possible grades\")\\n\\n    plt.ylabel(\"Number of students\")\\n\\n    plt.show()\\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/evLkI.png',\n",
       "  '<python-3.x><matplotlib><plot>',\n",
       "  datetime.date(2016, 11, 9),\n",
       "  '2017-05-23 12:34:20',\n",
       "  'ImportanceOfBeingErnest (4124317), URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '3715.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2601',\n",
       "  '39337192',\n",
       "  'Answer',\n",
       "  'Testing if a pandas DataFrame exists',\n",
       "  '> In my code, I have several variables which can either contain a pandas DataFrame or nothing at all\\n\\n\\n\\nThey Pythonic way of indicating \"nothing\" is via `None`, and for checking \"not nothing\" via\\n\\n\\n\\n    if df1 is not None:\\n\\n        ...\\n\\n\\n\\nI am not sure how critical time is here, but since you measured things:\\n\\n\\n\\n    In [82]: t = timeit.Timer(\\'if x is not None: pass\\', setup=\\'x=None\\')\\n\\n\\n\\n    In [83]: t.timeit()\\n\\n    Out[83]: 0.022536039352416992\\n\\n\\n\\n    In [84]: t = timeit.Timer(\\'if isinstance(x, type(None)): pass\\', setup=\\'x=None\\')\\n\\n\\n\\n    In [85]: t.timeit()\\n\\n    Out[85]: 0.11571192741394043\\n\\n\\n\\nSo checking that something `is not None`, is also faster than the `isinstance` alternative.',\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 9, 5),\n",
       "  '',\n",
       "  '',\n",
       "  '11',\n",
       "  '',\n",
       "  '15084.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2607',\n",
       "  '39778483',\n",
       "  'Answer',\n",
       "  'scatter plot with single pixel marker in matplotlib',\n",
       "  '### The problem\\n\\nI fear that the bugfix discussed at matplotlib git repository that you\\'re citing is only valid for `plt.plot()` and not for `plt.scatter()`\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    fig = plt.figure(figsize=(4,2))\\n\\n    ax = fig.add_subplot(121)\\n\\n    ax2 = fig.add_subplot(122, sharex=ax, sharey=ax)\\n\\n    ax.plot([1, 2],[0.4,0.4],color=\\'black\\',marker=\\',\\',lw=0, linestyle=\"\")\\n\\n    ax.set_title(\"ax.plot\")\\n\\n    ax2.scatter([1,2],[0.4,0.4],color=\\'black\\',marker=\\',\\',lw=0, s=1)\\n\\n    ax2.set_title(\"ax.scatter\")\\n\\n    ax.set_xlim(0,8)\\n\\n    ax.set_ylim(0,1)\\n\\n    fig.tight_layout()\\n\\n    print fig.dpi #prints 80 in my case\\n\\n    fig.savefig(\\'plot.png\\', dpi=fig.dpi)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n### The solution: Setting the markersize\\n\\n\\n\\nThe solution is to use a usual `\"o\"` or `\"s\"` marker, but set the markersize to be exactly one pixel. Since the markersize is given in points, one would need to use the figure dpi to calculate the size of one pixel in points. This is `72./fig.dpi`.\\n\\n\\n\\n* `For a `plot`, the markersize is directly\\n\\n\\n\\n        ax.plot(..., marker=\"o\", ms=72./fig.dpi)\\n\\n* For a `scatter` the markersize is given through the  `s` argument, which is in square points, \\n\\n\\n\\n        ax.scatter(..., marker=\\'o\\', s=(72./fig.dpi)**2)\\n\\n\\n\\nComplete example:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    fig = plt.figure(figsize=(4,2))\\n\\n    ax = fig.add_subplot(121)\\n\\n    ax2 = fig.add_subplot(122, sharex=ax, sharey=ax)\\n\\n    ax.plot([1, 2],[0.4,0.4], marker=\\'o\\',ms=72./fig.dpi, mew=0, \\n\\n            color=\\'black\\', linestyle=\"\", lw=0)\\n\\n    ax.set_title(\"ax.plot\")\\n\\n    ax2.scatter([1,2],[0.4,0.4],color=\\'black\\', marker=\\'o\\', lw=0, s=(72./fig.dpi)**2)\\n\\n    ax2.set_title(\"ax.scatter\")\\n\\n    ax.set_xlim(0,8)\\n\\n    ax.set_ylim(0,1)\\n\\n    fig.tight_layout()\\n\\n    fig.savefig(\\'plot.png\\', dpi=fig.dpi)\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/98OO7.png\\n\\n  [2]: https://i.stack.imgur.com/3dOH6.png',\n",
       "  '<python><matplotlib><scatter-plot>',\n",
       "  datetime.date(2016, 9, 29),\n",
       "  '2018-05-21 23:07:44',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '7',\n",
       "  '',\n",
       "  '3736.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2609',\n",
       "  '40554494',\n",
       "  'Answer',\n",
       "  'Python, summarize daily data in dataframe to monthly and quarterly',\n",
       "  \"First convert your Date column into a datetime index:\\n\\n\\n\\n    df.Date = pd.to_datetime(df.Date)\\n\\n    df.set_index('Date', inplace=True)\\n\\n\\n\\nThen use `resample`. The list of offset aliases is in the [pandas documentation][1]. For begin of month resample, use `MS`, and `QS` for the quarters:\\n\\n\\n\\n    df.resample('QS').sum()\\n\\n    Out[46]: \\n\\n                  Price\\n\\n    Date               \\n\\n    2012-10-01   830.49\\n\\n    2013-01-01  1311.21\\n\\n    2013-04-01   437.05\\n\\n    2013-07-01   437.93\\n\\n    \\n\\n    df.resample('MS').sum()\\n\\n    Out[47]: \\n\\n                 Price\\n\\n    Date              \\n\\n    2012-12-01  830.49\\n\\n    2013-01-01  874.14\\n\\n    2013-02-01  145.11\\n\\n    2013-03-01  291.96\\n\\n    2013-04-01  145.97\\n\\n    2013-05-01  145.97\\n\\n    2013-06-01  145.11\\n\\n    2013-07-01  145.99\\n\\n    2013-08-01  145.97\\n\\n    2013-09-01  145.97\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 11, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '1623.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2611',\n",
       "  '40555566',\n",
       "  'Answer',\n",
       "  'Pandas: flag consecutive values',\n",
       "  \"Similar idea using `shift`, but writing the result as a single Boolean column:\\n\\n\\n\\n    # Boolean indexers for recession start and stops.\\n\\n    rec_start = (df['signal'] == 1) & (df['signal'].shift(-1) == 1)\\n\\n    rec_end = (df['signal'] == 0) & (df['signal'].shift(-1) == 0)\\n\\n    \\n\\n    # Mark the recession start/stops as True/False.\\n\\n    df.loc[rec_start, 'recession'] = True\\n\\n    df.loc[rec_end, 'recession'] = False\\n\\n    \\n\\n    # Forward fill the recession column with the last known Boolean.\\n\\n    # Fill any NaN's as False (i.e. locations before the first start/stop).\\n\\n    df['recession'] = df['recession'].ffill().fillna(False)\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n        signal recession\\n\\n    0        0     False\\n\\n    1        1     False\\n\\n    2        0     False\\n\\n    3        1      True\\n\\n    4        1      True\\n\\n    5        1      True\\n\\n    6        0     False\\n\\n    7        0     False\\n\\n    8        1      True\\n\\n    9        1      True\\n\\n    10       0      True\\n\\n    11       1      True\\n\\n    12       0     False\\n\\n    13       0     False\\n\\n    14       1     False\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 11, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2588.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2613',\n",
       "  '37354256',\n",
       "  'Answer',\n",
       "  'Find the end of the month Pandas DataFrame Series',\n",
       "  'You can use `pandas.tseries.offsets.MonthEnd`:\\n\\n\\n\\n    from pandas.tseries.offsets import MonthEnd\\n\\n\\n\\n    df[\\'Date\\'] = pd.to_datetime(df[\\'Date\\'], format=\"%Y%m\") + MonthEnd(1)\\n\\n\\n\\nThe `1` in `MonthEnd` just specifies to move one step forward to the next date that\\'s a month end. (Using `0` or leaving it blank would also work in your case). If you wanted the last day of the next month, you\\'d use `MonthEnd(2)`, etc.  This should work for any month, so you don\\'t need to know the number days in the month, or anything like that.  More offset information can be found in the [documentation][1].\\n\\n\\n\\nExample usage and output:\\n\\n\\n\\n    df = pd.DataFrame({\\'Date\\': [200104, 200508, 201002, 201602, 199912, 200611]})\\n\\n    df[\\'EndOfMonth\\'] = pd.to_datetime(df[\\'Date\\'], format=\"%Y%m\") + MonthEnd(1)\\n\\n\\n\\n         Date EndOfMonth\\n\\n    0  200104 2001-04-30\\n\\n    1  200508 2005-08-31\\n\\n    2  201002 2010-02-28\\n\\n    3  201602 2016-02-29\\n\\n    4  199912 1999-12-31\\n\\n    5  200611 2006-11-30\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offset-semantics',\n",
       "  '<python><date><datetime><pandas>',\n",
       "  datetime.date(2016, 5, 20),\n",
       "  '2016-05-20 20:30:38',\n",
       "  'root (3339965)',\n",
       "  '45',\n",
       "  '',\n",
       "  '15526.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2615',\n",
       "  '37645393',\n",
       "  'Answer',\n",
       "  'Pandas Convert Timestamp Column to Datetime',\n",
       "  \"If you are keen on extracting only the date from a given `Timestamp` object, you can get the raw `datetime.date` objects by calling the unbound `Timestamp.date` method as shown:\\n\\n \\n\\n    import pandas as pd\\n\\n    from pandas import Timestamp, Series, date_range\\n\\n    \\n\\n    start = Timestamp('2016-01-01')\\n\\n    stop = Timestamp('2016-01-02')\\n\\n    s = Series(date_range(start, stop, freq = 'H'))\\n\\n\\n\\n    print s[0]\\n\\n    2016-01-01 00:00:00\\n\\n     \\n\\n    print s.map(Timestamp.date)[0]\\n\\n    2016-01-01\\n\\n    dtype: object\",\n",
       "  '<python-3.x><datetime><timestamp>',\n",
       "  datetime.date(2016, 6, 5),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '3958.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2616',\n",
       "  '39787608',\n",
       "  'Answer',\n",
       "  'How to use cross_val_score with random_state',\n",
       "  \"The mistake you are making is calling the [`RandomForestClassifier`][1] whose default arg, `random_state` is None. So, it picks up the seed generated by `np.random` to produce the random output.\\n\\n\\n\\nThe `random_state` in both [`StratifiedKFold`][2] and `RandomForestClassifier` need to be the same inorder to produce equal arrays of scores of cross validation.\\n\\n\\n\\n\\n\\n\\n\\n**Illustration:**\\n\\n\\n\\n    X=np.random.random((100,5))\\n\\n    y=np.random.randint(0,2,(100,))\\n\\n    \\n\\n    clf = RandomForestClassifier(random_state=1)\\n\\n    cv = StratifiedKFold(y, random_state=1)        # Setting random_state is not necessary here\\n\\n    s = cross_val_score(clf, X,y,scoring='roc_auc', cv=cv)\\n\\n    print(s)\\n\\n    ##[ 0.57612457  0.29044118  0.30514706]\\n\\n    print(s)\\n\\n    ##[ 0.57612457  0.29044118  0.30514706]\\n\\n\\n\\nAnother way of countering it would be to not provide `random_state` args for both RFC and SKF. But, simply providing the `np.random.seed(value)` to create the random integers at the beginning. These would also create equal arrays at the output.\\n\\n\\n\\n\\n\\n\\n\\n[1]: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\\n\\n[2]: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold\",\n",
       "  '<python><machine-learning><scikit-learn>',\n",
       "  datetime.date(2016, 9, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2842.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2618',\n",
       "  '39806180',\n",
       "  'Answer',\n",
       "  \"What does a 4-element tuple argument for 'bbox_to_anchor' mean in matplotlib?\",\n",
       "  \"You're right, the 4-tuple in `plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3)` is set as `(x0, y0, width, height)` where `(x0,y0)` are the lower left corner coordinates of the bounding box.\\n\\n\\n\\nWhile those parameters set the bounding box for the legend, the legend's actual vertical size is shrunk to the size that is needed to put the elements in. Further its position is determined only in conjunction with the `loc` parameter. The loc parameter sets the alignment of the legend inside the bounding box, such that for some cases, no difference will by seen when changing the `height`, compare e.g. plot (2) and (4).\\n\\n\\n\\n[![enter image description here][1]][1] \\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/OtHbK.png\",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2016, 10, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '23',\n",
       "  '',\n",
       "  '3955.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2622',\n",
       "  '39839489',\n",
       "  'Answer',\n",
       "  'Updating matplotlib plot during code execution',\n",
       "  \"That is interesting. I'm used to draw interactive plots a little bit differently: \\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    from time import sleep\\n\\n    \\n\\n    x = np.array([0])\\n\\n    y = np.array([0])\\n\\n    \\n\\n    plt.ion()\\n\\n    fig = plt.figure()\\n\\n    ax=fig.add_subplot(111)\\n\\n    ax.set_xlim([0,50])\\n\\n    ax.set_ylim([0,2500])\\n\\n    line,  = ax.plot(x,y)\\n\\n    plt.show()\\n\\n    for i in range(51):\\n\\n        x = np.append(x,[x[-1]+1])\\n\\n        y = np.append(y,[x[-1]**2])\\n\\n        line.set_data(x,y)\\n\\n        plt.draw()\\n\\n        sleep(0.01)\\n\\n\\n\\nCan you (or anyone) check if this shows the same problems in Matplotlib 1.5?\",\n",
       "  '<python><matplotlib><plot>',\n",
       "  datetime.date(2016, 10, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1024.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2627',\n",
       "  '40658320',\n",
       "  'Answer',\n",
       "  'How to get percentage of counts of a column after groupby in Pandas',\n",
       "  'Group your data by name and rank levels, and use `transform` to get the total of your series and broadcast it to the entire Series. Use that series to divide the current one:\\n\\n\\n\\n    grade_count.groupby(level = [0,1]).transform(sum)\\n\\n    Out[19]: \\n\\n    name  rank  grade\\n\\n    Bob   1     A        4\\n\\n                B        4\\n\\n                C        4\\n\\n          2     B        1\\n\\n          3     C        1\\n\\n    Joe   1     C        1\\n\\n          2     B        2\\n\\n          3     A        3\\n\\n                B        3\\n\\n    dtype: int64\\n\\n    \\n\\n    grade_count / grade_count.groupby(level = [0,1]).transform(sum)\\n\\n    Out[20]: \\n\\n    name  rank  grade\\n\\n    Bob   1     A        0.500000\\n\\n                B        0.250000\\n\\n                C        0.250000\\n\\n          2     B        1.000000\\n\\n          3     C        1.000000\\n\\n    Joe   1     C        1.000000\\n\\n          2     B        1.000000\\n\\n          3     A        0.333333\\n\\n                B        0.666667\\n\\n\\n\\n',\n",
       "  '<python><pandas><group-by><aggregate><percentage>',\n",
       "  datetime.date(2016, 11, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2082.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2628',\n",
       "  '40570696',\n",
       "  'Answer',\n",
       "  'Find max value and the corresponding column/index name in entire dataframe',\n",
       "  \"Turn your dataframe into a MultipleIndex Series and ask for the index of the max element with `argmax` or `idxmax` function:\\n\\n\\n\\n    coord = a.stack().argmax()\\n\\n    coord\\n\\n    (20, 'R20D')\\n\\n\\n\\nTo get the value, use the coordinates against `loc`:\\n\\n\\n\\n    df.loc[coord]\\n\\n    31\\n\\n\",\n",
       "  '<python><pandas><dataframe><max>',\n",
       "  datetime.date(2016, 11, 13),\n",
       "  '2016-11-13 04:58:56',\n",
       "  'Boud (624829)',\n",
       "  '4',\n",
       "  '',\n",
       "  '3462.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2631',\n",
       "  '37777624',\n",
       "  'Answer',\n",
       "  'How could I do one hot encoding with multiple values in one cell?',\n",
       "  'You need to make your variables to be [`categorical`][1] and then you can use [`one hot encoding`][2] as shown:\\n\\n\\n\\n    In [18]: df1 = pd.DataFrame({\"class\":pd.Series([\\'2\\',\\'1\\',\\'3\\']).astype(\\'category\\',categories=[\\'1\\',\\'2\\',\\'3\\',\\'4\\',\\'5\\'])})\\n\\n    \\n\\n    In [19]: df2 = pd.DataFrame({\"class\":pd.Series([\\'3\\',\\'3\\',\\'5\\']).astype(\\'category\\',categories=[\\'1\\',\\'2\\',\\'3\\',\\'4\\',\\'5\\'])})\\n\\n    \\n\\n    In [20]: df_1 = pd.get_dummies(df1)\\n\\n    \\n\\n    In [21]: df_2 = pd.get_dummies(df2)\\n\\n    \\n\\n    In [22]: df_1.add(df_2).apply(lambda x: x * [i for i in range(1,len(df_1.columns)+1)], axis = 1).astype(int).rename_axis(\\'id\\')\\n\\n    Out[22]: \\n\\n        class_1  class_2  class_3  class_4  class_5\\n\\n    id                                             \\n\\n    0         0        2        3        0        0\\n\\n    1         1        0        3        0        0\\n\\n    2         0        0        3        0        5\\n\\n\\n\\n[1]:http://pandas.pydata.org/pandas-docs/stable/categorical.html\\n\\n[2]:http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html',\n",
       "  '<python><one-hot>',\n",
       "  datetime.date(2016, 6, 12),\n",
       "  '2016-06-12 19:05:20',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '4',\n",
       "  '',\n",
       "  '2300.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2632',\n",
       "  '37819973',\n",
       "  'Answer',\n",
       "  'ValueError: too many values to unpack when using itertuples() on pandas dataframe',\n",
       "  \"When using [`itertuples()`][1], the index is included as part of the tuple, so the `for index, value in grouped.itertuples():` doesn't really make sense.  In fact, `itertuples()` uses [`namedtuple`][2] with `Index` being one of the names.\\n\\n\\n\\nConsider the following setup:\\n\\n\\n\\n    data = {'A': list('aabbc'), 'B': [0, 1, 0, 1, 0], 'C': list('vwxyz'), 'D': range(5,10)}\\n\\n    df = pd.DataFrame(data).set_index(['A', 'B'])\\n\\n\\n\\nYielding the following DataFrame:\\n\\n\\n\\n         C  D\\n\\n    A B      \\n\\n    a 0  v  5\\n\\n      1  w  6\\n\\n    b 0  x  7\\n\\n      1  y  8\\n\\n    c 0  z  9\\n\\n\\n\\nThen printing each tuple in `df.itertuples()` yields:\\n\\n\\n\\n    Pandas(Index=('a', 0), C='v', D=5)\\n\\n    Pandas(Index=('a', 1), C='w', D=6)\\n\\n    Pandas(Index=('b', 0), C='x', D=7)\\n\\n    Pandas(Index=('b', 1), C='y', D=8)\\n\\n    Pandas(Index=('c', 0), C='z', D=9)\\n\\n\\n\\nSo, what you'll probably want to do is something like the code below, with `value` being replaced by `t[1:]`:\\n\\n\\n\\n    for t in grouped.itertuples():\\n\\n        for i, key in enumerate(t.Index):\\n\\n            ...\\n\\n\\n\\n  \\n\\n\\n\\nIf you want to access components of the `namedtuple`, you can access things positionally, or by name. So, in the case of your DataFrame, `t[1]` and `t.firstname` should be equivalent.  Just remember that `t[0]` is the index, so your first column starts at `1`.\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.itertuples.html\\n\\n  [2]: https://docs.python.org/2/library/collections.html#collections.namedtuple\",\n",
       "  '<python><json><pandas><nested>',\n",
       "  datetime.date(2016, 6, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '3649.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2639',\n",
       "  '39406674',\n",
       "  'Answer',\n",
       "  'Adding Lat Lon coordinates to separate columns (python/dataframe)',\n",
       "  \"\\n\\n    df['gridReference'].str.strip('()')                               \\\\\\n\\n                       .str.split(', ', expand=True)                   \\\\\\n\\n                       .rename(columns={0:'Latitude', 1:'Longitude'}) \\n\\n\\n\\n                 Latitude           Longitude\\n\\n    0   56.37769816725615  -4.325049868061924\\n\\n    1   56.37769816725615  -4.325049868061924\\n\\n    2  51.749167440074324  -4.963575226888083\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 9, 9),\n",
       "  '2016-09-09 08:17:56',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1307.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2646',\n",
       "  '40615034',\n",
       "  'Question',\n",
       "  'Understanding scipy deconvolve',\n",
       "  'I\\'m trying to understand [`scipy.signal.deconvolve`](https://docs.scipy.org/doc/scipy-0.15.0/reference/generated/scipy.signal.deconvolve.html).  \\n\\n\\n\\nFrom the mathematical point of view a convolution is just the multiplication in fourier space so I would expect\\n\\nthat for two functions `f` and `g`:  \\n\\n`Deconvolve(Convolve(f,g) , g) == f`\\n\\n\\n\\nIn numpy/scipy this is either not the case or I\\'m missing an important point. \\n\\nAlthough there are some questions related to deconvolve on SO already (like [here][1] and [here][2]) they do not address this point, others remain unclear ([this][3]) or unanswered ([here][4]). There are also two questions on SignalProcessing SE ([this][5] and [this][6]) the answers to which are not helpful in understanding how scipy\\'s deconvolve function works.\\n\\n\\n\\nThe question would be:\\n\\n   \\n\\n * How do you reconstruct the original signal `f` from a convoluted signal, \\n\\nassuming you know the convolving function g.?\\n\\n * Or in other words: How does this pseudocode `Deconvolve(Convolve(f,g) , g) == f` translate into numpy / scipy?\\n\\n\\n\\n**Edit**: *Note that this question is not targeted at preventing numerical inaccuracies (although this is also an [open question](https://stackoverflow.com/questions/36204207/what-are-the-constraints-on-the-divisor-argument-of-scipy-signal-deconvolve-to-e)) but at understanding how convolve/deconvolve work together in scipy.*\\n\\n\\n\\nThe following code tries to do that with a Heaviside function and a gaussian filter. \\n\\nAs can be seen in the image, the result of the deconvolution of the convolution is not at \\n\\nall the original Heaviside function. I would be glad if someone could shed some light into this issue.\\n\\n\\n\\n\\n\\n    import numpy as np\\n\\n    import scipy.signal\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    # Define heaviside function\\n\\n    H = lambda x: 0.5 * (np.sign(x) + 1.)\\n\\n    #define gaussian\\n\\n    gauss = lambda x, sig: np.exp(-( x/float(sig))**2 )\\n\\n    \\n\\n    X = np.linspace(-5, 30, num=3501)\\n\\n    X2 = np.linspace(-5,5, num=1001)\\n\\n    \\n\\n    # convolute a heaviside with a gaussian\\n\\n    H_c = np.convolve( H(X),  gauss(X2, 1),  mode=\"same\"  )\\n\\n    # deconvolute a the result\\n\\n    H_dc, er = scipy.signal.deconvolve(H_c, gauss(X2, 1) )\\n\\n    \\n\\n    \\n\\n    #### Plot #### \\n\\n    fig , ax = plt.subplots(nrows=4, figsize=(6,7))\\n\\n    ax[0].plot( H(X),          color=\"#907700\", label=\"Heaviside\",    lw=3 ) \\n\\n    ax[1].plot( gauss(X2, 1),  color=\"#907700\", label=\"Gauss filter\", lw=3 )\\n\\n    ax[2].plot( H_c/H_c.max(), color=\"#325cab\", label=\"convoluted\" ,  lw=3 ) \\n\\n    ax[3].plot( H_dc,          color=\"#ab4232\", label=\"deconvoluted\", lw=3 ) \\n\\n    for i in range(len(ax)):\\n\\n    \\tax[i].set_xlim([0, len(X)])\\n\\n    \\tax[i].set_ylim([-0.07, 1.2])\\n\\n    \\tax[i].legend(loc=4)\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][7]][7]\\n\\n\\n\\n**Edit**: *Note that there is a [matlab example](https://terpconnect.umd.edu/~toh/spectrum/Deconvolution.html), showing how to convolve/deconvolve a rectangular signal using*\\n\\n\\n\\n    yc=conv(y,c,\\'full\\')./sum(c); \\n\\n    ydc=deconv(yc,c).*sum(c); \\n\\n\\n\\n*In the spirit of this question it would also help if someone was able to translate this example into python.*\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/questions/17063775/convolution-and-deconvolution-in-python-using-scipy-signal\\n\\n  [2]: https://stackoverflow.com/questions/15483346/scipy-deconvolution-function\\n\\n  [3]: https://stackoverflow.com/questions/21990327/convolution-deconvolution-using-scipy\\n\\n  [4]: https://stackoverflow.com/questions/36204207/what-are-the-constraints-on-the-divisor-argument-of-scipy-signal-deconvolve-to-e\\n\\n  [5]: https://dsp.stackexchange.com/questions/8287/deconvolution-in-python\\n\\n  [6]: https://dsp.stackexchange.com/questions/32181/how-does-scipy-signal-deconvolve-work\\n\\n  [7]: https://i.stack.imgur.com/wPbpy.png',\n",
       "  '<python><numpy><scipy><signals><deconvolution>',\n",
       "  datetime.date(2016, 11, 15),\n",
       "  '2017-05-23 12:10:01',\n",
       "  'ImportanceOfBeingErnest (4124317), URL Rewriter Bot (n/a)',\n",
       "  '14',\n",
       "  '6.0',\n",
       "  '5283.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2647',\n",
       "  '39817113',\n",
       "  'Answer',\n",
       "  'Qt QString toInt() fails',\n",
       "  'If you look at [the documentation](http://doc.qt.io/qt-4.8/qstring.html#toInt), it says\\n\\n\\n\\n> Returns 0 if the conversion fails.\\n\\n\\n\\nYou should use\\n\\n\\n\\n    bool ok;\\n\\n    strTest.toInt(&ok);\\n\\n\\n\\nand then check the value of `ok` - otherwise, you won\\'t be sure if the 0 is the actual value, or an indication of failure.\\n\\n\\n\\nIn this case it\\'s failing because it is not actually an integer (it has a decimal point). Note that you can use [`toDouble`](http://doc.qt.io/qt-4.8/qstring.html#toDouble) (and check `ok` there too!), and then cast the result as you see fit.\\n\\n\\n\\n    QString strTest = \"-3.5\";\\n\\n\\n\\n    bool ok;\\n\\n    double t = strTest.toDouble(&ok);\\n\\n\\n\\n    if(ok)\\n\\n        qDebug() << static_cast<int>(t);',\n",
       "  '<c++><qstring>',\n",
       "  datetime.date(2016, 10, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '2002.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2658',\n",
       "  '37956071',\n",
       "  'Answer',\n",
       "  'Pandas select the second to last column which is also not nan',\n",
       "  'Similar idea to @piRSquared.  Essentially, use `loc` to keep the non-null columns, then use `iloc` to select the second to last.\\n\\n\\n\\n    df.loc[:, ~df.isnull().all()].iloc[:, -2]\\n\\n\\n\\nSample input:\\n\\n\\n\\n       a  b  c   d   e   f   g   h   i   j\\n\\n    0  0  3  6   9  12  15  18  21 NaN NaN\\n\\n    1  1  4  7  10  13  16  19  22 NaN NaN\\n\\n    2  2  5  8  11  14  17  20  23 NaN NaN\\n\\n\\n\\nSample output:\\n\\n\\n\\n    0    18\\n\\n    1    19\\n\\n    2    20\\n\\n    Name: g, dtype: int32',\n",
       "  '<python><pandas><select><nan>',\n",
       "  datetime.date(2016, 6, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1388.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2660',\n",
       "  '38100431',\n",
       "  'Answer',\n",
       "  'Bokeh interactively changing the columns being plotted',\n",
       "  'The following script seems to work just fine. It does not use the Bokeh Server but the CustomJS client side style instead.\\n\\n\\n\\n    import numpy as np\\n\\n\\n\\n    from bokeh.models import ColumnDataSource\\n\\n    from bokeh.plotting import Figure\\n\\n    \\n\\n    from bokeh.models.widgets import Select,TextInput\\n\\n    from bokeh.models.layouts import HBox, VBox\\n\\n    import bokeh.io \\n\\n    from bokeh.models import CustomJS\\n\\n    \\n\\n    N = 200\\n\\n    \\n\\n    # Define the data to be used\\n\\n    x = np.linspace(0,4.*np.pi,N)\\n\\n    y = 3*np.cos(2*np.pi*x + np.pi*0.2)\\n\\n    z = 0.5*np.sin(2*np.pi*0.8*x + np.pi*0.4)\\n\\n    \\n\\n    source = ColumnDataSource(data={\\'x\\':x,\\'y\\':y, \\'X\\': x, \\'cos\\':y,\\'sin\\':z})\\n\\n    \\n\\n    \\n\\n    code=\"\"\"\\n\\n            var data = source.get(\\'data\\');\\n\\n            var r = data[cb_obj.get(\\'value\\')];\\n\\n            var {var} = data[cb_obj.get(\\'value\\')];\\n\\n            //window.alert( \"{var} \" + cb_obj.get(\\'value\\') + {var}  );\\n\\n            for (i = 0; i < r.length; i++) {{\\n\\n                {var}[i] = r[i] ;\\n\\n                data[\\'{var}\\'][i] = r[i];\\n\\n            }}\\n\\n            source.trigger(\\'change\\');\\n\\n        \"\"\"\\n\\n    \\n\\n    callbackx = CustomJS(args=dict(source=source), code=code.format(var=\"x\"))\\n\\n    callbacky = CustomJS(args=dict(source=source), code=code.format(var=\"y\"))\\n\\n    \\n\\n    # create a new plot \\n\\n    plot = Figure(title=None)\\n\\n    \\n\\n    # Make a line and connect to data source\\n\\n    plot.line(x=\"x\", y=\"y\", line_color=\"#F46D43\", line_width=6, line_alpha=0.6, source=source)\\n\\n    \\n\\n    \\n\\n    # Add list boxes for selecting which columns to plot on the x and y axis\\n\\n    yaxis_select = Select(title=\"Y axis:\", value=\"cos\",\\n\\n                               options=[\\'X\\',\\'cos\\',\\'sin\\'], callback=callbacky)\\n\\n    \\n\\n    \\n\\n    xaxis_select = Select(title=\"X axis:\", value=\"x\",\\n\\n                               options=[\\'X\\',\\'cos\\',\\'sin\\'], callback=callbackx)\\n\\n    \\n\\n    \\n\\n    # Text input as a title\\n\\n    text = TextInput(title=\"title\", value=\\'my sine wave plotter\\')\\n\\n    \\n\\n    # Layout widgets next to the plot                     \\n\\n    controls = VBox(text,yaxis_select,xaxis_select)\\n\\n    \\n\\n    layout = HBox(controls,plot,width=800)\\n\\n    \\n\\n    bokeh.io.show(layout)\\n\\n\\n\\n\\n\\nHowever, I tinkered it together until it did what it should. And I am not sure why the part in the CustomJS-code needs to be so complicated. \\n\\n\\n\\nEspecially it seems that both lines inside the for-loop in the JS-part are actually needed, although they seem to do the same thing.\\n\\n\\n\\nThus I wouldn\\'t call this question answered until someone can actually come up with something that is explainable.\\n\\n\\n\\n',\n",
       "  '<python><plot><interactive><bokeh>',\n",
       "  datetime.date(2016, 6, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '2434.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2664',\n",
       "  '39862735',\n",
       "  'Answer',\n",
       "  'pandas.concat of multiple data frames using only common columns',\n",
       "  \"You can find the common columns with Python's [`set.intersection`](https://docs.python.org/2/library/sets.html):\\n\\n\\n\\n    common_cols = list(set.intersection(*(set(df.columns) for df in frames)))\\n\\n\\n\\nTo concatenate using only the common columns, you can use\\n\\n\\n\\n    pd.concat([df[common_cols] for df in frames], ignore_index=True)\\n\\n\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 10, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1555.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2671',\n",
       "  '39438029',\n",
       "  'Answer',\n",
       "  'Find parent in Binary Search Tree?',\n",
       "  \"As your code currently works, it is impossible that you're turning toward a `None` left or right child. This is because your code starts with\\n\\n\\n\\n    if not self._exists(key):\\n\\n        return None,None\\n\\n\\n\\nSo `key` must exist, and if it must exist, it must exist on the search path. \\n\\n\\n\\nIt should be noted that you're effectively performing the search twice, though, which is not that efficient. Instead, you could try something like this:\\n\\n\\n\\n    def findpar(self,key):\\n\\n        parent, node = None, self.root\\n\\n        while True:\\n\\n            if node is None:\\n\\n                return (None, None)\\n\\n        \\n\\n            if node.item == key:\\n\\n                return (parent, node)\\n\\n\\n\\n            parent, node = node, node.left if key < node.item else node, node.right\\n\\n\",\n",
       "  '<python><python-3.x>',\n",
       "  datetime.date(2016, 9, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1121.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2673',\n",
       "  '40727744',\n",
       "  'Answer',\n",
       "  'Matplotlib scatter plot - Remove white padding',\n",
       "  'The problem is that all the solutions given at [Matplotlib plots: removing axis, legends and white spaces](https://stackoverflow.com/questions/9295026/matplotlib-plots-removing-axis-legends-and-white-spaces) are actually meant to work with `imshow`. \\n\\n\\n\\nSo, the following clearly works\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax=fig.add_axes([0,0,1,1])\\n\\n    ax.set_axis_off()\\n\\n        \\n\\n    im = ax.imshow([[2,3,4,1], [2,4,4,2]], origin=\"lower\", extent=[1,4,2,8])\\n\\n    ax.plot([1,2,3,4], [2,3,4,8], lw=5)\\n\\n\\n\\n    ax.set_aspect(\\'auto\\')\\n\\n    plt.show()\\n\\n\\n\\nand produces\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nBut here, you are using `scatter`. Adding a scatter plot\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax=fig.add_axes([0,0,1,1])\\n\\n    ax.set_axis_off()\\n\\n    \\n\\n    \\n\\n    im = ax.imshow([[2,3,4,1], [2,4,4,2]], origin=\"lower\", extent=[1,4,2,8])\\n\\n    ax.plot([1,2,3,4], [2,3,4,8], lw=5)\\n\\n    \\n\\n    ax.scatter([2,3,4,1], [2,3,4,8], c=\"r\", s=2500)\\n\\n    \\n\\n    ax.set_aspect(\\'auto\\')\\n\\n    plt.show()\\n\\n\\n\\nproduces \\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n`Scatter` has the particularity that matplotlib tries to make all points visible by default, which means that the axes limits are set such that all scatter points are visible as a whole. \\n\\n\\n\\nTo overcome this, we need to specifically set the axes limits: \\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax=fig.add_axes([0,0,1,1])\\n\\n    ax.set_axis_off()\\n\\n    \\n\\n    im = ax.imshow([[2,3,4,1], [2,4,4,2]], origin=\"lower\", extent=[1,4,2,8])\\n\\n    ax.plot([1,2,3,4], [2,3,4,8], lw=5)\\n\\n    \\n\\n    ax.scatter([2,3,4,1], [2,3,4,8], c=\"r\", s=2500)\\n\\n    \\n\\n    ax.set_xlim([1,4])\\n\\n    ax.set_ylim([2,8])\\n\\n    \\n\\n    ax.set_aspect(\\'auto\\')\\n\\n    plt.show()\\n\\n\\n\\nsuch that we will get the desired behaviour.\\n\\n\\n\\n[![enter image description here][3]][3]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/TDYsr.png\\n\\n  [2]: https://i.stack.imgur.com/HBywU.png\\n\\n  [3]: https://i.stack.imgur.com/fqh2e.png',\n",
       "  '<python><matplotlib><data-visualization><scatter-plot>',\n",
       "  datetime.date(2016, 11, 21),\n",
       "  '2017-05-23 10:31:34',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1578.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2678',\n",
       "  '39916895',\n",
       "  'Question',\n",
       "  'PyQt - Reducing margins and spacing in widget *expands* layout',\n",
       "  'Consider the case of a `QMainWindow` with a `QWidget` as central widget. This widget has a `QHBoxLayout`. I add two other widgets to it, each with a `QVBoxLayout`. \\n\\nI now want to bring the Widgets inside `QVBoxLayout` closer to each other. The attempt is to use `.setMargin(0)`, `.setSpacing(0)` and `.setContentsMargins(0,0,0,0)` for this purpose.\\n\\n\\n\\nHowever the result is that their separation is actually **increased** instead of decreased - as can be seen in the picture (where Gain is the widget where I set the margins and spacings to zero).\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nThe code to reproduce this issue is appended below. (And the same actually happens when using a `QGridLayout`.)\\n\\n\\n\\nHere is the question on two different complexity levels: \\n\\n\\n\\n**(a)** Since the only difference between the two widgets is that one has the margins and spacings set to zero, one of the used method calls must have done something else to the layout or widget-properties. Which other property is changed by setting any of  `.setMargin(0)`, `.setSpacing(0)` and `setContentsMargins(0,0,0,0)` ?  \\n\\n\\n\\n**(b)** How do I make the spacing between the text label and the combobox in this example smaller?\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[1]: http://i.stack.imgur.com/O2BrI.png\\n\\n<hr>\\n\\n\\n\\n\\n\\n    from PyQt4 import QtGui\\n\\n    import sys\\n\\n    \\n\\n    class LabeledComboBox(QtGui.QWidget):\\n\\n        def __init__(self, text=\"\", items=[], parent=None):\\n\\n            super(LabeledComboBox, self).__init__(parent)\\n\\n            self.parent = parent\\n\\n            self.widgetlayout = QtGui.QVBoxLayout(self)\\n\\n            self.widgetlayout.addWidget(QtGui.QLabel(text))\\n\\n            self.Combo = QtGui.QComboBox()\\n\\n            self.Combo.addItems(items)\\n\\n            self.widgetlayout.addWidget(self.Combo)\\n\\n            self.parent.mylayout.addWidget(self)\\n\\n                \\n\\n        def printParams(self):\\n\\n            # print some margin/spacing parameters for testing\\n\\n            m = self.widgetlayout.margin()\\n\\n            s = self.widgetlayout.spacing()\\n\\n            cm = self.widgetlayout.getContentsMargins()\\n\\n            print \"margin: {m}, spacing: {s}, ContentsMargin: {cm}\".format(m=m, s=s, cm=cm)\\n\\n    \\n\\n    class App(QtGui.QMainWindow):\\n\\n        def __init__(self, parent=None):\\n\\n            super(App, self).__init__(parent)\\n\\n            self.mainbox = QtGui.QWidget()\\n\\n            self.mylayout = QtGui.QHBoxLayout()\\n\\n            self.mainbox.setLayout(self.mylayout)\\n\\n            self.setCentralWidget(self.mainbox)\\n\\n            \\n\\n            self.GainWidget = LabeledComboBox(\"Gain\", [\\'low\\', \\'medium\\', \\'high\\'],  self)\\n\\n            self.RevolutionsWidget = LabeledComboBox(\"Revolutions\", [\\'100\\', \\'200\\', \\'400\\'],  self)\\n\\n            \\n\\n            \\n\\n            self.GainWidget.printParams()\\n\\n            # this outputs: margin: 9, spacing: 6, ContentsMargin: (9, 9, 9, 9)\\n\\n            # now I set everything to zero \\n\\n            self.GainWidget.widgetlayout.setMargin(0)\\n\\n            self.GainWidget.widgetlayout.setSpacing(0)\\n\\n            self.GainWidget.widgetlayout.setContentsMargins(0,0,0,0)\\n\\n            # check\\n\\n            self.GainWidget.printParams()\\n\\n            # margin: 0, spacing: 0, ContentsMargin: (0, 0, 0, 0)\\n\\n    \\n\\n    \\n\\n    if __name__==\\'__main__\\':\\n\\n        app = QtGui.QApplication(sys.argv)\\n\\n        thisapp = App()\\n\\n        thisapp.show()\\n\\n        sys.exit(app.exec_())',\n",
       "  '<layout><pyqt>',\n",
       "  datetime.date(2016, 10, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2353.0',\n",
       "  '2.0',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2680',\n",
       "  '40622842',\n",
       "  'Answer',\n",
       "  'Pandas DataFrame: How to print single row horizontally?',\n",
       "  'Use the transpose property:\\n\\n\\n\\n    df.T\\n\\n\\n\\n         0    1    2\\n\\n    0  100  200  300',\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 11, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '7898.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2684',\n",
       "  '40018765',\n",
       "  'Answer',\n",
       "  'Binning of data along one axis in numpy',\n",
       "  \"You could use [`np.apply_along_axis`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.apply_along_axis.html):\\n\\n\\n\\n    x = np.array([range(20), range(1, 21), range(2, 22)])\\n\\n\\n\\n    nbins = 2\\n\\n    >>> np.apply_along_axis(lambda a: np.histogram(a, bins=nbins)[0], 1, x)\\n\\n    array([[10, 10],\\n\\n           [10, 10],\\n\\n           [10, 10]])\\n\\n\\n\\nThe main advantage (if any) is that it's slightly shorter, but I wouldn't expect much of a performance gain. It's possibly marginally more efficient in the assembly of the per-row results.\\n\\n\",\n",
       "  '<python><numpy><histogram><binning>',\n",
       "  datetime.date(2016, 10, 13),\n",
       "  '2016-10-13 15:00:13',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '7',\n",
       "  '',\n",
       "  '1762.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2687',\n",
       "  '38128675',\n",
       "  'Answer',\n",
       "  'append page to existing pdf file using python (and matplotlib?)',\n",
       "  'You may want to use [pyPdf][1] for this.  \\n\\n\\n\\n    # Merge two PDFs\\n\\n    from PyPDF2 import PdfFileReader, PdfFileWriter\\n\\n    \\n\\n    output = PdfFileWriter()\\n\\n    pdfOne = PdfFileReader(open(\"path/to/pdf1.pdf\", \"rb\"))\\n\\n    pdfTwo = PdfFileReader(open(\"path/to/pdf2.pdf\", \"rb\"))\\n\\n    \\n\\n    output.addPage(pdfOne.getPage(0))\\n\\n    output.addPage(pdfTwo.getPage(0))\\n\\n    \\n\\n    outputStream = open(r\"output.pdf\", \"wb\")\\n\\n    output.write(outputStream)\\n\\n    outputStream.close()\\n\\n\\n\\n[example taken from here][2]\\n\\n\\n\\nThereby you detach the plotting from the pdf-merging.\\n\\n\\n\\n  [1]: https://github.com/mstamy2/PyPDF2/\\n\\n  [2]: http://www.blog.pythonlibrary.org/2010/05/15/manipulating-pdfs-with-python-and-pypdf/',\n",
       "  '<python><pdf><matplotlib>',\n",
       "  datetime.date(2016, 6, 30),\n",
       "  '2018-06-12 17:33:03',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '4',\n",
       "  '',\n",
       "  '4082.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2689',\n",
       "  '38134049',\n",
       "  'Answer',\n",
       "  'Pandas dataframe fillna() only some columns in place',\n",
       "  \"You can select your desired columns and do it by assignment:\\n\\n\\n\\n    df[['a', 'b']] = df[['a','b']].fillna(value=0)\\n\\n\\n\\nThe resulting output is as expected:\\n\\n\\n\\n         a    b    c\\n\\n    0  1.0  4.0  NaN\\n\\n    1  2.0  5.0  NaN\\n\\n    2  3.0  0.0  7.0\\n\\n    3  0.0  6.0  8.0\",\n",
       "  '<python><python-2.7><pandas><dataframe><pandas-fillna>',\n",
       "  datetime.date(2016, 6, 30),\n",
       "  '2016-06-30 22:19:27',\n",
       "  'root (3339965)',\n",
       "  '108',\n",
       "  '',\n",
       "  '62064.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2695',\n",
       "  '39442599',\n",
       "  'Answer',\n",
       "  'pandas: groupby and aggregate without losing the column which was grouped',\n",
       "  \"If you don't want the groupby as an index, there is an argument for it to avoid further reset:\\n\\n\\n\\n    df.groupby('Id', as_index=False).agg(lambda x: set(x))\",\n",
       "  '<python><pandas><dataframe><group-by>',\n",
       "  datetime.date(2016, 9, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '13',\n",
       "  '',\n",
       "  '7902.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2702',\n",
       "  '39984629',\n",
       "  'Answer',\n",
       "  'Master theorem: issue when f(n) contains negative power of log',\n",
       "  \"As Matt Timmermans correctly notes, the statement doesn't follow from the master theorem, but it does follow from an extended version of it.\\n\\n\\n\\nIt's quite simple to solve this problem directly using the [tree method](http://www.cs.cornell.edu/courses/cs3110/2012sp/lectures/lec20-master/lec20.html).\\n\\n\\n\\nStarting with *T(n) = 2T (n/2)+ n / log n*:\\n\\n\\n\\n* Level 0 has 1 node with value *n / log(n)*.\\n\\n\\n\\n* Level 1 has 2 nodes, each with value *(n / 2) / log(n / 2)*.\\n\\n\\n\\n* ...\\n\\n\\n\\n* Level *i* has *2<sup>i</sup>* nodes, each with value *(n / 2<sup>i</sup>) / log(n / 2<sup>i</sup>)*\\n\\n\\n\\nSimplifying, level *i* contributes *n / (log(n) - i)*.\\n\\n\\n\\nNote that, altogether, there are *~log(n) - 1* levels to reach a constant.\\n\\n\\n\\nConsequently, the sum of all levels is *&sum;<sub>i = 0</sub><sup>~log(n) - 1</sup>[n / (log(n) - i)] ~ n &sum;<sub>i = 0</sub><sup>k</sup>[1 / k]*, \\n\\n\\n\\nfor *k = log(n)*.\\n\\n\\n\\nNote that the sigma is the [*k*th harmonic series, which is *&Theta;(log(k))*](https://stackoverflow.com/questions/25905118/finding-big-o-of-the-harmonic-series). Setting *k = log(n)* gives altogether *n &Theta;(log(log(n))) = &Theta;(n log(log(n)))*.\",\n",
       "  '<algorithm><time-complexity><master-theorem>',\n",
       "  datetime.date(2016, 10, 11),\n",
       "  '2017-05-23 12:33:17',\n",
       "  'Ami Tavory (3510736), URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1326.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2703',\n",
       "  '40005796',\n",
       "  'Answer',\n",
       "  'How does sp_randint work?',\n",
       "  '[`sklearn.grid_search.RandomizedSearchCV`](http://scikit-learn.org/0.17/modules/generated/sklearn.grid_search.RandomizedSearchCV.html) can get a `param_distributions` parameter, mapping parameters to random distributions supporting the `rvs` method.\\n\\n\\n\\nIn your example, this object will return random integers in the range $[1, 11)$:\\n\\n\\n\\n    In [8]: g = sp_randint(1, 11)\\n\\n\\n\\n    In [9]: g.rvs(20)\\n\\n    Out[9]: \\n\\n    array([ 5,  2,  9, 10,  6,  9,  9,  8,  1,  5,  1,  8,  1,  5,  5,  4,  6,\\n\\n            5,  8,  4])\\n\\n\\n\\nYou can change it to any other object meaningfully supporting the `rvs` method, or even a list. For example:\\n\\n\\n\\n    param_dist = {\"n_estimators\": [1, 3, 4], \\n\\n                  \"max_depth\": [3, None],\\n\\n                  \"max_features\": [1, 3, 4],\\n\\n                  \"min_samples_split\": [1, 3, 4],\\n\\n                  \"min_samples_leaf\": [1, 3, 4],\\n\\n                 }\\n\\n\\n\\nwill work as well.\\n\\n',\n",
       "  '<machine-learning><python><optimization><scikit-learn><scipy>',\n",
       "  datetime.date(2016, 10, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '1957.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2706',\n",
       "  '40794945',\n",
       "  'Answer',\n",
       "  'Python figure and axes object',\n",
       "  'In general axes are bound to a figure. The reason is, that matplotlib usually performs some operations in the background to make them look nice in the figure. \\n\\n\\n\\nThere are [some hacky ways around this](https://stackoverflow.com/questions/6309472/matplotlib-can-i-create-axessubplot-objects-then-add-them-to-a-figure-instance#comment18919455_6309636), also [this one](https://stackoverflow.com/questions/27763875/matplotlib-duplicate-axes-or-figure), but the general consensus seems to be that one should avoid trying to copy axes.\\n\\n\\n\\nOn the other hand this need not be a problem or a restriction at all.\\n\\n\\n\\nYou can always define a function which does the plotting and use this on several figures like so:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    def plot1(ax,  **kwargs):\\n\\n        x = range(5)\\n\\n        y = [5,4,5,1,2]\\n\\n        ax.plot(x,y, c=kwargs.get(\"c\", \"r\"))\\n\\n        ax.set_xlim((0,5))\\n\\n        ax.set_title(kwargs.get(\"title\", \"Some title\"))\\n\\n        # do some more specific stuff with your axes\\n\\n    \\n\\n    #create a figure    \\n\\n    fig, (ax1, ax2) = plt.subplots(1,2)\\n\\n    # add the same plot to it twice\\n\\n    plot1(ax1)\\n\\n    plot1(ax2, c=\"b\", title=\"Some other title\")\\n\\n    plt.savefig(__file__+\".png\")\\n\\n    \\n\\n    plt.close(\"all\")\\n\\n    \\n\\n    # add the same plot to a different figure\\n\\n    fig, ax1 = plt.subplots(1,1)\\n\\n    plot1(ax1)\\n\\n    plt.show()\\n\\n\\n\\n',\n",
       "  '<python><matplotlib><figures>',\n",
       "  datetime.date(2016, 11, 24),\n",
       "  '2017-05-23 12:34:11',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1306.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2707',\n",
       "  '40145561',\n",
       "  'Answer',\n",
       "  'How to select the last column of dataframe',\n",
       "  'Use iloc and select all rows (`:`) against the last column (`-1`):\\n\\n\\n\\n    df.iloc[:,-1]',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 10, 20),\n",
       "  '2017-05-02 16:01:15',\n",
       "  'Nate (4325994)',\n",
       "  '64',\n",
       "  '',\n",
       "  '35600.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2709',\n",
       "  '40203539',\n",
       "  'Answer',\n",
       "  'How to subplot multiple graphs when calling a function that plots the graph?',\n",
       "  'In case you know the number of plots you want to produce beforehands, you can first create as many subplots as you need\\n\\n\\n\\n    fig, axes = plt.subplots(nrows=1, ncols=5)\\n\\n\\n\\n(in this case 5) and then provide the axes to the function\\n\\n\\n\\n    def plt_graph(x, graph_title, horiz_label, ax):\\n\\n        df[x].plot(kind=\\'barh\\', ax=ax)\\n\\n\\n\\nFinally, call every plot like this\\n\\n\\n\\n    plt_graph(\"framekey\", \"Some title\", \"some label\", axes[4])\\n\\n\\n\\n(where 4 stands for the fifth and last plot)\\n\\n',\n",
       "  '<python><pandas><matplotlib><graph>',\n",
       "  datetime.date(2016, 10, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1222.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2714',\n",
       "  '40088346',\n",
       "  'Answer',\n",
       "  'Algorithm: Check if max flow is unique',\n",
       "  \"Your question leaves a few details open, e.g., is this an integer flow graph (probably yes, although Ford-Fulkerson, if it converges, can run on other networks as well), and how exactly do you define whether two flows are different (is it enough that the function mapping edges to flows be different, or must the set of edges actually flowing something be different, which is a stronger requirement).\\n\\n\\n\\n---------------------\\n\\n\\n\\nIf the network is not necessarily integer flows, then, no, this will not necessarily work. Consider the following graph, where, on each edge, the number within the parentheses represents the actual flow, and the number to the left of the parentheses represents the capacity (e.g., the capacity of each of *(a, c)* and *(c, d)* is 1.1, and the flow of each is 1.):\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\nIn this graph, the flow is non-unique. It's possible to flow a total of 1 by floating 0.5 through *(a, b)* and *(b, d)*. Your algorithm, however, won't find this by reducing the capacity of each of the edges to 1 below its current flow.\\n\\n\\n\\n---------------------------\\n\\n\\n\\nIf the network is integer, it is not guaranteed to find a different set of participating edges than the current one. You can see it through the following graph:\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n------------------------\\n\\n\\n\\nFinally, though, if the network is an integer flow network, and the meaning of a different flow is simply a different function of edges to flows, then your algorithm is correct.\\n\\n\\n\\n1. *Sufficiency* If your algorithm finds a different flow with the same total result, then obviously the new flow is legal, and, also, necessarily, at least one of the edges is flowing a different amount than it did before. \\n\\n\\n\\n2. *Necessity* Suppose there is a different flow than the original one (with the same total value), with at least one of the edges flowing a different amount. Say that, for each edge, the flow in the alternative solution is not less than the flow in the original solution. Since the flows are different, there must be at least a single edge where the flow in the alternative solution increased. Without a different edge decreasing the flow, though, there is either a violation of the conservation of flow, or the original solution was suboptimal. Hence there is some edge *e* where the flow in the alternative solution is lower than in the original solution. Since it is an integer flow network, the flow must be at least 1 lower on *e*. By definition, though, reducing the capacity of *e* to at least 1 lower than the current flow, will not make the alternative flow illegal. Hence some alternative flow must be found if the capacity is decreased for *e*.\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/80Ar3.jpg\\n\\n  [2]: https://i.stack.imgur.com/1TjPo.jpg\",\n",
       "  '<algorithm><graph><max><unique><flow>',\n",
       "  datetime.date(2016, 10, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1303.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2720',\n",
       "  '38565547',\n",
       "  'Answer',\n",
       "  'Why did std::allocator lose member types/functions in C++17?',\n",
       "  'If you look at [the relevant isocpp paper](http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2016/p0174r1.html#2.2) you can see that the first set you mention is now thought to be better placed in [`std::allocator_traits`](http://en.cppreference.com/w/cpp/memory/allocator_traits). Since the STL (not even standard library) came out, there\\'s been more of a shift to use traits.\\n\\n\\n\\n`rebind` is also a relic. When the STL first came out, aliases and template-template parameters were not supported. With these language features in existence, `rebind` seems fairly convoluted. E.g., as you can see in [an answer to this question](https://stackoverflow.com/questions/12362363/why-is-allocatorrebind-necessary-when-we-have-template-template-parameters), in The C++ Programming Language, 4th edition, section 34.4.1, p. 998, commenting the \\'classical\\' rebind member in default allocator class :\\n\\n\\n\\n    template<typename U>\\n\\n         struct rebind { using other = allocator<U>;};\\n\\n\\n\\nBjarne Stroustupr writes this : \"The curious rebind template is an archaic alias. It should have been:\\n\\n\\n\\n    template<typename U>\\n\\n    using other = allocator<U>;\\n\\n\\n\\nHowever, allocator was defined before such aliases were supported by C++.\"\\n\\n\\n\\n\\n\\nSo, altogether, it\\'s the standard library catching up with the language and paradigm shifts.',\n",
       "  '<c++><memory-management><stl><allocator><c++17>',\n",
       "  datetime.date(2016, 7, 25),\n",
       "  '2017-05-23 10:30:45',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '13',\n",
       "  '',\n",
       "  '1338.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2722',\n",
       "  '40091479',\n",
       "  'Question',\n",
       "  'Scipy.optimize - curve fitting with fixed parameters',\n",
       "  \"I'm performing curve fitting with `scipy.optimize.leastsq`. E.g. for a gaussian:\\n\\n\\n\\n    def fitGaussian(x, y, init=[1.0,0.0,4.0,0.1]):\\n\\n        fitfunc = lambda p, x: p[0]*np.exp(-(x-p[1])**2/(2*p[2]**2))+p[3] # Target function\\n\\n        errfunc = lambda p, x, y: fitfunc(p, x) - y # Distance to the target function\\n\\n        final, success = scipy.optimize.leastsq(errfunc, init[:], args=(x, y))\\n\\n        return fitfunc, final\\n\\n\\n\\n\\n\\n\\n\\nNow, I want to optionally fix the values of some of the parameters in the fit. I found that suggestions are to use a different package [lmfit][1], which I want to avoid, or are very general, like [here][2].\\n\\nSince I need a solution which \\n\\n\\n\\n1. works with numpy/scipy (no further packages etc.)\\n\\n2. is independent of the parameters themselves,\\n\\n3. is flexible, in which parameters are fixed or not, \\n\\n\\n\\nI came up with the following, using a condition on each of the parameters:\\n\\n\\n\\n    def fitGaussian2(x, y, init=[1.0,0.0,4.0,0.1], fix = [False, False, False, False]):\\n\\n        fitfunc = lambda p, x: (p[0] if not fix[0] else init[0])*np.exp(-(x-(p[1] if not fix[1] else init[1]))**2/(2*(p[2] if not fix[2] else init[2])**2))+(p[3] if not fix[3] else init[3]) \\n\\n        errfunc = lambda p, x, y: fitfunc(p, x) - y # Distance to the target function\\n\\n        final, success = scipy.optimize.leastsq(errfunc, init[:], args=(x, y))\\n\\n        return fitfunc, final\\n\\n\\n\\n\\n\\nWhile this works fine, it's neither practical, nor beautiful. \\n\\nSo my question is: Are there better ways of performing curve fitting in scipy for fixed parameters? Or are there wrappers, which already include such parameter fixing?\\n\\n\\n\\n  [1]: https://lmfit.github.io/lmfit-py/intro.html\\n\\n  [2]: http://scipy-user.10969.n7.nabble.com/curve-fitting-with-fixed-parameters-td3310.html\",\n",
       "  '<numpy><scipy><curve-fitting>',\n",
       "  datetime.date(2016, 10, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1075.0',\n",
       "  '2.0',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2726',\n",
       "  '39489834',\n",
       "  'Answer',\n",
       "  'Import CSV to pandas with two delimiters',\n",
       "  \"Asides from the other fine answers here, which are more pandas-specific, it should be noted that Python itself is pretty powerful when it comes to string processing. You can just place the result of replacing `';'` with `','` in a [`StringIO`](https://docs.python.org/2/library/stringio.html) object, and work normally from there:\\n\\n\\n\\n    In [8]: import pandas as pd\\n\\n\\n\\n    In [9]: from cStringIO import StringIO\\n\\n\\n\\n    In [10]: pd.read_csv(StringIO(''.join(l.replace(';', ',') for l in open('stuff.csv'))))\\n\\n    Out[10]: \\n\\n                       vin  vorgangid  eventkm  D_8_lamsoni_w_time  \\\\\\n\\n    V345578 295234545   13    -1000.0   -980.0            7.992188   \\n\\n    V346670 329781064   13     -960.0   -940.0            7.992188   \\n\\n\\n\\n                       D_8_lamsoni_w_value  \\n\\n    V345578 295234545            11.984375  \\n\\n    V346670 329781064            11.984375  \\n\\n\",\n",
       "  '<python><csv><pandas><delimiter><csv-import>',\n",
       "  datetime.date(2016, 9, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '3447.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2730',\n",
       "  '40186692',\n",
       "  'Answer',\n",
       "  'Pandas DataFrame with tuple of strings as index',\n",
       "  \"Your tuple in the selection brackets is seen as a sequence containing the elements you want to retrieve. It's like you would have passed `['1', 'b']` as argument. Thus the KeyError message: pandas tries to find the key `'1'` and obviously doesn't find it.\\n\\n\\n\\nThat's why it works when you add additional brackets, as now the argument becomes a sequence of one element - your tuple.\\n\\n\\n\\nYou should avoid dealing with ambiguities around list and tuple arguments in selection. The behavior can be also different depending on the index being a simple index or a multiindex.\\n\\n\\n\\nIn any case, if you ask about recommendations here, the one I see is that you should try to not build simple indexes made of tuples: pandas will work better and will be more powerful to use if you actually build a multiindex instead:\\n\\n\\n\\n    df = pd.DataFrame(columns=['Col 1', 'Col 2', 'Col 3'],\\n\\n                      index=pd.MultiIndex.from_tuples([('1', 'a'), ('2', 'a'), ('1', 'b'), ('2', 'b')]))\\n\\n    \\n\\n    df['Col 2'].loc[('1', 'b')] = 6\\n\\n    \\n\\n    df['Col 2'].loc[('1', 'b')]\\n\\n    Out[13]: 6\\n\\n    \\n\\n    df\\n\\n    Out[14]: \\n\\n        Col 1 Col 2 Col 3\\n\\n    1 a   NaN   NaN   NaN\\n\\n    2 a   NaN   NaN   NaN\\n\\n    1 b   NaN     6   NaN\\n\\n    2 b   NaN   NaN   NaN\",\n",
       "  '<python><pandas><indexing>',\n",
       "  datetime.date(2016, 10, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '5441.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2739',\n",
       "  '40836567',\n",
       "  'Answer',\n",
       "  'Removing parenthesis from a string in pandas with str.replace',\n",
       "  '`str.replace` uses regex to perform replacements. The parentheses must be escaped to keep them as simple characters:\\n\\n\\n\\n    energy[\\'Country\\'].str.replace(\"Bolivia \\\\(Plurinational State of\\\\)\",\"Bolivia\")\\n\\n\\n\\nYou can automate escaping like this:\\n\\n\\n\\n    import re\\n\\n    energy[\\'Country\\'].str.replace(re.escape(\\'Bolivia (Plurinational State of)\\'),\"Bolivia\")\\n\\n',\n",
       "  '<regex><pandas>',\n",
       "  datetime.date(2016, 11, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '6699.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2743',\n",
       "  '40861804',\n",
       "  'Answer',\n",
       "  '3D discrete heatmap in matplotlib',\n",
       "  '###New answer:\\n\\nIt seems we really want to have a 3D Tetris game here ;-)\\n\\n\\n\\nSo here is a way to plot cubes of different color to fill the space given by the arrays `(x,y,z)`. \\n\\n\\n\\n    from mpl_toolkits.mplot3d import Axes3D\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    import matplotlib.cm\\n\\n    import matplotlib.colorbar\\n\\n    import matplotlib.colors\\n\\n    \\n\\n    def cuboid_data(center, size=(1,1,1)):\\n\\n        # code taken from\\n\\n        # http://stackoverflow.com/questions/30715083/python-plotting-a-wireframe-3d-cuboid?noredirect=1&lq=1\\n\\n        # suppose axis direction: x: to left; y: to inside; z: to upper\\n\\n        # get the (left, outside, bottom) point\\n\\n        o = [a - b / 2 for a, b in zip(center, size)]\\n\\n        # get the length, width, and height\\n\\n        l, w, h = size\\n\\n        x = [[o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in bottom surface\\n\\n             [o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in upper surface\\n\\n             [o[0], o[0] + l, o[0] + l, o[0], o[0]],  # x coordinate of points in outside surface\\n\\n             [o[0], o[0] + l, o[0] + l, o[0], o[0]]]  # x coordinate of points in inside surface\\n\\n        y = [[o[1], o[1], o[1] + w, o[1] + w, o[1]],  # y coordinate of points in bottom surface\\n\\n             [o[1], o[1], o[1] + w, o[1] + w, o[1]],  # y coordinate of points in upper surface\\n\\n             [o[1], o[1], o[1], o[1], o[1]],          # y coordinate of points in outside surface\\n\\n             [o[1] + w, o[1] + w, o[1] + w, o[1] + w, o[1] + w]]    # y coordinate of points in inside surface\\n\\n        z = [[o[2], o[2], o[2], o[2], o[2]],                        # z coordinate of points in bottom surface\\n\\n             [o[2] + h, o[2] + h, o[2] + h, o[2] + h, o[2] + h],    # z coordinate of points in upper surface\\n\\n             [o[2], o[2], o[2] + h, o[2] + h, o[2]],                # z coordinate of points in outside surface\\n\\n             [o[2], o[2], o[2] + h, o[2] + h, o[2]]]                # z coordinate of points in inside surface\\n\\n        return x, y, z\\n\\n    \\n\\n    def plotCubeAt(pos=(0,0,0), c=\"b\", alpha=0.1, ax=None):\\n\\n        # Plotting N cube elements at position pos\\n\\n        if ax !=None:\\n\\n            X, Y, Z = cuboid_data( (pos[0],pos[1],pos[2]) )\\n\\n            ax.plot_surface(X, Y, Z, color=c, rstride=1, cstride=1, alpha=0.1)\\n\\n    \\n\\n    def plotMatrix(ax, x, y, z, data, cmap=\"jet\", cax=None, alpha=0.1):\\n\\n        # plot a Matrix \\n\\n        norm = matplotlib.colors.Normalize(vmin=data.min(), vmax=data.max())\\n\\n        colors = lambda i,j,k : matplotlib.cm.ScalarMappable(norm=norm,cmap = cmap).to_rgba(data[i,j,k]) \\n\\n        for i, xi in enumerate(x):\\n\\n                for j, yi in enumerate(y):\\n\\n                    for k, zi, in enumerate(z):\\n\\n                        plotCubeAt(pos=(xi, yi, zi), c=colors(i,j,k), alpha=alpha,  ax=ax)\\n\\n        \\n\\n        \\n\\n    \\n\\n        if cax !=None:\\n\\n            cbar = matplotlib.colorbar.ColorbarBase(cax, cmap=cmap,\\n\\n                                    norm=norm,\\n\\n                                    orientation=\\'vertical\\')  \\n\\n            cbar.set_ticks(np.unique(data))\\n\\n            # set the colorbar transparent as well\\n\\n            cbar.solids.set(alpha=alpha)              \\n\\n            \\n\\n              \\n\\n    \\n\\n    if __name__ == \\'__main__\\':\\n\\n        \\n\\n        # x and y and z coordinates\\n\\n        x = np.array(range(10))\\n\\n        y = np.array(range(10,15))\\n\\n        z = np.array(range(15,20))\\n\\n        data_value = np.random.randint(1,4, size=(len(x), len(y), len(z)) )\\n\\n        print data_value.shape\\n\\n        \\n\\n        fig = plt.figure(figsize=(10,4))\\n\\n        ax = fig.add_axes([0.1, 0.1, 0.7, 0.8], projection=\\'3d\\')\\n\\n        ax_cb = fig.add_axes([0.8, 0.3, 0.05, 0.45])\\n\\n        ax.set_aspect(\\'equal\\')\\n\\n    \\n\\n        plotMatrix(ax, x, y, z, data_value, cmap=\"jet\", cax = ax_cb)\\n\\n        \\n\\n        plt.savefig(__file__+\".png\")\\n\\n        plt.show()\\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\nI find it really hard to see anything here, but that may be a question of taste and now hopefully also answers the question.\\n\\n\\n\\n<hR>\\n\\n###Original Answer: \\n\\n*It seems I misunderstood the question. Therefore the following does not answer the question. For the moment I leave it here, to keep the comments below available for others.*\\n\\n\\n\\nI think [`plot_surface`](http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html#surface-plots) is fine for the specified task. \\n\\n\\n\\nEssentially you would plot a surface with the shape given by your points `X,Y,Z` in 3D and colorize it using the values from `data_values` as shown in the code below. \\n\\n\\n\\n\\n\\n    from mpl_toolkits.mplot3d import Axes3D\\n\\n    from matplotlib import cm\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax = fig.gca(projection=\\'3d\\')\\n\\n    \\n\\n    # as plot_surface needs 2D arrays as input\\n\\n    x = np.arange(10)\\n\\n    y = np.array(range(10,15))\\n\\n    # we make a meshgrid from the x,y data\\n\\n    X, Y = np.meshgrid(x, y)\\n\\n    Z = np.sin(np.sqrt(X**2 + Y**2))\\n\\n    \\n\\n    # data_value shall be represented by color\\n\\n    data_value = np.random.rand(len(y), len(x))\\n\\n    # map the data to rgba values from a colormap\\n\\n    colors = cm.ScalarMappable(cmap = \"viridis\").to_rgba(data_value)\\n\\n    \\n\\n    \\n\\n    # plot_surface with points X,Y,Z and data_value as colors\\n\\n    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1, facecolors=colors,\\n\\n                           linewidth=0, antialiased=True)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/mVsjM.png\\n\\n  [2]: https://i.stack.imgur.com/oqFab.png',\n",
       "  '<python><matplotlib><heatmap>',\n",
       "  datetime.date(2016, 11, 29),\n",
       "  '2016-11-29 20:21:38',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '13',\n",
       "  '',\n",
       "  '7953.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2749',\n",
       "  '40256147',\n",
       "  'Answer',\n",
       "  'Get the y value of a given x',\n",
       "  'The following code might do what you want. The interpolation of y(x) is straight forward, as the x-values are monotonically increasing. The problem of finding the x-values for a given y is not so easy anymore, once the function is not monotonically increasing as in this case. So you still need to know roughly where to expect the values to be.\\n\\n\\n\\n    import numpy as np\\n\\n    import scipy.interpolate\\n\\n    import scipy.optimize\\n\\n    \\n\\n    x=np.array([0,1,2,3,4])\\n\\n    y=np.array([5,3,40,20,1])\\n\\n    \\n\\n    #if the independent variable is monotonically increasing\\n\\n    print np.interp(1.3, x, y)\\n\\n    \\n\\n    # if not, as in the case of finding x(y) here,\\n\\n    # we need to find the zeros of an interpolating function\\n\\n    y0 = 30.\\n\\n    initial_guess = 1.5 #for the first zero, \\n\\n    #initial_guess = 3.0 # for the secon zero\\n\\n    f = scipy.interpolate.interp1d(x,y,kind=\"linear\")\\n\\n    fmin = lambda x: np.abs(f(x)-y0)\\n\\n    s = scipy.optimize.fmin(fmin, initial_guess, disp=False)\\n\\n    print s',\n",
       "  '<numpy><matplotlib><scipy>',\n",
       "  datetime.date(2016, 10, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2091.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2752',\n",
       "  '40302225',\n",
       "  'Answer',\n",
       "  'Pandas: make pivot table with percentage',\n",
       "  \"Divide the individual count values obtained with the total number of rows of the `DF` to get it's percentage distribution as shown:\\n\\n\\n\\n    func = lambda x: 100*x.count()/df.shape[0]\\n\\n    pd.pivot_table(df, index='used_at', columns='domain', values='ID', aggfunc=func)\\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/gOylN.png\",\n",
       "  '<python><pandas><unique><pivot-table><percentage>',\n",
       "  datetime.date(2016, 10, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '3835.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2762',\n",
       "  '40332984',\n",
       "  'Answer',\n",
       "  'SUMIF like functions in Pandas',\n",
       "  \"Use a combination of groupby and sum operator:\\n\\n\\n\\n    df.groupby('Date').sum()\\n\\n    Out[34]: \\n\\n              Attribute1  Attribute2  Attribute3\\n\\n    Date                                        \\n\\n    6/2/2014          23          21          24\\n\\n    6/3/2014          11          16          11\\n\\n    6/4/2014           9           7           5\\n\\n    6/5/2014          15          16          16\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 10, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1318.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2767',\n",
       "  '38675092',\n",
       "  'Answer',\n",
       "  'Pivot table error:1 ndim Categorical are not supported at this time',\n",
       "  'You really should be using [`pivot_table`][1] as you have got duplicate entries in your `date` column.\\n\\n\\n\\n    pd.pivot_table(df, values=\\'Score\\', index=[\\'date\\', \\'Cusip\\'], columns=[\\'Label\\']).boxplot()\\n\\n  \\n\\n![alt text](http://i.stack.imgur.com/8nx0m.png \"Resulting Boxplot\")\\n\\n\\n\\n [1]:http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html',\n",
       "  '<python><pandas><pivot>',\n",
       "  datetime.date(2016, 7, 30),\n",
       "  '2016-07-30 14:53:37',\n",
       "  'Nickil Maveli (6207849), Alberto Garcia-Raboso (509824)',\n",
       "  '2',\n",
       "  '',\n",
       "  '2052.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2768',\n",
       "  '38678718',\n",
       "  'Answer',\n",
       "  'Convert strings in column into categorical variable',\n",
       "  \"As @ayhan noted in the comments, you probably want to use [dummy variables](http://www.socialresearchmethods.net/kb/dummyvar.php) here. This is because it seems highly unlikely from your data that there is really any ordering in your text labels. \\n\\n\\n\\nThis can easily be done via [`pandas.get_dummies`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.get_dummies.html), e.g.:\\n\\n\\n\\n    pd.get_dummies(df.C1)\\n\\n\\n\\nNote that this returns a regular DataFrame:\\n\\n\\n\\n    >>> pd.get_dummies(df.C1).columns\\n\\n    Index([u'05db9164', u'1464facd', u'241546e0', u'287e684f', u'3c9d8785',\\n\\n         u'439a44a4', u'5a9ed9b0', u'68fd1e64', u'8cf07265', u'be589b51'],\\n\\n         dtype='object')\\n\\n\\n\\nYou'd probably want to use this with a horizontal [`concat`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html), therefore.\\n\\n\\n\\n\\n\\n----------------\\n\\n\\n\\nIf you actually are actually looking to transform the labels into something numeric (which does not seem likely), you might look at [`sklearn.preprocessing.LabelEncoder`](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html).\",\n",
       "  '<python><string><pandas><statistics><categorical-data>',\n",
       "  datetime.date(2016, 7, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '4011.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2769',\n",
       "  '38682103',\n",
       "  'Answer',\n",
       "  'pandas not condition with filtering',\n",
       "  \"IIUC, you can use [`isin`][1] to check for bool conditions and take only the `NOT(~)` values of the grouped dataframe:   \\n\\n\\n\\n     df[~df.isin(grouped.filter(lambda x: (len(x) == 1 and x['template_fk'] == exterior_template)))]\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.isin.html\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 7, 31),\n",
       "  '2016-07-31 11:45:46',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '5',\n",
       "  '',\n",
       "  '2890.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2777',\n",
       "  '40467529',\n",
       "  'Answer',\n",
       "  'Add columns in pandas dataframe dynamically',\n",
       "  \"If the list of indices is `l`, you can use [`pd.Series.cat`](http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.Series.str.cat.html):\\n\\n\\n\\n    df[df.columns[l[0]]].astype(str).str.cat([df[df.columns[i]].astype(str) for i in l[1: ]], sep=',')\\n\\n\\n\\n-----------------\\n\\n\\n\\n**Example**\\n\\n\\n\\n    In [18]: df = pd.DataFrame({'a': [1, 2], 'b': [2, 'b'], 'c': [3, 'd']})\\n\\n\\n\\n    In [19]: df[df.columns[l[0]]].astype(str).str.cat([df[df.columns[i]].astype(str) for i in l[1: ]], sep=',')\\n\\n    Out[19]: \\n\\n    0    1,2\\n\\n    1    2,b\\n\\n    Name: a, dtype: object\\n\\n\",\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 11, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '2510.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2781',\n",
       "  '40348884',\n",
       "  'Answer',\n",
       "  'Pandas \"diff()\" with string',\n",
       "  \"I get better performance with `ne` instead of using the actual `!=` comparison:\\n\\n\\n\\n    df['changed'] = df['ColumnB'].ne(df['ColumnB'].shift().bfill()).astype(int)\\n\\n\\n\\n**Timings**\\n\\n\\n\\nUsing the following setup to produce a larger dataframe:\\n\\n\\n\\n    df = pd.concat([df]*10**5, ignore_index=True) \\n\\n\\n\\nI get the following timings:\\n\\n\\n\\n    %timeit df['ColumnB'].ne(df['ColumnB'].shift().bfill()).astype(int)\\n\\n    10 loops, best of 3: 38.1 ms per loop\\n\\n    \\n\\n    %timeit (df.ColumnB != df.ColumnB.shift()).astype(int)\\n\\n    10 loops, best of 3: 77.7 ms per loop\\n\\n    \\n\\n    %timeit df['ColumnB'] == df['ColumnB'].shift(1).fillna(df['ColumnB'])\\n\\n    10 loops, best of 3: 99.6 ms per loop\\n\\n\\n\\n    %timeit (df.ColumnB.ne(df.ColumnB.shift())).astype(int)\\n\\n    10 loops, best of 3: 19.3 ms per loop\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 10, 31),\n",
       "  '2016-10-31 19:18:22',\n",
       "  'root (3339965)',\n",
       "  '14',\n",
       "  '',\n",
       "  '3515.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2788',\n",
       "  '40923440',\n",
       "  'Answer',\n",
       "  'How do I make the width of the title box span the entire plot?',\n",
       "  'It is of course possible to get the bounding box of the title, which is a `Text` element. This can be done with \\n\\n\\n\\n    title = ax.set_title(...) \\n\\n    bb = title.get_bbox_patch() \\n\\n\\n\\nIn principle, one can then manipulate the bounding box, e.g. via \\n\\n`bb.set_width(...)`. However all settings are lost, once matplotlib draws the title to the canvas. At least this is how I interprete the `Text`\\'s `draw()` method.\\n\\n\\n\\nI\\'m not aware of other methods of setting the bounding box. For example a `legend`\\'s bounding box can be set via  \\n\\n`plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3, mode=\"expand\")` such that it expands over the full axes range (see [here](http://matplotlib.org/users/legend_guide.html#legend-location)). It would be very useful to have the same option for `Text` as well. But as for now, we don\\'t.\\n\\n\\n\\nThe `Text` object allows to set a [`bbox`](http://matplotlib.org/api/text_api.html#matplotlib.text.Text) argument which is normally meant for setting the style of the bounding box. There is no way to set the bounding box extents, but it accepts some dictionary of properties of the surrounding box. And one of the accepted properties is a [`boxstyle`](http://matplotlib.org/users/annotations_guide.html#annotating-axes). Per default this is a `square`, but can be set to a circle or arrow or other strange shapes. \\n\\n\\n\\nThose `boxstyle`s are actually the key to a possible solution. They all inherit from `BoxStyle._Base` and - as can be seen at [the bottom of the annotations guide](http://matplotlib.org/users/annotations_guide.html#define-custom-boxstyle) - one can define a custom shape, subclassing `BoxStyle._Base`. \\n\\n\\n\\n**The following solution is based on subclassing `BoxStyle._Base` in a way that it accepts the width of the axes as argument and draws the title\\'s rectangle path such that it has exactly this width.**\\n\\n\\n\\nAs a bonus we can register an event handler such that this width, once it changes due to resizing of the window, is adapted.\\n\\n\\n\\nHere is the code:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import pandas as pd\\n\\n    import numpy as np\\n\\n    \\n\\n    from matplotlib.path import Path\\n\\n    from matplotlib.patches import BoxStyle\\n\\n    \\n\\n    \\n\\n    class ExtendedTextBox(BoxStyle._Base):\\n\\n        \"\"\"\\n\\n        An Extended Text Box that expands to the axes limits \\n\\n                            if set in the middle of the axes\\n\\n        \"\"\"\\n\\n    \\n\\n        def __init__(self, pad=0.3, width=500.):\\n\\n            \"\"\"\\n\\n            width: \\n\\n                width of the textbox. \\n\\n                Use `ax.get_window_extent().width` \\n\\n                       to get the width of the axes.\\n\\n            pad: \\n\\n                amount of padding (in vertical direction only)\\n\\n            \"\"\"\\n\\n            self.width=width\\n\\n            self.pad = pad\\n\\n            super(ExtendedTextBox, self).__init__()\\n\\n    \\n\\n        def transmute(self, x0, y0, width, height, mutation_size):\\n\\n            \"\"\"\\n\\n            x0 and y0 are the lower left corner of original text box\\n\\n            They are set automatically by matplotlib\\n\\n            \"\"\"\\n\\n            # padding\\n\\n            pad = mutation_size * self.pad\\n\\n    \\n\\n            # we add the padding only to the box height\\n\\n            height = height + 2.*pad\\n\\n            # boundary of the padded box\\n\\n            y0 = y0 - pad\\n\\n            y1 = y0 + height\\n\\n            _x0 = x0\\n\\n            x0 = _x0 +width /2. - self.width/2.\\n\\n            x1 = _x0 +width /2. + self.width/2.\\n\\n    \\n\\n            cp = [(x0, y0),\\n\\n                  (x1, y0), (x1, y1), (x0, y1),\\n\\n                  (x0, y0)]\\n\\n    \\n\\n            com = [Path.MOVETO,\\n\\n                   Path.LINETO, Path.LINETO, Path.LINETO,\\n\\n                   Path.CLOSEPOLY]\\n\\n    \\n\\n            path = Path(cp, com)\\n\\n    \\n\\n            return path\\n\\n    \\n\\n    dpi = 80\\n\\n    \\n\\n    # register the custom style\\n\\n    BoxStyle._style_list[\"ext\"] = ExtendedTextBox\\n\\n    \\n\\n    plt.figure(dpi=dpi)\\n\\n    s = pd.Series(np.random.lognormal(.001, .01, 100))\\n\\n    ax = s.cumprod().plot()\\n\\n    # set the title position to the horizontal center (0.5) of the axes\\n\\n    title = ax.set_title(\\'My Log Normal Example\\', position=(.5, 1.02), \\n\\n                 backgroundcolor=\\'black\\', color=\\'white\\')\\n\\n    # set the box style of the title text box toour custom box\\n\\n    bb = title.get_bbox_patch()\\n\\n    # use the axes\\' width as width of the text box\\n\\n    bb.set_boxstyle(\"ext\", pad=0.4, width=ax.get_window_extent().width )\\n\\n    \\n\\n    \\n\\n    # Optionally: use eventhandler to resize the title box, in case the window is resized\\n\\n    def on_resize(event):\\n\\n        print \"resize\"\\n\\n        bb.set_boxstyle(\"ext\", pad=0.4, width=ax.get_window_extent().width )\\n\\n        \\n\\n    cid = plt.gcf().canvas.mpl_connect(\\'resize_event\\', on_resize)\\n\\n    \\n\\n    # use the same dpi for saving to file as for plotting on screen\\n\\n    plt.savefig(__file__+\".png\", dpi=dpi)\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n<hr>\\n\\n\\n\\nJust in case someone is interested in a lighter solution, there is also the option to play around with the `mutation_aspect` of the title\\'s bounding box, which is apparently left unchanged when drawing the title. While the `mutation_aspect` itself basically only changes the height of the box, one can use an extremely large padding for the box and set `mutation_aspect` to a very small number such that at the end the box appears extended in width. The clear drawback of this solution is, that the values for the padding and aspect have to be found by trial and error and will change for different font and figure sizes.\\n\\nIn my case the values of `mutation_aspect = 0.04` and `pad=11.9` produce the desired result, but on other systems they may of course be different.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import pandas as pd\\n\\n    import numpy as np\\n\\n    \\n\\n    s = pd.Series(np.random.lognormal(.001, .01, 100))\\n\\n    ax = s.cumprod().plot()\\n\\n    title = ax.set_title(\\'My Log Normal Example\\', position=(.5, 1.02),\\n\\n                 backgroundcolor=\\'black\\', color=\\'white\\',\\n\\n                 verticalalignment=\"bottom\", horizontalalignment=\"center\")\\n\\n    title._bbox_patch._mutation_aspect = 0.04\\n\\n    title.get_bbox_patch().set_boxstyle(\"square\", pad=11.9)\\n\\n    plt.tight_layout()\\n\\n    plt.savefig(__file__+\".png\")\\n\\n    plt.show()\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/IG7CM.png\\n\\n',\n",
       "  '<python><pandas><matplotlib>',\n",
       "  datetime.date(2016, 12, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '22',\n",
       "  '',\n",
       "  '2106.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2791',\n",
       "  '40952872',\n",
       "  'Answer',\n",
       "  \"Python equivalent for Matlab's Demcmap (elevation +/- appropriate colormap)\",\n",
       "  'Unfortunaly, matplotlib does not provide the functionality of Matlab\\'s `demcmap`.\\n\\nThere might actually be some build-in features in the python `basemap` package, of which I\\'m not aware.\\n\\n\\n\\nSo, sticking to matplotlib on-board options, we can subclass [`Normalize`](http://matplotlib.org/users/colormapnorms.html) to build a color normalization centered around a point in the middle of the colormap. This technique can be found in [another question](https://stackoverflow.com/questions/20144529/shifted-colorbar-matplotlib) on StackOverflow and adapted to the specific needs, namely to set a `sealevel` (which is probably best chosen as `0`) and the value in the colormap `col_val` (ranging between 0 and 1) to which this sealevel should correspond. In the case of the terrain map, it seems that `0.22`, corresponding to a turqoise color, might be a good choice.  \\n\\nThe Normalize instance can then be given as an argument to `imshow`. The resulting figures can be seen down below in the first row of the picture.  \\n\\n\\n\\nDue to the smooth transition around the sealevel the values around `0` appear in a turqoise color, making it hard to distinguish between land and sea.  \\n\\nWe can therefore change the terrain map a bit and cut out those colors, such that the coastline is better visible. This is done by [combining two parts](https://stackoverflow.com/questions/31051488/combining-two-matplotlib-colormaps) of the map, ranging from 0 to 0.17 and from 0.25 to 1, and thus cutting out a part of it.\\n\\n\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    import matplotlib.colors\\n\\n    \\n\\n    class FixPointNormalize(matplotlib.colors.Normalize):\\n\\n        \"\"\" \\n\\n        Inspired by https://stackoverflow.com/questions/20144529/shifted-colorbar-matplotlib\\n\\n        Subclassing Normalize to obtain a colormap with a fixpoint \\n\\n        somewhere in the middle of the colormap.\\n\\n        \\n\\n        This may be useful for a `terrain` map, to set the \"sea level\" \\n\\n        to a color in the blue/turquise range. \\n\\n        \"\"\"\\n\\n        def __init__(self, vmin=None, vmax=None, sealevel=0, col_val = 0.21875, clip=False):\\n\\n            # sealevel is the fix point of the colormap (in data units)\\n\\n            self.sealevel = sealevel\\n\\n            # col_val is the color value in the range [0,1] that should represent the sealevel.\\n\\n            self.col_val = col_val\\n\\n            matplotlib.colors.Normalize.__init__(self, vmin, vmax, clip)\\n\\n    \\n\\n        def __call__(self, value, clip=None):\\n\\n            x, y = [self.vmin, self.sealevel, self.vmax], [0, self.col_val, 1]\\n\\n            return np.ma.masked_array(np.interp(value, x, y))\\n\\n    \\n\\n    # Combine the lower and upper range of the terrain colormap with a gap in the middle\\n\\n    # to let the coastline appear more prominently.\\n\\n    # inspired by https://stackoverflow.com/questions/31051488/combining-two-matplotlib-colormaps\\n\\n    colors_undersea = plt.cm.terrain(np.linspace(0, 0.17, 56))\\n\\n    colors_land = plt.cm.terrain(np.linspace(0.25, 1, 200))\\n\\n    # combine them and build a new colormap\\n\\n    colors = np.vstack((colors_undersea, colors_land))\\n\\n    cut_terrain_map = matplotlib.colors.LinearSegmentedColormap.from_list(\\'cut_terrain\\', colors)\\n\\n    \\n\\n    \\n\\n    \\n\\n    # invent some data (height in meters relative to sea level)\\n\\n    data = np.linspace(-1000,2400,15**2).reshape((15,15))\\n\\n    \\n\\n    \\n\\n    # plot example data\\n\\n    fig, ax = plt.subplots(nrows = 2, ncols=3, figsize=(11,6) )\\n\\n    plt.subplots_adjust(left=0.08, right=0.95, bottom=0.05, top=0.92, hspace = 0.28, wspace = 0.15)\\n\\n    \\n\\n    plt.figtext(.5, 0.95, \"Using \\'terrain\\' and FixedPointNormalize\", ha=\"center\", size=14)\\n\\n    norm = FixPointNormalize(sealevel=0, vmax=3400)\\n\\n    im = ax[0,0].imshow(data+1000, norm=norm, cmap=plt.cm.terrain)\\n\\n    fig.colorbar(im, ax=ax[0,0])\\n\\n    \\n\\n    norm2 = FixPointNormalize(sealevel=0, vmax=3400)\\n\\n    im2 = ax[0,1].imshow(data, norm=norm2, cmap=plt.cm.terrain)\\n\\n    fig.colorbar(im2, ax=ax[0,1])\\n\\n    \\n\\n    norm3 = FixPointNormalize(sealevel=0, vmax=0)\\n\\n    im3 = ax[0,2].imshow(data-2400.1, norm=norm3, cmap=plt.cm.terrain)\\n\\n    fig.colorbar(im3, ax=ax[0,2])\\n\\n    \\n\\n    plt.figtext(.5, 0.46, \"Using custom cut map and FixedPointNormalize (adding hard edge between land and sea)\", ha=\"center\", size=14)\\n\\n    norm4 = FixPointNormalize(sealevel=0, vmax=3400)\\n\\n    im4 = ax[1,0].imshow(data+1000, norm=norm4, cmap=cut_terrain_map)\\n\\n    fig.colorbar(im4, ax=ax[1,0])\\n\\n    \\n\\n    norm5 = FixPointNormalize(sealevel=0, vmax=3400)\\n\\n    im5 = ax[1,1].imshow(data, norm=norm5, cmap=cut_terrain_map)\\n\\n    cbar = fig.colorbar(im5, ax=ax[1,1])\\n\\n    \\n\\n    norm6 = FixPointNormalize(sealevel=0, vmax=0)\\n\\n    im6 = ax[1,2].imshow(data-2400.1, norm=norm6, cmap=cut_terrain_map)\\n\\n    fig.colorbar(im6, ax=ax[1,2])\\n\\n    \\n\\n    for i, name in enumerate([\"land only\", \"coast line\", \"sea only\"]):\\n\\n        for j in range(2):\\n\\n            ax[j,i].text(0.96,0.96,name, ha=\"right\", va=\"top\", transform=ax[j,i].transAxes, color=\"w\" )\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/nchSZ.png',\n",
       "  '<python><matlab><matplotlib><colormap><topography>',\n",
       "  datetime.date(2016, 12, 3),\n",
       "  '2017-05-23 11:54:46',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '7',\n",
       "  '',\n",
       "  '1069.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2795',\n",
       "  '40358227',\n",
       "  'Answer',\n",
       "  'Python Min and Max range for Color bar on Matplotlib Contour Graph',\n",
       "  \"I think the question is indeed valid.\\n\\n@Fatma90: You need to provide a working example, providing x,y,z in your case.\\n\\n\\n\\nAnyways, we can invent some values ourselves. So the problem is, that vmin and vmax are simply ignored by `plt.tricontourf()` and I don't know any good solution for that.\\n\\n\\n\\nHowever here is a workaround, manually setting the `levels`\\n\\n\\n\\n    plt.tricontourf(x, y, z, levels=np.linspace(0,0.12,11), cmap='Blues' )\\n\\n\\n\\nHere we use 10 different levels, which looks nicely (a problem might be to have nice tickmarks, if different number of levels are used).\\n\\n\\n\\nI provide a working example to see the effect:\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    #random numbers for tricontourf plot\\n\\n    x = (np.random.ranf(100)-0.5)*2.\\n\\n    y = (np.random.ranf(100)-0.5)*2.\\n\\n    #uniform number grid for pcolor\\n\\n    X, Y = np.meshgrid(np.linspace(-1,1), np.linspace(-1,1))\\n\\n    \\n\\n    z = lambda x,y : np.exp(-x**2 - y**2)*0.12\\n\\n    \\n\\n    fig, ax = plt.subplots(2,1)\\n\\n    \\n\\n    # tricontourf ignores the vmin, vmax, so we need to manually set the levels\\n\\n    # in this case we use 11-1=10 equally spaced levels.\\n\\n    im = ax[0].tricontourf(x, y, z(x,y), levels=np.linspace(0,0.12,11), cmap='Blues' )\\n\\n    # pcolor works as expected\\n\\n    im2 = ax[1].pcolor(z(X,Y), cmap='Blues', vmin=0, vmax=0.12 )\\n\\n    \\n\\n    plt.colorbar(im, ax=ax[0])\\n\\n    plt.colorbar(im2, ax=ax[1])\\n\\n    \\n\\n    for axis in ax:\\n\\n        axis.set_yticks([])\\n\\n        axis.set_xticks([])\\n\\n    plt.tight_layout()\\n\\n    plt.show()\\n\\n\\n\\nThis produces\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/LyX0a.png\",\n",
       "  '<python><matplotlib><colors><contour><colorbar>',\n",
       "  datetime.date(2016, 11, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '3458.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2797',\n",
       "  '40364737',\n",
       "  'Answer',\n",
       "  'Python pandas select rows by list of dates',\n",
       "  'Convert your entry into a DateTimeIndex:\\n\\n\\n\\n    df.loc[pd.to_datetime(myDates)]\\n\\n    \\n\\n                       A         B         C         D\\n\\n    2013-01-02 -0.047710 -1.827593 -0.944548 -0.149460\\n\\n    2013-01-04  1.437924  0.126788  0.641870  0.198664\\n\\n    2013-01-06  0.408820 -1.842112 -0.287346  0.071397',\n",
       "  '<python><pandas><select><time-series>',\n",
       "  datetime.date(2016, 11, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1330.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2815',\n",
       "  '40979878',\n",
       "  'Answer',\n",
       "  'Compare 2 consecutive rows and assign increasing value if different (using Pandas)',\n",
       "  \"Use [`shift`][1] and [`any`][2] to compare consecutive rows, using `True` to indicate where the value should change.  Then take the cumulative sum with [`cumsum`][3] to get the increasing value:\\n\\n\\n\\n    df_in['value'] = (df_in[['A', 'B']] != df_in[['A', 'B']].shift()).any(axis=1)\\n\\n    df_in['value'] = df_in['value'].cumsum()\\n\\n\\n\\nAlternatively, condensing it to one line:\\n\\n\\n\\n    df_in['value'] = (df_in[['A', 'B']] != df_in[['A', 'B']].shift()).any(axis=1).cumsum()\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n         A    B   C  value\\n\\n    0   aa  200  da      1\\n\\n    1   aa  200  cs      1\\n\\n    2   bb  200  fr      2\\n\\n    3   cc  400  fs      3\\n\\n    4   cc  400  se      3\\n\\n    5   cc  500  at      4\\n\\n    6   cc  700  yu      5\\n\\n    7   dd  700  j5      6\\n\\n    8   dd  900  31      7\\n\\n    9   dd  900  ds      7\\n\\n    10  ee  200  sz      8\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.shift.html\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.any.html\\n\\n  [3]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.cumsum.html\",\n",
       "  '<python><pandas><dataframe><replace><compare>',\n",
       "  datetime.date(2016, 12, 5),\n",
       "  '2016-12-05 17:29:03',\n",
       "  'root (3339965)',\n",
       "  '7',\n",
       "  '',\n",
       "  '2273.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2819',\n",
       "  '38764537',\n",
       "  'Answer',\n",
       "  'Pandas Divide dataframe by index values',\n",
       "  \"Another way of doing this is \\n\\n\\n\\n    df.div(df.index.values, axis=0)\\n\\n\\n\\nExample:\\n\\n\\n\\n\\n\\n    In [7]: df = pd.DataFrame({'a': range(5), 'b': range(1, 6), 'c': range(2, 7)}).set_index('a')\\n\\n\\n\\n    In [8]: df.divide(df.index.values, axis=0)\\n\\n    Out[8]: \\n\\n              b         c\\n\\n    a                    \\n\\n    0       inf       inf\\n\\n    1  2.000000  3.000000\\n\\n    2  1.500000  2.000000\\n\\n    3  1.333333  1.666667\\n\\n    4  1.250000  1.500000\\n\\n\",\n",
       "  '<python><pandas><indexing><dataframe>',\n",
       "  datetime.date(2016, 8, 4),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1083.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2822',\n",
       "  '40438856',\n",
       "  'Answer',\n",
       "  'Pandas sort row values',\n",
       "  'Starting from `0.19.0`, you could sort the columns based on row values.\\n\\n\\n\\n    df.sort_values(by=1, ascending=False, axis=1)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n----------\\n\\n***Bar chart:***\\n\\n\\n\\nUsing ggplot:\\n\\n\\n\\n    melt_df = pd.melt(df, var_name=\\'Cols\\')\\n\\n    ggplot(aes(x=\"Cols\", weight=\"value\"), melt_df) + geom_bar()\\n\\n\\n\\n[![Image][2]][2]\\n\\n\\n\\nUsing built-in:\\n\\n\\n\\n    melt_df.plot.bar(x=[\\'Cols\\'], y=[\\'value\\'], legend=False, cmap=plt.cm.Spectral)\\n\\n    plt.show()\\n\\n\\n\\n[![Image][3]][3]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/slc6X.png\\n\\n  [2]: https://i.stack.imgur.com/OsOIc.png\\n\\n  [3]: https://i.stack.imgur.com/h50hO.png',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 11, 5),\n",
       "  '2016-11-05 14:09:10',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '9',\n",
       "  '',\n",
       "  '6386.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2832',\n",
       "  '40539507',\n",
       "  'Answer',\n",
       "  'Merging multiindex dataframe in pandas',\n",
       "  \"Don't use `axis=1` when using [`concat`][1], as it means appending column-wise, not row-wise.  You want `axis=0` for row-wise, which happens to be the default, so you don't need to specify it:\\n\\n\\n\\n    df3 = pd.concat([df, df2]).sort_index()\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n                           col1\\n\\n    interval device stats      \\n\\n    Day 1    D1     Mean      1\\n\\n                    Ratio   100\\n\\n                    StDev     2\\n\\n                    StErr     3\\n\\n             D2     Mean      4\\n\\n                    Ratio   200\\n\\n                    StDev     5\\n\\n                    StErr     6\\n\\n    Day 2    D1     Mean      7\\n\\n                    Ratio   300\\n\\n                    StDev     8\\n\\n                    StErr     9\\n\\n             D2     Mean     10\\n\\n                    Ratio   400\\n\\n                    StDev    11\\n\\n                    StErr    12\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.concat.html\",\n",
       "  '<python><python-3.x><pandas>',\n",
       "  datetime.date(2016, 11, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '6285.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2836',\n",
       "  '40569207',\n",
       "  'Answer',\n",
       "  'Python pandas dataframe: find max for each unique values of an another column',\n",
       "  \"Sample data (note that you posted an image which can't be used by potential answerers without retyping, so I'm making a simple example in its place):\\n\\n\\n\\n    df=pd.DataFrame({ 'id':[1,1,1,1,2,2,2,2],\\n\\n                       'a':range(8), 'b':range(8,0,-1) })\\n\\n    \\n\\nThe key to this is just using `idxmax` and `idxmin` and then futzing with the indexes so that you can merge things in a readable way.  Here's the whole answer and you may wish to examine intermediate dataframes to see how this is working.\\n\\n\\n\\n    df_max = df.groupby('id').idxmax()\\n\\n    df_max['type'] = 'max'\\n\\n    df_min = df.groupby('id').idxmin()\\n\\n    df_min['type'] = 'min'\\n\\n    \\n\\n    df2 = df_max.append(df_min).set_index('type',append=True).stack().rename('index')\\n\\n\\n\\n    df3 = pd.concat([ df2.reset_index().drop('id',axis=1).set_index('index'), \\n\\n                      df.loc[df2.values] ], axis=1 )\\n\\n\\n\\n    df3.set_index(['id','level_2','type']).sort_index()\\n\\n     \\n\\n                     a  b\\n\\n    id level_2 type      \\n\\n    1  a       max   3  5\\n\\n               min   0  8\\n\\n       b       max   0  8\\n\\n               min   3  5\\n\\n    2  a       max   7  1\\n\\n               min   4  4\\n\\n       b       max   4  4\\n\\n               min   7  1\\n\\n\\n\\nNote in particular that df2 looks like this:\\n\\n\\n\\n    id  type   \\n\\n    1   max   a    3\\n\\n              b    0\\n\\n    2   max   a    7\\n\\n              b    4\\n\\n    1   min   a    0\\n\\n              b    3\\n\\n    2   min   a    4\\n\\n              b    7\\n\\n\\n\\nThe last column there holds the index values in `df` that were derived with `idxmax` & `idxmin`.  So basically all the information you need is in `df2`.  The rest of it is just a matter of merging back with `df` and making it more readable.\",\n",
       "  '<python><pandas><dataframe><grouping>',\n",
       "  datetime.date(2016, 11, 13),\n",
       "  '2016-11-13 17:40:00',\n",
       "  'JohnE (3877338)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2140.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2838',\n",
       "  '41089685',\n",
       "  'Answer',\n",
       "  'How to have actual values in matplotlib Pie Chart displayed (Python)?',\n",
       "  '### Using the `autopct` keyword\\n\\nAs we know that the percentage shown times the sum of all actual values must be the actual value, we can define this as a function and supply this function to `plt.pie` using the `autopct` keyword.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy\\n\\n    \\n\\n    labels = \\'Frogs\\', \\'Hogs\\', \\'Dogs\\'\\n\\n    sizes = numpy.array([5860, 677, 3200])\\n\\n    colors = [\\'yellowgreen\\', \\'gold\\', \\'lightskyblue\\']\\n\\n    \\n\\n    def absolute_value(val):\\n\\n        a  = numpy.round(val/100.*sizes.sum(), 0)\\n\\n        return a\\n\\n    \\n\\n    plt.pie(sizes, labels=labels, colors=colors,\\n\\n            autopct=absolute_value, shadow=True)\\n\\n    \\n\\n    plt.axis(\\'equal\\')\\n\\n    plt.show()\\n\\n\\n\\nCare must be taken since the calculation involves some error, so the supplied value is only accurate to some decimal places. \\n\\n\\n\\nA little bit more advanced may be the following function, that tries to get the original value from the input array back by comparing the difference between the calculated value and the input array. This method does not have the problem of inaccuracy but relies on input values which are sufficiently distinct from one another.\\n\\n\\n\\n    def absolute_value2(val):\\n\\n        a  = sizes[ numpy.abs(sizes - val/100.*sizes.sum()).argmin() ]\\n\\n        return a\\n\\n\\n\\n### Changing text after pie creation\\n\\n\\n\\nThe other option is to first let the pie being drawn with the percentage values and replace them afterwards. To this end, one would store the autopct labels returned by `plt.pie()` and loop over them to replace the text with the values from the original array. Attention, `plt.pie()` only returns three arguments, the last one being the labels of interest, when `autopct` keyword is provided so we set it to an empty string here.\\n\\n\\n\\n    labels = \\'Frogs\\', \\'Hogs\\', \\'Dogs\\'\\n\\n    sizes = numpy.array([5860, 677, 3200])\\n\\n    colors = [\\'yellowgreen\\', \\'gold\\', \\'lightskyblue\\']\\n\\n    \\n\\n    p, tx, autotexts = plt.pie(sizes, labels=labels, colors=colors,\\n\\n            autopct=\"\", shadow=True)\\n\\n    \\n\\n    for i, a in enumerate(autotexts):\\n\\n        a.set_text(\"{}\".format(sizes[i]))\\n\\n    \\n\\n    plt.axis(\\'equal\\')\\n\\n    plt.show()',\n",
       "  '<python><matplotlib><graph><charts><pie-chart>',\n",
       "  datetime.date(2016, 12, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '8772.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2845',\n",
       "  '41110568',\n",
       "  'Answer',\n",
       "  'Python Pandas: Find the maximum for each row in a dataframe column containing a numpy array',\n",
       "  \"I would just forget the 'max_val_idx' column.  I don't think it saves time and actually is more of a pain for syntax.  Sample data:\\n\\n \\n\\n    df = pd.DataFrame({ 'x': range(3) }).applymap( lambda x: np.random.randn(3) )\\n\\n\\n\\n                                                       x\\n\\n    0  [-1.17106202376, -1.61211460669, 0.0198122724315]\\n\\n    1    [0.806819945736, 1.49139051675, -0.21434675401]\\n\\n    2  [-0.427272615966, 0.0939459129359, 0.496474566...\\n\\n\\n\\nYou could extract the max like this:\\n\\n\\n\\n    df.applymap( lambda x: x.max() )\\n\\n   \\n\\n              x  \\n\\n    0  0.019812\\n\\n    1  1.491391\\n\\n    2  0.496475\\n\\n\\n\\nBut generally speaking, life is easier if you have one number per cell.  If each cell has an array of length 3, you could rearrange like this:\\n\\n\\n\\n    for i, v in enumerate(list('abc')): df[v] = df.x.map( lambda x: x[i] )\\n\\n    df = df[list('abc')]\\n\\n\\n\\n              a         b         c\\n\\n    0 -1.171062 -1.612115  0.019812\\n\\n    1  0.806820  1.491391 -0.214347\\n\\n    2 -0.427273  0.093946  0.496475\\n\\n\\n\\nAnd then do a standard pandas operation:\\n\\n\\n\\n    df.apply( max, axis=1 )\\n\\n\\n\\n              x  \\n\\n    0  0.019812\\n\\n    1  1.491391\\n\\n    2  0.496475\\n\\n\\n\\nAdmittedly, this is not much easier than above, but overall the data will be much easier to work with in this form.\\n\\n\",\n",
       "  '<python><pandas><numpy>',\n",
       "  datetime.date(2016, 12, 12),\n",
       "  '2016-12-12 22:54:08',\n",
       "  'JohnE (3877338)',\n",
       "  '4',\n",
       "  '',\n",
       "  '7308.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2850',\n",
       "  '38851432',\n",
       "  'Answer',\n",
       "  'Adding a column header to a csv in python',\n",
       "  \"You could use [`header`][1] parameter in `to_csv` as you have just 1 column in your dataframe.\\n\\n\\n\\n    df = pd.read_csv(data, header=None)\\n\\n    df.to_csv('out.csv', header=['domain'], index=False)\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\",\n",
       "  '<python><csv><pandas>',\n",
       "  datetime.date(2016, 8, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2982.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2851',\n",
       "  '38857733',\n",
       "  'Answer',\n",
       "  'What does the group_keys argument to pandas.groupby actually do?',\n",
       "  \"  `group_keys` parameter in [`groupby`][1] comes handy during [`apply`][2] operations that creates an additional index column corresponding to the grouped columns[`group_keys=True`] and eliminates in the case[`group_keys=False`] especially during the case when trying to perform operations on individual columns.\\n\\n\\n\\nOne such instance:\\n\\n\\n\\n    In [21]: gby = df.groupby('x',group_keys=True).apply(lambda row: row['x'])\\n\\n    \\n\\n    In [22]: gby\\n\\n    Out[22]: \\n\\n    x   \\n\\n    0  0    0\\n\\n    2  3    2\\n\\n       4    2\\n\\n    3  1    3\\n\\n       2    3\\n\\n    Name: x, dtype: int64\\n\\n\\n\\n    In [23]: gby_k = df.groupby('x', group_keys=False).apply(lambda row: row['x'])\\n\\n    \\n\\n    In [24]: gby_k\\n\\n    Out[24]: \\n\\n    0    0\\n\\n    3    2\\n\\n    4    2\\n\\n    1    3\\n\\n    2    3\\n\\n    Name: x, dtype: int64\\n\\n\\n\\nOne of it's intended application could be to group by one of the levels of the hierarchy by converting it to a `Multi-index` dataframe object.\\n\\n\\n\\n    In [27]: gby.groupby(level='x').sum()\\n\\n    Out[27]: \\n\\n    x\\n\\n    0    0\\n\\n    2    4\\n\\n    3    6\\n\\n    Name: x, dtype: int64\\n\\n\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html\\n\\n[2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 8, 9),\n",
       "  '2016-08-09 20:26:33',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '6',\n",
       "  '',\n",
       "  '2630.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2852',\n",
       "  '38901102',\n",
       "  'Answer',\n",
       "  'Convert complex NumPy array into (n, 2)-array of real and imaginary parts',\n",
       "  'You can use [`column_stack`][1] and stack the two 1-D arrays as columns to make a single 2D array.\\n\\n\\n\\n    In [9]: np.column_stack((u.real,u.imag))\\n\\n    Out[9]: \\n\\n    array([[ 1.,  2.],\\n\\n           [ 2.,  4.],\\n\\n           [ 3.,  6.],\\n\\n           [ 4.,  8.]])\\n\\n\\n\\n[1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.column_stack.html',\n",
       "  '<python><arrays><numpy>',\n",
       "  datetime.date(2016, 8, 11),\n",
       "  '2016-08-11 16:05:05',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1158.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2855',\n",
       "  '40622671',\n",
       "  'Answer',\n",
       "  'Jupyter python3 notebook cannot recognize pandas',\n",
       "  'If you use anaconda already as a distribution, stop using pip in that context. Use conda instead and you will stop having headaches. The command lines and procedures for setting up a new environment are pretty well documented [here][1].\\n\\n\\n\\nBasically upgrading python or having specific branches:\\n\\n\\n\\n    conda update python\\n\\n    conda install python=3.5\\n\\n\\n\\nOr using specific environments:\\n\\n\\n\\n    conda create -n py35 python=3.5 anaconda\\n\\n\\n\\n  [1]: http://conda.pydata.org/docs/py2or3.html\\n\\n',\n",
       "  '<python><pandas><anaconda><jupyter-notebook>',\n",
       "  datetime.date(2016, 11, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '11194.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2857',\n",
       "  '40712041',\n",
       "  'Answer',\n",
       "  'replacing empty strings with NaN in Pandas',\n",
       "  \"Indicate it has to start with blank and end with blanks with ^ and $ :\\n\\n\\n\\n    df.replace(r'^\\\\s*$', np.nan, regex=True, inplace = True)\",\n",
       "  '<python><pandas><replace>',\n",
       "  datetime.date(2016, 11, 21),\n",
       "  '2017-06-18 07:17:13',\n",
       "  'Boud (624829)',\n",
       "  '8',\n",
       "  '',\n",
       "  '5440.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2868',\n",
       "  '40748713',\n",
       "  'Answer',\n",
       "  'Slow ploting using Animation function in Matplotlib, Python',\n",
       "  'It\\'s hard to say anything about your special case, since we do not have the serial connection part that you\\'re using. \\n\\n\\n\\nPlotting should however be much faster than 3 fps in matplotlib if this is only a line plot with some points in it. \\n\\nOne thing you can directly try it not to replot everything at every iteration step, but plot it once and then only update the data using `.set_data()`\\n\\n\\n\\nThe following example is closely related to your code and runs with 90 fps on my machine. So maybe you try that one out and see if it helps speeding up your case.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import matplotlib.animation as animation\\n\\n    import time\\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax1 = fig.add_subplot(1, 1, 1)\\n\\n    \\n\\n    cnt=0\\n\\n    xComponent=[]\\n\\n    \\n\\n    line,  = ax1.plot([0], [0])\\n\\n    text = ax1.text(0.97,0.97, \"\", transform=ax1.transAxes, ha=\"right\", va=\"top\")\\n\\n     \\n\\n    plt.ylim(0,25)\\n\\n    plt.xlim(0,100)\\n\\n    last_time = {0: time.time()}\\n\\n    def animate(i):\\n\\n        \\n\\n        if len(xComponent)>100:\\n\\n            xComponent.pop(0)\\n\\n        y = i % 25\\n\\n        xComponent.append(y)\\n\\n    \\n\\n        line.set_data(range( len(xComponent) ) ,xComponent)\\n\\n        new_time = time.time()\\n\\n        text.set_text(\"{0:.2f} fps\".format(1./(new_time-last_time[0])))\\n\\n        last_time.update({0:new_time})\\n\\n    \\n\\n    \\n\\n    ani = animation.FuncAnimation(fig, animate, interval=0)\\n\\n    plt.show()',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2016, 11, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1110.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2872',\n",
       "  '41126971',\n",
       "  'Answer',\n",
       "  'Plotting Probability Density Function with Z scores on pandas/python',\n",
       "  \"*Preparing a dummy data:*\\n\\n\\n\\n    import pandas as pd\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    plt.style.use('fivethirtyeight')\\n\\n\\n\\n    np.random.seed([314, 42])\\n\\n    df = pd.DataFrame(dict(ZScore=np.sort(np.random.uniform(-2, 2, 50)), \\n\\n                           FreqDist=np.random.randint(1, 30, 50)))\\n\\n    df.head()\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n*Plotting:*\\n\\n\\n\\n    ax = df.plot(x='ZScore', y='FreqDist', kind='kde', figsize=(10, 6))\\n\\n    # get the x axis values corresponding to this slice (See beneath the plot)\\n\\n    arr = ax.get_children()[0]._x\\n\\n    # take the first and last element of this array to constitute the xticks and \\n\\n    # also rotate the ticklabels to avoid overlapping\\n\\n    plt.xticks(np.linspace(arr[0], arr[-1]), rotation=90)\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\nOutput of the `list` of child artists obtained after plot:\\n\\n\\n\\n    ax.get_children()\\n\\n    [<matplotlib.lines.Line2D at 0x1d68b5c6d68>, <--- first element in list of child artists\\n\\n     <matplotlib.spines.Spine at 0x1d6895f14a8>,\\n\\n     <matplotlib.spines.Spine at 0x1d6895f1f98>,\\n\\n     <matplotlib.spines.Spine at 0x1d68d881828>,\\n\\n     <matplotlib.spines.Spine at 0x1d68b995048>,\\n\\n     <matplotlib.axis.XAxis at 0x1d689aeb978>,\\n\\n     <matplotlib.axis.YAxis at 0x1d68d7ff908>,\\n\\n     <matplotlib.text.Text at 0x1d689b55cf8>,\\n\\n     <matplotlib.text.Text at 0x1d689b55a20>,\\n\\n     <matplotlib.text.Text at 0x1d689b55c88>,\\n\\n     <matplotlib.legend.Legend at 0x1d687645390>,\\n\\n     <matplotlib.patches.Rectangle at 0x1d689b55080>]\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/iGw8Y.png\\n\\n  [2]: https://i.stack.imgur.com/81YY3.png\",\n",
       "  '<python><pandas><matplotlib><plot>',\n",
       "  datetime.date(2016, 12, 13),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1360.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2877',\n",
       "  '40795901',\n",
       "  'Answer',\n",
       "  'Rotate matplotlib pyplot with curve by 90 degrees',\n",
       "  'A rotation must always happen about a point in space (let\\'s call it `origin`).\\n\\n\\n\\nTo implement a rotation, you would need to shift your points to the origin, rotate them about an angle of choice and shift them back. In case your angle is 90°, rotation is straight forward \\n\\n\\n\\n    x_new = -y\\n\\n    y_new = x\\n\\n\\n\\nIn such a way the image can be rotated:\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    a = np.array([4, 4, 4, 4, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 8, 9])\\n\\n    b = np.array([i/float(len(a)) for i in range(1, len(a)+1)])\\n\\n    A = np.array([i/10. for i in range(40, 91)])\\n\\n    B = np.array([ 0.06200455,  0.07389492,  0.08721351,  0.10198928,  0.11823225,\\n\\n                    0.13593267,  0.15506088,  0.1755675 ,  0.19738431,  0.22042543,\\n\\n                    0.244589  ,  0.26975916,  0.29580827,  0.32259936,  0.34998862,\\n\\n                    0.377828  ,  0.40596767,  0.43425846,  0.46255411,  0.49071331,\\n\\n                    0.51860153,  0.54609255,  0.57306977,  0.5994272 ,  0.62507019,\\n\\n                    0.64991591,  0.67389356,  0.69694438,  0.71902138,  0.74008905,\\n\\n                    0.76012273,  0.77910799,  0.79703987,  0.81392209,  0.82976609,\\n\\n                    0.84459023,  0.85841887,  0.87128143,  0.88321163,  0.89424658,\\n\\n                    0.90442608,  0.91379189,  0.92238706,  0.93025537,  0.93744079,\\n\\n                    0.94398702,  0.94993712,  0.95533313,  0.96021585,  0.96462454,\\n\\n                    0.96859684]) \\n\\n    \\n\\n    \\n\\n    \\n\\n    def rotate(x,y, origin=(0,0)):\\n\\n        # shift to origin\\n\\n        x1 = x - origin[0]\\n\\n        y1 = y - origin[1]\\n\\n        \\n\\n        #rotate\\n\\n        x2 = -y1\\n\\n        y2 = x1\\n\\n        \\n\\n        # shift back\\n\\n        x3 = x2 + origin[1]\\n\\n        y3 = y2 + origin[0]\\n\\n        \\n\\n        return x3, y3\\n\\n    \\n\\n    # now let\\'s do the rotation\\n\\n    origin = (9.,0.5)\\n\\n    a1, b1 = rotate(a,b, origin )\\n\\n    A1, B1 = rotate(A,B, origin ) \\n\\n    \\n\\n    \\n\\n    fig, (ax1, ax2) = plt.subplots(1,2, figsize=(7,3.3))\\n\\n    \\n\\n    ax1.set_title(\"original\")\\n\\n    ax1.scatter(a, b, color = \"blue\", marker = \\'o\\', s = 20)\\n\\n    ax1.plot   (A, B, \\'r-\\')\\n\\n    \\n\\n    ax2.set_title(u\"90° ccw rotated\")\\n\\n    ax2.scatter(a1, b1, color = \"blue\", marker = \\'o\\', s = 20)\\n\\n    ax2.plot   (A1, B1, \\'r-\\')\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/e7JeL.png',\n",
       "  '<python><python-3.x><matplotlib>',\n",
       "  datetime.date(2016, 11, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1614.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2880',\n",
       "  '38913459',\n",
       "  'Answer',\n",
       "  'convert Series to DataFrame',\n",
       "  \"There's a special method for that - [`pd.Series.to_frame`](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.to_frame.html)\\n\\n\\n\\n    In [2]: df = pd.DataFrame({'a': range(4)})\\n\\n\\n\\n    In [3]: df.a\\n\\n    Out[3]: \\n\\n    0    0\\n\\n    1    1\\n\\n    2    2\\n\\n    3    3\\n\\n    Name: a, dtype: int64\\n\\n\\n\\n    In [4]: df.a.to_frame()\\n\\n    Out[4]: \\n\\n       a\\n\\n    0  0\\n\\n    1  1\\n\\n    2  2\\n\\n    3  3\\n\\n\",\n",
       "  '<python><pandas><dataframe><series><categorical-data>',\n",
       "  datetime.date(2016, 8, 12),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '11806.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2884',\n",
       "  '40581346',\n",
       "  'Answer',\n",
       "  'How to create a frequency table in pandas python',\n",
       "  \"Use `value_counts`:\\n\\n\\n\\n    df = pd.value_counts(df.Col1).to_frame().reset_index()\\n\\n    df\\n\\n    A    3\\n\\n    B    2\\n\\n    C    1\\n\\n\\n\\nthen rename your columns if needed:\\n\\n\\n\\n    df.columns = ['Col_value','Count']\\n\\n    \\n\\n    df\\n\\n      Col_value  Count\\n\\n    0         A      3\\n\\n    1         B      2\\n\\n    2         C      1\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 11, 14),\n",
       "  '2016-11-14 04:09:22',\n",
       "  'Boud (624829)',\n",
       "  '4',\n",
       "  '',\n",
       "  '6266.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2892',\n",
       "  '41138038',\n",
       "  'Answer',\n",
       "  'How can I add markers on a bar graph in python?',\n",
       "  'You can simply add a `plot` command, plotting the `y_pos` against the `lengths`. Make sure to specify a maker and set linestyle to `\"\"` (or `\"none\"`) otherwise the markers will be connected by straight lines.  \\n\\nThe following code may be what you\\'re after.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    \\n\\n    lengths = [11380, 44547, 166616, 184373, 193068, 258004, 369582, 462795, 503099, 581158, 660724, 671812, 918449]\\n\\n    \\n\\n    y_pos = np.arange(len(lengths))\\n\\n    error = np.array(lengths)*0.08\\n\\n    \\n\\n    plt.barh(y_pos, lengths, xerr=error, align=\\'center\\', alpha=0.4)\\n\\n    plt.plot(lengths, y_pos, marker=\"D\", linestyle=\"\", alpha=0.8, color=\"r\")\\n\\n    plt.yticks(y_pos, lengths)\\n\\n    plt.xlabel(\\'Lengths\\')\\n\\n    plt.title(\\'Comparison of different cuts\\')\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/tO0KO.png',\n",
       "  '<python-2.7><matplotlib>',\n",
       "  datetime.date(2016, 12, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1598.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2896',\n",
       "  '41145815',\n",
       "  'Answer',\n",
       "  'Showing the mean line in a density plot in pandas',\n",
       "  \"    # Set seed to reproduce the results\\n\\n    np.random.seed(42)\\n\\n    # Generate random data\\n\\n    df = pd.DataFrame(dict(age=(np.random.uniform(-20, 50, 100))))\\n\\n\\n\\n    # KDE plot\\n\\n    ax = df['age'].plot(kind='density')\\n\\n    # Access the child artists and calculate the mean of the resulting array\\n\\n    mean_val = np.mean(ax.get_children()[0]._x)\\n\\n    # Annotate points\\n\\n    ax.annotate('mean', xy=(mean_val, 0.008), xytext=(mean_val+10, 0.010),\\n\\n                arrowprops=dict(facecolor='black', shrink=0.05),\\n\\n                )\\n\\n    # vertical dotted line originating at mean value\\n\\n    plt.axvline(mean_val, linestyle='dashed', linewidth=2)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nSlice 0 was selected as it corresponds to the position of the `matplotlib.lines.Line2D` axes object.\\n\\n\\n\\n    >>> np.mean(ax.get_children()[0]._x)\\n\\n    14.734316880344197\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/Xs48p.png\",\n",
       "  '<python><pandas><matplotlib><plot>',\n",
       "  datetime.date(2016, 12, 14),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1522.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2904',\n",
       "  '40619921',\n",
       "  'Answer',\n",
       "  'How to force python to write 3 channel png image',\n",
       "  'If it doesn\\'t need to be `matplotlib` you could use `scipy.misc.toimage()`\\n\\n\\n\\n    import matplotlib.image as mpimg\\n\\n    import scipy.misc\\n\\n    \\n\\n    original_image = mpimg.imread(\"Bc11g.png\")\\n\\n    print original_image.shape\\n\\n    # prints (200L, 300L, 3L)\\n\\n    \\n\\n    mpimg.imsave(\\'Bc11g_new.png\\', original_image)\\n\\n    unchanged_original_image = mpimg.imread(\\'Bc11g_new.png\\')\\n\\n    print unchanged_original_image.shape\\n\\n    # prints (200L, 300L, 4L)\\n\\n    \\n\\n    #now use scipy.misc\\n\\n    scipy.misc.toimage(original_image).save(\\'Bc11g_new2.png\\')\\n\\n    unchanged_original_image2 = mpimg.imread(\\'Bc11g_new2.png\\')\\n\\n    print unchanged_original_image2.shape\\n\\n    # prints (200L, 300L, 3L)',\n",
       "  '<image><python-2.7><matplotlib>',\n",
       "  datetime.date(2016, 11, 15),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1326.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2905',\n",
       "  '40623799',\n",
       "  'Answer',\n",
       "  'Pandas DataFrame: How to print single row horizontally?',\n",
       "  'Sorta combining the two previous answers, you could do:\\n\\n\\n\\n    for index, ser in df.iterrows():\\n\\n        print( pd.DataFrame(ser).T )\\n\\n\\n\\n         0    1    2\\n\\n    0  100  200  300\\n\\n         0    1    2\\n\\n    1  400  500  600\\n\\n\\n\\nBasically what happens is that if you extract a row or column from a dataframe, you get a series which displays as a column.  And doesn\\'t matter if you do `ser` or `ser.T`, it \"looks\" like a column.  I mean, series are one dimensional, not two, but you get the point...\\n\\n\\n\\nSo anyway, you can convert the series to a dataframe with one row.  (I changed the name from \"row\" to \"ser\" to emphasize what is happening above.)  The key is you have to convert to a dataframe first (which will be a column by default), then transpose it.\\n\\n\\n\\n',\n",
       "  '<python><pandas><dataframe>',\n",
       "  datetime.date(2016, 11, 16),\n",
       "  '2016-11-16 04:07:57',\n",
       "  'JohnE (3877338)',\n",
       "  '2',\n",
       "  '',\n",
       "  '7898.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2907',\n",
       "  '40666123',\n",
       "  'Answer',\n",
       "  'matplot imshow add label to each color and put them in legend',\n",
       "  'I suppose putting a legend for all values in a matrix only makes sense if there aren\\'t too many of them. So let\\'s assume you have 8 different values in your matrix. We can then create a proxy artist of the respective color for each of them and put them into a legend like this\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import matplotlib.patches as mpatches\\n\\n    import numpy as np\\n\\n    \\n\\n    # create some data\\n\\n    data = np.random.randint(0, 8, (5,5))\\n\\n    # get the unique values from data\\n\\n    # i.e. a sorted list of all values in data\\n\\n    values = np.unique(data.ravel())\\n\\n    \\n\\n    plt.figure(figsize=(8,4))\\n\\n    im = plt.imshow(data, interpolation=\\'none\\')\\n\\n    \\n\\n    # get the colors of the values, according to the \\n\\n    # colormap used by imshow\\n\\n    colors = [ im.cmap(im.norm(value)) for value in values]\\n\\n    # create a patch (proxy artist) for every color \\n\\n    patches = [ mpatches.Patch(color=colors[i], label=\"Level {l}\".format(l=values[i]) ) for i in range(len(values)) ]\\n\\n    # put those patched as legend-handles into the legend\\n\\n    plt.legend(handles=patches, bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0. )\\n\\n    \\n\\n    plt.grid(True)\\n\\n    plt.show()\\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/J88Et.png',\n",
       "  '<python><matplotlib><label><legend><imshow>',\n",
       "  datetime.date(2016, 11, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '1070.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2911',\n",
       "  '40873683',\n",
       "  'Answer',\n",
       "  'Common legend for subplot matplotlib',\n",
       "  'Of course you need to show the legend on one of the subplots. It\\'s your decision which one you chose.\\n\\n\\n\\nIn order to show all four lines in the legend, you need to provide a reference to the lines to the legend\\n\\n\\n\\n    plt.legend(handles = [line1, line2, ...])\\n\\n\\n\\nSee also the [Matplotlib legend guide](http://matplotlib.org/users/legend_guide.html).  \\n\\nSo here is a working example\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    x = np.random.randint(0,12,size=(12,4))\\n\\n    y = np.random.randint(0,8,size=(12,4))\\n\\n    \\n\\n    fig, (ax, ax2, ax3) = plt.subplots(3, 1, sharex=True, figsize=(5,5))\\n\\n    \\n\\n    l, = ax.plot(x[:,0],y[:,0], marker = \\'o\\', label=\\'1\\')\\n\\n    l2, =ax2.plot(x[:,1],y[:,1], marker = \\'o\\', label=\\'2\\',color=\\'r\\')\\n\\n    l3, =ax2.plot(x[:,2],y[:,2], marker = \\'o\\', label=\\'3\\',color=\\'turquoise\\')\\n\\n    l4, =ax3.plot(x[:,3],y[:,3], marker = \\'o\\', label=\\'4\\',color=\\'g\\')\\n\\n    \\n\\n    \\n\\n    plt.legend( handles=[l, l2, l3, l4],loc=\"upper left\", bbox_to_anchor=[0, 1],\\n\\n               ncol=2, shadow=True, title=\"Legend\", fancybox=True)\\n\\n\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/hH9gI.png',\n",
       "  '<python><matplotlib><plot>',\n",
       "  datetime.date(2016, 11, 29),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '4908.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2914',\n",
       "  '40711772',\n",
       "  'Answer',\n",
       "  'How to replace inf in a numpy array with zero',\n",
       "  \"You have to save the operation in your dataframe. One way is to use the parameter `inplace=True`:\\n\\n\\n\\n    df_fund['dly_retn'].replace(np.inf, 0, inplace=True)\\n\\n    na_fund['dly_retn'].replace(np.inf, 0, inplace=True)\",\n",
       "  '<pandas>',\n",
       "  datetime.date(2016, 11, 21),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '2945.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2918',\n",
       "  '40946483',\n",
       "  'Answer',\n",
       "  'Using scikit-learn LinearRegression to plot a linear fit',\n",
       "  'There are two main issues here:\\n\\n\\n\\n 1. Getting the data out of the source\\n\\n 2. Getting the data into the shape that [`sklearn.LinearRegression.fit`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) understands\\n\\n\\n\\n**1. Getting the data out**  \\n\\nThe source file contains a header line with the column names. We do not want to column names in our data, so after reading in the whole data into the dataframe `df`, we can tell it to use the first line as headers by  \\n\\n`df.head()`. This allows to later query the dataframe by the column names as usual, i.e. `df[\\'Father\\']`.\\n\\n\\n\\n**2. Getting the data into shape**  \\n\\nThe [`sklearn.LinearRegression.fit`](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit) takes two arguments. First the \"training data\", which should be a 2D array, and second the \"target values\". In the case considered here, we simply what to make a fit, so we do not care about the notions too much, but we need to bring the first input to that function into the desired shape. This can be easily done by creating a new axis to one of the arrays, i.e. `df[\\'Father\\'].values[:,np.newaxis]`  \\n\\n\\n\\n\\n\\n**The complete working skript:**  \\n\\n \\n\\n\\n\\n    import numpy as np\\n\\n    import pandas as pd\\n\\n    from matplotlib import pyplot as plt\\n\\n    import seaborn as sns\\n\\n    \\n\\n    from sklearn.linear_model import LinearRegression\\n\\n    \\n\\n    df = pd.read_csv(\\'http://www.math.uah.edu/stat/data/Pearson.txt\\',\\n\\n                     delim_whitespace=True)\\n\\n    df.head() # prodce a header from the first data row\\n\\n    \\n\\n    \\n\\n    # LinearRegression will expect an array of shape (n, 1) \\n\\n    # for the \"Training data\"\\n\\n    X = df[\\'Father\\'].values[:,np.newaxis]\\n\\n    # target data is array of shape (n,) \\n\\n    y = df[\\'Son\\'].values\\n\\n    \\n\\n    \\n\\n    model2 = LinearRegression()\\n\\n    model2.fit(X, y)\\n\\n    \\n\\n    plt.scatter(X, y,color=\\'g\\')\\n\\n    plt.plot(X, model2.predict(X),color=\\'k\\')\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/K0yNG.png',\n",
       "  '<numpy><matplotlib><scikit-learn><curve-fitting>',\n",
       "  datetime.date(2016, 12, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '9',\n",
       "  '',\n",
       "  '6002.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2920',\n",
       "  '41152160',\n",
       "  'Answer',\n",
       "  'Paging/scrolling through set of 2D heat maps in matplotlib',\n",
       "  'The solution could indeed be to use a Slider as in the excellent answer by @hashmuke. In his answer he mentioned that \\n\\n*\"The slider is continuous while the layer index is a discrete integer [...]\"*\\n\\n\\n\\nThis brought me to think about a solution that wouldn\\'t have this restriction and have  \\n\\n**a more page-like look and feel**.\\n\\n\\n\\nThe outcome is `PageSlider`. Subclassing  `Slider` it makes use of the slider functionality, but displays the slider in integer steps starting at `1`. It takes the number of pages `numpages` as init argument, but except of that works as `Slider` seen from the outside. Additionally it also provides a back- and forward button.\\n\\n\\n\\nAn example, similar to the one from  @hashmuke, is given below the class.\\n\\n\\n\\n\\n\\n    import matplotlib.widgets\\n\\n    import matplotlib.patches\\n\\n    import mpl_toolkits.axes_grid1\\n\\n    \\n\\n    class PageSlider(matplotlib.widgets.Slider):\\n\\n        \\n\\n        def __init__(self, ax, label, numpages = 10, valinit=0, valfmt=\\'%1d\\', \\n\\n                     closedmin=True, closedmax=True,  \\n\\n                     dragging=True, **kwargs):\\n\\n            \\n\\n            self.facecolor=kwargs.get(\\'facecolor\\',\"w\")\\n\\n            self.activecolor = kwargs.pop(\\'activecolor\\',\"b\")\\n\\n            self.fontsize = kwargs.pop(\\'fontsize\\', 10)\\n\\n            self.numpages = numpages\\n\\n            \\n\\n            super(PageSlider, self).__init__(ax, label, 0, numpages, \\n\\n                                valinit=valinit, valfmt=valfmt, **kwargs)\\n\\n            \\n\\n            self.poly.set_visible(False)\\n\\n            self.vline.set_visible(False)\\n\\n            self.pageRects = []\\n\\n            for i in range(numpages):\\n\\n                facecolor = self.activecolor if i==valinit else self.facecolor\\n\\n                r  = matplotlib.patches.Rectangle((float(i)/numpages, 0), 1./numpages, 1, \\n\\n                                    transform=ax.transAxes, facecolor=facecolor)\\n\\n                ax.add_artist(r)\\n\\n                self.pageRects.append(r)\\n\\n                ax.text(float(i)/numpages+0.5/numpages, 0.5, str(i+1),  \\n\\n                        ha=\"center\", va=\"center\", transform=ax.transAxes,\\n\\n                        fontsize=self.fontsize)\\n\\n            self.valtext.set_visible(False)\\n\\n            \\n\\n            divider = mpl_toolkits.axes_grid1.make_axes_locatable(ax)\\n\\n            bax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\\n\\n            fax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\\n\\n            self.button_back = matplotlib.widgets.Button(bax, label=ur\\'$\\\\u25C0$\\', \\n\\n                            color=self.facecolor, hovercolor=self.activecolor)\\n\\n            self.button_forward = matplotlib.widgets.Button(fax, label=ur\\'$\\\\u25B6$\\', \\n\\n                            color=self.facecolor, hovercolor=self.activecolor)\\n\\n            self.button_back.label.set_fontsize(self.fontsize)\\n\\n            self.button_forward.label.set_fontsize(self.fontsize)\\n\\n            self.button_back.on_clicked(self.backward)\\n\\n            self.button_forward.on_clicked(self.forward)\\n\\n    \\n\\n        def _update(self, event):\\n\\n            super(PageSlider, self)._update(event)\\n\\n            i = int(self.val)\\n\\n            if i >=self.valmax:\\n\\n                return\\n\\n            self._colorize(i)\\n\\n                \\n\\n        def _colorize(self, i):\\n\\n            for j in range(self.numpages):\\n\\n                self.pageRects[j].set_facecolor(self.facecolor)\\n\\n            self.pageRects[i].set_facecolor(self.activecolor)\\n\\n        \\n\\n        def forward(self, event):\\n\\n            current_i = int(self.val)\\n\\n            i = current_i+1\\n\\n            if (i < self.valmin) or (i >= self.valmax):\\n\\n                return\\n\\n            self.set_val(i)\\n\\n            self._colorize(i)\\n\\n        \\n\\n        def backward(self, event):\\n\\n            current_i = int(self.val)\\n\\n            i = current_i-1\\n\\n            if (i < self.valmin) or (i >= self.valmax):\\n\\n                return\\n\\n            self.set_val(i)\\n\\n            self._colorize(i)\\n\\n    \\n\\n    \\n\\n    if __name__ == \"__main__\":\\n\\n        import numpy as np\\n\\n        from matplotlib import pyplot as plt\\n\\n        \\n\\n        \\n\\n        num_pages = 23\\n\\n        data = np.random.rand(9, 9, num_pages)\\n\\n        \\n\\n        fig, ax = plt.subplots()\\n\\n        fig.subplots_adjust(bottom=0.18)\\n\\n        \\n\\n        im = ax.imshow(data[:, :, 0], cmap=\\'viridis\\', interpolation=\\'nearest\\')\\n\\n        \\n\\n        ax_slider = fig.add_axes([0.1, 0.05, 0.8, 0.04])\\n\\n        slider = PageSlider(ax_slider, \\'Page\\', num_pages, activecolor=\"orange\")\\n\\n        \\n\\n        def update(val):\\n\\n            i = int(slider.val)\\n\\n            im.set_data(data[:,:,i])\\n\\n        \\n\\n        slider.on_changed(update)\\n\\n        \\n\\n        plt.show() \\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/hOGc2.png',\n",
       "  '<python><matplotlib><plot>',\n",
       "  datetime.date(2016, 12, 14),\n",
       "  '2016-12-16 00:34:19',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '6',\n",
       "  '',\n",
       "  '1238.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2926',\n",
       "  '40985866',\n",
       "  'Answer',\n",
       "  'Update range of colorbar in matplotlib',\n",
       "  'Can you work with `imshow` instead of `contour`? In that case it is easy to just update both, the plot and the colorbar.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax = fig.add_subplot(111)\\n\\n    \\n\\n    # Random data\\n\\n    data = np.random.rand(10, 10)\\n\\n    \\n\\n    # Plot data\\n\\n    plot = ax.imshow(data)\\n\\n    \\n\\n    \\n\\n    # Create colorbar\\n\\n    cbar = plt.colorbar(plot)\\n\\n    cbar_ticks = np.linspace(0., 1., num=6, endpoint=True)\\n\\n    cbar.set_ticks(cbar_ticks)\\n\\n    \\n\\n    plt.show(block=False)\\n\\n    \\n\\n    def update():\\n\\n    \\n\\n        new_data   = 2.*np.random.rand(10, 10)\\n\\n    \\n\\n        plot.set_data(new_data)\\n\\n        cbar.set_clim(vmin=0,vmax=2)\\n\\n        cbar_ticks = np.linspace(0., 2., num=11, endpoint=True)\\n\\n        cbar.set_ticks(cbar_ticks) \\n\\n        cbar.draw_all() \\n\\n        plt.draw()\\n\\n    \\n\\n        plt.show()\\n\\n    \\n\\n    update()',\n",
       "  '<python><matplotlib><colorbar><contourf>',\n",
       "  datetime.date(2016, 12, 6),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '3486.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2928',\n",
       "  '41002545',\n",
       "  'Answer',\n",
       "  'Log x-axis for histogram',\n",
       "  'The following works. \\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    \\n\\n    data = [1.2, 14, 150 ]\\n\\n    bins = 10**(np.arange(0,4))\\n\\n    print \"bins: \", bins\\n\\n    plt.xscale(\\'log\\')\\n\\n    plt.hist(data,bins=bins) \\n\\n    \\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n\\n\\nIn your code the probelm is the `bins` array. It has only two values, `[1, 10]`, while if you want tickmarks at `1,10,100,and 1000` you need to provide those numbers as `bins`. ',\n",
       "  '<matplotlib><plot><histogram><axis>',\n",
       "  datetime.date(2016, 12, 6),\n",
       "  '',\n",
       "  '',\n",
       "  '9',\n",
       "  '',\n",
       "  '6894.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2931',\n",
       "  '38977743',\n",
       "  'Answer',\n",
       "  'Algorithm - minimal number of bits to distinguish a set of given binary numbers',\n",
       "  'The [Set Cover](https://en.wikipedia.org/wiki/Vertex_cover) is defined in terms of a set *U*, and a set *S* of subsets of *U*. Each element in *U* must be covered by (at least) one of the sets in *S*. \\n\\n\\n\\nIf you can solve Set Cover, you can solve this problem as well. Suppose you build a set *U* whose each entry, *u<sub>i, j</sub>* (where *i < j*), corresponds to the pairs *(i, j)* and *(j, i)* of your *k* numbers (hence *|U| = k (k - 1) /2*). Now build *n* sets, *S<sub>1</sub>, ..., S<sub>n</sub>*, corresponding to the *n* possible bit positions. Set *S<sub>j</sub>* is the subset of all the elements corresponding to pairs position *j* differentiates. That is, if numbers *k, l* are different in position *j*, then set *u<sub>k, l</sub> &in; S<sub>j</sub>*.\\n\\n\\n\\nAs such, the [simple greedy algorithm for set cover](https://en.wikipedia.org/wiki/Set_cover_problem#Greedy_algorithm), can give you an bounded approximation of the minimal number of bits. Unfortunately, it will not give you the *minimal* number of bits. As noted by @augurar in the comments, this problem is NP-Hard by reduction, and, as such, probably does not have a feasible exact optimal algorithm.\\n\\n',\n",
       "  '<algorithm>',\n",
       "  datetime.date(2016, 8, 16),\n",
       "  '2016-08-17 10:21:44',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1204.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2932',\n",
       "  '38982066',\n",
       "  'Answer',\n",
       "  'Pandas setting multi-index on rows, then transposing to columns',\n",
       "  'You could use [`pivot_table`][1] followed by a series of manipulations on the dataframe to get the desired form:\\n\\n\\n\\n    df_pivot = pd.pivot_table(df, index=[\\'one\\', \\'two\\'], values=\\'three\\', aggfunc=np.sum)\\n\\n\\n\\n    def rename_duplicates(old_list):    # Replace duplicates in the index with an empty string\\n\\n        seen = {}\\n\\n        for x in old_list:\\n\\n            if x in seen:\\n\\n                seen[x] += 1\\n\\n                yield \" \" \\n\\n            else:\\n\\n                seen[x] = 0\\n\\n                yield x\\n\\n\\n\\n    col_group = df_pivot.unstack().stack().reset_index(level=-1)\\n\\n    col_group.index = rename_duplicates(col_group.index.tolist())\\n\\n    col_group.index.name = df_pivot.index.names[0]\\n\\n    col_group.T\\n\\n\\n\\n    one  A     B     C   \\n\\n    two  1  2  1  2  1  2\\n\\n    0    a  b  c  d  e  f\\n\\n\\n\\n  \\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.pivot_table.html',\n",
       "  '<python><pandas><dataframe><transpose><multi-index>',\n",
       "  datetime.date(2016, 8, 16),\n",
       "  '2016-08-16 19:27:29',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '11291.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2933',\n",
       "  '41210491',\n",
       "  'Answer',\n",
       "  'Histogram values of a Pandas Series',\n",
       "  'Inorder to get the frequency counts of the values in a given interval binned range, we could make use of [`pd.cut`][1] which returns indices of half open bins for each element along with [`value_counts`][2] for computing their respective counts. \\n\\n\\n\\nTo plot their counts, a bar plot can be then made.\\n\\n\\n\\n    step = 50\\n\\n    bin_range = np.arange(-200, 1000+step, step)\\n\\n    out, bins  = pd.cut(s, bins=bin_range, include_lowest=True, right=False, retbins=True)\\n\\n    out.value_counts(sort=False).plot.bar()\\n\\n\\n\\n[![enter image description here][3]][3]\\n\\n\\n\\nFrequency for each interval sorted in descending order of their counts:\\n\\n\\n\\n    out.value_counts().head()\\n\\n    [-100, -50)    18\\n\\n    [0, 50)        16\\n\\n    [800, 850)      2\\n\\n    [-50, 0)        2\\n\\n    [950, 1000)     1\\n\\n    dtype: int64\\n\\n\\n\\n----------\\n\\nTo modify the plot to include just the lower closed interval of the range for aesthetic purpose, you could do:\\n\\n\\n\\n    out.cat.categories = bins[:-1]\\n\\n    out.value_counts(sort=False).plot.bar()\\n\\n\\n\\n[![enter image description here][4]][4]\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/version/0.19.0/generated/pandas.cut.html\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html\\n\\n  [3]: https://i.stack.imgur.com/sZBWd.png\\n\\n  [4]: https://i.stack.imgur.com/Wu0XU.png',\n",
       "  '<python><pandas><numpy><matplotlib>',\n",
       "  datetime.date(2016, 12, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '14',\n",
       "  '',\n",
       "  '42230.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2941',\n",
       "  '41209383',\n",
       "  'Answer',\n",
       "  '[matplotlib]: understanding \"set_ydata\" method',\n",
       "  'It is not surprising that you see nothing if you remove the line that you set the data to.\\n\\n\\n\\nAs the name of the function `set_data` suggests, it sets the data points of a `Line2D` object. `set_ydata` is a special case which does only set the ydata.\\n\\n\\n\\nThe use of `set_data` mostly makes sense when updating a plot, as in your example (just without removing the line).\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    \\n\\n    x = np.arange(-3, 3, 0.01)\\n\\n    j = 1\\n\\n    y = np.sin( np.pi*x*j ) / ( np.pi*x*j )\\n\\n    fig = plt.figure()\\n\\n    ax = fig.add_subplot(111)\\n\\n    #plot a line along points x,y\\n\\n    line, = ax.plot(x, y)\\n\\n    #update data\\n\\n    j = 2\\n\\n    y2 = np.sin( np.pi*x*j ) / ( np.pi*x*j )\\n\\n    #update the line with the new data\\n\\n    line.set_ydata(y2)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n\\n\\nIt is obvious that it would have been much easier to directly plot `ax.plot(x, y2)`. Therefore `set_data` is commonly only used in cases where it makes sense and to which you refer as \"large and difficult to understand codes\".',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2016, 12, 18),\n",
       "  '2016-12-18 14:44:03',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '4',\n",
       "  '',\n",
       "  '4732.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2944',\n",
       "  '40746133',\n",
       "  'Answer',\n",
       "  'Pandas stacked bar chart with sorted values',\n",
       "  \"1. Sorting the first level index according to it's total sum:\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n\\n\\n    s_sort = s.groupby(level=[0]).sum().sort_values(ascending=False)\\n\\n    s_sort\\n\\n    qux    47\\n\\n    foo    34\\n\\n    baz    32\\n\\n    bar    30\\n\\n    dtype: int64\\n\\n    \\n\\n2. Reindex back using the new sorted index values in the first level + `unstack` + plot:\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n\\n\\n    cmp = plt.cm.get_cmap('jet')\\n\\n    s.reindex(index=s_sort.index, level=0).unstack().plot.bar(stacked=True, cmap=cmp)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/xE1JX.png\",\n",
       "  '<python><pandas><dataframe><bar-chart><stacked>',\n",
       "  datetime.date(2016, 11, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '1830.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2953',\n",
       "  '41213032',\n",
       "  'Answer',\n",
       "  'Python Jupyter Notebook: Put two histogram subplots side by side in one figure',\n",
       "  \"Yes this is possible. See the following code. \\n\\n\\n\\n    %matplotlib inline\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    \\n\\n    list1 = np.random.rand(10)*2.1\\n\\n    list2 = np.random.rand(10)*3.\\n\\n    bins = np.linspace(0, 1, 3)\\n\\n    \\n\\n    fig, ax = plt.subplots(1,2)\\n\\n    ax[0].hist(list1, bins, alpha = 0.5, color = 'r')\\n\\n    ax[1].hist(list2, bins, alpha = 0.5, color = 'g')\\n\\n    plt.show()\",\n",
       "  '<python><python-3.x><matplotlib><histogram>',\n",
       "  datetime.date(2016, 12, 18),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '5222.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2956',\n",
       "  '41109029',\n",
       "  'Answer',\n",
       "  'Heat Map half-sphere plot',\n",
       "  '[`Axes3D.plot_surface`](http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html#mpl_toolkits.mplot3d.Axes3D.plot_surface) accepts 2D arrays as inputs. It provides the `facecolors` argument, which accepts an array of the same shape as the input arrays. This array should have the color for each face as rgba tuple in it. One can therefore normalize the array values to the range up to 1 and supply it the a colormap from `matplotlib.cm`. \\n\\n\\n\\nThe remaining problem is then to obtain this array from the 3 column list which is provided. Given a the datatable of length `n*m` where the first column denotes `x` values, second `y` values and the third some value, and where the sorting is first by `x` and then by `y`. One can then reshape the last column to an `(n,m)` array, where `n` is the number of `x` values and `m` of y values, using `.reshape((m,n)).T`.\\n\\n\\n\\nSome further remarks:\\n\\n \\n\\n1. In the solution below, I needed to mimic this array and directly used angles in radiant, instead of degrees.\\n\\n2. The number of points, 180*720 seems a bit high. In order for the window not to take ages to rotate, I decreased that number.  \\n\\n3. I renamed the angles, such that they match with the usual textbook definition, phi = azimuthal angle, theta=inclination angle (from z axis).\\n\\n4. The use of `plot_wireframe` may not make too much sense, since it will hide the surface below. If a wireframe is desired, one can play with the number of points to be drawn and the `linewidth` keyword argument. Setting `linewidth` to something big, like 3 or 5 makes the surface look nice, setting it to 1 leaves some wireframe look.\\n\\n\\n\\nHere is the complete solution.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    from matplotlib import cm\\n\\n    from mpl_toolkits.mplot3d import Axes3D\\n\\n    import numpy as np\\n\\n    \\n\\n    #theta inclination angle\\n\\n    #phi azimuthal angle\\n\\n    n_theta = 50 # number of values for theta\\n\\n    n_phi = 200  # number of values for phi\\n\\n    r = 2        #radius of sphere\\n\\n    \\n\\n    theta, phi = np.mgrid[0.0:0.5*np.pi:n_theta*1j, 0.0:2.0*np.pi:n_phi*1j]\\n\\n    \\n\\n    x = r*np.sin(theta)*np.cos(phi)\\n\\n    y = r*np.sin(theta)*np.sin(phi)\\n\\n    z = r*np.cos(theta)\\n\\n    \\n\\n    # mimic the input array\\n\\n    # array columns phi, theta, value\\n\\n    # first n_theta entries: phi=0, second n_theta entries: phi=0.0315..\\n\\n    inp = []\\n\\n    for j in phi[0,:]:\\n\\n        for i in theta[:,0]:\\n\\n            val = 0.7+np.cos(j)*np.sin(i+np.pi/4.)# put something useful here\\n\\n            inp.append([j, i, val])\\n\\n    inp = np.array(inp)\\n\\n    print inp.shape\\n\\n    print inp[49:60, :]\\n\\n    \\n\\n    #reshape the input array to the shape of the x,y,z arrays. \\n\\n    c = inp[:,2].reshape((n_phi,n_theta)).T\\n\\n    print z.shape\\n\\n    print c.shape\\n\\n    \\n\\n    \\n\\n    #Set colours and render\\n\\n    fig = plt.figure(figsize=(10, 8))\\n\\n    ax = fig.add_subplot(111, projection=\\'3d\\')\\n\\n    #use facecolors argument, provide array of same shape as z\\n\\n    # cm.<cmapname>() allows to get rgba color from array.\\n\\n    # array must be normalized between 0 and 1\\n\\n    ax.plot_surface(\\n\\n        x,y,z,  rstride=1, cstride=1, facecolors=cm.hot(c/c.max()), alpha=0.9, linewidth=1) \\n\\n    ax.set_xlim([-2.2,2.2])\\n\\n    ax.set_ylim([-2.2,2.2])\\n\\n    ax.set_zlim([0,4.4])\\n\\n    ax.set_aspect(\"equal\")\\n\\n    #ax.plot_wireframe(x, y, z, color=\"k\") #not needed?!\\n\\n    plt.savefig(__file__+\".png\")\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/ee2jD.png',\n",
       "  '<python><matplotlib><3d><heatmap><mplot3d>',\n",
       "  datetime.date(2016, 12, 12),\n",
       "  '2016-12-12 20:58:24',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1060.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2971',\n",
       "  '40831803',\n",
       "  'Answer',\n",
       "  'sort_values versus sort giving different answers, but sort_values is the correct answer',\n",
       "  'Apart from the fact that sort is deprecated, it returns `None` when you use `inplace=True`\\n\\n\\n\\n`sort_values` still returns the self updated dataframe is you use that argument as you can see in frame.py source code:\\n\\n\\n\\n        if inplace:\\n\\n            return self._update_inplace(new_data)\\n\\n        else:\\n\\n            return self._constructor(new_data).__finalize__(self)',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 11, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1570.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2972',\n",
       "  '40832170',\n",
       "  'Answer',\n",
       "  'Apply function to every row in Pandas Dataframe',\n",
       "  'If you want to keep a kind of readable math function, and an easy conversion of the current function, use [`eval`][1]:\\n\\n\\n\\n    df.eval(\"\"\"\\n\\n    northing = 3189068.5 * log((1.0 + sin(latitude * 0.017453292519943295)) / (1.0 - sin(latitude * 0.017453292519943295)))\\n\\n    easting = 6378137.0 * longitude * 0.017453292519943295\"\"\", inplace=False)\\n\\n    Out[51]: \\n\\n             id  longitude   latitude      northing       easting\\n\\n    0  11135465 -73.986893  40.761093  4.977167e+06 -8.236183e+06\\n\\n    1   1113546 -73.979645  40.747814  4.975215e+06 -8.235376e+06\\n\\n    2  11135467 -74.001244  40.743172  4.974533e+06 -8.237781e+06\\n\\n    3  11135468 -73.997818  40.726055  4.972018e+06 -8.237399e+06\\n\\n\\n\\nYou will have to work a bit on the syntax as you cannot use `if` statements, but you can easily filter out the out-of-boundaries data before calling `eval`. You can also use `inplace=True` if you want to directly assign the new columns.\\n\\n\\n\\nIf you aren\\'t that interested in keeping the math syntax and is searching for full speed, it is likely the numpy answer will perform still faster.\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/enhancingperf.html#expression-evaluation-via-eval-experimental',\n",
       "  '<python><python-3.x><pandas><geolocation>',\n",
       "  datetime.date(2016, 11, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1039.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2984',\n",
       "  '39068669',\n",
       "  'Answer',\n",
       "  'How to solve ValueError: Index contains duplicate entries, cannot reshape',\n",
       "  'You can set `append=True` in [`DF.set_index`][1] so that it avoids the default writing entries all over again during `unstack` operation. It only adds the entries not present in the unstacked column before.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import seaborn as sns\\n\\n    sns.set_style(\"whitegrid\")\\n\\n\\n\\n    df = pd.concat(dicts, names=(\\'test\\', \\'displacement\\')).reset_index()\\n\\n    labels = np.unique(df[\\'test\\'].values).tolist()\\n\\n    df.set_index([\\'test\\', \\'displacement\\'], append=True, inplace=True)\\n\\n    df.unstack(level=\\'test\\').plot(figsize=(10,10), use_index=False,               \\n\\n                                  legend=False, title=\"Grouped Plot\")\\n\\n    plt.legend(loc=\\'upper left\\', fontsize=12, frameon=True, labels=labels)\\n\\n    plt.show()\\n\\n\\n\\n[![Image1][2]][2]\\n\\n\\n\\n\\n\\n----------\\n\\nIncase you want all the plots to start at the origin, you could use [`array_split`][4] to split the unstacked `dataframe` object into equal size based on the total length of the unique labels i.e 5 [*Test0* → *Test4*] as follows: \\n\\n\\n\\n    df = pd.concat(dicts, names=(\\'test\\', \\'displacement\\')).reset_index()\\n\\n    labels = np.unique(df[\\'test\\'].values).tolist()\\n\\n    df.set_index([\\'test\\', \\'displacement\\'], append=True, inplace=True)\\n\\n    \\n\\n    fig, ax = plt.subplots(figsize=(10,10))\\n\\n    for test_sample in range(len(labels)):\\n\\n        np.array_split(df.unstack(\\'test\\'), len(labels))[test_sample].plot(grid=True, \\n\\n                       use_index=False, ax=ax, legend=False, cmap=plt.cm.get_cmap(\\'jet\\'))\\n\\n    plt.legend(loc=\\'upper left\\', fontsize=12, frameon=True, labels=labels)\\n\\n    plt.xlim(0,50)\\n\\n    plt.title(\"Grouped Plot\")\\n\\n    plt.show()\\n\\n\\n\\n[![Image2][3]][3]\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.set_index.html\\n\\n  [2]: http://i.stack.imgur.com/TZ1Hb.png\\n\\n  [3]: http://i.stack.imgur.com/Ypz8f.png\\n\\n  [4]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.array_split.html#numpy.array_split',\n",
       "  '<python><pandas><matplotlib><seaborn>',\n",
       "  datetime.date(2016, 8, 21),\n",
       "  '2017-01-21 08:47:43',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '0',\n",
       "  '',\n",
       "  '1198.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2986',\n",
       "  '40835814',\n",
       "  'Answer',\n",
       "  'Python: Error tokenizing data. C error: Calling read(nbytes) on source failed with input nzip file',\n",
       "  'The input zip file is corrupted. Get a proper copy of this file from the source of try to use zip repairing tools before you pass it along to pandas.',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 11, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '5469.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2988',\n",
       "  '41277685',\n",
       "  'Answer',\n",
       "  'Output a Document (preferably a PDF) from Python',\n",
       "  'You can do all of this directly in matplotlib. \\n\\nIt is possible to create several figures and afterwards save them all to the same pdf document using `matplotlib.backends.backend_pdf.PdfPages`.  \\n\\nThe text can be set using the `text()` command. \\n\\n\\n\\nHere is a basic example on how that would work.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    from matplotlib.backends.backend_pdf import PdfPages\\n\\n    import numpy as np\\n\\n    \\n\\n    N=10\\n\\n    text =  \"The maximum is {}, the minimum is {}\"\\n\\n    \\n\\n    # create N figures\\n\\n    for i in range(N):\\n\\n        data = np.random.rand(28)\\n\\n        fig = plt.figure(i+1)\\n\\n        ax= fig.add_subplot(111)\\n\\n        ax.plot(np.arange(28), data)\\n\\n        # add some text to the figure\\n\\n        ax.text(0.1, 1.05,text.format(data.max(),data.min()), transform=ax.transAxes)\\n\\n        \\n\\n    # Saving all figures to the same pdf\\n\\n    pp = PdfPages(\\'multipage.pdf\\')\\n\\n    for i in range(N):\\n\\n        fig = plt.figure(i+1)\\n\\n        fig.savefig(pp, format=\\'pdf\\')\\n\\n    pp.close()',\n",
       "  '<python><pdf><matplotlib><automation><pdf-generation>',\n",
       "  datetime.date(2016, 12, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1182.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2990',\n",
       "  '41363938',\n",
       "  'Answer',\n",
       "  'Python+sqlAlchemy: change dtype object to string dynamically',\n",
       "  \"You can use *sqlalchemy* `String` type instead of the default `Text` type after identifying the `object` columns present in the dataframe. \\n\\n\\n\\nUse the `dtype` argument in [`to_sql`][1] and supply a dictionary mapping of those columns with the `sqlalchemy.sql.sqltypes.String` as shown:\\n\\n\\n\\n    from sqlalchemy.types import String\\n\\n \\n\\n    obj_cols = pandasDF.select_dtypes(include=[object]).columns.values.tolist()\\n\\n    pandasDF.to_sql(tableName, engine, if_exists='append', dtype={c: String for c in obj_cols})\\n\\n\\n\\nThese would map your `object` data to the `String` SQL data type.\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_sql.html\\n\\n    \\n\\n\",\n",
       "  '<python><pandas><sqlalchemy>',\n",
       "  datetime.date(2016, 12, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1559.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['2998',\n",
       "  '40978321',\n",
       "  'Answer',\n",
       "  'Pandas (python) plot() without a legend',\n",
       "  'In order to remove a legend that has once been drawn, use\\n\\n\\n\\n    plt.gca().get_legend().remove()\\n\\n\\n\\nassuming that you have `import`ed `matplotlib.pyplot as plt` or \\n\\n\\n\\n    ax.get_legend().remove()\\n\\n\\n\\nif `ax` is the axes where the legend resides.\\n\\n \\n\\nAlternatively, see Nipun Batras answer if there is some choice to turn the legend off from the beginning in which case one can simply use \\n\\n\\n\\n    df.plot(legend=False)',\n",
       "  '<python><matplotlib><plot><pandas>',\n",
       "  datetime.date(2016, 12, 5),\n",
       "  '2018-11-08 01:03:56',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '15',\n",
       "  '',\n",
       "  '21453.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3006',\n",
       "  '41247697',\n",
       "  'Answer',\n",
       "  'Pandas convert dataframe to array of tuples',\n",
       "  \"Here's a vectorized approach (assuming the dataframe, `data_set` to be defined as `df` instead) that returns a `list` of `tuples` as shown:\\n\\n\\n\\n    >>> df.set_index(['data_date'])[['data_1', 'data_2']].to_records().tolist()\\n\\n\\n\\nproduces:\\n\\n\\n\\n    [(datetime.datetime(2012, 2, 17, 0, 0), 24.75, 25.03),\\n\\n     (datetime.datetime(2012, 2, 16, 0, 0), 25.0, 25.07),\\n\\n     (datetime.datetime(2012, 2, 15, 0, 0), 24.99, 25.15),\\n\\n     (datetime.datetime(2012, 2, 14, 0, 0), 24.68, 25.05),\\n\\n     (datetime.datetime(2012, 2, 13, 0, 0), 24.62, 24.77),\\n\\n     (datetime.datetime(2012, 2, 10, 0, 0), 24.38, 24.61)]\\n\\n\\n\\n\\n\\nThe idea of setting datetime column as the index axis is to aid in the conversion of the `Timestamp` value to it's corresponding `datetime.datetime` format equivalent by making use of the `convert_datetime64` argument in [`DF.to_records`][1] which does so for a `DateTimeIndex` dataframe.\\n\\n\\n\\nThis returns a `recarray` which could be then made to return a `list` using `.tolist`\\n\\n\\n\\n----------\\n\\nMore generalized solution depending on the use case would be:\\n\\n\\n\\n    df.to_records().tolist()                              # Supply index=False to exclude index\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_records.html\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 12, 20),\n",
       "  '2016-12-26 13:47:19',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '8',\n",
       "  '',\n",
       "  '76110.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3007',\n",
       "  '41251483',\n",
       "  'Answer',\n",
       "  '%matplotlib notebook showing a blank histogram',\n",
       "  'Seeing that my comment above has indeed helped someone to solve the problem I will post it as an answer. \\n\\n\\n\\nThe problem occurs if you switch from `%matplotlib inline` to `%matplotlib notebook` without restarting the kernel. \\n\\n\\n\\nSwitching from `%matplotlib notebook` to `%matplotlib inline` works fine. \\n\\n\\n\\nSo the solution is to either restart the kernel or start a new notebook.\\n\\n\\n\\nIt seems that in some cases it helps to repeat the setting of the notebook backend, i.e. call it twice like\\n\\n\\n\\n    %matplotlib notebook\\n\\n    %matplotlib notebook',\n",
       "  '<python><matplotlib><jupyter-notebook>',\n",
       "  datetime.date(2016, 12, 20),\n",
       "  '2017-06-20 14:26:34',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '50',\n",
       "  '',\n",
       "  '7535.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3015',\n",
       "  '39152639',\n",
       "  'Answer',\n",
       "  'Annotate seaborn Factorplot',\n",
       "  'You could make use of [`plt.annotate`][2] method provided by `matplotlib` to make annotations for the `factorplot` as shown:\\n\\n\\n\\n  **Setup:** \\n\\n    \\n\\n    df = pd.DataFrame({\\'groups\\':[\\'A\\', \\'B\\', \\'C\\', \\'D\\'],\\n\\n                       \\'nb_opportunities\\':[674, 140, 114, 99],\\n\\n                       \\'actual_group\\':[False, False, True, False],\\n\\n                       \\'adviced_group\\':[False, True, True, True]})\\n\\n    print (df)\\n\\n\\n\\n      actual_group adviced_group groups  nb_opportunities\\n\\n    0        False         False      A               674\\n\\n    1        False          True      B               140\\n\\n    2         True          True      C               114\\n\\n    3        False          True      D                99\\n\\n                       \\n\\n**Data Operations:**\\n\\n\\n\\nChoosing the subset of `df` where the values of `actual_group` are True. The `index` value and the `nb_opportunities` value become the arguments for x and y that become the location of the annotation.\\n\\n\\n\\n    actual_group = df.loc[df[\\'actual_group\\']==True]\\n\\n    x = actual_group.index.tolist()[0]\\n\\n    y = actual_group[\\'nb_opportunities\\'].values[0]\\n\\n   \\n\\n**Plotting:** \\n\\n\\n\\n    sns.factorplot(x=\"groups\", y=\"nb_opportunities\", hue=\"adviced_group\", kind=\\'bar\\', data=df, \\n\\n                   size=4, aspect=2)\\n\\n\\n\\nAdding some padding to the location of the annotation as well as the location of text to account for the width of the bars being plotted.\\n\\n\\n\\n    plt.annotate(\\'actual group\\', xy=(x+0.2,y), xytext=(x+0.3, 300),\\n\\n                 arrowprops=dict(facecolor=\\'black\\', shrink=0.05, headwidth=20, width=7))\\n\\n    plt.show()\\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/GZHtJ.png\\n\\n  [2]: http://matplotlib.org/users/annotations_intro.html',\n",
       "  '<python><pandas><matplotlib><seaborn>',\n",
       "  datetime.date(2016, 8, 25),\n",
       "  '2016-08-25 18:55:53',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '6',\n",
       "  '',\n",
       "  '3343.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3020',\n",
       "  '41449714',\n",
       "  'Answer',\n",
       "  'pandas combine two columns with null values',\n",
       "  \"Use [`fillna`][1] on one column with the fill values being the other column:\\n\\n\\n\\n    df['foodstuff'].fillna(df['type'])\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n    0      apple-martini\\n\\n    1          apple-pie\\n\\n    2    strawberry-tart\\n\\n    3            dessert\\n\\n    4               None\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.fillna.html\",\n",
       "  '<python><pandas><dataframe><nonetype>',\n",
       "  datetime.date(2017, 1, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '20',\n",
       "  '',\n",
       "  '5792.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3023',\n",
       "  '39158831',\n",
       "  'Answer',\n",
       "  'confused about random_state in decision tree of scikit learn',\n",
       "  \"This is explained in [the documentation](http://scikit-learn.org/stable/modules/tree.html#tree)\\n\\n\\n\\n> The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree. This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement.\\n\\n\\n\\nSo, basically, a sub-optimal greedy algorithm is repeated a number of times using random selections of features and samples (a similar technique used in random forests). The `random_state` parameter allows controlling these random choices.\\n\\n\\n\\nThe [interface documentation](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html) specifically states:\\n\\n\\n\\n> If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random.\\n\\n\\n\\nSo, the random algorithm will be used in any case. Passing any value (whether a specific int, e.g., 0, or a `RandomState` instance), will not change that. The only rationale for passing in an int value (0 or otherwise) is to make the outcome consistent across calls: if you call this with `random_state=0` (or any other value), then each and every time, you'll get the same result.\",\n",
       "  '<python><python-2.7><machine-learning><scikit-learn><decision-tree>',\n",
       "  datetime.date(2016, 8, 26),\n",
       "  '2016-08-26 18:57:44',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '9',\n",
       "  '',\n",
       "  '8798.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3024',\n",
       "  '39179180',\n",
       "  'Answer',\n",
       "  'SystemError: error return without exception set',\n",
       "  'You\\'re correct that your function doesn\\'t really need to return anything (in this case), but, in Python, \"not returning anything\" means returning `None`. Try replacing \\n\\n\\n\\n    return 0;\\n\\n\\n\\nin your `printHello` function with\\n\\n\\n\\n    Py_INCREF(Py_None);\\n\\n    return Py_None;\\n\\n\\n\\nThe first line increments the refcount of `None`, and the second line returns it.\\n\\n\\n\\nEven better, just use [`Py_RETURN_NONE`](https://docs.python.org/2/c-api/none.html) (which is shorthand for the same):\\n\\n\\n\\n    extern \"C\" PyObject * printHello(PyObject *self, PyObject *args) {\\n\\n        printf(\"hello from C\\\\n\");\\n\\n        Py_RETURN_NONE;\\n\\n    }\\n\\n\\n\\n',\n",
       "  '<c++><python-embedding>',\n",
       "  datetime.date(2016, 8, 27),\n",
       "  '2016-08-27 08:53:03',\n",
       "  'sashoalm (492336)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1283.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3027',\n",
       "  '41124286',\n",
       "  'Answer',\n",
       "  'How to group by two Columns using Pandas?',\n",
       "  \"After the `Groupby.size` operation, a `multi-index`(2 level index) `series` object gets created that needs to be converted into a dataframe, which could be done by [`unstacking`][2] the 2nd level index and optionally filling `NaNs` obtained with 0.\\n\\n\\n\\n    df.groupby(['Segment', 'Type']).size().unstack(level=1, fill_value=0)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/LjJiJ.png\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.unstack.html\",\n",
       "  '<excel><python-3.x><pandas>',\n",
       "  datetime.date(2016, 12, 13),\n",
       "  '2016-12-13 15:09:40',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '5',\n",
       "  '',\n",
       "  '1040.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3029',\n",
       "  '41553592',\n",
       "  'Answer',\n",
       "  'PyQt4 equivilent to time.sleep?',\n",
       "  'You cannot use `time.sleep` in the pyqt main event loop as it would stop the GUI event loop from responding. \\n\\n\\n\\nA solution in pyqt could look like this, using `QTimer`\\n\\n\\n\\n    import sys\\n\\n    from PyQt4 import QtGui, QtCore\\n\\n    \\n\\n    application = QtGui.QApplication(sys.argv)\\n\\n    \\n\\n    i=0\\n\\n    timer = QtCore.QTimer()\\n\\n    \\n\\n    def num():\\n\\n        global i, timer\\n\\n        if i <999:\\n\\n            print ( i )\\n\\n            i += 1\\n\\n        else:\\n\\n            timer.stop()\\n\\n    \\n\\n    timer.timeout.connect(num)\\n\\n    timer.start(2000)\\n\\n    \\n\\n    sys.exit(application.exec_())\\n\\n\\n\\n ',\n",
       "  '<python><pyqt><pyqt4>',\n",
       "  datetime.date(2017, 1, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '3524.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3030',\n",
       "  '41564971',\n",
       "  'Answer',\n",
       "  'A weighted version of random.choice',\n",
       "  'As of Python `v3.6`, [`random.choices`][1] could be used to return a `list` of elements of specified size from the given population with optional weights.\\n\\n\\n\\n    \\n\\n\\n\\n> `random.choices(population, weights=None, *, cum_weights=None, k=1)`\\n\\n\\n\\n - *population* : `list` containing unique observations. (If empty, raises `IndexError`)\\n\\n \\n\\n - *weights* : More precisely relative weights required to make selections.\\n\\n\\n\\n - *cum_weights* : cumulative weights required to make selections.\\n\\n\\n\\n - *k* : size(`len`) of the `list` to be outputted. (Default `len()=1`)\\n\\n\\n\\n\\n\\n----------\\n\\n***Few Caveats:***\\n\\n\\n\\n1) It makes use of weighted sampling with replacement so the drawn items would be later replaced. The values in the weights sequence in itself do not matter, but their relative ratio does.\\n\\n\\n\\nUnlike `np.random.choice` which can only take on probabilities as weights and also which must ensure summation of individual probabilities upto 1 criteria, there are no such regulations here. As long as they belong to numeric types (`int/float/fraction` except `Decimal` type) , these would still perform.\\n\\n\\n\\n\\n\\n\\n\\n    >>> import random\\n\\n    # weights being integers\\n\\n    >>> random.choices([\"white\", \"green\", \"red\"], [12, 12, 4], k=10)\\n\\n    [\\'green\\', \\'red\\', \\'green\\', \\'white\\', \\'white\\', \\'white\\', \\'green\\', \\'white\\', \\'red\\', \\'white\\']\\n\\n    # weights being floats\\n\\n    >>> random.choices([\"white\", \"green\", \"red\"], [.12, .12, .04], k=10)\\n\\n    [\\'white\\', \\'white\\', \\'green\\', \\'green\\', \\'red\\', \\'red\\', \\'white\\', \\'green\\', \\'white\\', \\'green\\']\\n\\n    # weights being fractions\\n\\n    >>> random.choices([\"white\", \"green\", \"red\"], [12/100, 12/100, 4/100], k=10)\\n\\n    [\\'green\\', \\'green\\', \\'white\\', \\'red\\', \\'green\\', \\'red\\', \\'white\\', \\'green\\', \\'green\\', \\'green\\']\\n\\n\\n\\n2) If neither *weights* nor *cum_weights* are specified, selections are made with equal probability.  If a *weights* sequence is supplied, it must be the same length as the *population* sequence. \\n\\n\\n\\nSpecifying both *weights* and *cum_weights* raises a `TypeError`.\\n\\n\\n\\n    >>> random.choices([\"white\", \"green\", \"red\"], k=10)\\n\\n    [\\'white\\', \\'white\\', \\'green\\', \\'red\\', \\'red\\', \\'red\\', \\'white\\', \\'white\\', \\'white\\', \\'green\\']\\n\\n\\n\\n3) *cum_weights* are typically a result of [`itertools.accumulate`][2] function which are really handy in such situations. \\n\\n\\n\\n> <sub> From the documentation linked: </sub>\\n\\n>\\n\\n> Internally, the relative weights are converted to cumulative weights\\n\\n> before making selections, so supplying the cumulative weights saves\\n\\n> work.\\n\\n\\n\\nSo, either supplying `weights=[12, 12, 4]` or `cum_weights=[12, 24, 28]` for our contrived case produces the same outcome and the latter seems to be more faster / efficient.\\n\\n\\n\\n[1]: https://docs.python.org/3/library/random.html#random.choices\\n\\n[2]: https://docs.python.org/3/library/itertools.html#itertools.accumulate\\n\\n',\n",
       "  '<python><optimization>',\n",
       "  datetime.date(2017, 1, 10),\n",
       "  '2017-01-10 09:12:03',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '10',\n",
       "  '',\n",
       "  '98345.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3036',\n",
       "  '39903944',\n",
       "  'Answer',\n",
       "  'Efficiently replace values from a column to another column Pandas DataFrame',\n",
       "  \"Using [`np.where`][1] is faster.  Using a similar pattern as you used with `replace`:\\n\\n\\n\\n    df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\\n\\n    df['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\\n\\n\\n\\nHowever, using a nested `np.where` is slightly faster:\\n\\n\\n\\n    df['col1'] = np.where(df['col1'] == 0, \\n\\n                          np.where(df['col2'] == 0, df['col3'], df['col2']),\\n\\n                          df['col1'])\\n\\n\\n\\n**Timings**\\n\\n\\n\\nUsing the following setup to produce a larger sample DataFrame and timing functions:\\n\\n\\n\\n    df = pd.concat([df]*10**4, ignore_index=True)\\n\\n    \\n\\n    def root_nested(df):\\n\\n        df['col1'] = np.where(df['col1'] == 0, np.where(df['col2'] == 0, df['col3'], df['col2']), df['col1'])\\n\\n        return df\\n\\n    \\n\\n    def root_split(df):\\n\\n        df['col1'] = np.where(df['col1'] == 0, df['col2'], df['col1'])\\n\\n        df['col1'] = np.where(df['col1'] == 0, df['col3'], df['col1'])\\n\\n        return df\\n\\n\\n\\n    def pir2(df):\\n\\n        df['col1'] = df.where(df.ne(0), np.nan).bfill(axis=1).col1.fillna(0)\\n\\n        return df\\n\\n\\n\\n    def pir2_2(df):\\n\\n        slc = (df.values != 0).argmax(axis=1)\\n\\n        return df.values[np.arange(slc.shape[0]), slc]\\n\\n\\n\\n    def andrew(df):\\n\\n        df.col1[df.col1 == 0] = df.col2\\n\\n        df.col1[df.col1 == 0] = df.col3\\n\\n        return df\\n\\n    \\n\\n    def pablo(df):\\n\\n        df['col1'] = df['col1'].replace(0,df['col2'])\\n\\n        df['col1'] = df['col1'].replace(0,df['col3'])\\n\\n        return df\\n\\n\\n\\nI get the following timings:\\n\\n\\n\\n    %timeit root_nested(df.copy())\\n\\n    100 loops, best of 3: 2.25 ms per loop\\n\\n    \\n\\n    %timeit root_split(df.copy())\\n\\n    100 loops, best of 3: 2.62 ms per loop\\n\\n\\n\\n    %timeit pir2(df.copy())\\n\\n    100 loops, best of 3: 6.25 ms per loop\\n\\n\\n\\n    %timeit pir2_2(df.copy())\\n\\n    1 loop, best of 3: 2.4 ms per loop\\n\\n    \\n\\n    %timeit andrew(df.copy())\\n\\n    100 loops, best of 3: 8.55 ms per loop\\n\\n\\n\\nI tried timing your method, but it's been running for multiple minutes without completing.  As a comparison, timing your method on just the 6 row example DataFrame (not the much larger one tested above) took 12.8 ms.\\n\\n\\n\\n\\n\\n  [1]: http://docs.scipy.org/doc/numpy/reference/generated/numpy.where.html\\n\\n\",\n",
       "  '<python><pandas><replace><dataframe>',\n",
       "  datetime.date(2016, 10, 6),\n",
       "  '2016-10-06 20:02:56',\n",
       "  'root (3339965)',\n",
       "  '12',\n",
       "  '',\n",
       "  '11928.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3037',\n",
       "  '41491052',\n",
       "  'Answer',\n",
       "  'Summing Rows in Pandas Dataframe returning NAN',\n",
       "  \"When you supply `axis=0` in the [`DF.sum`][0] method, it performs summation along the indexes (vertical direction if it's easier to comprehend). As a result, you get just 4 values computed corresponding to the 4 columns of the dataframe. Then, you are assigning this result to a new column of the dataframe. As they do not share the same index axis to reindex upon, you get a series of `NaN` elements.\\n\\n\\n\\nYou actually want to do the summation across the columns(horizontal direction).\\n\\n\\n\\nChange that line to:\\n\\n\\n\\n    new_df['cash_change'] = new_df.sum(axis=1)  # sum row-wise across each column\\n\\n\\n\\nNow you would get finite computed summed values.\\n\\n\\n\\n[0]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html\",\n",
       "  '<python-2.7><pandas><numpy><dataframe><nan>',\n",
       "  datetime.date(2017, 1, 5),\n",
       "  '2017-01-05 17:29:07',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1225.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3038',\n",
       "  '41547816',\n",
       "  'Answer',\n",
       "  'Python: Iterate over a data frame column, check for a condition-value stored in array, and get the values to a list',\n",
       "  \"A quick way to do would be to leverage NumPy's broadcasting techniques as an extension of [***this answer***][1] from the same post linked, although an answer related to the use of `DF.where` was actually asked.\\n\\n\\n\\nBroadcasting eliminates the need to iterate through every element of the array and it's highly efficient at the same time.\\n\\n\\n\\nThe only addition to this post is the use of `np.argmax` to grab the indices of the first `True` instance along each column (traversing ↓ direction).\\n\\n\\n\\n    conditions = np.array([10, 15, 23])\\n\\n    tol = 0\\n\\n    num_albums = df.Num_Albums.values\\n\\n    num_albums_cumsum = df.Num_Albums.cumsum().values\\n\\n    slices = np.argmax(np.isclose(num_albums_cumsum[:, None], conditions, atol=tol), axis=0)\\n\\n\\n\\nRetrieved slices:\\n\\n\\n\\n    slices\\n\\n    Out[692]:\\n\\n    array([0, 2, 4], dtype=int64)\\n\\n\\n\\nCorresponding array produced:\\n\\n\\n\\n    num_albums[slices]\\n\\n    Out[693]:\\n\\n    array([10,  4,  1], dtype=int64)\\n\\n\\n\\n\\n\\n----------\\n\\nIf you still prefer using `DF.where`, here is another solution using `list-comprehension` -\\n\\n\\n\\n    [df.where((df['cumsum'] >= cond - tol) & (df['cumsum'] <= cond + tol), -1)['Num_Albums']\\n\\n       .max() for cond in conditions]\\n\\n    Out[695]:\\n\\n    [10, 4, 1]\\n\\n\\n\\nThe conditions not fulfilling the given criteria would be replaced by -1. Doing this way preserves the `dtype` at the end.\\n\\n\\n\\n[1]: https://stackoverflow.com/questions/41488676/python-data-frame-cumulative-sum-of-column-until-condition-is-reached-and-retur/41489086#41489086\",\n",
       "  '<python><arrays><pandas><dataframe>',\n",
       "  datetime.date(2017, 1, 9),\n",
       "  '2017-05-23 12:33:49',\n",
       "  'Nickil Maveli (6207849), URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1538.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3060',\n",
       "  '41228272',\n",
       "  'Answer',\n",
       "  'Calculate time difference between Pandas Dataframe indices',\n",
       "  \"We can create a series with both index and values equal to the index keys using [`to_series`][1] and then compute the differences between successive rows which would result in `timedelta64[ns]` dtype. After obtaining this, via the `.dt` property, we could access the seconds attribute of the time portion and finally divide each element by 60 to get it outputted in minutes(optionally filling the first value with 0).\\n\\n\\n\\n    In [13]: df['deltaT'] = df.index.to_series().diff().dt.seconds.div(60, fill_value=0)\\n\\n        ...: df                                 # use .astype(int) to obtain integer values\\n\\n    Out[13]: \\n\\n                         value  deltaT\\n\\n    time                              \\n\\n    2012-03-16 23:50:00      1     0.0\\n\\n    2012-03-16 23:56:00      2     6.0\\n\\n    2012-03-17 00:08:00      3    12.0\\n\\n    2012-03-17 00:10:00      4     2.0\\n\\n    2012-03-17 00:12:00      5     2.0\\n\\n    2012-03-17 00:20:00      6     8.0\\n\\n    2012-03-20 00:43:00      7    23.0\\n\\n\\n\\n\\n\\n----------\\n\\n***simplification:***\\n\\n\\n\\nWhen we perform `diff`:\\n\\n\\n\\n    In [8]: ser_diff = df.index.to_series().diff()\\n\\n    \\n\\n    In [9]: ser_diff\\n\\n    Out[9]: \\n\\n    time\\n\\n    2012-03-16 23:50:00               NaT\\n\\n    2012-03-16 23:56:00   0 days 00:06:00\\n\\n    2012-03-17 00:08:00   0 days 00:12:00\\n\\n    2012-03-17 00:10:00   0 days 00:02:00\\n\\n    2012-03-17 00:12:00   0 days 00:02:00\\n\\n    2012-03-17 00:20:00   0 days 00:08:00\\n\\n    2012-03-20 00:43:00   3 days 00:23:00\\n\\n    Name: time, dtype: timedelta64[ns]\\n\\n\\n\\nSeconds to minutes conversion:\\n\\n\\n\\n    In [10]: ser_diff.dt.seconds.div(60, fill_value=0)\\n\\n    Out[10]: \\n\\n    time\\n\\n    2012-03-16 23:50:00     0.0\\n\\n    2012-03-16 23:56:00     6.0\\n\\n    2012-03-17 00:08:00    12.0\\n\\n    2012-03-17 00:10:00     2.0\\n\\n    2012-03-17 00:12:00     2.0\\n\\n    2012-03-17 00:20:00     8.0\\n\\n    2012-03-20 00:43:00    23.0\\n\\n    Name: time, dtype: float64\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n\\n\\nIf suppose you want to include even the `date` portion as it was excluded previously(only time portion was considered), [`dt.total_seconds`][2] would give you the elapsed duration in seconds with which minutes could then be calculated again by division.\\n\\n\\n\\n    In [12]: ser_diff.dt.total_seconds().div(60, fill_value=0)\\n\\n    Out[12]: \\n\\n    time\\n\\n    2012-03-16 23:50:00       0.0\\n\\n    2012-03-16 23:56:00       6.0\\n\\n    2012-03-17 00:08:00      12.0\\n\\n    2012-03-17 00:10:00       2.0\\n\\n    2012-03-17 00:12:00       2.0\\n\\n    2012-03-17 00:20:00       8.0\\n\\n    2012-03-20 00:43:00    4343.0    # <-- number of minutes in 3 days 23 minutes\\n\\n    Name: time, dtype: float64\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Index.to_series.html\\n\\n[2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.total_seconds.html\",\n",
       "  '<python><dataframe><pandas>',\n",
       "  datetime.date(2016, 12, 19),\n",
       "  '2017-01-12 13:50:21',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '20',\n",
       "  '',\n",
       "  '34302.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3061',\n",
       "  '41228807',\n",
       "  'Answer',\n",
       "  'extracting days from a numpy.timedelta64 value',\n",
       "  \"Use [`dt.days`][1] to obtain the days attribute as integers.\\n\\n\\n\\nFor eg:\\n\\n    \\n\\n    In [14]: s = pd.Series(pd.timedelta_range(start='1 days', end='12 days', freq='3000T'))\\n\\n    \\n\\n    In [15]: s\\n\\n    Out[15]: \\n\\n    0    1 days 00:00:00\\n\\n    1    3 days 02:00:00\\n\\n    2    5 days 04:00:00\\n\\n    3    7 days 06:00:00\\n\\n    4    9 days 08:00:00\\n\\n    5   11 days 10:00:00\\n\\n    dtype: timedelta64[ns]\\n\\n    \\n\\n    In [16]: s.dt.days\\n\\n    Out[16]: \\n\\n    0     1\\n\\n    1     3\\n\\n    2     5\\n\\n    3     7\\n\\n    4     9\\n\\n    5    11\\n\\n    dtype: int64\\n\\n\\n\\n\\n\\nMore generally - You can use the [`.components`][2] property to access a reduced form of `timedelta`.\\n\\n\\n\\n    In [17]: s.dt.components\\n\\n    Out[17]: \\n\\n       days  hours  minutes  seconds  milliseconds  microseconds  nanoseconds\\n\\n    0     1      0        0        0             0             0            0\\n\\n    1     3      2        0        0             0             0            0\\n\\n    2     5      4        0        0             0             0            0\\n\\n    3     7      6        0        0             0             0            0\\n\\n    4     9      8        0        0             0             0            0\\n\\n    5    11     10        0        0             0             0            0\\n\\n\\n\\nNow, to get the `hours` attribute:\\n\\n\\n\\n    In [23]: s.dt.components.hours\\n\\n    Out[23]: \\n\\n    0     0\\n\\n    1     2\\n\\n    2     4\\n\\n    3     6\\n\\n    4     8\\n\\n    5    10\\n\\n    Name: hours, dtype: int64\\n\\n\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.dt.days.html\\n\\n[2]: http://pandas.pydata.org/pandas-docs/version/0.17.0/generated/pandas.Series.dt.components.html\",\n",
       "  '<python><numpy><pandas>',\n",
       "  datetime.date(2016, 12, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '19',\n",
       "  '',\n",
       "  '54855.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3071',\n",
       "  '40050882',\n",
       "  'Answer',\n",
       "  'Exclude between time in pandas',\n",
       "  'You can combine `between_time` with [`drop`][1]:\\n\\n\\n\\n    df2 = df.drop(df.between_time(\"16:00\", \"17:00\").index)\\n\\n\\n\\n**Edit**\\n\\n\\n\\nAn alternate method is to exploit the fact that `between_time` operates circularly, so you can switch the order of your input times to exclude the range between them:\\n\\n\\n\\n    df.between_time(\"17:00\", \"16:00\", include_start=False, include_end=False)\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html',\n",
       "  '<python><pandas><numpy>',\n",
       "  datetime.date(2016, 10, 14),\n",
       "  '2017-05-02 20:53:21',\n",
       "  'root (3339965)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1313.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3080',\n",
       "  '41303259',\n",
       "  'Answer',\n",
       "  'Rounding up one column in pandas dataframe',\n",
       "  \"You are filtering the columns upside down, do this instead:\\n\\n\\n\\n    cols = [col for col in  df.columns if col == 'price_cleaning']\\n\\n\\n\\nNow, if you need to cleanup only one columns, then no need to create `cols`. Just do:\\n\\n\\n\\n    df['price_cleaning'] = df['price_cleaning'].apply(roundup)\",\n",
       "  '<python><python-3.x><pandas><python-3.5>',\n",
       "  datetime.date(2016, 12, 23),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '3555.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3086',\n",
       "  '40165711',\n",
       "  'Answer',\n",
       "  'matplotlib: subplots of same size?',\n",
       "  'In order to get help, you need to provide a [minimal working example][1]. You will find out that producing such a [minimal working example][1], **almost always** makes you find the problem and a corresponding solution yourself.   \\n\\nAlso, structure your code!! \\n\\n\\n\\nAs we do not have the necessary knowledge of your data and the variables you are using, it is almost impossible to come up with a solution.\\n\\n\\n\\nWhat you need to do is break down the problem. You are looking for the difference between two things - so make them as equal as possible. If you apply everything to both plots simultaneously, how can they be different after all?\\n\\n\\n\\nThe following code shows how you would do that and it acutally shows no difference in size of the two plots. So start from there and add the stuff you might need accordingly. One step at a time, until you find the piece of code that causes problems.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    from mpl_toolkits.axes_grid1 import make_axes_locatable\\n\\n    \\n\\n    x = np.linspace(0,100,101)\\n\\n    X, Y = np.meshgrid(x,x)\\n\\n    data = np.sin(X/2.3)*np.cos(Y/2.7)*np.cos(X*Y/20.)\\n\\n    \\n\\n    fig = plt.figure(figsize=(10,5))\\n\\n    ax1=fig.add_subplot(121)\\n\\n    ax2=fig.add_subplot(122)\\n\\n    plt.subplots_adjust(wspace = 0.33 )\\n\\n    \\n\\n    heatmap = ax1.pcolor(data, cmap=\"RdPu\")\\n\\n    resmap =  ax2.scatter(X,Y, c=data, cmap=\"YlGnBu\",edgecolors=\\'none\\',alpha=0.5)\\n\\n    \\n\\n    \\n\\n    for ax in [ax1,ax2]:\\n\\n        ax.set_frame_on(False)\\n\\n        \\n\\n        ax.set_aspect(\\'equal\\')\\n\\n    \\n\\n        ax.invert_yaxis()\\n\\n        ax.xaxis.tick_top()\\n\\n    \\n\\n        ax.set_xmargin(0)\\n\\n        ax.set_ymargin(0)\\n\\n    \\n\\n        ax.grid(False)\\n\\n    \\n\\n        for t in ax.xaxis.get_major_ticks():\\n\\n            t.tick1On = False\\n\\n            t.tick2On = False\\n\\n        for t in ax.yaxis.get_major_ticks():\\n\\n            t.tick1On = False\\n\\n            t.tick2On = False\\n\\n    \\n\\n        ax.set_xlim([x[0],x[-1]])\\n\\n        ax.set_ylim([x[0],x[-1]])\\n\\n    \\n\\n    \\n\\n    divider1 = make_axes_locatable(ax1)       \\n\\n    cax1 = divider1.append_axes(\"right\", size=\"5%\", pad=0.05)\\n\\n    plt.colorbar(heatmap,cax=cax1)\\n\\n    \\n\\n    divider2 = make_axes_locatable(ax2)       \\n\\n    cax2 = divider2.append_axes(\"right\", size=\"5%\", pad=0.05)\\n\\n    plt.colorbar(resmap,cax=cax2)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n\\n\\nAnd btw, `fig = plt.figure(figsize=(10,5))` produces a rectangle, while `fig = plt.figure(figsize=(20,20))` produces a square.\\n\\n\\n\\n\\n\\n  [1]: https://stackoverflow.com/help/mcve',\n",
       "  '<python><matplotlib><plot><scatter-plot><subplot>',\n",
       "  datetime.date(2016, 10, 20),\n",
       "  '2017-05-23 12:24:35',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '2105.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3093',\n",
       "  '39371197',\n",
       "  'Answer',\n",
       "  'efficient loop over numpy array',\n",
       "  'Python itself is a highly-dynamic, slow, language. The idea in numpy is to use [vectorization](https://www.safaribooksonline.com/library/view/python-for-data/9781449323592/ch04.html), and avoid explicit loops. In this case, you can use [`np.equal.outer`](http://docs.scipy.org/doc/numpy/reference/generated/numpy.ufunc.outer.html). You can start with\\n\\n\\n\\n    a = np.equal.outer(vect, vect)\\n\\n\\n\\nNow, for example, to find the sum:\\n\\n\\n\\n     >>> np.sum(a)\\n\\n     10006\\n\\n\\n\\nTo find the indices of *i* that are equal, you can do\\n\\n\\n\\n    np.fill_diagonal(a, 0)\\n\\n\\n\\n    >>> np.nonzero(np.any(a, axis=0))[0]\\n\\n    array([   1, 2500, 5000])\\n\\n\\n\\n\\n\\n-----------------------------------\\n\\n\\n\\n**Timing**\\n\\n\\n\\n    def find_vec():\\n\\n        a = np.equal.outer(vect, vect)\\n\\n        s = np.sum(a)\\n\\n        np.fill_diagonal(a, 0)\\n\\n        return np.sum(a), np.nonzero(np.any(a, axis=0))[0]\\n\\n\\n\\n    >>> %timeit find_vec()\\n\\n    1 loops, best of 3: 214 ms per loop\\n\\n\\n\\n    def find_loop():\\n\\n        dupl = []\\n\\n        counter = 0\\n\\n        for i in range(N):\\n\\n            for j in range(i+1, N):\\n\\n                 if vect[i] == vect[j]:\\n\\n                     dupl.append(j)\\n\\n                     counter += 1\\n\\n        return dupl\\n\\n\\n\\n    >>> % timeit find_loop()\\n\\n    1 loops, best of 3: 8.51 s per loop\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "  '<python><arrays><loops><numpy><optimization>',\n",
       "  datetime.date(2016, 9, 7),\n",
       "  '2016-09-07 13:38:55',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '5883.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3098',\n",
       "  '40258165',\n",
       "  'Answer',\n",
       "  'Plotting a column containing lists using Pandas',\n",
       "  \"Consider this `DF` containing values as lists as shown:\\n\\n\\n\\n    np.random.seed(42)\\n\\n    df = pd.DataFrame({'list1': np.random.randint(0, 10, (5,2)).tolist(), \\n\\n                       'list2': np.random.randint(0, 10, (5,3)).tolist()}, \\n\\n                       index=list('ABCDE'))\\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n***Q-1*** *Plot both list into one single plot:*\\n\\n\\n\\nUnstack the `DF` to make the column names appear as index and make individual values present in the list to individual series objects.\\n\\n\\n\\n    df_lists = df[['list1','list2']].unstack().apply(pd.Series)\\n\\n    df_lists.plot.bar(rot=0, cmap=plt.cm.jet, fontsize=8, width=0.7, figsize=(8,4))\\n\\n\\n\\n[![Image][2]][2]\\n\\n\\n\\n***Q-2*** *Plot the first elements of each list only into one single grouped bar plot:*\\n\\n\\n\\nUse [`DF.applymap`][4] to select first element of the required columns to obtain the grouped bar plot.\\n\\n\\n\\n    df[['list1','list2']].applymap(lambda x: x[0]).plot.bar(rot=0, color=list('br'))\\n\\n\\n\\n[![Image][3]][3]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/7vTuM.png\\n\\n  [2]: https://i.stack.imgur.com/RsU3n.png\\n\\n  [3]: https://i.stack.imgur.com/hqOpN.png\\n\\n  [4]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.applymap.html\",\n",
       "  '<python><list><pandas><plot><dataframe>',\n",
       "  datetime.date(2016, 10, 26),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1633.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3103',\n",
       "  '41489086',\n",
       "  'Answer',\n",
       "  'Python Data Frame: cumulative sum of column until condition is reached and return the index',\n",
       "  \"***Opt - 1:***\\n\\n\\n\\nYou could compute the cumulative sum using [`cumsum`][1]. Then use [`np.isclose`][2] with it's inbuilt tolerance parameter to check if the values  present in this series  lies within the specified threshold of 15 +/- 2. This returns a boolean array. \\n\\n\\n\\nThrough [`np.flatnonzero`][3], return the ordinal values of the indices for which the `True` condition holds. We select the first instance of a `True` value.\\n\\n\\n\\nFinally, use `.iloc` to retrieve value of the column name you require based on the index computed earlier.\\n\\n\\n\\n    val = np.flatnonzero(np.isclose(df.Num_Albums.cumsum().values, 15, atol=2))[0]\\n\\n    df['Num_authors'].iloc[val]      # for faster access, use .iat \\n\\n    4\\n\\n\\n\\nWhen performing `np.isclose` on the `series` later converted to an array:\\n\\n\\n\\n    np.isclose(df.Num_Albums.cumsum().values, 15, atol=2)\\n\\n    array([False, False,  True, False, False, False], dtype=bool)\\n\\n\\n\\n***Opt - 2:***\\n\\n\\n\\nUse [`pd.Index.get_loc`][4] on the `cumsum` calculated series which also supports a `tolerance` parameter on the `nearest` method.\\n\\n\\n\\n    val = pd.Index(df.Num_Albums.cumsum()).get_loc(15, 'nearest', tolerance=2)\\n\\n    df.get_value(val, 'Num_authors')\\n\\n    4\\n\\n\\n\\n***Opt - 3:***\\n\\n\\n\\nUse [`idxmax`][5] to find the first index of a `True` value for the boolean mask created after `sub` and `abs` operations on the `cumsum` series:\\n\\n\\n\\n    df.get_value(df.Num_Albums.cumsum().sub(15).abs().le(2).idxmax(), 'Num_authors')\\n\\n    4\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.cumsum.html\\n\\n[2]: https://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.isclose.html\\n\\n[3]: https://docs.scipy.org/doc/numpy/reference/generated/numpy.flatnonzero.html\\n\\n[4]: https://pandas-docs.github.io/pandas-docs-travis/generated/pandas.Index.get_loc.html\\n\\n[5]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.idxmax.html\",\n",
       "  '<python><pandas><dataframe><sum>',\n",
       "  datetime.date(2017, 1, 5),\n",
       "  '2017-01-05 17:05:34',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '6',\n",
       "  '',\n",
       "  '2864.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3109',\n",
       "  '41667906',\n",
       "  'Answer',\n",
       "  'Interactive boxplot with pandas and Jupyter notebook',\n",
       "  'You can simply filter the dataframe by the number of days and then plot the respective boxplot.\\n\\n\\n\\n    numer_of_days = 42\\n\\n    df_filtered= df.loc[df[\\'days\\'] < numer_of_days]  # use operators like ==, >=, <, etc.\\n\\n    df_filtered[[\"category\", \"value\"]].boxplot( by=\"category\", return_type=\\'axes\\')\\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n<hr>\\n\\nIn order to get a dropdown field, you can use the `ipywidgets.interact()` function, to which you provide a function that plots the dataframe for that specific day.\\n\\n(In the following I restricted the number of days to 12, such that a dropdown actually makes sense for selecting a single day out of those.)\\n\\n\\n\\n\\n\\n    import numpy as np\\n\\n    import pandas as pd\\n\\n    from ipywidgets import interact\\n\\n    %matplotlib notebook\\n\\n    \\n\\n    categories=(\\'A\\',\\'B\\',\\'C\\')\\n\\n    \\n\\n    data = {\\n\\n                \\'days\\':      np.random.randint(12, size=100), \\n\\n                \\'category\\':  np.random.choice(categories, 100),\\n\\n                \\'value\\':     100.0 * np.random.random_sample(100)\\n\\n           }\\n\\n    \\n\\n    df = pd.DataFrame(data)\\n\\n    \\n\\n    def select_days(number_of_days):\\n\\n        df_filtered= df.loc[df[\\'days\\'] == int(number_of_days)] \\n\\n        ax = df_filtered[[\"category\", \"value\"]].boxplot( by=\"category\", return_type=\\'axes\\')\\n\\n        ax[\"value\"].set_title(\"Day \" + number_of_days)\\n\\n        print df_filtered\\n\\n        \\n\\n    days = [str(day) for day in np.arange(12)]\\n\\n    \\n\\n    interact(select_days, number_of_days=days)\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/SUASL.png\\n\\n  [2]: https://i.stack.imgur.com/Q3Pbx.png',\n",
       "  '<python><pandas><matplotlib><jupyter-notebook>',\n",
       "  datetime.date(2017, 1, 16),\n",
       "  '2017-01-16 09:13:13',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '4',\n",
       "  '',\n",
       "  '3671.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3110',\n",
       "  '41671112',\n",
       "  'Answer',\n",
       "  \"Python ValueError: non-broadcastable output operand with shape (124,1) doesn't match the broadcast shape (124,13)\",\n",
       "  'The partitioning of train/test data must be specified in the same order as the input array to the [`train_test_split()`][1] function for it to unpack them corresponding to that order. \\n\\n\\n\\nClearly, when the order was specified as `X_train, y_train, X_test, y_test`, the resulting shapes of `y_train` (`len(y_train)=54`) and `X_test` (`len(X_test)=124`) got swapped resulting in the `ValueError`.\\n\\n\\n\\nInstead, you must:\\n\\n\\n\\n    # Split into train/test data.\\n\\n    #                   _________________________________\\n\\n    #                   |       |                        \\\\\\n\\n    #                   |       |                         \\\\\\n\\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)                                        \\n\\n    # |          |                                      /\\n\\n    # |__________|_____________________________________/\\n\\n    # (or)\\n\\n    # y_train, y_test, X_train, X_test = train_test_split(y, X, test_size=0.3, random_state=0)\\n\\n\\n\\n    # Normalize features using min-max scaling.\\n\\n    from sklearn.preprocessing import MinMaxScaler\\n\\n    mms = MinMaxScaler()\\n\\n    X_train_norm = mms.fit_transform(X_train)\\n\\n    X_test_norm = mms.transform(X_test)\\n\\n\\n\\nproduces:\\n\\n\\n\\n    X_train_norm[0]\\n\\n    array([ 0.72043011,  0.20378151,  0.53763441,  0.30927835,  0.33695652,\\n\\n            0.54316547,  0.73700306,  0.25      ,  0.40189873,  0.24068768,\\n\\n            0.48717949,  1.        ,  0.5854251 ])\\n\\n    \\n\\n    X_test_norm[0]\\n\\n    array([ 0.72849462,  0.16386555,  0.47849462,  0.29896907,  0.52173913,\\n\\n            0.53956835,  0.74311927,  0.13461538,  0.37974684,  0.4364852 ,\\n\\n            0.32478632,  0.70695971,  0.60566802])\\n\\n\\n\\n[1]: http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html',\n",
       "  '<python><python-2.7><numpy><scikit-learn>',\n",
       "  datetime.date(2017, 1, 16),\n",
       "  '2017-01-16 07:28:56',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '1',\n",
       "  '',\n",
       "  '6074.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3111',\n",
       "  '41693529',\n",
       "  'Answer',\n",
       "  'How to merge two rows in a dataframe pandas',\n",
       "  \"You can make use of [`DF.combine_first()`][1] method after separating the `DF` into 2 parts where the null values in the first half would be replaced with the finite values in the other half while keeping it's other finite values untouched:\\n\\n\\n\\n    df.head(1).combine_first(df.tail(1))\\n\\n    # Practically this is same as → df.head(1).fillna(df.tail(1))\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\nIncase there are columns of mixed datatype, partitioning them into it's constituent `dtype` columns and then performing various operations on it would be feasible by chaining them across.\\n\\n\\n\\n    obj_df = df.select_dtypes(include=[np.object])\\n\\n    num_df = df.select_dtypes(exclude=[np.object])\\n\\n    \\n\\n    obj_df.head(1).combine_first(obj_df.tail(1)).join(num_df.head(1).add(num_df.tail(1)))\\n\\n\\n\\n[![enter image description here][3]][3]\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.combine_first.html\\n\\n  [2]: https://i.stack.imgur.com/5uK90.png\\n\\n  [3]: https://i.stack.imgur.com/1OtFD.png\",\n",
       "  '<pandas><dataframe><merge>',\n",
       "  datetime.date(2017, 1, 17),\n",
       "  '2017-01-17 12:51:23',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '4',\n",
       "  '',\n",
       "  '6991.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3119',\n",
       "  '40320495',\n",
       "  'Answer',\n",
       "  'PushDown Automaton (PDA) for L={a^(n)b^(n)c^(n)|n>=1}',\n",
       "  \"One reason you've not managed to construct a pushdown automaton for this language, is because there isn't any. The [Bar Hillel pumping lemma](https://en.wikipedia.org/wiki/Pumping_lemma_for_context-free_languages) shows this. \\n\\n\\n\\nTo outline the proof, suppose it can be done. Then, for some *p*, each string larger than *p* can be partitioned to *uvwxy*, s.t., \\n\\n\\n\\n* *|vwx| < p*\\n\\n\\n\\n* *|vx| > 1*\\n\\n\\n\\n* *uv<sup>n</sup>wx<sup>n</sup>y* is also accepted by the automaton, for any *n*.\\n\\n\\n\\nThe first rule implies that *vwx* can't span the three regions, only at most two (for large enough strings). The second and third rules now imply that you can pump so that the un-spanned region is smaller than the at least one of the other regions.\",\n",
       "  '<computation><pushdown-automaton><pda><automata-theory><jflap>',\n",
       "  datetime.date(2016, 10, 29),\n",
       "  '2016-10-29 15:22:26',\n",
       "  'Ami Tavory (3510736)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1961.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3120',\n",
       "  '40353653',\n",
       "  'Answer',\n",
       "  'How to apply custom function to pandas data frame for each row',\n",
       "  'Apply will pass you along the entire row with axis=1. Adjust like this assuming your two columns are called `initial_pop`and `growth_rate`\\n\\n    def final_pop(row):\\n        return row.initial_pop*math.e**(row.growth_rate*35)',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 11, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '7',\n",
       "  '',\n",
       "  '12244.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3122',\n",
       "  '40361359',\n",
       "  'Answer',\n",
       "  'Numpy roll vertical in 2d array',\n",
       "  'Roll with a negative shift:\\n\\n\\n\\n    x2[:, 0] = np.roll(x2[:, 0], -2)\\n\\n\\n\\nRoll with a positive shift:\\n\\n\\n\\n    x2[:, 4] = np.roll(x2[:, 4], 2)\\n\\n    \\n\\ngives: \\n\\n\\n\\n    >>>x2\\n\\n    array([[14,  1,  2,  3, 39,  5,  6],\\n\\n           [21,  8,  9, 10, 46, 12, 13],\\n\\n           [28, 15, 16, 17,  4, 19, 20],\\n\\n           [35, 22, 23, 24, 11, 26, 27],\\n\\n           [42, 29, 30, 31, 18, 33, 34],\\n\\n           [ 0, 36, 37, 38, 25, 40, 41],\\n\\n           [ 7, 43, 44, 45, 32, 47, 48]])\\n\\n\\n\\n',\n",
       "  '<python><arrays><numpy>',\n",
       "  datetime.date(2016, 11, 1),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1018.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3124',\n",
       "  '41580938',\n",
       "  'Answer',\n",
       "  'Remove duplicates values on one column in a data frame using conditions',\n",
       "  \"You can perform a `groupby` on `'ID'` and take the maximum:\\n\\n\\n\\n    df = df.groupby('ID', as_index=False)['salary'].max()\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n       ID  salary\\n\\n    0   1  1000.0\\n\\n    1   2  1500.0\\n\\n    2   3  1000.0\\n\\n    3   4  2000.0\",\n",
       "  '<python><pandas><dataframe><duplicates>',\n",
       "  datetime.date(2017, 1, 11),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1190.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3125',\n",
       "  '41609238',\n",
       "  'Answer',\n",
       "  'Matplotlib 3D scatter animations',\n",
       "  'The **scatter plot** in 3D is a `mpl_toolkits.mplot3d.art3d.Path3DCollection` object. This provides an attribute `_offsets3d` which hosts a tuple `(x,y,z)` and can be used to update the scatter points\\' coordinates. Therefore it may be beneficial not to create the whole plot on every iteration of the animation, but instead only update its points.\\n\\n\\n\\nThe following is a working example on how to do this.\\n\\n\\n\\n    import numpy as np\\n\\n    from matplotlib import pyplot as plt\\n\\n    from mpl_toolkits.mplot3d import Axes3D\\n\\n    import matplotlib.animation\\n\\n    import pandas as pd\\n\\n    \\n\\n    \\n\\n    a = np.random.rand(2000, 3)*10\\n\\n    t = np.array([np.ones(100)*i for i in range(20)]).flatten()\\n\\n    df = pd.DataFrame({\"time\": t ,\"x\" : a[:,0], \"y\" : a[:,1], \"z\" : a[:,2]})\\n\\n    \\n\\n    def update_graph(num):\\n\\n        data=df[df[\\'time\\']==num]\\n\\n        graph._offsets3d = (data.x, data.y, data.z)\\n\\n        title.set_text(\\'3D Test, time={}\\'.format(num))\\n\\n    \\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax = fig.add_subplot(111, projection=\\'3d\\')\\n\\n    title = ax.set_title(\\'3D Test\\')\\n\\n    \\n\\n    data=df[df[\\'time\\']==0]\\n\\n    graph = ax.scatter(data.x, data.y, data.z)\\n\\n    \\n\\n    ani = matplotlib.animation.FuncAnimation(fig, update_graph, 19, \\n\\n                                   interval=40, blit=False)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\nThis solution does not allow for blitting. However, depending on the usage case, it may not be necessary to use a scatter plot at all; using a **normal `plot`** might be equally possible, which allows for blitting - as seen in the following example.\\n\\n\\n\\n    import numpy as np\\n\\n    from matplotlib import pyplot as plt\\n\\n    from mpl_toolkits.mplot3d import Axes3D\\n\\n    import matplotlib.animation\\n\\n    import pandas as pd\\n\\n    \\n\\n    \\n\\n    a = np.random.rand(2000, 3)*10\\n\\n    t = np.array([np.ones(100)*i for i in range(20)]).flatten()\\n\\n    df = pd.DataFrame({\"time\": t ,\"x\" : a[:,0], \"y\" : a[:,1], \"z\" : a[:,2]})\\n\\n    \\n\\n    def update_graph(num):\\n\\n        data=df[df[\\'time\\']==num]\\n\\n        graph.set_data (data.x, data.y)\\n\\n        graph.set_3d_properties(data.z)\\n\\n        title.set_text(\\'3D Test, time={}\\'.format(num))\\n\\n        return title, graph, \\n\\n    \\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax = fig.add_subplot(111, projection=\\'3d\\')\\n\\n    title = ax.set_title(\\'3D Test\\')\\n\\n    \\n\\n    data=df[df[\\'time\\']==0]\\n\\n    graph, = ax.plot(data.x, data.y, data.z, linestyle=\"\", marker=\"o\")\\n\\n    \\n\\n    ani = matplotlib.animation.FuncAnimation(fig, update_graph, 19, \\n\\n                                   interval=40, blit=True)\\n\\n    \\n\\n    plt.show()',\n",
       "  '<python><animation><matplotlib><plot><3d>',\n",
       "  datetime.date(2017, 1, 12),\n",
       "  '2017-08-18 13:34:56',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '10',\n",
       "  '',\n",
       "  '5816.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3134',\n",
       "  '41771968',\n",
       "  'Answer',\n",
       "  'How to change ticks fontsize of a plot using PyQtgraph',\n",
       "  'I think the only way to change the font size of the ticklabels in pyqtgraph is to first create a new font within PyQt and set the fontsize to it. Then this font can be applied to the ticks. \\n\\n\\n\\n    font=QtGui.QFont()\\n\\n    font.setPixelSize(20)\\n\\n    plot.getAxis(\"bottom\").tickFont = font\\n\\n\\n\\nInitially I would have thought that something like  \\n\\n`plot.getAxis(\"bottom\").setStyle(tickFont = font)`  \\n\\nshould work as well, but for some reason it doesn\\'t. \\n\\n\\n\\nOnce the font size has increased, it may make sense to adapt the tickOffset as well. Find a complete running code is below.\\n\\n\\n\\n\\n\\n    import numpy as np\\n\\n    from pyqtgraph.Qt import QtGui, QtCore\\n\\n    import pyqtgraph as pg\\n\\n    \\n\\n    \\n\\n    app = QtGui.QApplication([])\\n\\n    \\n\\n    x = np.linspace(-50, 50, 1000)\\n\\n    y = np.sin(x) / x\\n\\n    \\n\\n    win = pg.GraphicsWindow()\\n\\n    plot = win.addPlot(x=x, y=y, title=\"Plot\")\\n\\n    plot.setLabel(\\'bottom\\', \"some x axis label\")\\n\\n    \\n\\n    font=QtGui.QFont()\\n\\n    font.setPixelSize(20)\\n\\n    plot.getAxis(\"bottom\").tickFont = font\\n\\n    plot.getAxis(\"bottom\").setStyle(tickTextOffset = 20)\\n\\n    \\n\\n    \\n\\n    if __name__ == \\'__main__\\':\\n\\n        import sys\\n\\n        if (sys.flags.interactive != 1) or not hasattr(QtCore, \\'PYQT_VERSION\\'):\\n\\n            QtGui.QApplication.instance().exec_()\\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/5Lm3i.png\\n\\n',\n",
       "  '<python><plot><font-size><pyqtgraph>',\n",
       "  datetime.date(2017, 1, 20),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1440.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3142',\n",
       "  '41687202',\n",
       "  'Answer',\n",
       "  'Live Plotting with PyQtGraph in PyQt4',\n",
       "  \"There are several things to consider in the code.  \\n\\nFirst, going in the same direction as @luddek's answer, you need to keep track of your variables. If you define a variable inside a class method and this class method finished executing, the variable is lost. Also it is not visible from the outside.  \\n\\nIt is therefore a good idea to use instance variables,   \\n\\n`self.plot` instead of `plot`  \\n\\n`self.timer` instead of `timer`  \\n\\n`self.login_widget` instead of `login_widget`  \\n\\n(also, I would not recomment using `global` in a PyQt programm, although it is valid code)\\n\\n\\n\\nNext, `PlotWidget` does not have a `setData` method. Plotting the data to the PlotItem's plot is a little more involved:   \\n\\n`pg.PlotWidget()` has a `PlotItem` which you can get via `.getPlotItem()`. Then you need to invoke `plot()` on it, which returns the actual curve you want to add data to. In the example below I introduced the new variable `self.curve` to which you can add the data via `self.curve.setData(self.data)`\\n\\n\\n\\nHere is the complete working code.\\n\\n\\n\\n\\n\\n    from PyQt4 import QtCore, QtGui\\n\\n    import pyqtgraph as pg\\n\\n    import random\\n\\n    \\n\\n    class MainWindow(QtGui.QMainWindow):\\n\\n        def __init__(self, parent=None):\\n\\n            super(MainWindow, self).__init__(parent)\\n\\n            self.central_widget = QtGui.QStackedWidget()\\n\\n            self.setCentralWidget(self.central_widget)\\n\\n            self.login_widget = LoginWidget(self)\\n\\n            self.login_widget.button.clicked.connect(self.plotter)\\n\\n            self.central_widget.addWidget(self.login_widget)\\n\\n    \\n\\n        def plotter(self):\\n\\n            self.data =[0]\\n\\n            self.curve = self.login_widget.plot.getPlotItem().plot()\\n\\n            \\n\\n            self.timer = QtCore.QTimer()\\n\\n            self.timer.timeout.connect(self.updater)\\n\\n            self.timer.start(0)\\n\\n    \\n\\n        def updater(self):\\n\\n    \\n\\n            self.data.append(self.data[-1]+0.2*(0.5-random.random()) )\\n\\n            self.curve.setData(self.data)\\n\\n    \\n\\n    class LoginWidget(QtGui.QWidget):\\n\\n        def __init__(self, parent=None):\\n\\n            super(LoginWidget, self).__init__(parent)\\n\\n            layout = QtGui.QHBoxLayout()\\n\\n            self.button = QtGui.QPushButton('Start Plotting')\\n\\n            layout.addWidget(self.button)\\n\\n            self.plot = pg.PlotWidget()\\n\\n            layout.addWidget(self.plot)\\n\\n            self.setLayout(layout)\\n\\n    \\n\\n    if __name__ == '__main__':\\n\\n        app = QtGui.QApplication([])\\n\\n        window = MainWindow()\\n\\n        window.show()\\n\\n        app.exec_()\",\n",
       "  '<python><pyqt><pyqt4><pyqtgraph>',\n",
       "  datetime.date(2017, 1, 17),\n",
       "  '2018-05-01 12:00:34',\n",
       "  'ImportanceOfBeingErnest (4124317), Randy Welt (4132112)',\n",
       "  '3',\n",
       "  '',\n",
       "  '3367.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3144',\n",
       "  '40369002',\n",
       "  'Answer',\n",
       "  'Converting month to quarter in python dataframe',\n",
       "  \"Same idea as @MaxU but using `astype`:\\n\\n\\n\\n    hp2['Qtr'] = pd.to_datetime(hp2['Mth'].values, format='%Y-%m').astype('period[Q]')\\n\\n\\n\\nThe resulting output:\\n\\n\\n\\n            Mth    Qtr\\n\\n    0   2014-01 2014Q1\\n\\n    1   2017-02 2017Q1\\n\\n    2   2016-03 2016Q1\\n\\n    3   2017-04 2017Q2\\n\\n    4   2016-05 2016Q2\\n\\n    5   2016-06 2016Q2\\n\\n    6   2017-07 2017Q3\\n\\n    7   2016-08 2016Q3\\n\\n    8   2017-09 2017Q3\\n\\n    9   2015-10 2015Q4\\n\\n    10  2017-11 2017Q4\\n\\n    11  2015-12 2015Q4\\n\\n\\n\\n**Timings**\\n\\n\\n\\nUsing the following setup to produce a large sample dataset:\\n\\n\\n\\n    n = 10**5\\n\\n    yrs = np.random.choice(range(2010, 2021), n)\\n\\n    mths = np.random.choice(range(1, 13), n)\\n\\n    df = pd.DataFrame({'Mth': ['{0}-{1:02d}'.format(*p) for p in zip(yrs, mths)]})\\n\\n\\n\\nI get the following timings:\\n\\n\\n\\n    %timeit pd.to_datetime(df['Mth'].values, format='%Y-%m').astype('period[Q]')\\n\\n    10 loops, best of 3: 33.4 ms per loop\\n\\n    \\n\\n    %timeit pd.PeriodIndex(pd.to_datetime(df.Mth), freq='Q')\\n\\n    1 loop, best of 3: 2.68 s per loop\\n\\n    \\n\\n    %timeit df['Mth'].map(lambda x: pd.Period(x,'Q'))\\n\\n    1 loop, best of 3: 6.26 s per loop\\n\\n\\n\\n    %timeit df.apply(lambda x: pd.Period(x['Mth'],'Q'),axis=1)\\n\\n    1 loop, best of 3: 9.49 s per loop\\n\\n\\n\\n\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 11, 1),\n",
       "  '2016-11-01 22:32:55',\n",
       "  'root (3339965)',\n",
       "  '0',\n",
       "  '',\n",
       "  '4606.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3147',\n",
       "  '41815114',\n",
       "  'Answer',\n",
       "  \"How to change colorbar's color (in some particular value interval)?\",\n",
       "  'You basically need to create your own colormap that has the particular features you want. Of course it is possible to make use of existing colormaps when doing so. \\n\\n\\n\\nColormaps are always ranged between `0` and `1`. This range will then be mapped to the data interval. So in order to create whites between `-0.5` and `0.5` we need to know the range of data - let\\'s say data goes from `-1` to `1`. We can then decide to have the lower (blues) part of the seismic map go from `-1` to `-0.5`, then have white between `-0.5` and `+0.5` and finally the upper part of the seismic map (reds) from `0.5` to `1`. In the language of a colormap this corresponds to the ranges `[0,0.25]`, `[0.25, 0.75]` and `[0.75,1]`. We can then create a list, with the first and last 25% percent being the colors of the seismic map and the middle 50% white.\\n\\n\\n\\nThis list can be used to create a colormap, using `matplotlib.colors.LinearSegmentedColormap.from_list(\"colormapname\", listofcolors)`.\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    import matplotlib.colors\\n\\n    \\n\\n    n=50\\n\\n    x = 0.5\\n\\n    lower = plt.cm.seismic(np.linspace(0, x, n))\\n\\n    white = plt.cm.seismic(np.ones(100)*0.5)\\n\\n    upper = plt.cm.seismic(np.linspace(1-x, 1, n))\\n\\n    colors = np.vstack((lower, white, upper))\\n\\n    tmap = matplotlib.colors.LinearSegmentedColormap.from_list(\\'terrain_map_white\\', colors)\\n\\n    \\n\\n    x = np.linspace(0,10)\\n\\n    X,Y = np.meshgrid(x,x)\\n\\n    z = np.sin(X) * np.cos(Y*0.4)\\n\\n    \\n\\n    fig, ax = plt.subplots()\\n\\n    im = ax.imshow(z, cmap=tmap)    \\n\\n    plt.colorbar(im)\\n\\n\\n\\n    plt.show() \\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nFor more general cases, you may need a color normalization (using `matplotlib.colors.Normalize`). See e.g. [this example](https://stackoverflow.com/questions/40895021/python-equivalent-for-matlabs-demcmap-elevation-appropriate-colormap), where a certain color in the colormap is always fixed at a data value of 0, independent of the data range.\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/xKm8Q.png',\n",
       "  '<matplotlib><colorbar>',\n",
       "  datetime.date(2017, 1, 23),\n",
       "  '2017-05-23 11:52:53',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1413.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3152',\n",
       "  '41738365',\n",
       "  'Answer',\n",
       "  'How to print numpy matrix nicely with text headers - python',\n",
       "  'Numpy does not provide such a functionality out of the box. \\n\\n\\n\\n###(a) pandas\\n\\nYou may look into pandas. Printing a `pandas.DataFrame` usually looks quite nice.\\n\\n\\n\\n    import numpy as np\\n\\n    import pandas as pd\\n\\n    cols = [\"T\", \"C\", \"S\", \"W\", \"Q\"]\\n\\n    a = np.random.randint(0,11,size=(5,5))\\n\\n    df = pd.DataFrame(a, columns=cols, index=cols)\\n\\n    print df\\n\\n\\n\\nwill produce\\n\\n\\n\\n       T  C   S  W  Q\\n\\n    T  9  5  10  0  0\\n\\n    C  3  8   0  7  2\\n\\n    S  0  2   6  5  8\\n\\n    W  4  4  10  1  5\\n\\n    Q  3  8   7  1  4\\n\\n\\n\\n###(b) pure python\\n\\nIf you only have pure python available, you can use the following function.\\n\\n\\n\\n    import numpy as np\\n\\n    \\n\\n    def print_array(a, cols, rows):\\n\\n        if (len(cols) != a.shape[1]) or (len(rows) != a.shape[0]):\\n\\n            print \"Shapes do not match\"\\n\\n            return\\n\\n        s = a.__repr__()\\n\\n        s = s.split(\"array(\")[1]\\n\\n        s = s.replace(\"      \", \"\")\\n\\n        s = s.replace(\"[[\", \" [\")\\n\\n        s = s.replace(\"]])\", \"]\")\\n\\n        pos = [i for i, ltr in enumerate(s.splitlines()[0]) if ltr == \",\"]\\n\\n        pos[-1] = pos[-1]-1\\n\\n        empty = \" \" * len(s.splitlines()[0])\\n\\n        s = s.replace(\"],\", \"]\")\\n\\n        s = s.replace(\",\", \"\")\\n\\n        lines = []\\n\\n        for i, l in enumerate(s.splitlines()):\\n\\n            lines.append(rows[i] + l)\\n\\n        s  =\"\\\\n\".join(lines)\\n\\n        empty = list(empty)\\n\\n        for i, p in enumerate(pos):\\n\\n            empty[p-i] = cols[i]\\n\\n        s = \"\".join(empty) + \"\\\\n\" + s\\n\\n        print s\\n\\n    \\n\\n    \\n\\n    \\n\\n    c = [\" \", \"T\", \"C\", \"G\", \"C\", \"A\"]\\n\\n    r = [\" \", \"T\", \"C\", \"C\", \"A\" ]\\n\\n    a = np.random.randint(-4,15,size=(5,6))    \\n\\n    print_array(a, c, r)\\n\\n\\n\\ngiving you\\n\\n\\n\\n           T  C  G  C  A      \\n\\n      [ 2  5 -3  7  1  9]\\n\\n    T [-3 10  3 -4  8  3]\\n\\n    C [ 6 11 -2  2  5  1]\\n\\n    C [ 4  6 14 11 10  0]\\n\\n    A [11 -4 -3 -4 14 14]\\n\\n\\n\\n',\n",
       "  '<python-2.7><numpy>',\n",
       "  datetime.date(2017, 1, 19),\n",
       "  '2017-01-20 09:38:07',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1169.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3154',\n",
       "  '41841117',\n",
       "  'Answer',\n",
       "  'plotting graph using strings as x axis labels using python',\n",
       "  \"The data columns you are reading in are stings, and because you specify `dtype='str'` in the call to `loadtxt`, your arrays `x`,`y` and `my_ticks` are as well. While matplotlib is happy to have strings provided in `plt.plot` it is not happy with strings in `plt.xticks`. So you need to convert the string array to float first, before passing it to `xticks`.\\n\\n\\n\\n    plt.xticks(x.astype(float), my_xticks)\",\n",
       "  '<python><matplotlib><graph>',\n",
       "  datetime.date(2017, 1, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1071.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3157',\n",
       "  '41681743',\n",
       "  'Answer',\n",
       "  'Rank within groups using python-pandas',\n",
       "  \"You require `method=dense` in [`SeriesGroupBy.rank()`][1] where the ranks increase by 1 between groups:\\n\\n\\n\\n    df['z_rank'] = df.groupby(['instance', 'D'])['z'].rank(method='dense').astype(int)\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.rank.html\\n\\n  [2]: https://i.stack.imgur.com/GCOsW.png\",\n",
       "  '<python-3.x><pandas><data-analysis>',\n",
       "  datetime.date(2017, 1, 16),\n",
       "  '',\n",
       "  '',\n",
       "  '8',\n",
       "  '',\n",
       "  '1963.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3160',\n",
       "  '41706895',\n",
       "  'Answer',\n",
       "  'How to obscure a line behind a surface plot in matplotlib?',\n",
       "  'The problem is that matplotlib is no ray tracer and it\\'s not really designed to be a 3D capable plotting library. As such it works with a system of layers in 2D space, and objects can be in a layer more in front or more to the back. This can be set with the `zorder` keyword argument to most plotting functions. However there is no awareness in matplotlib about whether an object is in front or behind another object in 3D space. Therefore you can either have the complete line visible (in front of the sphere) or hidden (behind it). \\n\\n\\n\\n**The solution would be to calculate the points that should be visible by yourself.** I\\'m talking about points here because a line would be connecting visible points \"through\" the sphere, which is unwanted. I therefore restrict myself to plotting points - but if you have enough of them, they look like a line :-).\\n\\n\\n\\nThe calculation of which points should be visible is not too hard for a perfect sphere, and the idea is the following:  \\n\\n\\n\\n1. Obtain the viewing angle of the 3D plot\\n\\n2. From that, calculate the normal vector to the plane of vision in data coordinates in direction of the view.\\n\\n3. Calculate the scalar product between this normal vector (called `X` in the code below) and the line points in order to use this scalar product as a condition on whether to show the points or not. If the scalar product is smaller than `0` then the respective point is on the other side of the viewing plane as seen from the observer and should therefore not be shown.\\n\\n5. Filter the points by the condition.\\n\\n\\n\\nOne further optional task is then to adapt the points shown for the case when the user rotates the view. This is accomplished by connecting the `motion_notify_event` to a function that updates the data using the procedure from above, based on the newly set viewing angle.\\n\\n\\n\\nSee the code below on how to implement this. \\n\\n\\n\\n    import matplotlib\\n\\n    import matplotlib.pyplot as plt\\n\\n    from mpl_toolkits.mplot3d import Axes3D\\n\\n    import numpy as np\\n\\n    \\n\\n    \\n\\n    NPoints_Phi         = 30\\n\\n    NPoints_Theta       = 30\\n\\n    \\n\\n    phi_array           = ((np.linspace(0, 1, NPoints_Phi))**1) * 2*np.pi\\n\\n    theta_array         = (np.linspace(0, 1, NPoints_Theta) **1) * np.pi\\n\\n    \\n\\n    radius=1\\n\\n    phi, theta          = np.meshgrid(phi_array, theta_array) \\n\\n    \\n\\n    x_coord             = radius*np.sin(theta)*np.cos(phi)\\n\\n    y_coord             = radius*np.sin(theta)*np.sin(phi)\\n\\n    z_coord             = radius*np.cos(theta)\\n\\n    \\n\\n    #Make colormap the fourth dimension\\n\\n    color_dimension     = x_coord \\n\\n    minn, maxx          = color_dimension.min(), color_dimension.max()\\n\\n    norm                = matplotlib.colors.Normalize(minn, maxx)\\n\\n    m                   = plt.cm.ScalarMappable(norm=norm, cmap=\\'jet\\')\\n\\n    m.set_array([])\\n\\n    fcolors             = m.to_rgba(color_dimension)\\n\\n    \\n\\n    theta2              = np.linspace(-np.pi,  0, 1000)\\n\\n    phi2                = np.linspace( 0, 5 * 2*np.pi , 1000)\\n\\n    \\n\\n    x_coord_2           = radius * np.sin(theta2) * np.cos(phi2)\\n\\n    y_coord_2           = radius * np.sin(theta2) * np.sin(phi2)\\n\\n    z_coord_2           = radius * np.cos(theta2)\\n\\n    \\n\\n    # plot\\n\\n    fig = plt.figure()\\n\\n    \\n\\n    ax = fig.gca(projection=\\'3d\\')\\n\\n    # plot empty plot, with points (without a line)\\n\\n    points, = ax.plot([],[],[],\\'k.\\', markersize=5, alpha=0.9)\\n\\n    #set initial viewing angles\\n\\n    azimuth, elev = 75, 21\\n\\n    ax.view_init(elev, azimuth )\\n\\n    \\n\\n    def plot_visible(azimuth, elev):\\n\\n        #transform viewing angle to normal vector in data coordinates\\n\\n        a = azimuth*np.pi/180. -np.pi\\n\\n        e = elev*np.pi/180. - np.pi/2.\\n\\n        X = [ np.sin(e) * np.cos(a),np.sin(e) * np.sin(a),np.cos(e)]  \\n\\n        # concatenate coordinates\\n\\n        Z = np.c_[x_coord_2, y_coord_2, z_coord_2]\\n\\n        # calculate dot product \\n\\n        # the points where this is positive are to be shown\\n\\n        cond = (np.dot(Z,X) >= 0)\\n\\n        # filter points by the above condition\\n\\n        x_c = x_coord_2[cond]\\n\\n        y_c = y_coord_2[cond]\\n\\n        z_c = z_coord_2[cond]\\n\\n        # set the new data points\\n\\n        points.set_data(x_c, y_c)\\n\\n        points.set_3d_properties(z_c, zdir=\"z\")\\n\\n        fig.canvas.draw_idle()\\n\\n        \\n\\n    plot_visible(azimuth, elev)\\n\\n    ax.plot_surface(x_coord,y_coord,z_coord, rstride=1, cstride=1, \\n\\n                facecolors=fcolors, vmin=minn, vmax=maxx, shade=False)\\n\\n    \\n\\n    # in order to always show the correct points on the sphere, \\n\\n    # the points to be shown must be recalculated one the viewing angle changes\\n\\n    # when the user rotates the plot\\n\\n    def rotate(event):\\n\\n        if event.inaxes == ax:\\n\\n            plot_visible(ax.azim, ax.elev)\\n\\n    \\n\\n    c1 = fig.canvas.mpl_connect(\\'motion_notify_event\\', rotate)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nAt the end one may have to play a bit with the `markersize`, `alpha` and the number of points in order to get the most visually appealing result out of this.\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/u7j32.png',\n",
       "  '<python><matplotlib><plot><mayavi>',\n",
       "  datetime.date(2017, 1, 17),\n",
       "  '2017-01-17 21:05:59',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '7',\n",
       "  '',\n",
       "  '1122.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3161',\n",
       "  '41709394',\n",
       "  'Answer',\n",
       "  'How to change the plot line color from blue to black?',\n",
       "  'The usual way to set the line color in matplotlib is to specify it in the plot command. This can either be done by a string after the data, e.g. `\"r-\"` for a red line, or by explicitely stating the `color` argument.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    plt.plot([1,2,3], [2,3,1], \"r-\") # red line\\n\\n    plt.plot([1,2,3], [5,5,3], color=\"blue\") # blue line\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\nSee also the [plot command\\'s documentation](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.plot).\\n\\n\\n\\nIn case you already have a line with a certain color, you can change that with the `lines2D.set_color()` method.\\n\\n\\n\\n    line, = plt.plot([1,2,3], [4,5,3], color=\"blue\")\\n\\n    line.set_color(\"black\")\\n\\n\\n\\n<hr>\\n\\nSetting the color of a line in a pandas plot is also best done at the point of creating the plot:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import pandas as pd\\n\\n    \\n\\n    df = pd.DataFrame({ \"x\" : [1,2,3,5], \"y\" : [3,5,2,6]})\\n\\n    df.plot(\"x\", \"y\", color=\"r\") #plot red line\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\nIf you want to change this color later on, you can do so by\\n\\n\\n\\n\\n\\n    plt.gca().get_lines()[0].set_color(\"black\")\\n\\n\\n\\nThis will get you the first (possibly the only) line of the current active axes.  \\n\\nIn case you have more axes in the plot, you could loop through them \\n\\n\\n\\n    for ax in plt.gcf().axes:\\n\\n        ax.get_lines()[0].set_color(\"black\")\\n\\n\\n\\nand if you have more lines you can loop over them as well.\\n\\n',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 1, 18),\n",
       "  '2017-01-20 18:47:21',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '30',\n",
       "  '',\n",
       "  '44905.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3162',\n",
       "  '39417275',\n",
       "  'Answer',\n",
       "  'Regex capturing group within non-capturing group',\n",
       "  \"Your question is phrased strictly about regex, but if you're willing to use a [recursive descent parser](https://en.wikipedia.org/wiki/Recursive_descent_parser) (e.g., [`pyparsing`](http://pyparsing.wikispaces.com/)), many things that require expertise in regex, become very simple.\\n\\n\\n\\nE.g., here what you're asking becomes\\n\\n\\n\\n    from pyparsing import *\\n\\n\\n\\n    p = Suppress(Literal('import')) + commaSeparatedList\\n\\n\\n\\n    >>> p.parseString('import pandas, os, sys').asList()\\n\\n    ['pandas', 'os', 'sys']\\n\\n\\n\\n    >>> p.parseString('import                    pandas,             os').asList()\\n\\n    ['pandas', 'os']\\n\\n\\n\\n-----------------------\\n\\n\\n\\nIt might be a matter of personal taste, but to me, \\n\\n\\n\\n    Suppress(Literal('import')) + commaSeparatedList\\n\\n\\n\\nis also more intuitive than a regex.\",\n",
       "  '<python><regex>',\n",
       "  datetime.date(2016, 9, 9),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1308.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3163',\n",
       "  '39419288',\n",
       "  'Answer',\n",
       "  'Find Magic Numbers C++',\n",
       "  'This question basically asks to verify the [Collatz Conjecture](https://en.wikipedia.org/wiki/Collatz_conjecture) up to 5B.\\n\\n\\n\\nI think the key here is, for each number *n* we are checking, to view the optimistic scenario and the pessimistic scenario, and to check the optimistic one before reverting to the pessimistic one. \\n\\n\\n\\nIn the optimistic scenario, as we we modify *n* according to the *n / 2 ; 3n + 1* rule, the sequence of numbers will:\\n\\n\\n\\n* in a finite number of steps become smaller than *n* (in which case we can check what we know about that smaller number).\\n\\n\\n\\n* will not cause an overflow in the steps.\\n\\n\\n\\n(on which, as TonyK correctly notes, we cannot rely (even not on the first)).\\n\\n\\n\\nSo, for the optimistic scenario, we can use the following function:\\n\\n\\n\\n    #include <unordered_set>\\n\\n    #include <set>\\n\\n    #include <iostream>\\n\\n    #include <memory>\\n\\n    #include <list>\\n\\n    #include <gmp.h>\\n\\n\\n\\n    using namespace std;\\n\\n\\n\\n    using found_set = unordered_set<size_t>;\\n\\n\\n\\n    bool fast_verify(size_t i, size_t max_tries, const found_set &found) {\\n\\n        size_t tries = 0;\\n\\n        size_t n = i;\\n\\n        while(n != 1) {\\n\\n            if(++tries == max_tries ) \\n\\n                return false;\\n\\n\\n\\n            if(n < i)\\n\\n                return found.empty() || found.find(i) == found.end();\\n\\n            if(n % 2 == 0)\\n\\n                n /= 2;\\n\\n            else if(__builtin_mul_overflow (n, 3, &n) || __builtin_add_overflow(n, 1, &n))\\n\\n                return false;\\n\\n        }   \\n\\n\\n\\n        return true;\\n\\n    }\\n\\n\\n\\nNote the following:\\n\\n\\n\\n1. The function only attempts to verify the conjecture for the number it receives. If it returns `true`, it has been verified. If it returns `false`, it just means it hasn\\'t been verified (i.e., it does not mean it has been disproved).\\n\\n\\n\\n2. It takes a parameter `max_tries`, and only verifies up to this number of steps. If the number has been exceeded, it does not try to discern whether this is part of an infinite loop or not - it just returns that verification failed.\\n\\n\\n\\n3. It takes an `unordered_set` of known numbers that have failed (of course, if the Collatz conjecture is true, this set will always be empty).\\n\\n\\n\\n4. It detects overflow via [`__builtin_*_overflow`](https://gcc.gnu.org/onlinedocs/gcc/Integer-Overflow-Builtins.html). (Unfortunately, this is gcc specific, then. A different platform might require a different set of such functions.)\\n\\n\\n\\nNow for the slow-but-sure function. This function uses the [GNU MP  multi-precision arithmetic library](https://gmplib.org/manual/index.html#Top). It checks for an infinite loop by maintaining the sequence of numbers it has already encountered. This function returns `true` if the conjecture has been proved for this number, and `false` if has been disproved for this number (note the difference from the previous fast verification).\\n\\n\\n\\n    bool slow_check(size_t i) {\\n\\n        mpz_t n_; \\n\\n        mpz_init(n_);\\n\\n\\n\\n        mpz_t rem_;\\n\\n        mpz_init(rem_);\\n\\n\\n\\n        mpz_t i_; \\n\\n        mpz_init(i_);\\n\\n\\n\\n        mpz_set_ui(i_, i); \\n\\n        mpz_set_ui(n_, i); \\n\\n\\n\\n        list<mpz_t> seen;\\n\\n        \\n\\n        while(mpz_cmp_ui(n_, 1) != 0) {\\n\\n            if(mpz_cmp_ui(n_, i) < 0)\\n\\n                return true;\\n\\n            mpz_mod_ui(rem_, n_, 2); \\n\\n            if(mpz_cmp_ui(rem_, 0) == 0) {\\n\\n                mpz_div_ui(n_, n_, 2); \\n\\n            }   \\n\\n            else {\\n\\n                mpz_mul_ui(n_, n_, 3); \\n\\n                mpz_add_ui(n_, n_, 1); \\n\\n           }   \\n\\n           seen.emplace_back(n_);\\n\\n           for(const auto &e0: seen)\\n\\n               for(const auto &e1: seen)\\n\\n                   if(&e0 != &e1 && mpz_cmp(e0, e1) == 0)\\n\\n                       return false;\\n\\n       }   \\n\\n\\n\\n       return true;\\n\\n    }\\n\\n\\n\\nFinally, `main` maintains an `unordered_set` of the disproven numbers. for each number, it optimistically tries to verify the conjecture, then, if it fails (for overflow or exceeding the number of iterations), uses the slow method:\\n\\n\\n\\n    int main()\\n\\n    {\\n\\n        const auto max_num = 5000000000;\\n\\n        found_set found;\\n\\n\\n\\n        for(size_t i = 1; i <= max_num; i++) {\\n\\n            if(i % 1000000000 == 0)\\n\\n                cout << \"iteration \" << i << endl;\\n\\n\\n\\n            auto const f = fast_verify(i, max_num, found);\\n\\n            if(!f && !slow_check(i))\\n\\n                found.insert(i);\\n\\n        }\\n\\n\\n\\n        for(auto e: found)\\n\\n            cout << e << endl;\\n\\n    }\\n\\n\\n\\n--------------------------\\n\\n\\n\\nRunning the full code (below) gives:\\n\\n\\n\\n    $ g++ -O3 --std=c++11 magic2.cpp -lgmp && time ./a.out\\n\\n    iteration 1000000000\\n\\n    iteration 2000000000\\n\\n    iteration 3000000000\\n\\n    iteration 4000000000\\n\\n    iteration 5000000000\\n\\n\\n\\n    real\\t1m3.933s\\n\\n    user\\t1m3.924s\\n\\n    sys\\t0m0.000s\\n\\n\\n\\n    $ uname -a\\n\\n    Linux atavory 4.4.0-38-generic #57-Ubuntu SMP Tue Sep 6 15:42:33 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux\\n\\n    $ sudo lshw | grep -i cpu\\n\\n          *-cpu\\n\\n               description: CPU\\n\\n               product: Intel(R) Core(TM) i7-4720HQ CPU @ 2.60GHz\\n\\n               bus info: cpu@0\\n\\n               version: Intel(R) Core(TM) i7-4720HQ CPU @ 2.60GHz\\n\\n               capabilities: x86-64 fpu fpu_exception wp vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm epb tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid xsaveopt dtherm ida arat pln pts cpufreq\\n\\n\\n\\nI.e., no disproven numbers found, and the running time at ~64 seconds.\\n\\n\\n\\n--------------------------\\n\\n\\n\\nFull code:\\n\\n\\n\\n    #include <unordered_set>\\n\\n    #include <set>\\n\\n    #include <iostream>\\n\\n    #include <memory>\\n\\n    #include <list>\\n\\n    #include <gmp.h>\\n\\n\\n\\n    using namespace std;\\n\\n\\n\\n    using found_set = unordered_set<size_t>;\\n\\n\\n\\n    bool fast_verify(size_t i, size_t max_tries, const found_set &found) {\\n\\n        size_t tries = 0;\\n\\n        size_t n = i;\\n\\n        while(n != 1) {\\n\\n            if(++tries == max_tries )\\n\\n                return false;\\n\\n\\n\\n            if(n < i)\\n\\n                return found.empty() || found.find(i) == found.end();\\n\\n            if(n % 2 == 0)\\n\\n                n /= 2;\\n\\n            else if(__builtin_mul_overflow (n, 3, &n) || __builtin_add_overflow(n, 1, &n))\\n\\n                return false;\\n\\n        }   \\n\\n        \\n\\n        return true;\\n\\n    }\\n\\n\\n\\n    bool slow_check(size_t i) {\\n\\n        mpz_t n_; \\n\\n        mpz_init(n_);\\n\\n\\n\\n        mpz_t rem_;\\n\\n        mpz_init(rem_);\\n\\n\\n\\n        mpz_t i_; \\n\\n        mpz_init(i_);\\n\\n\\n\\n        mpz_set_ui(i_, i); \\n\\n        mpz_set_ui(n_, i); \\n\\n\\n\\n        list<mpz_t> seen;\\n\\n            \\n\\n        while(mpz_cmp_ui(n_, 1) != 0) {\\n\\n            if(mpz_cmp_ui(n_, i) < 0)\\n\\n                return true;\\n\\n            mpz_mod_ui(rem_, n_, 2); \\n\\n            if(mpz_cmp_ui(rem_, 0) == 0) {\\n\\n                mpz_div_ui(n_, n_, 2); \\n\\n            }   \\n\\n            else {\\n\\n                mpz_mul_ui(n_, n_, 3); \\n\\n                mpz_add_ui(n_, n_, 1); \\n\\n           }   \\n\\n           seen.emplace_back(n_);\\n\\n           for(const auto &e0: seen)\\n\\n               for(const auto &e1: seen)\\n\\n                   if(&e0 != &e1 && mpz_cmp(e0, e1) == 0)\\n\\n                       return false;\\n\\n       }   \\n\\n\\n\\n       return true;\\n\\n    }\\n\\n\\n\\n\\n\\n    int main()\\n\\n    {\\n\\n        const auto max_num = 5000000000;\\n\\n        found_set found;\\n\\n\\n\\n        for(size_t i = 1; i <= max_num; i++) {\\n\\n            if(i % 1000000000 == 0)\\n\\n                cout << \"iteration \" << i << endl;\\n\\n\\n\\n            auto const f = fast_verify(i, max_num, found);\\n\\n            if(!f && !slow_check(i))\\n\\n                found.insert(i);\\n\\n        }\\n\\n\\n\\n        for(auto e: found)\\n\\n            cout << e << endl;\\n\\n\\n\\n        return 0;\\n\\n    }\\n\\n\\n\\n\\n\\n\\n\\n',\n",
       "  '<c++><magic-numbers>',\n",
       "  datetime.date(2016, 9, 9),\n",
       "  '2016-10-09 21:43:27',\n",
       "  'Ami Tavory (3510736), 2785528 (2785528)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1418.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3167',\n",
       "  '41746837',\n",
       "  'Answer',\n",
       "  '\"ValueError: year is out of range\" when attempting to use matplotlib pyplot',\n",
       "  'You need to convert your timestamp-like x data to a python datetime object, which can then be used in matplotlib and be understood by the `matplotlib.dates.DateFormatter`.\\n\\n\\n\\nThis can be done using the `datetime.datetime.fromtimestamp()` method.\\n\\n\\n\\n\\n\\n    import datetime\\n\\n    import matplotlib.dates\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    data = [\\n\\n            [1484611200.0, 844.4333],\\n\\n            [1484524800.0, 783.3373],\\n\\n            [1484438400.0, 774.194 ],\\n\\n            [1484352000.0, 769.2299]\\n\\n        ]\\n\\n    \\n\\n    x = [datetime.datetime.fromtimestamp(element[0]) for element in data]\\n\\n    y = [element[1] for element in data]\\n\\n    \\n\\n    plt.plot( x,  y,  ls=\"-\",  c= \"b\",  linewidth  = 2 )\\n\\n    plt.xlabel(\"Dates\")\\n\\n    \\n\\n    time_formatter = matplotlib.dates.DateFormatter(\"%Y-%m-%d\")\\n\\n    plt.axes().xaxis.set_major_formatter(time_formatter)\\n\\n    plt.axes().xaxis_date() # this is not actually necessary\\n\\n        \\n\\n    plt.show()',\n",
       "  '<datetime><matplotlib><time><unix-timestamp>',\n",
       "  datetime.date(2017, 1, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '3835.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3169',\n",
       "  '41765095',\n",
       "  'Answer',\n",
       "  'Single legend item with two lines',\n",
       "  'In the matplotlib legend guide there is a chapter about [custom legend handlers](http://matplotlib.org/users/legend_guide.html#legend-handlers). You could adapt it to your needs, e.g. like this:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    from matplotlib.legend_handler import HandlerBase\\n\\n    \\n\\n    \\n\\n    class AnyObjectHandler(HandlerBase):\\n\\n        def create_artists(self, legend, orig_handle,\\n\\n                           x0, y0, width, height, fontsize, trans):\\n\\n            l1 = plt.Line2D([x0,y0+width], [0.7*height,0.7*height], \\n\\n                                                    linestyle=\\'--\\', color=\\'k\\')\\n\\n            l2 = plt.Line2D([x0,y0+width], [0.3*height,0.3*height], color=\\'r\\')\\n\\n            return [l1, l2]\\n\\n    \\n\\n    \\n\\n    x = np.linspace(0, 3)\\n\\n    fig, axL = plt.subplots(figsize=(4,3))\\n\\n    axR = axL.twinx()\\n\\n    \\n\\n    axL.plot(x, np.sin(x), color=\\'k\\', linestyle=\\'--\\')\\n\\n    axR.plot(x, 100*np.cos(x), color=\\'r\\')\\n\\n    \\n\\n    axL.set_ylabel(\\'sin(x)\\', color=\\'k\\')\\n\\n    axR.set_ylabel(\\'100 cos(x)\\', color=\\'r\\')\\n\\n    axR.tick_params(\\'y\\', colors=\\'r\\')\\n\\n    \\n\\n    plt.legend([object], [\\'label\\'],\\n\\n               handler_map={object: AnyObjectHandler()})\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nIn order to have multiple such entries, one can supply some tuple of parameters that the `Handler` then uses to draw the legend.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    from matplotlib.legend_handler import HandlerBase\\n\\n    \\n\\n    \\n\\n    class AnyObjectHandler(HandlerBase):\\n\\n        def create_artists(self, legend, orig_handle,\\n\\n                           x0, y0, width, height, fontsize, trans):\\n\\n            l1 = plt.Line2D([x0,y0+width], [0.7*height,0.7*height],\\n\\n                               linestyle=orig_handle[1], color=\\'k\\')\\n\\n            l2 = plt.Line2D([x0,y0+width], [0.3*height,0.3*height], \\n\\n                               color=orig_handle[0])\\n\\n            return [l1, l2]\\n\\n    \\n\\n    \\n\\n    x = np.linspace(0, 3)\\n\\n    fig, axL = plt.subplots(figsize=(4,3))\\n\\n    axR = axL.twinx()\\n\\n    \\n\\n    axL.plot(x, np.sin(x), color=\\'k\\', linestyle=\\'--\\')\\n\\n    axR.plot(x, 100*np.cos(x), color=\\'r\\')\\n\\n    \\n\\n    axL.plot(x, .3*np.sin(x), color=\\'k\\', linestyle=\\':\\')\\n\\n    axR.plot(x, 20*np.cos(x), color=\\'limegreen\\')\\n\\n    \\n\\n    axL.set_ylabel(\\'sin(x)\\', color=\\'k\\')\\n\\n    axR.set_ylabel(\\'100 cos(x)\\', color=\\'r\\')\\n\\n    axR.tick_params(\\'y\\', colors=\\'r\\')\\n\\n    \\n\\n    plt.legend([(\"r\",\"--\"), (\"limegreen\",\":\")], [\\'label\\', \"label2\"],\\n\\n               handler_map={tuple: AnyObjectHandler()})\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/QE2QZ.png\\n\\n  [2]: https://i.stack.imgur.com/vlrcB.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 1, 20),\n",
       "  '2017-10-31 22:57:26',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1872.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3170',\n",
       "  '41740938',\n",
       "  'Answer',\n",
       "  'Python - split string into multiples columns',\n",
       "  \"You could use `str.split` and provide `expand=True` so that it enlarges into a dataframe for each of those individual splits.\\n\\n\\n\\nReindex these by providing an added range so that we can create an extra column with `NaNs`. Provide an optional prefix char later.\\n\\n\\n\\nThen, concatentate the original and the extracted `DF's` column-wise.\\n\\n\\n\\n    str_df = df['a'].str.split(expand=True).reindex(columns=np.arange(6)).add_prefix('a')\\n\\n    pd.concat([df, str_df], axis=1).replace({None:np.NaN})\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/SL0ev.png\",\n",
       "  '<python><string><pandas><split>',\n",
       "  datetime.date(2017, 1, 19),\n",
       "  '',\n",
       "  '',\n",
       "  '3',\n",
       "  '',\n",
       "  '1442.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3176',\n",
       "  '41760096',\n",
       "  'Answer',\n",
       "  'Replace outliers with column quantile in Pandas dataframe',\n",
       "  \"You can use [`DF.mask()`][2] method. Wherever there is a presence of a `True` instance, the values from the other series get's replaced aligned as per matching column names by providing `axis=1`.\\n\\n\\n\\n    df.mask(outliers_low, down_quantiles, axis=1)  \\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n----------\\n\\nAnother variant would be to use [`DF.where()`][3] method after inverting your boolean mask using the tilde (`~`) symbol.\\n\\n\\n\\n    df.where(~outliers_low, down_quantiles, axis=1)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/3vZjz.png\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mask.html\\n\\n  [3]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.where.html#pandas.DataFrame.where\",\n",
       "  '<python><pandas><dataframe><quantile>',\n",
       "  datetime.date(2017, 1, 20),\n",
       "  '2017-01-20 09:32:30',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '9',\n",
       "  '',\n",
       "  '3012.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3182',\n",
       "  '39442551',\n",
       "  'Answer',\n",
       "  'Reindexing a multiindex in pandas dataframe',\n",
       "  \"Turn it back into a simple datetimeindex and fill the gaps:\\n\\n\\n\\n    df = (df.unstack(level=0)\\n\\n            .reindex(pd.date_range(start='2014-03-03 07:45:00', \\n\\n                       end='2014-03-04 07:45:00', freq='15min')))\\n\\n    \\n\\n    \\n\\n    df = df.fillna(0)  # for the data, 0 is the desired value\\n\\n    \\n\\n    df.stack('station').swaplevel(0,1).sort_index()\\n\\n\",\n",
       "  '<python><pandas><multi-index>',\n",
       "  datetime.date(2016, 9, 12),\n",
       "  '2016-09-12 03:33:37',\n",
       "  'Boud (624829)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1698.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3183',\n",
       "  '39448291',\n",
       "  'Answer',\n",
       "  'Add data labels to Seaborn factor plot',\n",
       "  \"You could do it this way:\\n\\n\\n\\n    import math\\n\\n    # Set plotting style\\n\\n    sns.set_style('whitegrid')\\n\\n\\n\\n    # Rounding the integer to the next hundredth value plus an offset of 100\\n\\n    def roundup(x):\\n\\n        return 100 + int(math.ceil(x / 100.0)) * 100 \\n\\n        \\n\\n    df = pd.read_csv('train.csv')\\n\\n    sns.factorplot('Sex', data=df, kind='count', alpha=0.7, size=4, aspect=1)\\n\\n\\n\\n    # Get current axis on current figure\\n\\n    ax = plt.gca()\\n\\n\\n\\n    # ylim max value to be set\\n\\n    y_max = df['Sex'].value_counts().max() \\n\\n    ax.set_ylim([0, roundup(y_max)])\\n\\n\\n\\n    # Iterate through the list of axes' patches\\n\\n    for p in ax.patches:\\n\\n        ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), \\n\\n                fontsize=12, color='red', ha='center', va='bottom')\\n\\n                 \\n\\n    plt.show()\\n\\n\\n\\n[![Image][1]][1]\\n\\n\\n\\n\\n\\n  [1]: http://i.stack.imgur.com/tEPd9.png\",\n",
       "  '<python><matplotlib><seaborn>',\n",
       "  datetime.date(2016, 9, 12),\n",
       "  '2016-09-12 10:39:11',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '10',\n",
       "  '',\n",
       "  '5740.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3196',\n",
       "  '41770262',\n",
       "  'Answer',\n",
       "  'Matplotlib fill between method and datetime',\n",
       "  'A solution to this problem can be found in [this answer](https://stackoverflow.com/a/29329823/4124317). Thus, try\\n\\n\\n\\n    plt.fill_between(w.values[i:i+365],ampl[i:i+365],1)',\n",
       "  '<python><python-2.7><python-3.x><matplotlib><ipython>',\n",
       "  datetime.date(2017, 1, 20),\n",
       "  '2017-05-23 12:25:40',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1245.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3199',\n",
       "  '41789748',\n",
       "  'Answer',\n",
       "  'How to fill a region with half-transparent color in Matplotlib?',\n",
       "  'This can surely be done. The implementation would depend on how your mask looks like. \\n\\n\\n\\nHere is an example\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    \\n\\n    image = plt.imread(\"https://i.stack.imgur.com/9qe6z.png\")\\n\\n    \\n\\n    ar= np.zeros((image.shape[0],image.shape[1]) )\\n\\n    ar[100:300,50:150] = np.ones((200,100))\\n\\n    ar[:,322:] = np.zeros((image.shape[0],image.shape[1]-322) )*np.nan\\n\\n    \\n\\n    fig,ax=plt.subplots()\\n\\n    ax.imshow(image)\\n\\n    ax.imshow(ar, alpha=0.5, cmap=\"RdBu\")\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/VLjeI.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 1, 22),\n",
       "  '2017-01-22 12:31:16',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1446.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3201',\n",
       "  '41912731',\n",
       "  'Answer',\n",
       "  'Matplotlib pandas dataframe scatter type subplots according to the same column',\n",
       "  'One way would be to create the subplots externally and loop over the column names, creating a plot for each one of them.\\n\\n\\n\\n    import pandas as pd\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    df = pd.DataFrame({\\'AAA\\' : [4,5,6,7], \\'BBB\\' : [10,20,30,40],\\'CCC\\' : [100,50,-30,-50]})\\n\\n    \\n\\n    fig, axes = plt.subplots(1,len(df.columns.values)-1, sharey=True)\\n\\n    \\n\\n    for i, col in enumerate(df.columns.values[:-1]):\\n\\n        df.plot(x=[col], y=[\"CCC\"], kind=\"scatter\", ax=axes[i])\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n<img src=\"https://i.stack.imgur.com/ythlu.png\" width=\"450\">\\n\\n\\n\\n\\n\\n<hr>\\n\\nAnother method which might work in pandas 0.19 is to use the `subplots` argument. According to [the documentation](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.html)\\n\\n>subplots : boolean, default False  \\n\\n>           Make separate subplots for each column\\n\\n\\n\\nI interprete this such that the following should work, however, I haven\\'t been able to test it.\\n\\n\\n\\n    import pandas as pd\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    df = pd.DataFrame({\\'AAA\\' : [4,5,6,7], \\'BBB\\' : [10,20,30,40],\\'CCC\\' : [100,50,-30,-50]})\\n\\n    \\n\\n    df.plot(x=df.columns.values[:-1], y=[\"CCC\" for _ in df.columns.values[:-1]], \\n\\n                                kind=\"scatter\", subplots=True, sharey=True)\\n\\n    plt.show()\\n\\n\\n\\n',\n",
       "  '<python><matplotlib><scatter>',\n",
       "  datetime.date(2017, 1, 28),\n",
       "  '2017-01-28 20:19:10',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '4',\n",
       "  '',\n",
       "  '1978.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3206',\n",
       "  '41829883',\n",
       "  'Answer',\n",
       "  'Pandas, filter rows which column contain another column',\n",
       "  'You can use [`str.contains`][2] to match each of the substrings by using the regex `|` character which implies an `OR` selection from the contents of the other series:\\n\\n\\n\\n    df[df[\\'B\\'].str.contains(\"|\".join(df[\\'A\\']))]\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/AP6fn.png\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.contains.html',\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2017, 1, 24),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1315.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3208',\n",
       "  '41909861',\n",
       "  'Answer',\n",
       "  'Crop a Bounding Box from an Image which is a Numpy Array',\n",
       "  'In principle cropping is easily done simply by slicing the correct part out of the array. E.g. `image[100:200, 50:100, :]` slices the part between pixels 100 and 200 in y (vertical) direction, and the part between pixels 50 and 100 in x (horizontal) direction.\\n\\n\\n\\nSee this working example:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    mydic = {\\n\\n      \"annotations\": [\\n\\n      {\\n\\n        \"class\": \"rect\",\\n\\n        \"height\": 98,\\n\\n        \"width\": 113,\\n\\n        \"x\": 177,\\n\\n        \"y\": 12\\n\\n      },\\n\\n      {\\n\\n        \"class\": \"rect\",\\n\\n        \"height\": 80,\\n\\n        \"width\": 87,\\n\\n        \"x\": 373,\\n\\n        \"y\": 43\\n\\n      }\\n\\n     ],\\n\\n       \"class\": \"image\",\\n\\n       \"filename\": \"https://i.stack.imgur.com/9qe6z.png\"\\n\\n    }\\n\\n    \\n\\n    \\n\\n    def crop(dic, i):\\n\\n        image = plt.imread(dic[\"filename\"])\\n\\n        x0 = dic[\"annotations\"][i][\"x\"]\\n\\n        y0 = dic[\"annotations\"][i][\"y\"]\\n\\n        width = dic[\"annotations\"][i][\"width\"]\\n\\n        height = dic[\"annotations\"][i][\"height\"]\\n\\n        return image[y0:y0+height , x0:x0+width, :]\\n\\n        \\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax = fig.add_subplot(121)\\n\\n    ax.imshow(plt.imread(mydic[\"filename\"]))\\n\\n    \\n\\n    ax1 = fig.add_subplot(222)\\n\\n    ax1.imshow(crop(mydic, 0))\\n\\n    \\n\\n    ax2 = fig.add_subplot(224)\\n\\n    ax2.imshow(crop(mydic, 1))\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/ZFLDJ.png',\n",
       "  '<python><image><numpy><matplotlib>',\n",
       "  datetime.date(2017, 1, 28),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '9365.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3210',\n",
       "  '41793987',\n",
       "  'Answer',\n",
       "  'Plotting images side by side using matplotlib',\n",
       "  \"The problem you face is that you try to **assign** the return of `imshow` (which is an `matplotlib.image.AxesImage` to an existing axes object. \\n\\n\\n\\nThe correct way of plotting image data to the different axes in `axarr` would be\\n\\n\\n\\n    f, axarr = plt.subplots(2,2)\\n\\n    axarr[0,0].imshow(image_datas[0])\\n\\n    axarr[0,1].imshow(image_datas[1])\\n\\n    axarr[1,0].imshow(image_datas[2])\\n\\n    axarr[1,1].imshow(image_datas[3])\\n\\n\\n\\nThe concept is the same for all subplots, and in most cases the axes instance provide the same methods than the pyplot (plt) interface. \\n\\nE.g. if `ax` is one of your subplot axes, for plotting a normal line plot you'd use `ax.plot(..)` instead of `plt.plot()`. This can actually be found exactly in the source from [the page you link to](http://matplotlib.org/examples/pylab_examples/subplots_demo.html#pylab-examples-subplots-demo). \",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 1, 22),\n",
       "  '2017-01-22 17:41:37',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '23',\n",
       "  '',\n",
       "  '29519.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3214',\n",
       "  '41928733',\n",
       "  'Answer',\n",
       "  'How to randomly select some pandas dataframe rows?',\n",
       "  \"A minor tweak on @piRSquared's answer (using a boolean selection instead of query):\\n\\n\\n\\n    df.drop( df[df.amount == 0].sample(frac=.5).index )\\n\\n\\n\\nIt's about twice as fast as using query, but 3x slower than the numpy way. \",\n",
       "  '<python><python-3.x><pandas>',\n",
       "  datetime.date(2017, 1, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1121.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3215',\n",
       "  '41932843',\n",
       "  'Answer',\n",
       "  'How do I fill a region with only hatch (no background colour) in matplotlib 2.0',\n",
       "  '###matplotlib > 2.0.1\\n\\n\\n\\n*Since there was a lot of [discussion](https://github.com/matplotlib/matplotlib/issues/7901) on GitHub about hatching, there were now some changes introduced which make hatching much more intuitive.*\\n\\n\\n\\nThe example from the question now works again as expected, if the `facecolor` argument is used instead of the `color` argument.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    plt.plot([0,1],[0,1],ls=\"--\",c=\"b\")\\n\\n    plt.fill_between([0,1],[0,1], facecolor=\"none\", hatch=\"X\", edgecolor=\"b\", linewidth=0.0)\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n### matplotlib 2.0.0\\n\\n\\n\\n*To keep the original post, which has lead to this issue:*  \\n\\nIn matplotlib 2.0.0 you can get the old style back by using `plt.style.use(\\'classic\\')`. \\n\\n\\n\\n    ##Classic!\\n\\n    import matplotlib.pyplot as plt\\n\\n    plt.style.use(\\'classic\\')\\n\\n    plt.rcParams[\\'hatch.color\\'] = \\'b\\'\\n\\n    \\n\\n    plt.plot([0,1],[0,1],ls=\"--\",c=\"b\")\\n\\n    plt.fill_between([0,1],[0,1], color=\"none\", hatch=\"X\", edgecolor=\"b\", linewidth=0.0)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\nWithout setting the old style the following works by not setting the color to none, but instead make it transparent.\\n\\n\\n\\n    ## New\\n\\n    import matplotlib.pyplot as plt\\n\\n    plt.rcParams[\\'hatch.color\\'] = \\'b\\'\\n\\n    \\n\\n    plt.plot([0,1],[0,1],ls=\"--\",c=\"b\")\\n\\n    plt.fill_between([0,1],[0,1], hatch=\"X\", linewidth=0.0, alpha=0.0)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\nBoth methods rely on setting the hatch-color via `plt.rcParams[\\'hatch.color\\'] = \\'b\\'`.\\n\\n\\n\\n\\n\\nUnfortunately, there is currently no other way of setting the hatch color in matplotlib  2.0.0.  \\n\\nThe [matplotlib page that explains the changes](http://matplotlib.org/users/dflt_style_changes.html#hatching) states\\n\\n\\n\\n> There is no API level control of the hatch color or linewidth.\\n\\n\\n\\nThere is an [issue on this topic](https://github.com/matplotlib/matplotlib/issues/7901) open at github and API control may be (re)added in an upcoming version *(which is indeed done with version 2.0.1)*.\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/YyasQ.png',\n",
       "  '<python><python-2.7><matplotlib><plot>',\n",
       "  datetime.date(2017, 1, 30),\n",
       "  '2017-05-20 09:19:45',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '10',\n",
       "  '',\n",
       "  '4433.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3221',\n",
       "  '40656281',\n",
       "  'Answer',\n",
       "  'Understanding scipy deconvolve',\n",
       "  'After some trial and error I found out how to interprete the results of `scipy.signal.deconvolve()` and I post my findings as an answer.\\n\\n\\n\\nLet\\'s start with a working example code\\n\\n\\n\\n    import numpy as np\\n\\n    import scipy.signal\\n\\n    import matplotlib.pyplot as plt\\n\\n\\n\\n    # let the signal be box-like\\n\\n    signal = np.repeat([0., 1., 0.], 100)\\n\\n    # and use a gaussian filter\\n\\n    # the filter should be shorter than the signal\\n\\n    # the filter should be such that it\\'s much bigger then zero everywhere\\n\\n    gauss = np.exp(-( (np.linspace(0,50)-25.)/float(12))**2 )\\n\\n    print gauss.min()  # = 0.013 >> 0\\n\\n    \\n\\n    # calculate the convolution (np.convolve and scipy.signal.convolve identical)\\n\\n    # the keywordargument mode=\"same\" ensures that the convolution spans the same\\n\\n    #   shape as the input array.\\n\\n    #filtered = scipy.signal.convolve(signal, gauss, mode=\\'same\\') \\n\\n    filtered = np.convolve(signal, gauss, mode=\\'same\\') \\n\\n\\n\\n    deconv,  _ = scipy.signal.deconvolve( filtered, gauss )\\n\\n    #the deconvolution has n = len(signal) - len(gauss) + 1 points\\n\\n    n = len(signal)-len(gauss)+1\\n\\n    # so we need to expand it by \\n\\n    s = (len(signal)-n)/2\\n\\n    #on both sides.\\n\\n    deconv_res = np.zeros(len(signal))\\n\\n    deconv_res[s:len(signal)-s-1] = deconv\\n\\n    deconv = deconv_res\\n\\n    # now deconv contains the deconvolution \\n\\n    # expanded to the original shape (filled with zeros) \\n\\n    \\n\\n    \\n\\n    #### Plot #### \\n\\n    fig , ax = plt.subplots(nrows=4, figsize=(6,7))\\n\\n    \\n\\n    ax[0].plot(signal,            color=\"#907700\", label=\"original\",     lw=3 ) \\n\\n    ax[1].plot(gauss,          color=\"#68934e\", label=\"gauss filter\", lw=3 )\\n\\n    # we need to divide by the sum of the filter window to get the convolution normalized to 1\\n\\n    ax[2].plot(filtered/np.sum(gauss), color=\"#325cab\", label=\"convoluted\" ,  lw=3 )\\n\\n    ax[3].plot(deconv,         color=\"#ab4232\", label=\"deconvoluted\", lw=3 ) \\n\\n\\n\\n    for i in range(len(ax)):\\n\\n        ax[i].set_xlim([0, len(signal)])\\n\\n        ax[i].set_ylim([-0.07, 1.2])\\n\\n        ax[i].legend(loc=1, fontsize=11)\\n\\n        if i != len(ax)-1 :\\n\\n            ax[i].set_xticklabels([])\\n\\n    \\n\\n    plt.savefig(__file__ + \".png\")\\n\\n    plt.show()    \\n\\n\\n\\nThis code produces the following image, showing exactly what we want (`Deconvolve(Convolve(signal,gauss) , gauss) == signal`)\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\nSome important findings are:\\n\\n\\n\\n * The filter should be shorter than the signal\\n\\n * The filter should be much bigger than zero everywhere (here > 0.013 is good enough)\\n\\n * Using the keyword argument `mode = \\'same\\'` to the convolution ensures that it lives on the same array shape as the signal.\\n\\n * The deconvolution has `n = len(signal) - len(gauss) + 1` points. \\n\\nSo in order to let it also reside on the same original array shape we need to expand it by `s = (len(signal)-n)/2` on both sides.\\n\\n\\n\\nOf course, further findings, comments and suggestion to this question are still welcome.\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/VvPOs.png',\n",
       "  '<python><numpy><scipy><signals><deconvolution>',\n",
       "  datetime.date(2016, 11, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '8',\n",
       "  '',\n",
       "  '5283.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3222',\n",
       "  '41793591',\n",
       "  'Answer',\n",
       "  'Value error: numpy.loadtxt could not convert string to float',\n",
       "  \"*Just to keep this question not unanswered:*\\n\\n\\n\\nThe problem comes from the fact that you have a line containing a lot of white spaces (line 64 in this case) in your data. \\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nOne option is of course to manually delete them.  \\n\\nThe other option is to use [`np.genfromtxt()`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.genfromtxt.html) instead of `np.loadtxt()`.\\n\\n\\n\\n    x,y = np.genfromtxt('XRDdata.txt', unpack = True,  delimiter = ';' )\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/8HhFU.png\",\n",
       "  '<python><numpy><matplotlib>',\n",
       "  datetime.date(2017, 1, 22),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '2392.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3232',\n",
       "  '41946424',\n",
       "  'Answer',\n",
       "  'Apply global settings to all subplots in pyplot',\n",
       "  'Some settings concerning mostly the style of the figure can be set globally using the [matplotlib rc parameters](http://matplotlib.org/users/customizing.html).\\n\\nFor example, setting the grid on throughout the script, put\\n\\n\\n\\n    plt.rcParams[\\'axes.grid\\'] = True\\n\\n\\n\\nat the beginning of your file (after the imports).  \\n\\n\\n\\nOther things like axis limits are really specific to the plot itself, and there is no global parameter for that. But you can still go the way, as outlined in the linked questions, i.e. write your own function that does most of the stuff you need.\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    plt.rcParams[\\'axes.grid\\'] = True\\n\\n    \\n\\n    def plotfunction(fig, x1, y1, x2, y2,\\n\\n                     title1 = \\'Surface concentrations\\',\\n\\n                     title2 = \\'Surface temperature\\', **kwargs ):\\n\\n        ax = fig.add_subplot(211)\\n\\n        ax2 = fig.add_subplot(212, sharex=ax)\\n\\n        ax.set_title(title1)\\n\\n        ax2.set_title(title2)\\n\\n        ax.set_ylim(1e-3, None)\\n\\n        ax2.set_ylim(1e-3, None)\\n\\n        ax.plot(x1, y1, **kwargs)\\n\\n        ax2.plot(x2, y2, **kwargs)\\n\\n    \\n\\n    \\n\\n    fspec = plt.figure(1)\\n\\n    fener = plt.figure(2)\\n\\n    \\n\\n    x1, y1, x2, y2 = np.loadtxt(\"spectrum.txt\", unpack=True)\\n\\n    plotfunction(fspec,x1, y1, x2, y2)\\n\\n    \\n\\n    x1, y1, x2, y2 = np.loadtxt(\"energy.txt\", unpack=True)\\n\\n    plotfunction(fener,x1, y1, x2, y2, linewidth=3)\\n\\n\\n\\n',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 1, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '2418.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3240',\n",
       "  '41997956',\n",
       "  'Answer',\n",
       "  'Python Matplotlib Boxplot Color',\n",
       "  'To colorize the boxplot, you need to first use the `patch_artist=True` keyword to tell it that the boxes are patches and not just paths. Then you have two main options here:\\n\\n\\n\\n1. set the color via `...props` keyword argument, e.g.   \\n\\n`boxprops=dict(facecolor=\"red\")`. For all keyword arguments, refer to [the documentation](http://matplotlib.org/api/pyplot_api.html#matplotlib.pyplot.boxplot)\\n\\n2. Use the `plt.setp(item, properties)` functionality to set the properties of the boxes, whiskers, fliers, medians, caps.\\n\\n3. obtain the individual items of the boxes from the returned dictionary and use `item.set_<property>(...)` on them individually. This option is detailed in an answer to the following question: [python matplotlib filled boxplots](https://stackoverflow.com/questions/20289091/python-matplotlib-filled-boxplots), where it allows to change the color of the individual boxes separately. \\n\\n\\n\\nThe complete example, showing options 1 and 2:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    data = np.random.normal(0.1, size=(100,6))\\n\\n    data[76:79,:] = np.ones((3,6))+0.2\\n\\n    \\n\\n    plt.figure(figsize=(4,3))\\n\\n    # option 1, specify props dictionaries\\n\\n    c = \"red\"\\n\\n    plt.boxplot(data[:,:3], positions=[1,2,3], notch=True, patch_artist=True,\\n\\n                boxprops=dict(facecolor=c, color=c),\\n\\n                capprops=dict(color=c),\\n\\n                whiskerprops=dict(color=c),\\n\\n                flierprops=dict(color=c, markeredgecolor=c),\\n\\n                medianprops=dict(color=c),\\n\\n                )\\n\\n    \\n\\n    \\n\\n    # option 2, set all colors individually\\n\\n    c2 = \"purple\"\\n\\n    box1 = plt.boxplot(data[:,::-2]+1, positions=[1.5,2.5,3.5], notch=True, patch_artist=True)\\n\\n    for item in [\\'boxes\\', \\'whiskers\\', \\'fliers\\', \\'medians\\', \\'caps\\']:\\n\\n            plt.setp(box1[item], color=c2)\\n\\n    plt.setp(box1[\"boxes\"], facecolor=c2)\\n\\n    plt.setp(box1[\"fliers\"], markeredgecolor=c2)\\n\\n                \\n\\n    \\n\\n    plt.xlim(0.5,4)\\n\\n    plt.xticks([1,2,3], [1,2,3])\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/xgyF3.png',\n",
       "  '<python><matplotlib><plot><colors><box>',\n",
       "  datetime.date(2017, 2, 2),\n",
       "  '2017-05-23 11:47:35',\n",
       "  'ImportanceOfBeingErnest (4124317), URL Rewriter Bot (n/a)',\n",
       "  '9',\n",
       "  '',\n",
       "  '15058.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3241',\n",
       "  '42002193',\n",
       "  'Answer',\n",
       "  'threads not running parallel in python script',\n",
       "  'Here is an example on how you could use threading based on your code:\\n\\n\\n\\n    import threading\\n\\n    import time\\n\\n    threads = []\\n\\n    \\n\\n    print \"hello\"\\n\\n    \\n\\n    def doWork(i):\\n\\n        print \"i = \",i\\n\\n        for j in range(0,i):\\n\\n            print \"j = \",j\\n\\n            time.sleep(5)\\n\\n    \\n\\n    for i in range(1,4):\\n\\n        thread = threading.Thread(target=doWork, args=(i,))\\n\\n        threads.append(thread)\\n\\n        thread.start()\\n\\n    \\n\\n    # you need to wait for the threads to finish\\n\\n    for thread in threads:\\n\\n        thread.join()\\n\\n    \\n\\n    print \"Finished\"',\n",
       "  '<python><multithreading><python-2.7>',\n",
       "  datetime.date(2017, 2, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1802.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3242',\n",
       "  '41823827',\n",
       "  'Answer',\n",
       "  'Interactive charts in Python',\n",
       "  'There are several ways of producing a dropdown field in matplotlib. \\n\\n\\n\\n###(a) Jupyter notebook\\n\\n\\n\\nIn order to get a dropdown field in a Jupyter notebook, you can use the `ipywidgets.interact()` command. I have already provided [an answer](https://stackoverflow.com/a/41667906/4124317) on how to use that. \\n\\n\\n\\n\\n\\n<img src=\"https://i.stack.imgur.com/Q3Pbx.png\" width=\"400\">\\n\\n\\n\\n\\n\\n###(b) Matplotlib within PyQt.\\n\\n\\n\\nYou can embed matplotlib into PyQt and use the PyQt widgets to obtain a drop down field. [Here is a solution](https://stackoverflow.com/a/41677416/4124317) for that.\\n\\n\\n\\n\\n\\n<img src=\"https://i.stack.imgur.com/ldioR.png\" width=\"400\">\\n\\n\\n\\n###(c) Matplotlib widgets\\n\\n\\n\\nMatplotlib provides some interactive widgets out of the box. Unfortunately it does not provide a dropdown field. But if requirements are not too strict, one could use a slider to accomplish the effect of chosing different datasets. Find [an example here](https://stackoverflow.com/a/41152160/4124317). \\n\\n\\n\\n\\n\\n<img src=\"https://i.stack.imgur.com/hOGc2.png\" width=\"400\">\\n\\n\\n\\n\\n\\n [1]: https://i.stack.imgur.com/ldioR.png\\n\\n\\n\\n  [2]: https://i.stack.imgur.com/Q3Pbx.png\\n\\n[3]: https://i.stack.imgur.com/hOGc2.png',\n",
       "  '<python-3.x><matplotlib><charts>',\n",
       "  datetime.date(2017, 1, 24),\n",
       "  '2017-05-23 10:29:17',\n",
       "  'ImportanceOfBeingErnest (4124317), URL Rewriter Bot (n/a)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1105.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3243',\n",
       "  '41831182',\n",
       "  'Answer',\n",
       "  'In pandas, how do I flatten a group of rows',\n",
       "  \"***Steps:***\\n\\n\\n\\n1) Compute the cumulative counts for the *Groupby* object. Add 1 so that the headers are formatted as per the desired `DF`.\\n\\n\\n\\n2) Set the same grouped columns as the index axis along with the computed `cumcounts` and then `unstack` it. Additionally, sort the header according to the lowermost level.\\n\\n\\n\\n3) Rename the multi-index columns and flatten accordingly to obtain a single header.\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n\\n\\n    cc = df.groupby(['event','event_date','event_time']).cumcount() + 1\\n\\n    df = df.set_index(['event','event_date','event_time', cc]).unstack().sort_index(1, level=1)\\n\\n    df.columns = ['_'.join(map(str,i)) for i in df.columns]\\n\\n    df.reset_index()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/5YseY.png\",\n",
       "  '<python><csv><pandas><dataframe>',\n",
       "  datetime.date(2017, 1, 24),\n",
       "  '2017-01-24 15:03:52',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '8',\n",
       "  '',\n",
       "  '1376.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3244',\n",
       "  '41861615',\n",
       "  'Answer',\n",
       "  'One colorbar for multiple pandas subplots',\n",
       "  'The question [Matplotlib 2 Subplots, 1 Colorbar](https://stackoverflow.com/questions/13784201/matplotlib-2-subplots-1-colorbar) is probably more what you are looking for. **The problem is however that you do not directly have access to the mappable that is created in the pandas scatter plot.**\\n\\n\\n\\nThe solution here would be to distill this mappable (in this case a PatchCollection) from the axes using it `plt.gca().get_children()[0]`, which takes the first child artist from the axes.\\n\\n\\n\\nThis method is save as long as all scatterplots share the same colors and\\n\\n as long as there are no other artists in the axes.\\n\\n\\n\\n\\n\\n    import pandas as pd\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    df = pd.DataFrame({\"BASE\": np.random.randn(10),\\n\\n                \"A\": np.random.randn(10),\\n\\n                \"B\": np.random.randn(10),\\n\\n                 \"C\": np.random.randn(10), \\n\\n                 \"D\": np.random.randn(10),\\n\\n                \"color_col\": np.random.randn(10)})\\n\\n    \\n\\n    fig = plt.figure(1, figsize = (6,6))\\n\\n    plt.subplots_adjust(wspace=0.5, right=0.8, top=0.9, bottom=0.1)\\n\\n    for i, col in enumerate(\"ABCD\"):\\n\\n        plt.subplot(2,2,i+1)\\n\\n        df.plot.scatter(x = \"BASE\", y = col, ax = plt.gca(), \\n\\n                        c = df[\"color_col\"], cmap=\"jet\", colorbar=False)\\n\\n    \\n\\n    # we now take the first axes and \\n\\n    # create a colorbar for it\\'s first child (the PathCollection from scatter)\\n\\n    # this is save as long as all scatterplots share the same colors and\\n\\n    # as long as there are no other artists in the axes.\\n\\n    im = plt.gca().get_children()[0]\\n\\n    cax = fig.add_axes([0.85,0.1,0.03,0.8]) \\n\\n    fig.colorbar(im, cax=cax)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/cPqUQ.png',\n",
       "  '<python><pandas><matplotlib><plot>',\n",
       "  datetime.date(2017, 1, 25),\n",
       "  '2017-05-23 12:02:44',\n",
       "  'URL Rewriter Bot (n/a)',\n",
       "  '1',\n",
       "  '',\n",
       "  '1021.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3245',\n",
       "  '42013314',\n",
       "  'Answer',\n",
       "  'Clear figure subplots matplotlib python',\n",
       "  \"I'm not sure if I fully understand where the problem lies. \\n\\nIf you want to update the plot you would need a function that does this. I would call this function `plotData`. Before that you also need to set the plot up. That is what you currently have in `plotData`. So let's rename that to `generatePlot`.\\n\\n\\n\\n    class SomeClass():\\n\\n        ...\\n\\n        \\n\\n        def generatePlot(self):\\n\\n            # Setup figure to hold subplots\\n\\n            f = Figure(figsize=(10,8), dpi=100)\\n\\n        \\n\\n            # Setup subplots\\n\\n            self.subplot1=f.add_subplot(2,1,1)\\n\\n            self.subplot2=f.add_subplot(2,1,2)\\n\\n        \\n\\n            # Show plots\\n\\n            dataPlot = FigureCanvasTkAgg(f, master=app)\\n\\n            dataPlot.show()\\n\\n            dataPlot.get_tk_widget().pack(side=RIGHT, fill=BOTH, expand=1)\\n\\n        \\n\\n        ## Plot Data Function\\n\\n        def plotData(self, data, otherdata):\\n\\n            #clear subplots\\n\\n            self.subplot1.cla()\\n\\n            self.subplot2.cla()\\n\\n            #plot new data to the same axes\\n\\n            self.subplot1.plot(data)\\n\\n            self.subplot2.plot(otherdata)\\n\\n\\n\\nNow you need to call `generatePlot` only once at the beginning. Afterwards you can update your plot with new data whenever you want.\",\n",
       "  '<python><matplotlib><tkinter>',\n",
       "  datetime.date(2017, 2, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '5',\n",
       "  '',\n",
       "  '14696.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3247',\n",
       "  '42014009',\n",
       "  'Answer',\n",
       "  \"TypeError: ufunc 'multiply' did not contain a loop with signature matching types dtype('S32') dtype('S32') dtype('S32')\",\n",
       "  'From the [documentation of `raw_input`](https://docs.python.org/2/library/functions.html#raw_input):\\n\\n\\n\\n> The function then reads a line from input, converts it to a string (stripping a trailing newline), and returns that.\\n\\n\\n\\nSo what happens is that you try to multiply a string with a float, something like `y=\"3\" * x - 0.5 * \"3\" *x**2`, which is not defined. \\n\\n\\n\\nThe easiest way to circumvent this is to cast the input string to float first.\\n\\n\\n\\n    x = np.linspace(0., 9., 10)\\n\\n    a = float(raw_input(\\'Acceleration =\\'))\\n\\n    v = float(raw_input(\\'Velocity = \\'))\\n\\n    y=v*x-0.5*a*x**2\\n\\n\\n\\nMind that if you\\'re using python 3, you\\'d need to use `input`instead of `raw_input`, \\n\\n\\n\\n    a = float(input(\\'Acceleration =\\'))',\n",
       "  '<python><matplotlib><typeerror>',\n",
       "  datetime.date(2017, 2, 2),\n",
       "  '2017-04-26 17:42:33',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '8',\n",
       "  '',\n",
       "  '15942.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3256',\n",
       "  '41960255',\n",
       "  'Answer',\n",
       "  'filled 3d plot python',\n",
       "  \"You are using `plot` to draw the vertices. This gives you a line, as expected. In order to get a filled  polygon, use `Poly3DCollection`.\\n\\n\\n\\n    from mpl_toolkits.mplot3d import Axes3D\\n\\n    from mpl_toolkits.mplot3d.art3d import Poly3DCollection\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    \\n\\n    fig = plt.figure()\\n\\n    ax = fig.add_subplot(111, projection='3d')\\n\\n    plt.tight_layout()\\n\\n    \\n\\n    x,y,z = np.loadtxt('D:\\\\PyCharm\\\\File1.txt', skiprows=1, unpack=True)\\n\\n    \\n\\n    verts = [zip(x, y,z)]\\n\\n    ax.add_collection3d(Poly3DCollection(verts))\\n\\n    \\n\\n    ax.set_xlim(5712000, 5716000)\\n\\n    ax.set_ylim(5576000, 5579000)\\n\\n    ax.set_zlim(-1000, -600)\\n\\n    plt.show()\",\n",
       "  '<python><matplotlib><plot><3d>',\n",
       "  datetime.date(2017, 1, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1208.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3260',\n",
       "  '40660242',\n",
       "  'Answer',\n",
       "  'Get first row of dataframe in Python Pandas based on criteria',\n",
       "  \"For existing matches, use `query`:\\n\\n\\n\\n\\n\\n    df.query(' A > 3' ).head(1)\\n\\n    Out[33]: \\n\\n       A  B  C\\n\\n    2  4  6  3\\n\\n    \\n\\n    df.query(' A > 4 and B > 3' ).head(1)\\n\\n    Out[34]: \\n\\n       A  B  C\\n\\n    4  5  4  5\\n\\n    \\n\\n    df.query(' A > 3 and (B > 3 or C > 2)' ).head(1)\\n\\n    Out[35]: \\n\\n       A  B  C\\n\\n    2  4  6  3\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2016, 11, 17),\n",
       "  '',\n",
       "  '',\n",
       "  '8',\n",
       "  '',\n",
       "  '41944.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3263',\n",
       "  '40672660',\n",
       "  'Answer',\n",
       "  'Matplotlib customize the legend to show squares instead of rectangles',\n",
       "  'Matplotlib provides the [rcParams](http://matplotlib.org/users/customizing.html) \\n\\n\\n\\n    legend.handlelength  : 2.    # the length of the legend lines in fraction of fontsize\\n\\n    legend.handleheight  : 0.7   # the height of the legend handle in fraction of fontsize\\n\\n\\n\\nYou can set those within the call to [`plt.legend()`](http://matplotlib.org/api/legend_api.html#matplotlib.legend.Legend)\\n\\n\\n\\n`plt.legend(handlelength=1, handleheight=1)`\\n\\n\\n\\nor using the rcParams at the beginning of your script\\n\\n\\n\\n    import matplotlib\\n\\n    matplotlib.rcParams[\\'legend.handlelength\\'] = 1\\n\\n    matplotlib.rcParams[\\'legend.handleheight\\'] = 1\\n\\n\\n\\n \\n\\nUnfortunately providing equal `handlelength=1, handleheight=1` will not give a perfect rectange. It seems `handlelength=1, handleheight=1.125` will do the job, but this may depend on the font being used.\\n\\n\\n\\n<hr>\\n\\n\\n\\nAn alternative, if you want to use proxy artists may be to use the square markers from the plot/scatter methods.\\n\\n\\n\\n    bar1 = plt.plot([], marker=\"s\", markersize=15, linestyle=\"\", label=\"2015\")\\n\\n\\n\\nand supply it to the legend, `legend(handles=[bar1])`. Using this approach needs to have set `matplotlib.rcParams[\\'legend.numpoints\\'] = 1`, otherwise two markers would appear in the legend.\\n\\n\\n\\n<hr>\\n\\nHere is a full example of both methods\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    plt.rcParams[\\'legend.handlelength\\'] = 1\\n\\n    plt.rcParams[\\'legend.handleheight\\'] = 1.125\\n\\n    plt.rcParams[\\'legend.numpoints\\'] = 1\\n\\n    \\n\\n    \\n\\n    fig, ax = plt.subplots(ncols=2, figsize=(5,2.5))\\n\\n    \\n\\n    # Method 1: Set the handlesizes already in the rcParams\\n\\n    ax[0].set_title(\"Setting handlesize\")\\n\\n    ax[0].bar([0,2], [6,3], width=0.7, color=\"#a30e73\", label=\"2015\", align=\"center\")\\n\\n    ax[0].bar([1,3], [3,2], width=0.7, color=\"#0943a8\", label=\"2016\", align=\"center\" )\\n\\n    ax[0].legend()\\n\\n    \\n\\n    # Method 2: use proxy markers. (Needs legend.numpoints to be 1)\\n\\n    ax[1].set_title(\"Proxy markers\")\\n\\n    ax[1].bar([0,2], [6,3], width=0.7, color=\"#a30e73\", align=\"center\" )\\n\\n    ax[1].bar([1,3], [3,2], width=0.7, color=\"#0943a8\", align=\"center\" )\\n\\n    b1, =ax[1].plot([], marker=\"s\", markersize=15, linestyle=\"\", color=\"#a30e73\",  label=\"2015\")\\n\\n    b2, =ax[1].plot([], marker=\"s\", markersize=15, linestyle=\"\", color=\"#0943a8\",  label=\"2016\")\\n\\n    ax[1].legend(handles=[b1, b2])\\n\\n    \\n\\n    [a.set_xticks([0,1,2,3]) for a in ax]\\n\\n    plt.show()\\n\\n\\n\\nproducing \\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/25vzX.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2016, 11, 18),\n",
       "  '2016-11-18 09:30:53',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '3',\n",
       "  '',\n",
       "  '1008.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3267',\n",
       "  '41898583',\n",
       "  'Answer',\n",
       "  'Pandas dataframe error: matplotlib.axes._subplots.AxesSubplot',\n",
       "  'In case you want to see the plot inline, use\\n\\n\\n\\n    %matplotlib inline\\n\\n\\n\\nin the header (after the imports).\\n\\n\\n\\nIf you want to show the graphic in a window, add the line \\n\\n\\n\\n    plt.show()\\n\\n\\n\\nat the end (make sure you have imported `import matplotlib.pyplot as plt` in the header).',\n",
       "  '<python><pandas><matplotlib><bokeh>',\n",
       "  datetime.date(2017, 1, 27),\n",
       "  '',\n",
       "  '',\n",
       "  '18',\n",
       "  '',\n",
       "  '23385.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3277',\n",
       "  '42005287',\n",
       "  'Answer',\n",
       "  'Plotting data with a string as the x-axis',\n",
       "  'If you already have a dataframe with your data in, you could easily use a [seaborn swarmplot](http://seaborn.pydata.org/generated/seaborn.swarmplot.html?highlight=swarmplot#seaborn.swarmplot), which makes this a one-liner:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import pandas as pd\\n\\n    import seaborn as sns\\n\\n    \\n\\n    w = [100,123,156,90,40,320,198,174,175,146,238,120]\\n\\n    m = [\"morning\"]*4+ [\"noon\"]*4+[\"evening\"]*4\\n\\n    df = pd.DataFrame({\"Wattage\" : w, \"timeOfDay\":m})\\n\\n    \\n\\n    sns.swarmplot(x=\"timeOfDay\", y=\"Wattage\",  data=df)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/Y82N0.png',\n",
       "  '<python><matplotlib><scatter-plot>',\n",
       "  datetime.date(2017, 2, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '1',\n",
       "  '',\n",
       "  '1217.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3284',\n",
       "  '40703652',\n",
       "  'Answer',\n",
       "  'Extract nested JSON in pandas dataframe',\n",
       "  \"1. Use `json_normalize` to handle a `list` of dictionaries and break individual dicts into separate series after setting the common path, which is *info* here. Then, `unstack` + apply series which gets appended downwards for that level.\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n    from pandas.io.json import json_normalize\\n\\n\\n\\n    df_info = json_normalize(df.to_dict('list'), ['info']).unstack().apply(pd.Series)\\n\\n    df_info\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n2. Pivot the `DF` with an optional `aggfunc` to handle duplicated index axis:\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n    DF = df_info.pivot_table(index=df_info.index.get_level_values(1), columns=['b'], \\n\\n                             values=['a'], aggfunc=' '.join)\\n\\n\\n\\n    DF\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n3. Finally Concatenate sideways:\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n\\n\\n    pd.concat([df[['ID']], DF.xs('a', axis=1).rename_axis(None, 1)], axis=1)\\n\\n\\n\\n[![enter image description here][3]][3]\\n\\n\\n\\n\\n\\n----------\\n\\nStarting `DF` used:\\n\\n\\n\\n    df = pd.DataFrame(dict(ID=[0,1,2], info=[[{u'a': u'good', u'b': u'type1'}, {u'a': u'bad', u'b': u'type2'}], \\n\\n                                            [{u'a': u'bad', u'b': u'type1'}, {u'a': u'bad', u'b': u'type2'}],\\n\\n                                            [{u'a': u'good', u'b': u'type1'}, {u'a': u'good', u'b': u'type2'}]]))\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/AtijG.png\\n\\n  [2]: https://i.stack.imgur.com/LLSDJ.png\\n\\n  [3]: https://i.stack.imgur.com/u4nnl.png\",\n",
       "  '<python><json><pandas>',\n",
       "  datetime.date(2016, 11, 20),\n",
       "  '2016-11-20 11:56:56',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '5',\n",
       "  '',\n",
       "  '3488.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3285',\n",
       "  '40717427',\n",
       "  'Answer',\n",
       "  'How to extract all string matches from a column using a input corpus/list in pandas?',\n",
       "  '***Steps:***\\n\\n\\n\\n1. We perform lemmatization only on verbs by supplying `pos=\\'v\\'` and let the nouns remain as they were before by iterating thorugh each word in that list got by `str.split` operation.\\n\\n2. Then, take all the matches of words present in the lookup list and the lemmatized list using `set`. \\n\\n3. Finally, join them to return string as the output.\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n    from nltk.stem.wordnet import WordNetLemmatizer\\n\\n\\n\\n    action = [\\'jump\\',\\'fly\\',\\'run\\',\\'swim\\']     # lookup list\\n\\n    lem = WordNetLemmatizer() \\n\\n    fcn = lambda x: \" \".join(set([lem.lemmatize(w, \\'v\\') for w in x]).intersection(set(action)))\\n\\n    df[\\'action_description\\'] = df[\\'action_description\\'].str.split().apply(fcn)\\n\\n    df\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n----------\\n\\nStarting `DF` used:\\n\\n\\n\\n    df = pd.DataFrame(dict(action_description=[\"I love to run and while my friend prefer to swim\", \\n\\n                                               \"Allan excels at high jump but he is not a good at running\"]))\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\nTo generate binary flags (0/1), we can use [`str.get_dummies`][3] method by splitting strings on whitespace and computing it\\'s indicator variables as shown:\\n\\n\\n\\n    bin_flag = df[\\'action_description\\'].str.get_dummies(sep=\\' \\').add_suffix(\\'_flag\\')\\n\\n    pd.concat([df[\\'action_description\\'], bin_flag], axis=1)\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/utD8K.png\\n\\n  [2]: https://i.stack.imgur.com/6jkKE.png\\n\\n  [3]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.str.get_dummies.html',\n",
       "  '<python><regex><pandas><nltk><text-mining>',\n",
       "  datetime.date(2016, 11, 21),\n",
       "  '2016-11-23 09:42:09',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1030.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3297',\n",
       "  '42045987',\n",
       "  'Answer',\n",
       "  'How can I change the x axis in matplotlib so there is no white space?',\n",
       "  \"In matplotlib 2.x there is an automatic margin set at the edges, which ensures the data to be nicely fitting within the axis spines. In this case such a margin is probably desired on the y axis. By default it is set to `0.05` in units of axis span.\\n\\nTo set the margin to `0` on the x axis, use \\n\\n\\n\\n    plt.margins(x=0)\\n\\n\\n\\nor \\n\\n\\n\\n    ax.margins(x=0)\\n\\n\\n\\ndepending on the context. Also see [the documentation](http://matplotlib.org/devdocs/api/_as_gen/matplotlib.axes.Axes.margins.html).\\n\\n\\n\\nIn case you want to get rid of the margin in the whole script, you can use \\n\\n\\n\\n    plt.rcParams['axes.xmargin'] = 0\\n\\n\\n\\nat the beginning of your script (same for `y` of course). If you want to get rid of the margin entirely and forever, you might want to change the according line in the [matplotlib rc file](http://matplotlib.org/users/customizing.html).\\n\\n\\n\\n\\n\\n\\n\\n<hr>\\n\\nAlternatively to changing the margins, use `plt.xlim(..)` or `ax.set_xlim(..)` to manually set the limits of the axes such that there is no white space left. \",\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 2, 4),\n",
       "  '2017-02-04 23:43:53',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '28',\n",
       "  '',\n",
       "  '6909.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3301',\n",
       "  '42057207',\n",
       "  'Answer',\n",
       "  'matplotlib scatterplot with legend',\n",
       "  \"Actually both linked questions provide a way how to achieve the desired result.\\n\\n\\n\\nThe easiest method is to create as many scatter plots as unique classes exist and give each a single color and legend entry.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    x=[1,2,3,4]\\n\\n    y=[5,6,7,8]\\n\\n    classes = [2,4,4,2]\\n\\n    unique = list(set(classes))\\n\\n    colors = [plt.cm.jet(float(i)/max(unique)) for i in unique]\\n\\n    for i, u in enumerate(unique):\\n\\n        xi = [x[j] for j  in range(len(x)) if classes[j] == u]\\n\\n        yi = [y[j] for j  in range(len(x)) if classes[j] == u]\\n\\n        plt.scatter(xi, yi, c=colors[i], label=str(u))\\n\\n    plt.legend()\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\nIn case the classes are string labels, the solution would look slightly different, in that you need to get the colors from their index instead of using the classes themselves.\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    x=[1,2,3,4]\\n\\n    y=[5,6,7,8]\\n\\n    classes = ['X','Y','Z','X']\\n\\n    unique = np.unique(classes)\\n\\n    colors = [plt.cm.jet(i/float(len(unique)-1)) for i in range(len(unique))]\\n\\n    for i, u in enumerate(unique):\\n\\n        xi = [x[j] for j  in range(len(x)) if classes[j] == u]\\n\\n        yi = [y[j] for j  in range(len(x)) if classes[j] == u]\\n\\n        plt.scatter(xi, yi, c=colors[i], label=str(u))\\n\\n    plt.legend()\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/aJSBG.png\\n\\n  [2]: https://i.stack.imgur.com/Erihf.png\",\n",
       "  '<python><matplotlib><plot>',\n",
       "  datetime.date(2017, 2, 5),\n",
       "  '2018-07-04 09:39:04',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '5',\n",
       "  '',\n",
       "  '6591.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3312',\n",
       "  '41963227',\n",
       "  'Answer',\n",
       "  'N/A as N/A not as not a number in pandas and python',\n",
       "  '`keep_default_na=False` will indicate to `read_csv` to not interpret NA strings at load time. Thus any NA string from the csv file will be kept as such and your code will work:\\n\\n\\n\\n    df = pd.read_csv(\"abx.csv\", keep_default_na=False)',\n",
       "  '<python><regex><pandas><nan><na>',\n",
       "  datetime.date(2017, 1, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '4',\n",
       "  '',\n",
       "  '1048.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3313',\n",
       "  '41963267',\n",
       "  'Answer',\n",
       "  'Return Tuple of Index and .max() Value?',\n",
       "  \"Here is one way:\\n\\n\\n\\n    df['%'].idxmax(), df['%'].max()\",\n",
       "  '<python><python-3.x><pandas><dataframe><tuples>',\n",
       "  datetime.date(2017, 1, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '10',\n",
       "  '',\n",
       "  '1934.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3317',\n",
       "  '42008934',\n",
       "  'Answer',\n",
       "  \"AttributeError: 'module' object has no attribute 'BASE_COLORS'\",\n",
       "  \"The code is the well known [named_colors example](http://matplotlib.org/examples/color/named_colors.html) from the matplotlib page. \\n\\n\\n\\nDepending on the version of matplotlib you are running, the example will or will not work. However previous version's examples are still available:\\n\\n\\n\\nv2.0:   http://matplotlib.org/examples/color/named_colors.html  \\n\\nv1.5:   http://matplotlib.org/1.5.1/examples/color/named_colors.html  \\n\\nv*x*.*y*.*z*: http://matplotlib.org/x.y.z/examples/color/named_colors.html\",\n",
       "  '<python><matplotlib><colors>',\n",
       "  datetime.date(2017, 2, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '4648.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3324',\n",
       "  '42125202',\n",
       "  'Answer',\n",
       "  'Delete row based on nulls in certain columns (pandas)',\n",
       "  \"dropna has a parameter to apply the tests only on a subset of columns:\\n\\n\\n\\n    dropna(axis=0, how='all', subset=[your three columns in this list])\",\n",
       "  '<python><pandas>',\n",
       "  datetime.date(2017, 2, 8),\n",
       "  '2017-02-09 02:27:04',\n",
       "  'piRSquared (2336654)',\n",
       "  '2',\n",
       "  '',\n",
       "  '5150.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3326',\n",
       "  '42095870',\n",
       "  'Answer',\n",
       "  'Animating two curves in the same plot at the same time in Matplotlib',\n",
       "  'There is no reason to use two different `FuncAnimation`s here, as you want one single animation, animating two or more objects.\\n\\n\\n\\nSince `unpf` and `uexact` seem to have the same number of items, implementing this is straight forward.\\n\\n\\n\\n    ... # all other code\\n\\n    \\n\\n    line, = ax.plot(x, u0)\\n\\n    line2, = ax.plot(x, u0)\\n\\n    def animate(i):\\n\\n        line.set_ydata(uexact[i])  # update the data\\n\\n        line2.set_ydata(unpf[i])  # update the data\\n\\n        return line, line2,\\n\\n        \\n\\n    ani = animation.FuncAnimation(fig, animate, np.arange(1, len(unpf)), interval=100)\\n\\n    plt.show()',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 2, 7),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1016.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3327',\n",
       "  '42142888',\n",
       "  'Answer',\n",
       "  'How can I change a specific row label in a Pandas dataframe?',\n",
       "  \"You can get the last index like this:\\n\\n\\n\\n    last = df.index[-1]\\n\\n\\n\\nThen\\n\\n\\n\\n    df = df.rename(index={last: 'a'})\\n\\n\\n\\n\",\n",
       "  '<python><python-3.x><pandas><dataframe><label>',\n",
       "  datetime.date(2017, 2, 9),\n",
       "  '2017-11-03 17:16:43',\n",
       "  'Vaishali (6287308)',\n",
       "  '16',\n",
       "  '',\n",
       "  '5132.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3330',\n",
       "  '42019632',\n",
       "  'Answer',\n",
       "  'plot a nested list as multiple trendlines in python',\n",
       "  \"You need to call `plt.show()` outside the loop, otherwise it will show a plot on every iteration.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    nested=[[35,36,37],[34,35,36,37,38],[22,23,23,24]]\\n\\n    for y in nested:\\n\\n          x=range(1, len(y)+1)\\n\\n          plt.plot(x,y)\\n\\n    plt.show()\\n\\n\\n\\n(Also, don't use `list` as a variable name in python, because it is a datatype. While this does not cause harm here, it may produce complete chaos in other scenarios.)\",\n",
       "  '<python><matplotlib><plot><graphing><trendline>',\n",
       "  datetime.date(2017, 2, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '0',\n",
       "  '',\n",
       "  '1165.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3331',\n",
       "  '42026341',\n",
       "  'Answer',\n",
       "  'Plotting a continuous stream of data with MatPlotLib',\n",
       "  '###(1) \\n\\nYou can set `plt.ion()` at the beginning and plot all graphs to the same window. Within the loop use `plt.draw()` to show the graph and `plt.pause(t)` to make a pause. Note that `t` can be very small, but the command needs to be there for the animation to work on most backends.  \\n\\nYou might want to clear the axes before plotting new content using `plt.gca().cla()`.\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    plt.ion()\\n\\n    for i in range(100):\\n\\n        x = range(i)\\n\\n        y = range(i)\\n\\n        # plt.gca().cla() # optionally clear axes\\n\\n        plt.plot(x, y)\\n\\n        plt.title(str(i))\\n\\n        plt.draw()\\n\\n        plt.pause(0.1)\\n\\n    \\n\\n    plt.show(block=True) # block=True lets the window stay open at the end of the animation.\\n\\n\\n\\nAlternatively to this very simple approach, use any of the examples for animations provided in http://matplotlib.org/examples/animation/index.html\\n\\n\\n\\n###(2) \\n\\nIn order to get each plot in a new window, use `plt.figure()` and remove `plt.ion()`. Also only show the windows at the end:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    for i in range(100):\\n\\n        x = range(i)\\n\\n        y = range(i)\\n\\n        plt.figure()\\n\\n        plt.plot(x, y)\\n\\n        plt.title(str(i))\\n\\n        \\n\\n    plt.show()\\n\\n\\n\\nNote that you might find that in both cases the first plot is empty simply because for `i=0`, `range(i) == []` is an empty list without any points. Even for `i=1` there is only one point being plotted, but of course no line can connect a single point with itself.',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 2, 3),\n",
       "  '2017-02-03 15:00:36',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '2',\n",
       "  '',\n",
       "  '5754.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3335',\n",
       "  '41960656',\n",
       "  'Answer',\n",
       "  'Matplotlib 3D surface plot from 2D pandas dataframe',\n",
       "  'The general strategy you pursue is fine. The only error you have is that you create a meshgrid from a list of strings. Of course maplotlib cannot plot strings.\\n\\n\\n\\nYou can therfore create an array of the same length as the number of columns in your dataframe and plug that into the `meshgrid`.\\n\\n\\n\\n    x = np.arange(len(df.columns))',\n",
       "  '<python><pandas><numpy><matplotlib><3d>',\n",
       "  datetime.date(2017, 1, 31),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '3724.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3347',\n",
       "  '42033636',\n",
       "  'Answer',\n",
       "  'How to apply pos_tag_sents() to pandas dataframe efficiently',\n",
       "  \"Assign this to your new column instead:\\n\\n\\n\\n    dfData['POSTags'] = pos_tag_sents(dfData['SourceText'].apply(word_tokenize).tolist())\",\n",
       "  '<python><python-3.x><pandas><nltk><pos-tagger>',\n",
       "  datetime.date(2017, 2, 3),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '2161.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3348',\n",
       "  '42045225',\n",
       "  'Answer',\n",
       "  'Getting black plots with plt.imshow after multiplying image array by a scalar',\n",
       "  '**The solution for the problem in the question would be not to multiply the array with `255`.**  \\n\\nThe other option is to reduce the datatype of the image to unsigned int8,   \\n\\n`out_image = out_image.astype(np.uint8)` \\n\\n\\n\\nLet me explain why:\\n\\n\\n\\nA single channel image can have arbitrary values and datatype. The color will be determined by the colormap to be used, and if required, normalized to a certain range. \\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\nIn contrast, 3 channel RGB arrays are assumed by `imshow` to be in two ranges, `[0., 1.]` or `[0,255]`. (\"3-dimensional arrays must be of dtype unsigned byte, unsigned short, float32 or float64\").  \\n\\nThe range to use will be selected by the datatype of the array:\\n\\n\\n\\n1. A **float** array should be in the **`[0., 1.]`** range,\\n\\n2. an **integer** array should be in the range **`[0,255]`**. Also note that integer arrays must be of datatype int8 and not int32.\\n\\n\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\nAs can be seen in the RGB case, an integer array in the range `[0,1]` stays black, as well as a float array of range `[0., 255.]`.\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/Zfqqg.png\\n\\n  [2]: https://i.stack.imgur.com/m6iFK.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 2, 4),\n",
       "  '2017-02-05 00:23:29',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '9',\n",
       "  '',\n",
       "  '4690.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3356',\n",
       "  '42315650',\n",
       "  'Answer',\n",
       "  'Remove annotation while keeping plot matplotlib',\n",
       "  'You can remove an artist using `remove()`. \\n\\n\\n\\n    ann = plt.annotate (...)\\n\\n    ann.remove()\\n\\n\\n\\nAfter removal it may be necessary to redraw the canvas. \\n\\n\\n\\n\\n\\n<hr>\\n\\nHere is a complete example, removing several annotations within an animation:\\n\\n\\n\\n    import matplotlib.pyplot as plt\\n\\n    import numpy as np\\n\\n    import matplotlib.animation\\n\\n    \\n\\n    fig, ax = plt.subplots()\\n\\n    \\n\\n    x = np.arange(0, 2*np.pi, 0.01)\\n\\n    f = lambda x: np.sin(x)\\n\\n    line, = ax.plot(x, f(x))\\n\\n    \\n\\n    scat = plt.scatter([], [],  s=20, alpha=1, color=\"purple\", edgecolors=\\'none\\')\\n\\n    ann_list = []\\n\\n    \\n\\n    def animate(j):\\n\\n        for i, a in enumerate(ann_list):\\n\\n            a.remove()\\n\\n        ann_list[:] = []\\n\\n        \\n\\n        n = np.random.rand(5)*6\\n\\n        scat.set_offsets([(r, f(r)) for r in n])\\n\\n        for j in range(len(n)):\\n\\n            ann = plt.annotate(\"{:.2f}\".format(n[j]), xy = (n[j],f(n[j])), color = \"purple\", fontsize = 12)\\n\\n            ann_list.append(ann)\\n\\n    \\n\\n    ani = matplotlib.animation.FuncAnimation(fig, animate, frames=20, interval=360)\\n\\n    ani.save(__file__+\".gif\",writer=\\'imagemagick\\', fps=3)\\n\\n    plt.show()\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/OCENQ.gif',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 2, 18),\n",
       "  '2017-02-19 20:04:48',\n",
       "  'ImportanceOfBeingErnest (4124317)',\n",
       "  '8',\n",
       "  '',\n",
       "  '2732.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3366',\n",
       "  '42000164',\n",
       "  'Answer',\n",
       "  'What object does matplotlib.imshow() actually return? How to use this object?',\n",
       "  \"As you have already found out, the return type of `plt.imshow()` is a [`matplotlib.image.AxesImage`](http://matplotlib.org/api/image_api.html#matplotlib.image.AxesImage). The object `img` you get when calling `img = plt.imshow()` is an instance of that class. \\n\\n\\n\\nIn general, you do not have to care about this object, since it's bound to an axes and most of what you want to do, like showing a figure or saving it, does not require any control over that `AxesImage` itself. \\n\\n\\n\\nLike with any other object, it may be useful for getting access to some of its properties for later use. e.g. `img.cmap` returns the colormap of the imgage, `img.get_extent()` provides you with the image extent.\\n\\n\\n\\nThere are two applications coming to my mind, where the `AxesImage` is frequently used:\\n\\n\\n\\n1. Updating the image data: If you want to change the image data without producing a new plot, you may use  \\n\\n`img.set_data(data)`  \\n\\nThis may be useful when working with [user interactions](https://stackoverflow.com/questions/41143782/paging-scrolling-through-set-of-2d-heat-maps-in-matplotlib/41152160) or with animations.\\n\\n2. Creating a colorbar: When creating a colorbar in a plot with several images, it may not be obvious for which image this colorbar should be created, thus\\n\\n`plt.colorbar(img, cax=cax)` sets the colorbar of the image `img` to the axes `cax`.\\n\\n\\n\\nFor further details, you would probably need to refine your question, asking more specifically about some property, application, potential use etc.\\n\\n\\n\\n\\n\\n\",\n",
       "  '<python><matplotlib><plot><imshow>',\n",
       "  datetime.date(2017, 2, 2),\n",
       "  '2017-05-23 11:53:13',\n",
       "  'ImportanceOfBeingErnest (4124317), URL Rewriter Bot (n/a)',\n",
       "  '2',\n",
       "  '',\n",
       "  '1505.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3373',\n",
       "  '42009458',\n",
       "  'Answer',\n",
       "  \"Pandas datetime anchored offset for (-) MonthBegin doesn't work as expected\",\n",
       "  \"This is indeed the correct behavior that is witnessed which are part of the rules laid out in *Anchored Offset Semantics* for offsets supporting *start/end*  of a particular frequency offset.\\n\\n\\n\\nConsider the given example:\\n\\n\\n\\n    from pandas.tseries.offsets import MonthBegin\\n\\n    \\n\\n    pd.Timestamp('2017-01-02 00:00:00') - MonthBegin(n=0)\\n\\n    Out[18]:\\n\\n    Timestamp('2017-02-01 00:00:00')\\n\\n\\n\\nNote that the anchor point corresponding to `MonthBegin` offset is set as first of every month. Now, since the given timestamp clearly surpasses this day, these would automatically be treated as though it were a part of the next month and rolling (whether forward or backwards) would come into play only after that.\\n\\n\\n\\n\\n\\n> <sub> excerpt from [**docs**][1] </sub> <br>\\n\\n> For the case when n=0, the date is not moved if on an anchor point,\\n\\n> otherwise it is rolled forward to the next anchor point.\\n\\n\\n\\n\\n\\n----------\\n\\n\\n\\n\\n\\nTo get what you're after, you need to provide `n=1` which would roll the timestamp to the correct date.\\n\\n\\n\\n    pd.Timestamp('2017-01-02 00:00:00') - MonthBegin(n=1)\\n\\n    Out[20]:\\n\\n    Timestamp('2017-01-01 00:00:00')\\n\\n\\n\\nIf you had set the date on the exact anchor point, then also it would give you the desired result as per the attached docs.\\n\\n\\n\\n    pd.Timestamp('2017-01-01 00:00:00') - MonthBegin(n=0)\\n\\n    Out[21]:\\n\\n    Timestamp('2017-01-01 00:00:00')\\n\\n\\n\\n\\n\\n\\n\\n[1]: http://pandas.pydata.org/pandas-docs/stable/timeseries.html#anchored-offset-semantics\",\n",
       "  '<pandas>',\n",
       "  datetime.date(2017, 2, 2),\n",
       "  '',\n",
       "  '',\n",
       "  '2',\n",
       "  '',\n",
       "  '1265.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3380',\n",
       "  '40865731',\n",
       "  'Answer',\n",
       "  'Convert json to pandas DataFrame',\n",
       "  \"Use [`json_normalize`][2] on the list of dictionaries which performs reasonably faster on large number of json objects.\\n\\n\\n\\n    from pandas.io.json import json_normalize\\n\\n\\n\\n    my_list = []\\n\\n    with open('train.json') as f:\\n\\n        for line in f:\\n\\n            line = line.replace('\\\\\\\\','')\\n\\n            my_list.append(json.loads(line))\\n\\n\\n\\n    # avoid transposing if you want to keep keys as columns of the dataframe\\n\\n    result_df = json_normalize(my_list).T\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/frXRu.png\\n\\n  [2]: http://pandas.pydata.org/pandas-docs/stable/generated/pandas.io.json.json_normalize.html\",\n",
       "  '<python><json><pandas><dataframe>',\n",
       "  datetime.date(2016, 11, 29),\n",
       "  '2016-11-29 16:39:17',\n",
       "  'Nickil Maveli (6207849)',\n",
       "  '4',\n",
       "  '',\n",
       "  '6529.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3388',\n",
       "  '42190453',\n",
       "  'Answer',\n",
       "  'In matplotlib, how can I plot a multi-colored line, like a rainbow',\n",
       "  'Plotting parallel lines is not an easy task. Using a simple uniform offset will of course not show the desired result. This is shown in the left picture below.  \\n\\nSuch a simple offset can be produced in matplotlib as shown in the [transformation tutorial](http://matplotlib.org/users/transforms_tutorial.html#using-offset-transforms-to-create-a-shadow-effect).\\n\\n\\n\\n[![enter image description here][1]][1]\\n\\n\\n\\n###Method1\\n\\nA better solution may be to use the idea sketched on the right side. To calculate the offset of the `n`th point we can use the normal vector to the line between the `n-1`st and the `n+1`st point and use the same distance along this normal vector to calculate the offset point.\\n\\n\\n\\nThe advantage of this method is that we have the same number of points in the original line as in the offset line. The disadvantage is that it is not completely accurate, as can be see in the picture.\\n\\n\\n\\nThis method is implemented in the function `offset` in the code below.  \\n\\nIn order to make this useful for a matplotlib plot, we need to consider that the linewidth should be independent of the data units. Linewidth is usually given in units of points, and the offset would best be given in the same unit, such that e.g. the requirement from the question (\"two parallel lines of width 3\") can be met. \\n\\nThe idea is therefore to transform the coordinates from data to display coordinates, using  `ax.transData.transform`. Also the offset in points `o` can be transformed to the same units: Using the dpi and the standard of ppi=72, the offset in display coordinates is `o*dpi/ppi`. After the offset in display coordinates has been applied, the inverse transform (`ax.transData.inverted().transform`) allows a backtransformation.\\n\\n\\n\\nNow there is another dimension of the problem: How to assure that the offset remains the same independent of the zoom and size of the figure?\\n\\nThis last point can be addressed by recalculating the offset each time a zooming of resizing event has taken place. \\n\\n\\n\\nHere is how a rainbow curve would look like produced by this method.\\n\\n\\n\\n[![enter image description here][2]][2]\\n\\n\\n\\nAnd here is the code to produce the image.\\n\\n\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    dpi = 100\\n\\n    \\n\\n    def offset(x,y, o):\\n\\n        \"\"\" Offset coordinates given by array x,y by o \"\"\"\\n\\n        X = np.c_[x,y].T\\n\\n        m = np.array([[0,-1],[1,0]])\\n\\n        R = np.zeros_like(X)\\n\\n        S = X[:,2:]-X[:,:-2]\\n\\n        R[:,1:-1] = np.dot(m, S)\\n\\n        R[:,0] = np.dot(m, X[:,1]-X[:,0])\\n\\n        R[:,-1] = np.dot(m, X[:,-1]-X[:,-2])\\n\\n        On = R/np.sqrt(R[0,:]**2+R[1,:]**2)*o\\n\\n        Out = On+X\\n\\n        return Out[0,:], Out[1,:]\\n\\n    \\n\\n    \\n\\n    def offset_curve(ax, x,y, o):\\n\\n        \"\"\" Offset array x,y in data coordinates\\n\\n            by o in points \"\"\"\\n\\n        trans = ax.transData.transform\\n\\n        inv = ax.transData.inverted().transform\\n\\n        X = np.c_[x,y]\\n\\n        Xt = trans(X)\\n\\n        xto, yto = offset(Xt[:,0],Xt[:,1],o*dpi/72. )\\n\\n        Xto = np.c_[xto, yto]\\n\\n        Xo = inv(Xto)\\n\\n        return Xo[:,0], Xo[:,1]\\n\\n        \\n\\n    \\n\\n    # some single points\\n\\n    y = np.array([1,2,2,3,3,0])    \\n\\n    x = np.arange(len(y))\\n\\n    #or try a sinus\\n\\n    x = np.linspace(0,9)\\n\\n    y=np.sin(x)*x/3.\\n\\n    \\n\\n    \\n\\n    fig, ax=plt.subplots(figsize=(4,2.5), dpi=dpi)\\n\\n    \\n\\n    cols = [\"#fff40b\", \"#00e103\", \"#ff9921\", \"#3a00ef\", \"#ff2121\", \"#af00e7\"]\\n\\n    lw = 2.\\n\\n    lines = []\\n\\n    for i in range(len(cols)):\\n\\n        l, = plt.plot(x,y, lw=lw, color=cols[i])\\n\\n        lines.append(l)\\n\\n    \\n\\n    \\n\\n    def plot_rainbow(event=None):\\n\\n        xr = range(6); yr = range(6); \\n\\n        xr[0],yr[0] = offset_curve(ax, x,y, lw/2.)\\n\\n        xr[1],yr[1] = offset_curve(ax, x,y, -lw/2.)\\n\\n        xr[2],yr[2] = offset_curve(ax, xr[0],yr[0], lw)\\n\\n        xr[3],yr[3] = offset_curve(ax, xr[1],yr[1], -lw)\\n\\n        xr[4],yr[4] = offset_curve(ax, xr[2],yr[2], lw)\\n\\n        xr[5],yr[5] = offset_curve(ax, xr[3],yr[3], -lw)\\n\\n        \\n\\n        for i  in range(6):     \\n\\n            lines[i].set_data(xr[i], yr[i])\\n\\n        \\n\\n    \\n\\n    plot_rainbow()\\n\\n    \\n\\n    fig.canvas.mpl_connect(\"resize_event\", plot_rainbow)\\n\\n    fig.canvas.mpl_connect(\"button_release_event\", plot_rainbow)\\n\\n    \\n\\n    plt.savefig(__file__+\".png\", dpi=dpi)\\n\\n    plt.show()\\n\\n\\n\\n\\n\\n<hr>\\n\\n###Method2\\n\\n\\n\\nTo avoid overlapping lines, one has to use a more complicated solution. \\n\\nOne could first offset every point normal to the two line segments it is part of (green points in the picture below). Then calculate the line through those offset points and find their intersection.\\n\\n[![enter image description here][3]][3]\\n\\n\\n\\nA particular case would be when the slopes of two subsequent line segments equal. This has to be taken care of (`eps` in the code below).\\n\\n\\n\\n\\n\\n\\n\\n    from __future__ import division\\n\\n    import numpy as np\\n\\n    import matplotlib.pyplot as plt\\n\\n    \\n\\n    dpi = 100\\n\\n    \\n\\n    def intersect(p1, p2, q1, q2, eps=1.e-10):\\n\\n        \"\"\" given two lines, first through points pn, second through qn,\\n\\n            find the intersection \"\"\"\\n\\n        x1 = p1[0]; y1 = p1[1]; x2 = p2[0]; y2 = p2[1]\\n\\n        x3 = q1[0]; y3 = q1[1]; x4 = q2[0]; y4 = q2[1]\\n\\n        nomX = ((x1*y2-y1*x2)*(x3-x4)- (x1-x2)*(x3*y4-y3*x4)) \\n\\n        denom = float(  (x1-x2)*(y3-y4) - (y1-y2)*(x3-x4) )\\n\\n        nomY = (x1*y2-y1*x2)*(y3-y4) - (y1-y2)*(x3*y4-y3*x4)\\n\\n        if np.abs(denom) < eps:\\n\\n            #print \"intersection undefined\", p1\\n\\n            return np.array( p1 )\\n\\n        else:\\n\\n            return np.array( [ nomX/denom , nomY/denom ])\\n\\n        \\n\\n    \\n\\n    def offset(x,y, o, eps=1.e-10):\\n\\n        \"\"\" Offset coordinates given by array x,y by o \"\"\"\\n\\n        X = np.c_[x,y].T\\n\\n        m = np.array([[0,-1],[1,0]])\\n\\n        S = X[:,1:]-X[:,:-1]\\n\\n        R = np.dot(m, S)\\n\\n        norm = np.sqrt(R[0,:]**2+R[1,:]**2) / o\\n\\n        On = R/norm\\n\\n        Outa = On+X[:,1:]\\n\\n        Outb = On+X[:,:-1]\\n\\n        G = np.zeros_like(X)\\n\\n        for i in xrange(0, len(X[0,:])-2):\\n\\n            p = intersect(Outa[:,i], Outb[:,i], Outa[:,i+1], Outb[:,i+1], eps=eps)\\n\\n            G[:,i+1] = p\\n\\n        G[:,0] = Outb[:,0]\\n\\n        G[:,-1] = Outa[:,-1]\\n\\n        return G[0,:], G[1,:]\\n\\n    \\n\\n    \\n\\n    def offset_curve(ax, x,y, o, eps=1.e-10):\\n\\n        \"\"\" Offset array x,y in data coordinates\\n\\n            by o in points \"\"\"\\n\\n        trans = ax.transData.transform\\n\\n        inv = ax.transData.inverted().transform\\n\\n        X = np.c_[x,y]\\n\\n        Xt = trans(X)\\n\\n        xto, yto = offset(Xt[:,0],Xt[:,1],o*dpi/72., eps=eps )\\n\\n        Xto = np.c_[xto, yto]\\n\\n        Xo = inv(Xto)\\n\\n        return Xo[:,0], Xo[:,1]\\n\\n        \\n\\n    \\n\\n    # some single points\\n\\n    y = np.array([1,1,2,0,3,2,1.,4,3]) *1.e9   \\n\\n    x = np.arange(len(y))\\n\\n    x[3]=x[4]\\n\\n    #or try a sinus\\n\\n    #x = np.linspace(0,9)\\n\\n    #y=np.sin(x)*x/3.\\n\\n    \\n\\n    \\n\\n    fig, ax=plt.subplots(figsize=(4,2.5), dpi=dpi)\\n\\n    \\n\\n    cols = [\"r\", \"b\"]\\n\\n    lw = 11.\\n\\n    lines = []\\n\\n    for i in range(len(cols)):\\n\\n        l, = plt.plot(x,y, lw=lw, color=cols[i], solid_joinstyle=\"miter\")\\n\\n        lines.append(l)\\n\\n    \\n\\n    \\n\\n    def plot_rainbow(event=None):\\n\\n        xr = range(2); yr = range(2); \\n\\n        xr[0],yr[0] = offset_curve(ax, x,y,  lw/2.)\\n\\n        xr[1],yr[1] = offset_curve(ax, x,y, -lw/2.)\\n\\n    \\n\\n        for i  in range(2):     \\n\\n            lines[i].set_data(xr[i], yr[i])\\n\\n        \\n\\n    \\n\\n    plot_rainbow()\\n\\n    \\n\\n    fig.canvas.mpl_connect(\"resize_event\", plot_rainbow)\\n\\n    fig.canvas.mpl_connect(\"button_release_event\", plot_rainbow)\\n\\n    \\n\\n    plt.show()\\n\\n\\n\\n\\n\\n[![enter image description here][4]][4]\\n\\n\\n\\nNote that this method should work well as long as the offset between the lines is smaller then the distance between subsequent points on the line. Otherwise method 1 may be better suited.\\n\\n\\n\\n\\n\\n  [1]: https://i.stack.imgur.com/k34Hp.png\\n\\n  [2]: https://i.stack.imgur.com/8Xfax.png\\n\\n  [3]: https://i.stack.imgur.com/nQhPX.png\\n\\n  [4]: https://i.stack.imgur.com/SmQQ9.png',\n",
       "  '<python><matplotlib>',\n",
       "  datetime.date(2017, 2, 12),\n",
       "  '2017-11-18 12:03:15',\n",
       "  'ImportanceOfBeingErnest (4124317), Wolf (2932052)',\n",
       "  '51',\n",
       "  '',\n",
       "  '1600.0',\n",
       "  '',\n",
       "  'Not Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ['3397',\n",
       "  '40899490',\n",
       "  'Answer',\n",
       "  'Dask equivalent to Pandas replace?',\n",
       "  \"You can use [`mask`][1]:\\n\\n\\n\\n    df = df.mask(df == 'PASS', '0')\\n\\n    df = df.mask(df == 'FAIL', '1')\\n\\n\\n\\nOr equivalently chaining the `mask` calls:\\n\\n\\n\\n    df = df.mask(df == 'PASS', '0').mask(df == 'FAIL', '1')\\n\\n\\n\\n  [1]: http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.mask\\n\\n\",\n",
       "  '<pandas><dask>',\n",
       "  datetime.date(2016, 11, 30),\n",
       "  '',\n",
       "  '',\n",
       "  '6',\n",
       "  '',\n",
       "  '1094.0',\n",
       "  '',\n",
       "  'Accepted',\n",
       "  '',\n",
       "  ''],\n",
       " ...]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cleaned_pandas_rows_by_filter(pandas_min_date, pandas_max_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "16703790",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1137\">\n",
       "  (function() {\n",
       "    const xhr = new XMLHttpRequest()\n",
       "    xhr.responseType = 'blob';\n",
       "    xhr.open('GET', \"http://localhost:54863/autoload.js?bokeh-autoload-element=1137&bokeh-absolute-url=http://localhost:54863&resources=none\", true);\n",
       "    \n",
       "    xhr.onload = function (event) {\n",
       "      const script = document.createElement('script');\n",
       "      const src = URL.createObjectURL(event.target.response);\n",
       "      script.src = src;\n",
       "      document.body.appendChild(script);\n",
       "    };\n",
       "    xhr.send();\n",
       "  })();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "11555e5afada4a9aa0a20757420586c1"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def panel_2():\n",
    "        \n",
    "    data = {\n",
    "            'scores': default_score,\n",
    "             'views': default_views,\n",
    "             'title': [row[pandas_header[\"Title\"]] for row in cleaned_pandas_rows],\n",
    "             'accepted': [row[pandas_header[\"Accepted\"]] for row in cleaned_pandas_rows],\n",
    "        }\n",
    "\n",
    "    TOOLTIPS = [\n",
    "        (\"Title\", \"@title\"),\n",
    "    ]\n",
    "\n",
    "    source = ColumnDataSource(data=data)\n",
    "\n",
    "    \n",
    "    score_range = Range1d(min(default_score), max(default_score) * 1.05)\n",
    "    views_range = Range1d(min(default_views), max(default_views) * 1.05)\n",
    "    \n",
    "    p = figure(plot_width=950, plot_height=500, tooltips=TOOLTIPS,\n",
    "               title=\"Post Scores vs Views\", sizing_mode=\"fixed\", \n",
    "               x_range=score_range, y_range=views_range)\n",
    "    \n",
    "    color_mapper = CategoricalColorMapper(factors=['Not Accepted', 'Accepted'], palette=('#ea4335', '#34a853'))\n",
    "\n",
    "\n",
    "    \n",
    "    p.circle(\n",
    "        x='scores', \n",
    "        y='views', \n",
    "        color={'field': 'accepted', 'transform': color_mapper}, \n",
    "        source=source, \n",
    "        alpha=1, \n",
    "        legend_field='accepted', \n",
    "        size=5\n",
    "     )\n",
    "\n",
    "    p.xaxis.axis_label = \"Post score\"\n",
    "    p.yaxis.axis_label = \"Number of post views\"\n",
    "    p.yaxis.formatter = NumeralTickFormatter(format=\"0,0\") \n",
    "    p.xaxis.formatter = NumeralTickFormatter(format=\"0,0\") \n",
    "        \n",
    "    date_slider = DateRangeSlider(start=pandas_min_date, end=pandas_max_date, value=(pandas_min_date, pandas_max_date),\n",
    "                    title=\"Post date\",\n",
    "                    width=950)\n",
    "\n",
    "    \n",
    "    def filter_data():\n",
    "        min_date = date_slider.value_as_date[0]\n",
    "        max_date = date_slider.value_as_date[1]\n",
    "        \n",
    "        clen_rows = get_cleaned_pandas_rows_by_filter(min_date, max_date)\n",
    "        \n",
    "        new_data = {\n",
    "                'scores': [row[pandas_header[\"Score\"]] for row in clen_rows],\n",
    "                'views': [row[pandas_header[\"Views\"]] for row in clen_rows],\n",
    "                'title': [row[pandas_header[\"Title\"]] for row in clen_rows],\n",
    "                'accepted': [row[pandas_header[\"Accepted\"]] for row in clen_rows],\n",
    "            }\n",
    "   \n",
    "        return ColumnDataSource(data=new_data)\n",
    "\n",
    "    \n",
    "    def update_data(attr, old, new):\n",
    "        updated_source = filter_data()\n",
    "        source.data.update(updated_source.data)\n",
    "        \n",
    "    date_slider.on_change(\"value\", update_data)\n",
    "\n",
    "    return layout([p, date_slider])\n",
    "\n",
    "def wrapper(doc):\n",
    "     doc.add_root(panel_2())\n",
    "        \n",
    "handler = FunctionHandler(wrapper)\n",
    "app = Application(handler)\n",
    "show(app, notebook_url=\"http://localhost:8889\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c55bd9",
   "metadata": {},
   "source": [
    "## Tab 2 Considerations\n",
    "\n",
    "Three ouliers:\n",
    "- Converting string into datetime\n",
    "- Select ros froma DataFrame based on values in a column in pandas\n",
    "- What is the difference between @staticmethod and @classmethod?\n",
    "\n",
    "Posts with higher number of views are more likely to be not accepted. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5ebc52",
   "metadata": {},
   "source": [
    "## Pack all the panels togheter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2fd53b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.bokehjs_exec.v0+json": "",
      "text/html": [
       "\n",
       "<script id=\"1138\">\n",
       "  (function() {\n",
       "    const xhr = new XMLHttpRequest()\n",
       "    xhr.responseType = 'blob';\n",
       "    xhr.open('GET', \"http://localhost:54864/autoload.js?bokeh-autoload-element=1138&bokeh-absolute-url=http://localhost:54864&resources=none\", true);\n",
       "    \n",
       "    xhr.onload = function (event) {\n",
       "      const script = document.createElement('script');\n",
       "      const src = URL.createObjectURL(event.target.response);\n",
       "      script.src = src;\n",
       "      document.body.appendChild(script);\n",
       "    };\n",
       "    xhr.send();\n",
       "  })();\n",
       "</script>"
      ]
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "server_id": "27341174ae464299b4c083f991a911d8"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from bokeh.models import Panel, Tabs\n",
    "\n",
    "\n",
    "def wrapper(doc):\n",
    "    tab1 = Panel(child=panel_1(), title=\"SO Survey\")\n",
    "    tab2 = Panel(child=panel_2(), title=\"Pandas Survey\")\n",
    "    tabs = Tabs(tabs=[ tab1, tab2 ])\n",
    "    doc.add_root(tabs)\n",
    "        \n",
    "handler = FunctionHandler(wrapper)\n",
    "app = Application(handler)\n",
    "show(app, notebook_url=\"http://localhost:8889\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd2765",
   "metadata": {},
   "source": [
    "### Datasets Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134bdd76",
   "metadata": {},
   "source": [
    "**Used Cars**\n",
    "\n",
    "This dataset is scraped from Ebay. The content of the dataset is in German, but it should not impose critical issues in understanding the data. The fields included in the dataset are as following: \n",
    "* dateCrawled: when this ad was first crawled, all field-values are taken from this date\n",
    "* name: ”name” of the car\n",
    "* seller: private or dealer\n",
    "* offerType\n",
    "* price: the price in euro on the ad to sell the car\n",
    "* abtest\n",
    "* vehicleType\n",
    "* yearOfRegistration : at which year the car was first registered \n",
    "* gearbox\n",
    "* powerPS: power of the car in PS\n",
    "* model\n",
    "* kilometer: how many kilometers the car has driven\n",
    "* monthOfRegistration: at which month the car was first registered\n",
    "* fuelType: vehicle fuel type\n",
    "* brand: vehicle brand\n",
    "* notRepairedDamage: if the car has a damage which is not repaired yet \n",
    "* dateCreated: the date for which the ad at ebay was created\n",
    "* nrOfPictures: number of pictures in the ad\n",
    "* postalCode\n",
    "* lastSeenOnline: when the crawler saw this ad last online"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fa53d8",
   "metadata": {},
   "source": [
    "**Stack Overflow 2018 Developer Survey**\n",
    "\n",
    "This dataset is collected by Stack Overflow, which contains questions and answers of Stack Overflow developers community about their salaries, years of coding expeprience, education and more. This dataset is organized in two tables:\n",
    "\n",
    "*survey results public*: contains the main data from the surveys, one respondent per row and one column per question. \n",
    "\n",
    "*survey results schema*: contains each column name from the main results along with the question text corresponding to that column. You may use this table to better understand the content of *survey results public*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e846b5f",
   "metadata": {},
   "source": [
    "**Pandas QA on Stack Overflow**\n",
    "\n",
    "This dataset contains information on posts (e.g., questions, answers) about Pandas on Stack Overflow. The column of the data set are as following: \n",
    "* Serial Number\n",
    "* Post Link (unique)\n",
    "* Type - Either Question, Answer etc\n",
    "* Title - Question heading\n",
    "* Markdown - Body\n",
    "* Tags - Associated tags, max 5\n",
    "* Created - Creation date\n",
    "* LastEdit - Last edited date\n",
    "* Edited By - Name of the user, if the post has been edited by him/her.\n",
    "* Score - Total score for this post. 1 score = 10 reputation.\n",
    "* Favorites - Number of users who mark it as favorite.\n",
    "* Views - Total number of views.\n",
    "* Answers - If 'type' is 'question', then number of answers to the question.\n",
    "* Accepted - If 'type' is 'answer', is it accepted or not.\n",
    "* CW - If 'type' is 'answer', is the answer given by Stack Overflow Community Wiki or not.\n",
    "* Closed - If 'type' is 'question', is it closed, duplicate or blank for new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VAA",
   "language": "python",
   "name": "vaa"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
